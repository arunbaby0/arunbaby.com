<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Federated Learning - Arun Baby</title>
<meta name="description" content="“If data can’t move, move the model—and design the system so the server never sees what matters.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Federated Learning">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0051-federated-learning/">


  <meta property="og:description" content="“If data can’t move, move the model—and design the system so the server never sees what matters.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Federated Learning">
  <meta name="twitter:description" content="“If data can’t move, move the model—and design the system so the server never sees what matters.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0051-federated-learning/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0051-federated-learning/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Federated Learning">
    <meta itemprop="description" content="“If data can’t move, move the model—and design the system so the server never sees what matters.”">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0051-federated-learning/" itemprop="url">Federated Learning
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          21 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-problem-statement">1. Problem Statement</a></li><li><a href="#2-understanding-the-requirements">2. Understanding the Requirements</a><ul><li><a href="#21-functional-requirements">2.1 Functional requirements</a></li><li><a href="#22-non-functional-requirements-the-real-difficulty">2.2 Non-functional requirements (the real difficulty)</a></li></ul></li><li><a href="#3-high-level-architecture">3. High-Level Architecture</a><ul><li><a href="#31-architecture-diagram-control--data-plane">3.1 Architecture diagram (control + data plane)</a></li><li><a href="#32-key-responsibilities">3.2 Key responsibilities</a></li></ul></li><li><a href="#4-component-deep-dives">4. Component Deep-Dives</a><ul><li><a href="#41-client-selection-sampling-and-eligibility">4.1 Client selection (sampling) and eligibility</a></li><li><a href="#42-client-training-plan-what-executes-on-device">4.2 Client training plan (what executes on device)</a></li><li><a href="#43-aggregation--optimization">4.3 Aggregation + optimization</a></li><li><a href="#44-secure-aggregation-sa">4.4 Secure aggregation (SA)</a></li><li><a href="#45-differential-privacy-dp-and-clipping">4.5 Differential privacy (DP) and clipping</a></li><li><a href="#46-secure-aggregation-protocol-details-what-actually-happens">4.6 Secure aggregation protocol details (what actually happens)</a></li><li><a href="#47-robust-aggregation-byzantine-resilience">4.7 Robust aggregation (Byzantine resilience)</a></li><li><a href="#48-dp-accounting-epsilon-budgets-in-practice">4.8 DP accounting (epsilon budgets) in practice</a></li></ul></li><li><a href="#5-data-flow-one-round-end-to-end">5. Data Flow (One Round End-to-End)</a><ul><li><a href="#51-step-by-step">5.1 Step-by-step</a></li><li><a href="#52-what-makes-fl-different">5.2 What makes FL “different”</a></li></ul></li><li><a href="#6-scaling-strategies">6. Scaling Strategies</a><ul><li><a href="#61-participation-scaling-bandwidth-and-concurrency">6.1 Participation scaling: bandwidth and concurrency</a></li><li><a href="#62-straggler-and-dropout-handling">6.2 Straggler and dropout handling</a></li><li><a href="#63-dealing-with-non-iid-data">6.3 Dealing with non-IID data</a></li><li><a href="#64-communication-efficiency">6.4 Communication efficiency</a></li></ul></li><li><a href="#7-implementation-core-logic-in-python">7. Implementation (Core Logic in Python)</a></li><li><a href="#8-monitoring--metrics">8. Monitoring &amp; Metrics</a><ul><li><a href="#81-participation-health">8.1 Participation health</a></li><li><a href="#82-model-update-health-privacy-aware">8.2 Model update health (privacy-aware)</a></li><li><a href="#83-quality-metrics">8.3 Quality metrics</a></li><li><a href="#84-cost-metrics">8.4 Cost metrics</a></li><li><a href="#85-debugging-playbook-what-you-do-when-quality-drops">8.5 Debugging playbook (what you do when quality drops)</a></li><li><a href="#86-privacy-safe-evaluation-patterns">8.6 Privacy-safe evaluation patterns</a></li><li><a href="#87-fleet-health-metrics-client-runtime-as-a-distributed-system">8.7 Fleet health metrics (client runtime as a distributed system)</a></li><li><a href="#88-auditability-and-compliance">8.8 Auditability and compliance</a></li></ul></li><li><a href="#9-failure-modes-and-mitigations">9. Failure Modes (and Mitigations)</a><ul><li><a href="#91-client-poisoning">9.1 Client poisoning</a></li><li><a href="#92-privacy-leakage-via-updates">9.2 Privacy leakage via updates</a></li><li><a href="#93-non-iid-drift-and-catastrophic-regressions">9.3 Non-IID drift and catastrophic regressions</a></li><li><a href="#94-operational-instability">9.4 Operational instability</a></li><li><a href="#95-data-poisoning-without-malice-bugs-look-like-attacks">9.5 Data poisoning without “malice” (bugs look like attacks)</a></li><li><a href="#96-silent-sampling-bias-the-most-common-real-world-failure">9.6 Silent sampling bias (the most common real-world failure)</a></li></ul></li><li><a href="#10-real-world-case-study">10. Real-World Case Study</a><ul><li><a href="#101-google-gboard-next-word-prediction">10.1 Google Gboard next-word prediction</a></li><li><a href="#102-apple-on-device-personalization">10.2 Apple on-device personalization</a></li><li><a href="#103-a-speech-flavored-case-study-personalized-wake-words--accents">10.3 A speech-flavored case study (personalized wake words / accents)</a></li></ul></li><li><a href="#11-cost-analysis">11. Cost Analysis</a><ul><li><a href="#111-a-back-of-the-envelope-cost-model-so-you-can-reason-in-interviews">11.1 A back-of-the-envelope cost model (so you can reason in interviews)</a></li><li><a href="#112-the-client-compute-cost-you-dont-pay-but-your-users-do">11.2 The “client compute cost” you don’t pay (but your users do)</a></li><li><a href="#113-roi-framing">11.3 ROI framing</a></li></ul></li><li><a href="#12-key-takeaways">12. Key Takeaways</a><ul><li><a href="#121-a-concise-design-review-checklist-for-fl-systems">12.1 A concise “design review checklist” for FL systems</a></li><li><a href="#122-deployment-note-fl-still-needs-a-serving-story">12.2 Deployment note: FL still needs a serving story</a></li></ul></li></ul>
            </nav>
          </aside>
        
        <p><strong>“If data can’t move, move the model—and design the system so the server never sees what matters.”</strong></p>

<h2 id="1-problem-statement">1. Problem Statement</h2>

<p>Many of the most valuable datasets live on user devices:</p>
<ul>
  <li>typed text (next-word prediction)</li>
  <li>voice snippets (wake word, personalization)</li>
  <li>photos, app usage patterns, sensor streams</li>
</ul>

<p>But collecting raw data centrally is often unacceptable due to:</p>
<ul>
  <li>privacy expectations</li>
  <li>legal constraints (GDPR/CCPA, children’s data)</li>
  <li>trust and product risk</li>
  <li>bandwidth and battery cost</li>
</ul>

<p><strong>Federated Learning (FL)</strong> trains a global model by sending the model to devices, training locally, and sending only updates back.</p>

<p><strong>Goal</strong>: Design a production-grade federated learning system that:</p>
<ul>
  <li>improves model quality</li>
  <li>preserves privacy</li>
  <li>scales to millions of devices</li>
  <li>handles unreliable clients and skewed data</li>
  <li>is observable, debuggable, and safe</li>
</ul>

<p>Thematic link for today: <strong>binary search and distributed algorithms</strong>.
Federated learning is a distributed optimization algorithm under strict constraints; the engineering challenge is making that algorithm reliable at scale.</p>

<hr />

<h2 id="2-understanding-the-requirements">2. Understanding the Requirements</h2>

<h3 id="21-functional-requirements">2.1 Functional requirements</h3>

<ul>
  <li>Train a global model over many client datasets without centralizing raw data.</li>
  <li>Periodically produce a new model version for serving (on-device or server-side).</li>
  <li>Support multiple model types:</li>
  <li>logistic regression / small neural nets</li>
  <li>large embeddings</li>
  <li>speech models (acoustic / personalization heads)</li>
  <li>Support experimentation: A/B testing between FL variants (FedAvg vs FedProx, compression choices, DP).</li>
</ul>

<h3 id="22-non-functional-requirements-the-real-difficulty">2.2 Non-functional requirements (the real difficulty)</h3>

<ul>
  <li><strong>Privacy</strong>: server should not learn individual client examples (or even individual client updates, ideally).</li>
  <li><strong>Scalability</strong>: millions of eligible devices; only a fraction participate per round.</li>
  <li><strong>Reliability</strong>: client dropouts are normal. Devices sleep, go offline, change networks.</li>
  <li><strong>Heterogeneity</strong>: devices differ in CPU/GPU/NPU, memory, battery, OS version.</li>
  <li><strong>Data skew (non-IID)</strong>: one user types “football”, another types “biology”. Local distributions differ drastically.</li>
  <li><strong>Security</strong>: malicious clients can poison updates or try to infer other clients.</li>
  <li><strong>Observability</strong>: you need metrics without raw data; debugging is harder than centralized training.</li>
  <li><strong>Cost</strong>: bandwidth, device compute, server aggregation capacity.</li>
</ul>

<hr />

<h2 id="3-high-level-architecture">3. High-Level Architecture</h2>

<p>At a high level, an FL system runs repeated <strong>rounds</strong>:</p>
<ol>
  <li>select clients</li>
  <li>broadcast model + training plan</li>
  <li>local training on-device</li>
  <li>upload updates</li>
  <li>secure aggregation</li>
  <li>server optimization step</li>
  <li>evaluate and promote model</li>
</ol>

<h3 id="31-architecture-diagram-control--data-plane">3.1 Architecture diagram (control + data plane)</h3>

<p><code class="language-plaintext highlighter-rouge">
 +----------------------------------+
 | FL Orchestrator |
 | (round scheduler + policy engine)|
 +-------------------+--------------+
 |
 (model + plan) | (eligibility + sampling)
 v
 +----------------------+ +----------------------+ +----------------------+
 | Model Registry | | Client Selector | | Privacy / Security |
 | (versions, metadata) | | (sampling, quotas) | | (DP params, SA keys) |
 +----------+-----------+ +----------+-----------+ +----------+-----------+
 | | |
 | | |
 v v v
 +-------------------+ +-------------------+ +-------------------+
 | Broadcast Service |---------&gt; | Devices (clients)|------&gt; | Secure Aggregator |
 | (CDN / gRPC) | model+ | local train, | updates| (no raw updates) |
 +-------------------+ plan | clip/noise, | +---------+---------+
 | upload partials | |
 +---------+---------+ |
 | |
 v v
 +-------------------+ +-------------------+
 | Telemetry | | Server Optimizer |
 | (metrics only) | | (FedAvg/FedOpt) |
 +-------------------+ +---------+---------+
 |
 v
 +-------------------+
 | Eval + Promotion |
 | (holdout + guard) |
 +-------------------+
</code></p>

<h3 id="32-key-responsibilities">3.2 Key responsibilities</h3>

<ul>
  <li><strong>Orchestrator</strong>: defines “who participates, when, and with what constraints.”</li>
  <li><strong>Client runtime</strong>: runs on-device training safely, respecting battery/network policies.</li>
  <li><strong>Secure aggregation</strong>: ensures server only sees aggregated updates, not individual updates.</li>
  <li><strong>Model registry</strong>: versioning, metadata, reproducibility, rollback.</li>
  <li><strong>Evaluation</strong>: must work without raw client data (or with privacy-safe evaluation).</li>
</ul>

<hr />

<h2 id="4-component-deep-dives">4. Component Deep-Dives</h2>

<h3 id="41-client-selection-sampling-and-eligibility">4.1 Client selection (sampling) and eligibility</h3>

<p>In production, you rarely train on “all devices”.
You define an <strong>eligibility policy</strong>, for example:</p>
<ul>
  <li>device on Wi-Fi</li>
  <li>charging</li>
  <li>idle</li>
  <li>minimum battery %</li>
  <li>opted-in (consent)</li>
  <li>locale / language segment match</li>
  <li>model version compatibility (runtime can execute)</li>
</ul>

<p>Then you sample eligible devices:</p>
<ul>
  <li>uniform random sampling can over-represent highly active regions</li>
  <li>you may need stratified sampling (by geography, device class, language)</li>
</ul>

<p><strong>Why this matters</strong>: the sampling policy is your “data loader” in FL. If it’s biased, your model is biased.</p>

<h3 id="42-client-training-plan-what-executes-on-device">4.2 Client training plan (what executes on device)</h3>

<p>A “round” is typically packaged as:</p>
<ul>
  <li>base model weights</li>
  <li>optimizer configuration (learning rate, epochs, batch size)</li>
  <li>data selection rules (which local records are included)</li>
  <li>privacy rules (clipping norm, DP noise, secure aggregation protocol version)</li>
</ul>

<p>This is similar to shipping a mini training job to untrusted compute.
So you need:</p>
<ul>
  <li>strict sandboxing</li>
  <li>deterministic kernels where possible</li>
  <li>a way to revoke/break-glass if a plan misbehaves (battery drain incidents are real)</li>
</ul>

<h3 id="43-aggregation--optimization">4.3 Aggregation + optimization</h3>

<p>The canonical algorithm is <strong>FedAvg</strong>:</p>
<ol>
  <li>each client trains locally from (w_t) to (w_t^k)</li>
  <li>server averages client deltas weighted by number of examples</li>
</ol>

<p>[
w_{t+1} = \sum_{k \in S_t} \frac{n_k}{\sum_{j \in S_t} n_j}\; w_t^k
]</p>

<p>Variants:</p>
<ul>
  <li><strong>FedAvgM</strong> (server momentum)</li>
  <li><strong>FedAdam / FedYogi</strong> (server-side adaptive optimizers)</li>
  <li><strong>FedProx</strong> (adds proximal term to stabilize with non-IID data)</li>
</ul>

<p>Choosing the right optimizer is a systems decision:</p>
<ul>
  <li>adaptive optimizers can converge faster but may be more sensitive to client drift</li>
  <li>non-IID data often benefits from proximal terms or personalization layers</li>
</ul>

<h3 id="44-secure-aggregation-sa">4.4 Secure aggregation (SA)</h3>

<p>Even model updates can leak private information (membership inference, gradient inversion).
Secure aggregation aims to ensure:</p>
<ul>
  <li>server learns only the sum/average of updates across many clients</li>
  <li>individual updates remain hidden</li>
</ul>

<p>Typical properties:</p>
<ul>
  <li><strong>threshold</strong> (T): aggregation succeeds only if at least (T) clients complete</li>
  <li><strong>dropout tolerance</strong>: protocol must handle client dropouts without revealing individuals</li>
</ul>

<p>Operationally:</p>
<ul>
  <li>SA adds protocol complexity and latency</li>
  <li>SA affects observability (you can’t inspect individual updates)</li>
</ul>

<h3 id="45-differential-privacy-dp-and-clipping">4.5 Differential privacy (DP) and clipping</h3>

<p>DP is often layered on top of FL:</p>
<ul>
  <li>each client clips its update to a maximum norm (C)</li>
  <li>adds noise (local DP or central DP depending on threat model)</li>
</ul>

<p>Why clipping matters:</p>
<ul>
  <li>without clipping, a single user with extreme data can dominate the aggregate</li>
  <li>clipping bounds sensitivity, enabling DP guarantees</li>
</ul>

<p>There’s a “binary search” motif here:
in practice you often <strong>tune the clipping norm</strong> by searching for a value that preserves utility while controlling leakage and outliers—this becomes an iterative “search over constraints”, not unlike how we search for a partition in the median problem.</p>

<h3 id="46-secure-aggregation-protocol-details-what-actually-happens">4.6 Secure aggregation protocol details (what actually happens)</h3>

<p>Secure aggregation sounds like a single box (“the secure aggregator”), but it’s usually a multi-step protocol. A simplified view:</p>

<ol>
  <li><strong>Key agreement / setup</strong>
    <ul>
      <li>Clients establish pairwise secrets (or receive public keys) for masking.</li>
    </ul>
  </li>
  <li><strong>Masking</strong>
    <ul>
      <li>Each client creates a random mask vector (r_k).</li>
      <li>Client sends <em>shares</em> of masks such that masks can be reconstructed only if enough clients finish.</li>
      <li>The client uploads masked update: (u_k + r_k).</li>
    </ul>
  </li>
  <li><strong>Dropout handling</strong>
    <ul>
      <li>Some clients drop out mid-round.</li>
      <li>The protocol reconstructs masks for missing clients (or cancels their masks) so the final sum unmasks correctly.</li>
    </ul>
  </li>
  <li><strong>Aggregation</strong>
    <ul>
      <li>Server ends with (\sum_k u_k) (or weighted sum), but never sees any individual (u_k).</li>
    </ul>
  </li>
</ol>

<p>Design implications:</p>
<ul>
  <li>You need a <strong>threshold</strong> (e.g., 1k clients) to reduce privacy leakage and to make SA feasible.</li>
  <li>You need a <strong>protocol state store</strong> (often a database) to track which clients completed which phase.</li>
  <li>SA can become your biggest source of operational failures (timeouts, key exchange bugs, state corruption).</li>
</ul>

<h3 id="47-robust-aggregation-byzantine-resilience">4.7 Robust aggregation (Byzantine resilience)</h3>

<p>Even with privacy, you still need robustness. Some clients may be buggy or malicious.
Robust aggregators attempt to reduce the impact of outliers:</p>

<ul>
  <li><strong>Trimmed mean</strong>: drop top/bottom (p\%) per coordinate.</li>
  <li><strong>Coordinate-wise median</strong>: median per coordinate (very robust, but can be noisy in high dimensions).</li>
  <li><strong>Norm bounding</strong>: reject aggregates if norm spikes beyond historical ranges.</li>
</ul>

<p>Trade-off table:</p>

<table>
  <thead>
    <tr>
      <th>Aggregator</th>
      <th>Pros</th>
      <th>Cons</th>
      <th>When to use</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>FedAvg</td>
      <td>simple, fast</td>
      <td>sensitive to outliers</td>
      <td>trusted clients, small cohorts</td>
    </tr>
    <tr>
      <td>Trimmed mean</td>
      <td>robust to outliers</td>
      <td>loses signal if trimming too aggressive</td>
      <td>noisy client populations</td>
    </tr>
    <tr>
      <td>Median</td>
      <td>strong robustness</td>
      <td>can slow convergence</td>
      <td>high poisoning risk</td>
    </tr>
  </tbody>
</table>

<h3 id="48-dp-accounting-epsilon-budgets-in-practice">4.8 DP accounting (epsilon budgets) in practice</h3>

<p>DP is not “add noise once and forget”.
You need a privacy accountant to track privacy loss across rounds.</p>

<p>Operationally this means:</p>
<ul>
  <li>per model / per cohort DP budget (e.g., “this model can spend (\epsilon \le 3) over 30 days”)</li>
  <li>round-level parameters: clip norm, noise multiplier, cohort size</li>
  <li>stopping criteria: “stop training when budget is exhausted”</li>
</ul>

<p>The practical system design question:</p>
<blockquote>
  <p>Where does the DP accounting live, and how do you enforce it so teams can’t silently ship a ‘privacy-unsafe’ training plan?</p>
</blockquote>

<p>Good answer:</p>
<ul>
  <li>DP accounting is part of the orchestrator policy engine</li>
  <li>training plans are validated before launch</li>
  <li>model registry stores DP metadata (noise, clip, epsilon consumed)</li>
</ul>

<hr />

<h2 id="5-data-flow-one-round-end-to-end">5. Data Flow (One Round End-to-End)</h2>

<h3 id="51-step-by-step">5.1 Step-by-step</h3>

<ol>
  <li><strong>Round start</strong>
    <ul>
      <li>Orchestrator decides: model <code class="language-plaintext highlighter-rouge">v123</code>, plan <code class="language-plaintext highlighter-rouge">p77</code>, cohort: “English-US”, target: 50k clients.</li>
    </ul>
  </li>
  <li><strong>Client selection</strong>
    <ul>
      <li>Selector samples from eligible clients, respecting quotas.</li>
    </ul>
  </li>
  <li><strong>Broadcast</strong>
    <ul>
      <li>Devices fetch:</li>
      <li>model weights (CDN)</li>
      <li>training plan (gRPC / config service)</li>
    </ul>
  </li>
  <li><strong>Local training</strong>
    <ul>
      <li>Device trains for a small number of steps/epochs.</li>
      <li>Applies clipping and optional noise.</li>
    </ul>
  </li>
  <li><strong>Upload</strong>
    <ul>
      <li>Device uploads masked update shares for SA (or direct update if SA disabled).</li>
    </ul>
  </li>
  <li><strong>Secure aggregation</strong>
    <ul>
      <li>Aggregator reconstructs only the sum/average update once threshold reached.</li>
    </ul>
  </li>
  <li><strong>Server update</strong>
    <ul>
      <li>Server optimizer applies aggregated update to produce new model candidate <code class="language-plaintext highlighter-rouge">v124-candidate</code>.</li>
    </ul>
  </li>
  <li><strong>Evaluation</strong>
    <ul>
      <li>Evaluate on:</li>
      <li>server-side public/curated datasets</li>
      <li>privacy-safe on-device eval (clients compute metrics and aggregate)</li>
    </ul>
  </li>
  <li><strong>Promotion</strong>
    <ul>
      <li>If guardrails pass (quality + privacy + regressions), promote to <code class="language-plaintext highlighter-rouge">v124</code>.</li>
      <li>Otherwise rollback, adjust plan, or reduce cohort.</li>
    </ul>
  </li>
</ol>

<h3 id="52-what-makes-fl-different">5.2 What makes FL “different”</h3>
<p>In centralized training, you can log:</p>
<ul>
  <li>per-example losses</li>
  <li>per-batch gradients</li>
  <li>exact failure samples</li>
</ul>

<p>In FL, you mostly see:</p>
<ul>
  <li>aggregated metrics</li>
  <li>participation rates</li>
  <li>update norms distributions (sometimes)</li>
</ul>

<p>This is why your evaluation design matters as much as your optimizer.</p>

<hr />

<h2 id="6-scaling-strategies">6. Scaling Strategies</h2>

<h3 id="61-participation-scaling-bandwidth-and-concurrency">6.1 Participation scaling: bandwidth and concurrency</h3>

<p>Assume:</p>
<ul>
  <li>model size: 20 MB</li>
  <li>50k clients per round</li>
</ul>

<p>Naively broadcasting 1 TB per round is expensive.
So systems use:</p>
<ul>
  <li><strong>delta updates</strong> (send weight diffs between versions)</li>
  <li><strong>compression</strong> (quantization, sparsification)</li>
  <li><strong>CDN caching</strong> for base weights</li>
</ul>

<p>On upload:</p>
<ul>
  <li>each client might send 100 KB–5 MB depending on compression</li>
  <li>server must handle bursty uploads (thundering herd after a “charging + Wi-Fi” window)</li>
</ul>

<h3 id="62-straggler-and-dropout-handling">6.2 Straggler and dropout handling</h3>

<p>Dropouts are normal. Designs:</p>
<ul>
  <li>set a strict round deadline; accept partial participation</li>
  <li>over-sample clients anticipating dropouts</li>
  <li>SA must tolerate dropout while preserving privacy</li>
</ul>

<h3 id="63-dealing-with-non-iid-data">6.3 Dealing with non-IID data</h3>

<p>Non-IID is the default.
Techniques:</p>
<ul>
  <li><strong>FedProx</strong>: reduces client drift</li>
  <li><strong>personalization layers</strong>: shared backbone + user-specific head</li>
  <li><strong>clustered FL</strong>: group clients with similar distributions, train multiple models</li>
</ul>

<p>This is where FL resembles distributed algorithms beyond “just average”:
you’re effectively solving a global objective with local constraints and partial information.</p>

<h3 id="64-communication-efficiency">6.4 Communication efficiency</h3>

<p>Common approaches:</p>
<ul>
  <li><strong>8-bit / 4-bit quantization</strong> of updates</li>
  <li><strong>top-k sparsification</strong> (send largest gradient components)</li>
  <li><strong>sketching</strong> (CountSketch-style)</li>
  <li><strong>periodic averaging</strong> (clients do more local steps, fewer rounds)</li>
</ul>

<p>Trade-off:</p>
<ul>
  <li>more local steps reduces comms but increases drift on non-IID data</li>
</ul>

<hr />

<h2 id="7-implementation-core-logic-in-python">7. Implementation (Core Logic in Python)</h2>

<p>This is a simplified simulation of FedAvg. In production, the core is the same conceptually, but wrapped in client runtimes, orchestration, and secure aggregation.</p>

<p>``python
from dataclasses import dataclass
from typing import List, Tuple
import numpy as np</p>

<p>@dataclass
class ClientUpdate:
 n_examples: int
 delta: np.ndarray # flattened weights delta</p>

<p>def clip_update(delta: np.ndarray, clip_norm: float) -&gt; np.ndarray:
 “"”L2 clip to bound sensitivity and reduce outliers.”””
 norm = np.linalg.norm(delta)
 if norm &lt;= clip_norm or norm == 0.0:
 return delta
 return delta * (clip_norm / norm)</p>

<p>def client_train_step(w: np.ndarray, data: Tuple[np.ndarray, np.ndarray]) -&gt; ClientUpdate:
 “””
 Toy local training:</p>
<ul>
  <li>pretend we run a couple of SGD steps and return a model delta
 “””
 x, y = data
 # Fake gradient: (this is placeholder logic for illustration)
 grad = x.T @ (x @ w - y) / max(1, len(y))
 lr = 0.1
 w_new = w - lr * grad
 delta = w_new - w
 return ClientUpdate(n_examples=len(y), delta=delta)</li>
</ul>

<p>def fedavg_aggregate(updates: List[ClientUpdate]) -&gt; np.ndarray:
 “"”Weighted average of deltas.”””
 total = sum(u.n_examples for u in updates)
 if total == 0:
 raise ValueError(“No examples aggregated”)
 return sum((u.n_examples / total) * u.delta for u in updates)</p>

<p>def federated_round(w: np.ndarray, client_datas: List[Tuple[np.ndarray, np.ndarray]], clip_norm: float) -&gt; np.ndarray:
 “"”One round: local train -&gt; clip -&gt; aggregate.”””
 updates = []
 for data in client_datas:
 upd = client_train_step(w, data)
 upd.delta = clip_update(upd.delta, clip_norm)
 updates.append(upd)
 agg_delta = fedavg_aggregate(updates)
 return w + agg_delta
``</p>

<p>What’s missing (intentionally) compared to real FL:</p>
<ul>
  <li>secure aggregation (so the server never sees <code class="language-plaintext highlighter-rouge">upd.delta</code>)</li>
  <li>device eligibility and scheduling</li>
  <li>privacy noise (DP)</li>
  <li>anti-poisoning defenses</li>
  <li>robust aggregation (median/trimmed mean)</li>
</ul>

<p>Those missing pieces are the “system design interview”.</p>

<hr />

<h2 id="8-monitoring--metrics">8. Monitoring &amp; Metrics</h2>

<p>You need metrics in four buckets:</p>

<h3 id="81-participation-health">8.1 Participation health</h3>
<ul>
  <li>eligible devices / selected devices / completed devices</li>
  <li>dropout rate by stage (download, train, upload)</li>
  <li>round latency distribution</li>
  <li>SA threshold success rate</li>
</ul>

<h3 id="82-model-update-health-privacy-aware">8.2 Model update health (privacy-aware)</h3>
<ul>
  <li>aggregate update norm</li>
  <li>per-layer norm summaries (aggregated)</li>
  <li>fraction of updates clipped (clients can report “clipped yes/no” as a bit; aggregate counts)</li>
  <li>divergence signals (server loss spikes)</li>
</ul>

<h3 id="83-quality-metrics">8.3 Quality metrics</h3>
<ul>
  <li>on-device eval accuracy (aggregated)</li>
  <li>server-side eval (public dataset) accuracy</li>
  <li>segment metrics (locale, device class)</li>
</ul>

<h3 id="84-cost-metrics">8.4 Cost metrics</h3>
<ul>
  <li>bytes downloaded/uploaded per successful client</li>
  <li>device compute time per round</li>
  <li>server CPU time per aggregation</li>
</ul>

<p>Key challenge:
you must learn to debug using <strong>distributions and aggregates</strong>, not individual samples.</p>

<h3 id="85-debugging-playbook-what-you-do-when-quality-drops">8.5 Debugging playbook (what you do when quality drops)</h3>

<p>When a centralized model regresses, you can often:</p>
<ul>
  <li>inspect failed examples</li>
  <li>run slice analysis on raw data</li>
  <li>re-train with a cleaned dataset</li>
</ul>

<p>In FL, the playbook is different. A practical incident response flow:</p>

<ol>
  <li><strong>Check participation first</strong>
    <ul>
      <li>Did eligible devices drop (OS update, policy bug, backend outage)?</li>
      <li>Did completion rate change (network conditions, app crash)?</li>
    </ul>
  </li>
  <li><strong>Check update health</strong>
    <ul>
      <li>Did aggregate update norm spike or collapse?</li>
      <li>Did clipping rate jump (indicating outliers or plan misconfiguration)?</li>
    </ul>
  </li>
  <li><strong>Check segment shifts</strong>
    <ul>
      <li>Did the sampling distribution change (more low-end devices, different geos)?</li>
      <li>Are regressions concentrated in one locale/device segment?</li>
    </ul>
  </li>
  <li><strong>Check plan drift</strong>
    <ul>
      <li>Did someone change local epochs / learning rate / batch size?</li>
      <li>Did the DP policy change (more noise, smaller cohorts)?</li>
    </ul>
  </li>
  <li><strong>Roll back safely</strong>
    <ul>
      <li>FL systems should support rollback to last-known-good model + plan.</li>
      <li>Never “debug in production” by shipping risky plans to all devices.</li>
    </ul>
  </li>
</ol>

<p>This is why model registry metadata and orchestration policies are first-class engineering.</p>

<h3 id="86-privacy-safe-evaluation-patterns">8.6 Privacy-safe evaluation patterns</h3>

<p>To measure quality without collecting raw client data:</p>
<ul>
  <li>run a local eval on device (loss/accuracy on a held-out local slice)</li>
  <li>report only aggregated metrics (SA and/or DP)</li>
</ul>

<p>Common design:</p>
<ul>
  <li>the orchestrator ships an “eval-only plan” (no training)</li>
  <li>devices compute metrics and upload only counts/sums</li>
  <li>you can then compare cohorts without seeing a single example</li>
</ul>

<p>This becomes a reusable system primitive (“federated eval”) that product teams can use.</p>

<h3 id="87-fleet-health-metrics-client-runtime-as-a-distributed-system">8.7 Fleet health metrics (client runtime as a distributed system)</h3>

<p>Treat the client runtime like a large distributed compute cluster.
You need SRE-style metrics:</p>
<ul>
  <li>crash rate during training (by device model/OS)</li>
  <li>battery impact distributions</li>
  <li>CPU time / memory usage</li>
  <li>network bytes and failure codes</li>
</ul>

<p>The “training plan” is effectively code you deploy to that cluster.
So you need:</p>
<ul>
  <li>canarying</li>
  <li>staged rollout</li>
  <li>rapid disable switches</li>
</ul>

<h3 id="88-auditability-and-compliance">8.8 Auditability and compliance</h3>

<p>If FL is used for privacy reasons, you must be able to prove:</p>
<ul>
  <li>which models were trained with which privacy parameters</li>
  <li>which cohorts were eligible/participating</li>
  <li>what DP budget was consumed over time</li>
</ul>

<p>Operationally:</p>
<ul>
  <li>store DP metadata in the model registry</li>
  <li>sign training plans (policy enforcement)</li>
  <li>create an audit trail of promotion decisions</li>
</ul>

<hr />

<h2 id="9-failure-modes-and-mitigations">9. Failure Modes (and Mitigations)</h2>

<h3 id="91-client-poisoning">9.1 Client poisoning</h3>
<p>Attack: a malicious client sends a crafted update to cause targeted behavior.</p>

<p>Mitigations:</p>
<ul>
  <li>robust aggregation (trimmed mean, coordinate-wise median)</li>
  <li>anomaly detection on update norms (client-side + aggregate)</li>
  <li>limit per-client contribution via clipping</li>
  <li>attestations / trusted execution where possible</li>
</ul>

<h3 id="92-privacy-leakage-via-updates">9.2 Privacy leakage via updates</h3>
<p>Attack: gradient inversion or membership inference.</p>

<p>Mitigations:</p>
<ul>
  <li>secure aggregation + minimum cohort size</li>
  <li>clipping + DP noise</li>
  <li>restrict model capacity for sensitive tasks</li>
</ul>

<h3 id="93-non-iid-drift-and-catastrophic-regressions">9.3 Non-IID drift and catastrophic regressions</h3>
<p>Example: model overfits heavy users in one locale.</p>

<p>Mitigations:</p>
<ul>
  <li>stratified sampling</li>
  <li>segment-based guardrails</li>
  <li>personalization layers rather than fully global updates</li>
</ul>

<h3 id="94-operational-instability">9.4 Operational instability</h3>
<ul>
  <li>SA failures due to too many dropouts</li>
  <li>app version fragmentation breaks client runtime</li>
</ul>

<p>Mitigations:</p>
<ul>
  <li>over-sample, shorten round deadlines</li>
  <li>strict compatibility checks</li>
  <li>staged rollouts for new plan versions</li>
</ul>

<h3 id="95-data-poisoning-without-malice-bugs-look-like-attacks">9.5 Data poisoning without “malice” (bugs look like attacks)</h3>

<p>Not all bad updates are adversarial. Many incidents are plain bugs:</p>
<ul>
  <li>a client preprocessing change flips labels</li>
  <li>a corrupted local cache creates nonsense examples</li>
  <li>a new OS version changes tokenizer behavior</li>
</ul>

<p>In centralized training you might detect this via offline data validation.
In FL you must rely on:</p>
<ul>
  <li>plan versioning (so you can pinpoint which plan produced the regression)</li>
  <li>participation segmentation (so you can isolate which app/OS version is problematic)</li>
  <li>aggregate anomaly detection (norm spikes, sudden loss changes)</li>
</ul>

<p>This is one reason why FL orchestration feels like operating a distributed system:
the client fleet is not homogeneous, and “bad actors” often appear accidentally.</p>

<h3 id="96-silent-sampling-bias-the-most-common-real-world-failure">9.6 Silent sampling bias (the most common real-world failure)</h3>

<p>Even if your training algorithm is perfect, your sampling can drift:</p>
<ul>
  <li>you accidentally prefer devices that are always on Wi‑Fi and charging (a specific demographic)</li>
  <li>you exclude low-end phones due to memory limits (hurts global fairness)</li>
  <li>you oversample one geography because of timezone scheduling</li>
</ul>

<p>Mitigations:</p>
<ul>
  <li>explicit stratified quotas (by geo, device class, app version)</li>
  <li>sampling audits (“who participated last week vs target distribution?”)</li>
  <li>fairness guardrails (segment metrics must not regress)</li>
</ul>

<p>If you want to impress in interviews, say:</p>
<blockquote>
  <p>In FL, the client selector is your data loader. Sampling bias is a training bug.</p>
</blockquote>

<hr />

<h2 id="10-real-world-case-study">10. Real-World Case Study</h2>

<h3 id="101-google-gboard-next-word-prediction">10.1 Google Gboard next-word prediction</h3>
<p>One of the most cited FL deployments.
The model trains on-device on typed text without uploading raw keystrokes.</p>

<p>Key lessons:</p>
<ul>
  <li>device scheduling is as important as the optimizer</li>
  <li>evaluation is hard without raw data</li>
  <li>privacy constraints force different debugging techniques</li>
</ul>

<h3 id="102-apple-on-device-personalization">10.2 Apple on-device personalization</h3>
<p>Apple has publicly discussed privacy-preserving analytics and on-device ML patterns.
Even when not strictly FL, the constraints are similar:</p>
<ul>
  <li>data stays on device</li>
  <li>server learns via aggregates</li>
</ul>

<h3 id="103-a-speech-flavored-case-study-personalized-wake-words--accents">10.3 A speech-flavored case study (personalized wake words / accents)</h3>

<p>Speech is a great “stress test” for FL:</p>
<ul>
  <li>client data is extremely sensitive</li>
  <li>models are large</li>
  <li>non-IID is strong (accent, environment, microphone)</li>
</ul>

<p>A realistic production approach:</p>
<ul>
  <li>keep the base wake word detector global and stable</li>
  <li>personalize a small threshold or adapter layer per device</li>
  <li>aggregate only privacy-protected signals across devices to improve the global model slowly</li>
</ul>

<p>Engineering lesson:
you often split learning into:</p>
<ul>
  <li><strong>fast local personalization</strong> (small, device-specific)</li>
  <li><strong>slow global improvements</strong> (federated, heavily gated)</li>
</ul>

<p>This reduces risk while still benefiting from collective learning.</p>

<hr />

<h2 id="11-cost-analysis">11. Cost Analysis</h2>

<p>Cost drivers:</p>
<ul>
  <li><strong>bandwidth</strong> (download + upload)</li>
  <li><strong>server aggregation</strong> (compute + storage for protocol states)</li>
  <li><strong>engineering complexity</strong> (privacy/security requirements increase operational overhead)</li>
</ul>

<p>Practical levers:</p>
<ul>
  <li>compress models and updates</li>
  <li>reduce round frequency (but monitor for quality stagnation)</li>
  <li>select only clients likely to complete (charging + Wi-Fi windows)</li>
</ul>

<h3 id="111-a-back-of-the-envelope-cost-model-so-you-can-reason-in-interviews">11.1 A back-of-the-envelope cost model (so you can reason in interviews)</h3>

<p>Assume a mid-size on-device model:</p>
<ul>
  <li>model weights: 20 MB (download)</li>
  <li>update payload (compressed): 200 KB (upload)</li>
  <li>50k participating clients per day (across many rounds)</li>
</ul>

<p>Daily bandwidth:</p>
<ul>
  <li>downloads: (50{,}000 \times 20\text{MB} = 1{,}000{,}000\text{MB} \approx 1\text{TB})</li>
  <li>uploads: (50{,}000 \times 200\text{KB} = 10{,}000{,}000\text{KB} \approx 10\text{GB})</li>
</ul>

<p>Even if CDNs make downloads “cheap”, 1 TB/day is not free.
This is why production FL systems invest in:</p>
<ul>
  <li>delta updates between model versions</li>
  <li>caching via CDN</li>
  <li>smaller adapter-based updates rather than full model updates</li>
</ul>

<h3 id="112-the-client-compute-cost-you-dont-pay-but-your-users-do">11.2 The “client compute cost” you don’t pay (but your users do)</h3>

<p>Server-side training costs show up on your cloud bill.
Federated training moves a large part of compute to devices:</p>
<ul>
  <li>CPU/NPU cycles</li>
  <li>battery usage</li>
  <li>thermal constraints</li>
</ul>

<p>If training is too aggressive, you get:</p>
<ul>
  <li>user complaints (“phone gets hot”)</li>
  <li>OS throttling</li>
  <li>drops in completion rate (which also hurts model quality)</li>
</ul>

<p>So cost is not just “dollars”; it’s:</p>
<ul>
  <li>product risk</li>
  <li>engagement risk</li>
  <li>fleet health risk</li>
</ul>

<h3 id="113-roi-framing">11.3 ROI framing</h3>

<p>In many teams, FL is justified when:</p>
<ul>
  <li>privacy constraints block centralized data collection</li>
  <li>personalization materially improves retention/engagement</li>
  <li>the incremental engineering effort pays off via model quality and trust</li>
</ul>

<p>In interviews, a crisp way to say it:</p>
<blockquote>
  <p>FL is expensive to build, but sometimes it’s the only path to high-quality models under privacy constraints.</p>
</blockquote>

<hr />

<h2 id="12-key-takeaways">12. Key Takeaways</h2>

<ol>
  <li><strong>Federated learning is distributed optimization under constraints</strong>: unreliable clients, non-IID data, privacy/security.</li>
  <li><strong>System design dominates</strong>: client eligibility, secure aggregation, privacy, observability, and rollout safety are the hard parts.</li>
  <li><strong>Boundary-first thinking scales</strong>: like the median partition trick, FL often succeeds by optimizing what you exchange (bounded, aggregated signals) rather than moving raw data.</li>
</ol>

<h3 id="121-a-concise-design-review-checklist-for-fl-systems">12.1 A concise “design review checklist” for FL systems</h3>

<p>If you’re reviewing a federated learning design, ask:</p>

<ul>
  <li><strong>Client policy</strong></li>
  <li>How do we decide eligibility (Wi‑Fi, charging, opt-in)?</li>
  <li>
    <p>What is the target sampling distribution (and how do we audit it)?</p>
  </li>
  <li><strong>Privacy</strong></li>
  <li>Is secure aggregation enabled for sensitive models?</li>
  <li>Do we clip updates? Where is the DP budget accounted and enforced?</li>
  <li>
    <p>What is the minimum cohort size per segment?</p>
  </li>
  <li><strong>Robustness</strong></li>
  <li>What aggregator do we use (FedAvg vs robust variants)?</li>
  <li>How do we handle dropouts and stragglers?</li>
  <li>
    <p>What anti-poisoning mitigations exist?</p>
  </li>
  <li><strong>MLOps</strong></li>
  <li>Are plans versioned and validated before execution?</li>
  <li>Can we canary/roll back both model and plan quickly?</li>
  <li>What metrics exist without inspecting raw client data?</li>
</ul>

<p>If these questions have crisp answers, you’re operating FL like a real distributed system instead of a research prototype.</p>

<h3 id="122-deployment-note-fl-still-needs-a-serving-story">12.2 Deployment note: FL still needs a serving story</h3>

<p>Federated learning updates a model, but product impact comes from <strong>serving</strong>:</p>
<ul>
  <li>on-device inference (TFLite/CoreML)</li>
  <li>server-side inference (if the model isn’t privacy-sensitive at query time)</li>
</ul>

<p>Practical serving considerations:</p>
<ul>
  <li>keep runtime compatibility (model format + ops supported on older devices)</li>
  <li>stage rollouts with kill-switches (bad models are user-facing incidents)</li>
  <li>monitor on-device latency and battery impact (quality gains can be negated by UX regressions)</li>
</ul>

<p>In other words: FL is not just training infrastructure; it’s an end-to-end system from “round” to “release”.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0051-federated-learning/">arunbaby.com/ml-system-design/0051-federated-learning</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#distributed-training" class="page__taxonomy-item p-category" rel="tag">distributed-training</a><span class="sep">, </span>
    
      <a href="/tags/#federated-learning" class="page__taxonomy-item p-category" rel="tag">federated-learning</a><span class="sep">, </span>
    
      <a href="/tags/#mlops" class="page__taxonomy-item p-category" rel="tag">mlops</a><span class="sep">, </span>
    
      <a href="/tags/#on-device-ml" class="page__taxonomy-item p-category" rel="tag">on-device-ml</a><span class="sep">, </span>
    
      <a href="/tags/#privacy" class="page__taxonomy-item p-category" rel="tag">privacy</a><span class="sep">, </span>
    
      <a href="/tags/#secure-aggregation" class="page__taxonomy-item p-category" rel="tag">secure-aggregation</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0051-median-of-two-sorted-arrays/" rel="permalink">Median of Two Sorted Arrays
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Stop thinking ‘merge’. Think ‘partition’—the median is just the boundary between two halves.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0051-privacy-preserving-speech/" rel="permalink">Privacy-preserving Speech
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Speech is biometric. Treat every waveform like a password—design systems that learn without listening.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0051-knowledge-graphs-for-agents/" rel="permalink">Knowledge Graphs for Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“RAG gives you documents. A knowledge graph gives you facts with structure—and agents need structure to act reliably.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Federated+Learning%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0051-federated-learning%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0051-federated-learning%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0051-federated-learning/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0050-character-level-language-models/" class="pagination--pager" title="Character-Level Language Models">Previous</a>
    
    
      <a href="/ml-system-design/0052-anomaly-detection/" class="pagination--pager" title="Anomaly Detection">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
