<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Model Compression Techniques - Arun Baby</title>
<meta name="description" content="“Fitting billion-parameter models into megabytes.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Model Compression Techniques">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0041-model-compression/">


  <meta property="og:description" content="“Fitting billion-parameter models into megabytes.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Model Compression Techniques">
  <meta name="twitter:description" content="“Fitting billion-parameter models into megabytes.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0041-model-compression/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-14T23:01:57+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0041-model-compression/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Model Compression Techniques">
    <meta itemprop="description" content="“Fitting billion-parameter models into megabytes.”">
    <meta itemprop="datePublished" content="2025-12-14T23:01:57+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0041-model-compression/" itemprop="url">Model Compression Techniques
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          20 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-the-compression-imperative">1. The Compression Imperative</a></li><li><a href="#2-taxonomy-of-compression-techniques">2. Taxonomy of Compression Techniques</a></li><li><a href="#3-quantization">3. Quantization</a><ul><li><a href="#31-post-training-quantization-ptq">3.1. Post-Training Quantization (PTQ)</a></li><li><a href="#32-quantization-aware-training-qat">3.2. Quantization-Aware Training (QAT)</a></li><li><a href="#33-dynamic-vs-static-quantization">3.3. Dynamic vs Static Quantization</a></li></ul></li><li><a href="#4-pruning">4. Pruning</a><ul><li><a href="#41-unstructured-pruning">4.1. Unstructured Pruning</a></li><li><a href="#42-structured-pruning">4.2. Structured Pruning</a></li><li><a href="#43-lottery-ticket-hypothesis">4.3. Lottery Ticket Hypothesis</a></li></ul></li><li><a href="#5-knowledge-distillation">5. Knowledge Distillation</a><ul><li><a href="#51-distilbert">5.1. DistilBERT</a></li></ul></li><li><a href="#6-low-rank-factorization">6. Low-Rank Factorization</a></li><li><a href="#7-system-design-on-device-inference-pipeline">7. System Design: On-Device Inference Pipeline</a></li><li><a href="#8-deep-dive-mixed-precision-quantization">8. Deep Dive: Mixed-Precision Quantization</a></li><li><a href="#9-deep-dive-gradient-compression-for-distributed-training">9. Deep Dive: Gradient Compression for Distributed Training</a></li><li><a href="#10-case-study-tensorflow-lite">10. Case Study: TensorFlow Lite</a></li><li><a href="#11-interview-questions">11. Interview Questions</a></li><li><a href="#12-common-pitfalls">12. Common Pitfalls</a></li><li><a href="#13-hardware-specific-optimizations">13. Hardware-Specific Optimizations</a><ul><li><a href="#131-arm-neon-mobile-cpus">13.1. ARM NEON (Mobile CPUs)</a></li><li><a href="#132-nvidia-tensor-cores">13.2. NVIDIA Tensor Cores</a></li><li><a href="#133-google-tpu-tensor-processing-unit">13.3. Google TPU (Tensor Processing Unit)</a></li></ul></li><li><a href="#14-production-case-study-bert-compression-for-search">14. Production Case Study: BERT Compression for Search</a></li><li><a href="#15-advanced-technique-neural-architecture-search-for-compression">15. Advanced Technique: Neural Architecture Search for Compression</a></li><li><a href="#16-compression-benchmarks">16. Compression Benchmarks</a></li><li><a href="#17-deep-dive-weight-clustering">17. Deep Dive: Weight Clustering</a></li><li><a href="#18-deep-dive-dynamic-neural-networks">18. Deep Dive: Dynamic Neural Networks</a></li><li><a href="#19-production-deployment-model-serving">19. Production Deployment: Model Serving</a></li><li><a href="#20-cost-benefit-analysis">20. Cost-Benefit Analysis</a></li><li><a href="#21-ethical-considerations">21. Ethical Considerations</a></li><li><a href="#22-future-trends">22. Future Trends</a></li><li><a href="#23-conclusion">23. Conclusion</a></li><li><a href="#24-deep-dive-onnx-open-neural-network-exchange">24. Deep Dive: ONNX (Open Neural Network Exchange)</a></li><li><a href="#25-mobile-deployment-ios-core-ml">25. Mobile Deployment: iOS (Core ML)</a></li><li><a href="#26-mobile-deployment-android-tensorflow-lite">26. Mobile Deployment: Android (TensorFlow Lite)</a></li><li><a href="#27-advanced-technique-sparse-tensor-cores-nvidia">27. Advanced Technique: Sparse Tensor Cores (NVIDIA)</a></li><li><a href="#28-deep-dive-huffman-coding-for-weight-compression">28. Deep Dive: Huffman Coding for Weight Compression</a></li><li><a href="#29-production-case-study-mobilenet-deployment">29. Production Case Study: MobileNet Deployment</a></li><li><a href="#30-advanced-technique-neural-ode-compression">30. Advanced Technique: Neural ODE Compression</a></li><li><a href="#31-monitoring-compressed-models-in-production">31. Monitoring Compressed Models in Production</a></li><li><a href="#32-interview-deep-dive-compression-trade-offs">32. Interview Deep Dive: Compression Trade-offs</a></li><li><a href="#33-future-research-directions">33. Future Research Directions</a></li><li><a href="#34-conclusion--best-practices">34. Conclusion &amp; Best Practices</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Fitting billion-parameter models into megabytes.”</strong></p>

<h2 id="1-the-compression-imperative">1. The Compression Imperative</h2>

<p>Modern deep learning models are massive:</p>
<ul>
  <li><strong>GPT-3:</strong> 175B parameters = 700GB (FP32).</li>
  <li><strong>BERT-Large:</strong> 340M parameters = 1.3GB (FP32).</li>
  <li><strong>ResNet-50:</strong> 25M parameters = 100MB (FP32).</li>
</ul>

<p><strong>Challenges:</strong></p>
<ul>
  <li><strong>Deployment:</strong> Can’t fit on mobile devices (limited RAM).</li>
  <li><strong>Inference:</strong> Slow on CPUs without GPU acceleration.</li>
  <li><strong>Cost:</strong> Cloud inference costs scale with model size.</li>
</ul>

<p><strong>Goal:</strong> Reduce model size by 10-100x while maintaining 95%+ accuracy.</p>

<h2 id="2-taxonomy-of-compression-techniques">2. Taxonomy of Compression Techniques</h2>

<ol>
  <li><strong>Quantization:</strong> Reduce numerical precision (FP32 → INT8).</li>
  <li><strong>Pruning:</strong> Remove redundant weights or neurons.</li>
  <li><strong>Knowledge Distillation:</strong> Train a small model to mimic a large model.</li>
  <li><strong>Low-Rank Factorization:</strong> Decompose weight matrices.</li>
  <li><strong>Neural Architecture Search (NAS):</strong> Design efficient architectures.</li>
  <li><strong>Weight Sharing:</strong> Cluster weights and share values.</li>
</ol>

<h2 id="3-quantization">3. Quantization</h2>

<h3 id="31-post-training-quantization-ptq">3.1. Post-Training Quantization (PTQ)</h3>

<p><strong>Idea:</strong> Convert a trained FP32 model to INT8 without retraining.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Calibration:</strong> Run the model on a small dataset (e.g., 1000 samples).</li>
  <li><strong>Collect Statistics:</strong> Record min/max values of activations for each layer.</li>
  <li><strong>Compute Scale and Zero-Point:</strong>
    <ul>
      <li>$scale = \frac{max - min}{255}$ (for INT8).</li>
      <li>$zero_point = -\frac{min}{scale}$.</li>
    </ul>
  </li>
  <li><strong>Quantize Weights:</strong>
    <ul>
      <li>$W_{int8} = round(\frac{W_{fp32}}{scale} + zero_point)$.</li>
    </ul>
  </li>
  <li><strong>Dequantize for Inference:</strong>
    <ul>
      <li>$W_{fp32} = (W_{int8} - zero_point) \times scale$.</li>
    </ul>
  </li>
</ol>

<p><strong>Result:</strong> 4x memory reduction, 2-4x speedup on CPUs.</p>

<p><strong>Accuracy Drop:</strong> Typically 0.5-2% for CNNs, 1-5% for Transformers.</p>

<h3 id="32-quantization-aware-training-qat">3.2. Quantization-Aware Training (QAT)</h3>

<p><strong>Idea:</strong> Simulate quantization during training so the model learns to be robust to quantization noise.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Insert <strong>Fake Quantization</strong> nodes after each layer.</li>
  <li>During forward pass:
    <ul>
      <li>Quantize activations: $a_{int8} = round(\frac{a_{fp32}}{scale})$.</li>
      <li>Dequantize: $a_{fp32} = a_{int8} \times scale$.</li>
    </ul>
  </li>
  <li>During backward pass:
    <ul>
      <li>Use <strong>Straight-Through Estimator (STE):</strong> Treat the <code class="language-plaintext highlighter-rouge">round()</code> function as identity for gradients.</li>
    </ul>
  </li>
  <li>Train for a few epochs (fine-tuning).</li>
</ol>

<p><strong>Result:</strong> 1-2% better accuracy than PTQ.</p>

<h3 id="33-dynamic-vs-static-quantization">3.3. Dynamic vs Static Quantization</h3>

<p><strong>Static Quantization:</strong></p>
<ul>
  <li>Quantize both weights and activations.</li>
  <li>Requires calibration dataset.</li>
  <li><strong>Use Case:</strong> Inference on edge devices.</li>
</ul>

<p><strong>Dynamic Quantization:</strong></p>
<ul>
  <li>Quantize only weights. Activations remain FP32.</li>
  <li>No calibration needed.</li>
  <li><strong>Use Case:</strong> NLP models (BERT) on CPUs.</li>
</ul>

<h2 id="4-pruning">4. Pruning</h2>

<h3 id="41-unstructured-pruning">4.1. Unstructured Pruning</h3>

<p><strong>Idea:</strong> Remove individual weights with small magnitude.</p>

<p><strong>Algorithm (Magnitude-Based Pruning):</strong></p>
<ol>
  <li>Train the model to convergence.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Compute the magnitude of each weight: $</td>
          <td>W_{ij}</td>
          <td>$.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Sort weights by magnitude.</li>
  <li>Set the smallest $p\%$ of weights to zero (e.g., $p = 90$).</li>
  <li>Fine-tune the pruned model.</li>
  <li>Repeat (iterative pruning).</li>
</ol>

<p><strong>Result:</strong> 90% sparsity with &lt;1% accuracy drop.</p>

<p><strong>Challenge:</strong> Sparse matrices are not efficiently supported on all hardware. Need specialized libraries (e.g., NVIDIA Sparse Tensor Cores).</p>

<h3 id="42-structured-pruning">4.2. Structured Pruning</h3>

<p><strong>Idea:</strong> Remove entire channels, filters, or attention heads.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Compute the importance of each filter (e.g., L1 norm of weights).</li>
  <li>Remove the least important filters.</li>
  <li>Fine-tune.</li>
</ol>

<p><strong>Result:</strong> 50% compression with minimal accuracy drop. Works on standard hardware.</p>

<h3 id="43-lottery-ticket-hypothesis">4.3. Lottery Ticket Hypothesis</h3>

<p><strong>Discovery (Frankle &amp; Carbin, 2019):</strong>
A randomly initialized network contains a “winning ticket” subnetwork that, when trained in isolation, can match the full network’s accuracy.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Train the full network.</li>
  <li>Prune to sparsity $p\%$.</li>
  <li><strong>Rewind</strong> weights to their initial values (not random re-initialization).</li>
  <li>Train the pruned network from the initial weights.</li>
</ol>

<p><strong>Implication:</strong> We can find small, trainable networks by pruning and rewinding.</p>

<h2 id="5-knowledge-distillation">5. Knowledge Distillation</h2>

<p><strong>Idea:</strong> Train a small “student” model to mimic a large “teacher” model.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Teacher:</strong> Train a large, accurate model (e.g., BERT-Large).</li>
  <li><strong>Student:</strong> Define a smaller model (e.g., BERT-Tiny, 10x smaller).</li>
  <li><strong>Distillation Loss:</strong>
    <ul>
      <li><strong>Soft Targets:</strong> Use the teacher’s softmax probabilities (not hard labels).</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td>$L_{distill} = KL(P_{teacher}</td>
              <td> </td>
              <td>P_{student})$.</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li><strong>Temperature Scaling:</strong> $P_i = \frac{e^{z_i / T}}{\sum_j e^{z_j / T}}$ where $T &gt; 1$ (e.g., $T = 3$). Higher temperature makes the distribution “softer” (less peaked), revealing more information about the teacher’s uncertainty.</li>
    </ul>
  </li>
  <li><strong>Combined Loss:</strong>
    <ul>
      <li>$L = \alpha L_{CE}(y, P_{student}) + (1 - \alpha) L_{distill}$.</li>
    </ul>
  </li>
  <li>Train the student on the same dataset.</li>
</ol>

<p><strong>Result:</strong> Student achieves 95-98% of teacher’s accuracy with 10x fewer parameters.</p>

<h3 id="51-distilbert">5.1. DistilBERT</h3>

<p><strong>Architecture:</strong></p>
<ul>
  <li>6 layers (vs 12 in BERT-Base).</li>
  <li>Hidden size 768 (same as BERT).</li>
  <li>40% fewer parameters.</li>
</ul>

<p><strong>Training:</strong></p>
<ul>
  <li>Distilled from BERT-Base.</li>
  <li>Triple loss: Distillation + Masked LM + Cosine Embedding (hidden states).</li>
</ul>

<p><strong>Result:</strong></p>
<ul>
  <li>97% of BERT-Base accuracy on GLUE.</li>
  <li>60% faster inference.</li>
</ul>

<h2 id="6-low-rank-factorization">6. Low-Rank Factorization</h2>

<p><strong>Idea:</strong> Decompose a weight matrix $W \in \mathbb{R}^{m \times n}$ into $W = U V^T$ where $U \in \mathbb{R}^{m \times r}$ and $V \in \mathbb{R}^{n \times r}$ with $r \ll \min(m, n)$.</p>

<p><strong>Benefit:</strong> Reduce parameters from $m \times n$ to $r(m + n)$.</p>

<p><strong>Algorithm (SVD):</strong></p>
<ol>
  <li>Compute Singular Value Decomposition: $W = U \Sigma V^T$.</li>
  <li>Keep only the top $r$ singular values.</li>
  <li>$W_{approx} = U_r \Sigma_r V_r^T$.</li>
</ol>

<p><strong>Use Case:</strong> Compressing the embedding layer in NLP models.</p>

<h2 id="7-system-design-on-device-inference-pipeline">7. System Design: On-Device Inference Pipeline</h2>

<p><strong>Scenario:</strong> Deploy a BERT model on a smartphone for real-time text classification.</p>

<p><strong>Constraints:</strong></p>
<ul>
  <li><strong>Model Size:</strong> &lt; 50MB.</li>
  <li><strong>Latency:</strong> &lt; 100ms per inference.</li>
  <li><strong>Power:</strong> &lt; 500mW.</li>
</ul>

<p><strong>Solution:</strong></p>

<p><strong>Step 1: Compression</strong></p>
<ul>
  <li><strong>Distillation:</strong> BERT-Base (110M params) → DistilBERT (66M params).</li>
  <li><strong>Quantization:</strong> FP32 → INT8 (4x reduction).</li>
  <li><strong>Final Size:</strong> 66M params × 1 byte = 66MB → 50MB after compression.</li>
</ul>

<p><strong>Step 2: Optimization</strong></p>
<ul>
  <li><strong>ONNX Runtime:</strong> Convert PyTorch model to ONNX for optimized inference.</li>
  <li><strong>Operator Fusion:</strong> Fuse LayerNorm + GELU into a single kernel.</li>
  <li><strong>Graph Optimization:</strong> Remove redundant nodes.</li>
</ul>

<p><strong>Step 3: Hardware Acceleration</strong></p>
<ul>
  <li><strong>Android:</strong> Use NNAPI (Neural Networks API) to leverage GPU/DSP.</li>
  <li><strong>iOS:</strong> Use Core ML with ANE (Apple Neural Engine).</li>
</ul>

<p><strong>Step 4: Caching</strong></p>
<ul>
  <li>Cache embeddings for frequently seen inputs.</li>
</ul>

<h2 id="8-deep-dive-mixed-precision-quantization">8. Deep Dive: Mixed-Precision Quantization</h2>

<p>Not all layers need the same precision. Sensitive layers (e.g., first/last layer) can remain FP16, while others are INT8.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Sensitivity Analysis:</strong> For each layer, measure accuracy drop when quantized to INT8.</li>
  <li><strong>Pareto Frontier:</strong> Find the set of layer precisions that maximize accuracy for a given model size.</li>
  <li><strong>AutoML:</strong> Use Neural Architecture Search to find the optimal precision for each layer.</li>
</ol>

<p><strong>Example (MobileNetV2):</strong></p>
<ul>
  <li>First layer: FP16 (sensitive to quantization).</li>
  <li>Middle layers: INT8.</li>
  <li>Last layer: FP16 (classification head).</li>
</ul>

<h2 id="9-deep-dive-gradient-compression-for-distributed-training">9. Deep Dive: Gradient Compression for Distributed Training</h2>

<p>In distributed training, gradients are communicated across GPUs. Compressing gradients reduces bandwidth.</p>

<p><strong>Techniques:</strong></p>

<p><strong>1. Gradient Sparsification:</strong></p>
<ul>
  <li>Send only the top-k largest gradients.</li>
  <li>Accumulate the rest locally.</li>
  <li><strong>Result:</strong> 99% compression with minimal accuracy drop.</li>
</ul>

<p><strong>2. Gradient Quantization:</strong></p>
<ul>
  <li>Quantize gradients to 8-bit or even 1-bit.</li>
  <li><strong>1-bit SGD:</strong> Send only the sign of the gradient.</li>
</ul>

<p><strong>3. Error Feedback:</strong></p>
<ul>
  <li>Track the quantization error and add it to the next gradient.</li>
  <li>Ensures convergence despite lossy compression.</li>
</ul>

<h2 id="10-case-study-tensorflow-lite">10. Case Study: TensorFlow Lite</h2>

<p><strong>Goal:</strong> Run TensorFlow models on mobile and embedded devices.</p>

<p><strong>Features:</strong></p>
<ul>
  <li><strong>Converter:</strong> Converts TensorFlow/Keras models to <code class="language-plaintext highlighter-rouge">.tflite</code> format.</li>
  <li><strong>Quantization:</strong> Built-in PTQ and QAT.</li>
  <li><strong>Optimizations:</strong> Operator fusion, constant folding.</li>
  <li><strong>Delegates:</strong> Hardware acceleration (GPU, DSP, NPU).</li>
</ul>

<p><strong>Example:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># Load model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span><span class="sh">'</span><span class="s">model.h5</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Convert to TFLite with INT8 quantization
</span><span class="n">converter</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">lite</span><span class="p">.</span><span class="n">TFLiteConverter</span><span class="p">.</span><span class="nf">from_keras_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">converter</span><span class="p">.</span><span class="n">optimizations</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">lite</span><span class="p">.</span><span class="n">Optimize</span><span class="p">.</span><span class="n">DEFAULT</span><span class="p">]</span>
<span class="n">converter</span><span class="p">.</span><span class="n">target_spec</span><span class="p">.</span><span class="n">supported_ops</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">lite</span><span class="p">.</span><span class="n">OpsSet</span><span class="p">.</span><span class="n">TFLITE_BUILTINS_INT8</span><span class="p">]</span>

<span class="c1"># Calibration
</span><span class="k">def</span> <span class="nf">representative_dataset</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">calibration_data</span><span class="p">:</span>
        <span class="k">yield</span> <span class="p">[</span><span class="n">data</span><span class="p">]</span>

<span class="n">converter</span><span class="p">.</span><span class="n">representative_dataset</span> <span class="o">=</span> <span class="n">representative_dataset</span>
<span class="n">tflite_model</span> <span class="o">=</span> <span class="n">converter</span><span class="p">.</span><span class="nf">convert</span><span class="p">()</span>

<span class="c1"># Save
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">model.tflite</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">wb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">tflite_model</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="11-interview-questions">11. Interview Questions</h2>

<ol>
  <li><strong>Explain Quantization.</strong> What is the difference between PTQ and QAT?</li>
  <li><strong>Knowledge Distillation.</strong> Why use soft targets instead of hard labels?</li>
  <li><strong>Pruning.</strong> What is the Lottery Ticket Hypothesis?</li>
  <li><strong>Trade-offs.</strong> Quantization vs Pruning vs Distillation. When to use which?</li>
  <li><strong>Calculate Compression Ratio.</strong> A model has 100M parameters. After 90% pruning and INT8 quantization, what is the final size?
    <ul>
      <li>Original: 100M × 4 bytes = 400MB.</li>
      <li>After pruning: 10M params.</li>
      <li>After quantization: 10M × 1 byte = 10MB.</li>
      <li><strong>Compression:</strong> 40x.</li>
    </ul>
  </li>
</ol>

<h2 id="12-common-pitfalls">12. Common Pitfalls</h2>

<ul>
  <li><strong>Quantizing Batch Norm:</strong> Batch Norm layers should be fused with Conv layers before quantization.</li>
  <li><strong>Calibration Data:</strong> Using too little calibration data leads to poor quantization ranges.</li>
  <li><strong>Pruning Ratio:</strong> Pruning too aggressively (&gt;95%) often causes unrecoverable accuracy loss.</li>
  <li><strong>Distillation Temperature:</strong> Too high ($T &gt; 10$) makes the soft targets too uniform (no information). Too low ($T = 1$) is equivalent to hard labels.</li>
</ul>

<h2 id="13-hardware-specific-optimizations">13. Hardware-Specific Optimizations</h2>

<h3 id="131-arm-neon-mobile-cpus">13.1. ARM NEON (Mobile CPUs)</h3>

<p><strong>NEON</strong> is ARM’s SIMD (Single Instruction Multiple Data) instruction set.</p>

<p><strong>Optimization:</strong></p>
<ul>
  <li><strong>INT8 GEMM:</strong> Matrix multiplication using 8-bit integers.</li>
  <li><strong>Vectorization:</strong> Process 16 INT8 values in parallel.</li>
  <li><strong>Fused Operations:</strong> Combine Conv + ReLU + Quantization in a single kernel.</li>
</ul>

<p><strong>Performance Gain:</strong> 4-8x speedup over FP32 on ARM Cortex-A76.</p>

<h3 id="132-nvidia-tensor-cores">13.2. NVIDIA Tensor Cores</h3>

<p><strong>Tensor Cores</strong> are specialized hardware for matrix multiplication.</p>

<p><strong>Supported Precisions:</strong></p>
<ul>
  <li><strong>FP16:</strong> 312 TFLOPS on A100.</li>
  <li><strong>INT8:</strong> 624 TOPS (Tera Operations Per Second).</li>
  <li><strong>INT4:</strong> 1248 TOPS.</li>
</ul>

<p><strong>Optimization:</strong></p>
<ul>
  <li>Use <strong>Automatic Mixed Precision (AMP)</strong> in PyTorch.</li>
  <li>Ensure matrix dimensions are multiples of 8 (for INT8) or 16 (for FP16).</li>
</ul>

<h3 id="133-google-tpu-tensor-processing-unit">13.3. Google TPU (Tensor Processing Unit)</h3>

<p><strong>TPU v4:</strong></p>
<ul>
  <li><strong>Precision:</strong> BF16 (Brain Float 16).</li>
  <li><strong>Systolic Array:</strong> Optimized for matrix multiplications.</li>
  <li><strong>Memory:</strong> 32GB HBM (High Bandwidth Memory).</li>
</ul>

<p><strong>Optimization:</strong></p>
<ul>
  <li>Use <strong>XLA (Accelerated Linear Algebra)</strong> compiler for graph optimization.</li>
  <li>Batch size should be large (&gt;128) to saturate the TPU.</li>
</ul>

<h2 id="14-production-case-study-bert-compression-for-search">14. Production Case Study: BERT Compression for Search</h2>

<p><strong>Scenario:</strong> Deploy BERT for semantic search at Google scale (billions of queries/day).</p>

<p><strong>Challenges:</strong></p>
<ul>
  <li><strong>Latency:</strong> Must respond in &lt;50ms.</li>
  <li><strong>Cost:</strong> Running BERT-Base on every query costs millions/day.</li>
</ul>

<p><strong>Solution:</strong></p>

<p><strong>Step 1: Distillation</strong></p>
<ul>
  <li>BERT-Base (110M params) → DistilBERT (66M params).</li>
  <li><strong>Result:</strong> 40% smaller, 60% faster, 97% accuracy.</li>
</ul>

<p><strong>Step 2: Quantization</strong></p>
<ul>
  <li>DistilBERT FP32 → INT8.</li>
  <li><strong>Result:</strong> 4x smaller (66MB → 16MB), 2x faster.</li>
</ul>

<p><strong>Step 3: Pruning</strong></p>
<ul>
  <li>Structured pruning: Remove 30% of attention heads.</li>
  <li><strong>Result:</strong> 50MB → 35MB, 1.2x faster.</li>
</ul>

<p><strong>Step 4: Caching</strong></p>
<ul>
  <li>Cache embeddings for top 1M queries.</li>
  <li><strong>Hit Rate:</strong> 40% (reduces compute by 40%).</li>
</ul>

<p><strong>Final Result:</strong></p>
<ul>
  <li><strong>Latency:</strong> 15ms (vs 80ms for BERT-Base).</li>
  <li><strong>Cost:</strong> $100K/day (vs $1M/day).</li>
  <li><strong>Accuracy:</strong> 96% of BERT-Base.</li>
</ul>

<h2 id="15-advanced-technique-neural-architecture-search-for-compression">15. Advanced Technique: Neural Architecture Search for Compression</h2>

<p><strong>Problem:</strong> Manually designing compressed architectures is tedious.</p>

<p><strong>Solution:</strong> Use NAS to automatically find efficient architectures.</p>

<p><strong>Approaches:</strong></p>

<p><strong>1. Once-for-All (OFA) Networks:</strong></p>
<ul>
  <li>Train a single “super-network” that contains all possible sub-networks.</li>
  <li>At deployment, extract a sub-network that fits the target device.</li>
  <li><strong>Benefit:</strong> No need to retrain for each device.</li>
</ul>

<p><strong>2. ProxylessNAS:</strong></p>
<ul>
  <li>Search directly on the target hardware (e.g., mobile phone).</li>
  <li><strong>Objective:</strong> Maximize accuracy subject to latency &lt; 50ms.</li>
</ul>

<p><strong>3. EfficientNet:</strong></p>
<ul>
  <li>Use compound scaling: scale depth, width, and resolution together.</li>
  <li><strong>Result:</strong> EfficientNet-B0 achieves ResNet-50 accuracy with 10x fewer parameters.</li>
</ul>

<h2 id="16-compression-benchmarks">16. Compression Benchmarks</h2>

<p><strong>Model:</strong> ResNet-50 (25M params, 100MB FP32).</p>

<table>
  <thead>
    <tr>
      <th>Technique</th>
      <th>Size (MB)</th>
      <th>Accuracy (ImageNet)</th>
      <th>Speedup (CPU)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline (FP32)</td>
      <td>100</td>
      <td>76.1%</td>
      <td>1x</td>
    </tr>
    <tr>
      <td>INT8 Quantization</td>
      <td>25</td>
      <td>75.8%</td>
      <td>3x</td>
    </tr>
    <tr>
      <td>50% Pruning</td>
      <td>50</td>
      <td>75.5%</td>
      <td>1.5x</td>
    </tr>
    <tr>
      <td>Distillation (ResNet-18)</td>
      <td>45</td>
      <td>73.2%</td>
      <td>2x</td>
    </tr>
    <tr>
      <td>Pruning + Quantization</td>
      <td>12.5</td>
      <td>75.0%</td>
      <td>4x</td>
    </tr>
    <tr>
      <td>Distillation + Quantization</td>
      <td>11</td>
      <td>72.8%</td>
      <td>5x</td>
    </tr>
  </tbody>
</table>

<p><strong>Observation:</strong> Combining techniques yields the best compression ratio.</p>

<h2 id="17-deep-dive-weight-clustering">17. Deep Dive: Weight Clustering</h2>

<p><strong>Idea:</strong> Cluster weights into $K$ centroids (e.g., 256). Store only the centroid index (8 bits) instead of the full weight (32 bits).</p>

<p><strong>Algorithm (K-Means):</strong></p>
<ol>
  <li>Flatten all weights into a 1D array.</li>
  <li>Run K-Means clustering with $K = 256$.</li>
  <li>Replace each weight with its cluster centroid.</li>
  <li>Store: (1) Codebook (256 centroids), (2) Indices (8 bits per weight).</li>
</ol>

<p><strong>Compression Ratio:</strong></p>
<ul>
  <li>Original: 32 bits/weight.</li>
  <li>Compressed: 8 bits/weight + codebook overhead.</li>
  <li><strong>Result:</strong> ~4x compression.</li>
</ul>

<p><strong>Accuracy:</strong> Typically &lt;1% drop for $K = 256$.</p>

<h2 id="18-deep-dive-dynamic-neural-networks">18. Deep Dive: Dynamic Neural Networks</h2>

<p><strong>Idea:</strong> Adapt the model size based on input complexity.</p>

<p><strong>Techniques:</strong></p>

<p><strong>1. Early Exit:</strong></p>
<ul>
  <li>Add intermediate classifiers at different layers.</li>
  <li>For easy inputs, exit early (use fewer layers).</li>
  <li>For hard inputs, use the full network.</li>
  <li><strong>Example:</strong> BranchyNet, MSDNet.</li>
</ul>

<p><strong>2. Adaptive Computation Time (ACT):</strong></p>
<ul>
  <li>Each layer decides whether to continue processing or stop.</li>
  <li><strong>Benefit:</strong> Variable compute based on input.</li>
</ul>

<p><strong>3. Slimmable Networks:</strong></p>
<ul>
  <li>Train a single network that can run at different widths (e.g., 0.25x, 0.5x, 0.75x, 1x).</li>
  <li>At runtime, choose the width based on available resources.</li>
</ul>

<h2 id="19-production-deployment-model-serving">19. Production Deployment: Model Serving</h2>

<p><strong>Scenario:</strong> Serve a compressed model in production.</p>

<p><strong>Architecture:</strong></p>

<p><strong>1. Model Repository:</strong></p>
<ul>
  <li>Store compressed models in S3/GCS.</li>
  <li>Version control (v1, v2, v3).</li>
</ul>

<p><strong>2. Model Server:</strong></p>
<ul>
  <li><strong>TensorFlow Serving:</strong> Supports TFLite models.</li>
  <li><strong>TorchServe:</strong> Supports quantized PyTorch models.</li>
  <li><strong>ONNX Runtime:</strong> Cross-framework support.</li>
</ul>

<p><strong>3. Load Balancer:</strong></p>
<ul>
  <li>Distribute requests across multiple model servers.</li>
</ul>

<p><strong>4. Monitoring:</strong></p>
<ul>
  <li>Track latency (P50, P95, P99).</li>
  <li>Track accuracy (A/B test compressed vs full model).</li>
  <li>Track resource usage (CPU, memory).</li>
</ul>

<p><strong>5. Auto-Scaling:</strong></p>
<ul>
  <li>Scale up during peak hours.</li>
  <li>Scale down during off-peak to save cost.</li>
</ul>

<h2 id="20-cost-benefit-analysis">20. Cost-Benefit Analysis</h2>

<p><strong>Scenario:</strong> Deploying a compressed model for image classification (1M requests/day).</p>

<p><strong>Baseline (FP32 ResNet-50):</strong></p>
<ul>
  <li><strong>Latency:</strong> 100ms/request.</li>
  <li><strong>Compute:</strong> 1M requests × 100ms = 100K seconds/day = 28 hours/day.</li>
  <li><strong>Cost:</strong> 28 hours × $0.10/hour (AWS EC2 c5.xlarge) = $2.80/day = $1,022/year.</li>
</ul>

<p><strong>Compressed (INT8 ResNet-50):</strong></p>
<ul>
  <li><strong>Latency:</strong> 30ms/request (3x faster).</li>
  <li><strong>Compute:</strong> 1M requests × 30ms = 30K seconds/day = 8.3 hours/day.</li>
  <li><strong>Cost:</strong> 8.3 hours × $0.10/hour = $0.83/day = $303/year.</li>
</ul>

<p><strong>Savings:</strong> $719/year (70% cost reduction).</p>

<p><strong>Accuracy Trade-off:</strong> 76.1% → 75.8% (0.3% drop).</p>

<p><strong>ROI:</strong> If the accuracy drop is acceptable, compression is a no-brainer.</p>

<h2 id="21-ethical-considerations">21. Ethical Considerations</h2>

<p><strong>Bias Amplification:</strong></p>
<ul>
  <li>Compression can amplify biases in the training data.</li>
  <li><strong>Example:</strong> A pruned model might be less accurate on underrepresented groups.</li>
  <li><strong>Solution:</strong> Evaluate fairness metrics (e.g., demographic parity) after compression.</li>
</ul>

<p><strong>Environmental Impact:</strong></p>
<ul>
  <li>Training large models consumes energy (carbon footprint).</li>
  <li>Compression reduces inference energy, but the compression process itself (e.g., NAS) can be energy-intensive.</li>
  <li><strong>Solution:</strong> Use efficient compression methods (PTQ instead of NAS).</li>
</ul>

<h2 id="22-future-trends">22. Future Trends</h2>

<p><strong>1. Extreme Quantization:</strong></p>
<ul>
  <li><strong>Binary Neural Networks (BNNs):</strong> 1-bit weights and activations.</li>
  <li><strong>Ternary Neural Networks (TNNs):</strong> Weights in {-1, 0, 1}.</li>
  <li><strong>Challenge:</strong> Significant accuracy drop (5-10%).</li>
</ul>

<p><strong>2. Hardware-Software Co-Design:</strong></p>
<ul>
  <li>Design hardware specifically for compressed models.</li>
  <li><strong>Example:</strong> Google Edge TPU optimized for INT8.</li>
</ul>

<p><strong>3. On-Device Learning:</strong></p>
<ul>
  <li>Fine-tune compressed models on-device using user data.</li>
  <li><strong>Challenge:</strong> Privacy (Federated Learning).</li>
</ul>

<h2 id="23-conclusion">23. Conclusion</h2>

<p>Model compression is essential for deploying deep learning at scale. The key is to combine multiple techniques (quantization + pruning + distillation) to achieve the best compression ratio while maintaining acceptable accuracy.</p>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Quantization:</strong> 4x compression with minimal accuracy drop.</li>
  <li><strong>Pruning:</strong> 50-90% sparsity, but requires specialized hardware for speedup.</li>
  <li><strong>Distillation:</strong> 10x compression, but requires retraining.</li>
  <li><strong>Hardware Matters:</strong> Optimize for the target device (ARM, NVIDIA, TPU).</li>
  <li><strong>Production:</strong> Monitor latency, accuracy, and cost.</li>
</ul>

<p>The future of AI is edge AI. As models grow larger, compression will become even more critical. Mastering these techniques is a must for ML engineers.</p>

<h2 id="24-deep-dive-onnx-open-neural-network-exchange">24. Deep Dive: ONNX (Open Neural Network Exchange)</h2>

<p><strong>Problem:</strong> Models trained in PyTorch can’t run on TensorFlow Serving. Models trained in TensorFlow can’t run on ONNX Runtime.</p>

<p><strong>Solution:</strong> ONNX is a universal format for neural networks.</p>

<p><strong>Workflow:</strong></p>
<ol>
  <li><strong>Train</strong> in PyTorch/TensorFlow/Keras.</li>
  <li><strong>Export</strong> to ONNX format.</li>
  <li><strong>Optimize</strong> using ONNX Runtime.</li>
  <li><strong>Deploy</strong> on any platform (mobile, edge, cloud).</li>
</ol>

<p><strong>Example (PyTorch → ONNX):</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.onnx</span>

<span class="c1"># Load PyTorch model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">model.pth</span><span class="sh">'</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="c1"># Dummy input
</span><span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="c1"># Export to ONNX
</span><span class="n">torch</span><span class="p">.</span><span class="n">onnx</span><span class="p">.</span><span class="nf">export</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">dummy_input</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">model.onnx</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">export_params</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">opset_version</span><span class="o">=</span><span class="mi">13</span><span class="p">,</span>
    <span class="n">do_constant_folding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">input</span><span class="sh">'</span><span class="p">],</span>
    <span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">],</span>
    <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">input</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="sh">'</span><span class="s">batch_size</span><span class="sh">'</span><span class="p">},</span> <span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="sh">'</span><span class="s">batch_size</span><span class="sh">'</span><span class="p">}}</span>
<span class="p">)</span>
</code></pre></div></div>

<p><strong>ONNX Runtime Optimizations:</strong></p>
<ul>
  <li><strong>Graph Optimization:</strong> Fuse operators, eliminate redundant nodes.</li>
  <li><strong>Quantization:</strong> INT8 quantization.</li>
  <li><strong>Execution Providers:</strong> CPU, CUDA, TensorRT, DirectML, CoreML.</li>
</ul>

<h2 id="25-mobile-deployment-ios-core-ml">25. Mobile Deployment: iOS (Core ML)</h2>

<p><strong>Core ML</strong> is Apple’s framework for on-device ML.</p>

<p><strong>Workflow:</strong></p>
<ol>
  <li>Train model in PyTorch/TensorFlow.</li>
  <li>Convert to Core ML format (<code class="language-plaintext highlighter-rouge">.mlmodel</code>).</li>
  <li>Integrate into iOS app.</li>
</ol>

<p><strong>Conversion (PyTorch → Core ML):</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">coremltools</span> <span class="k">as</span> <span class="n">ct</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="c1"># Load PyTorch model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">model.pth</span><span class="sh">'</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="c1"># Trace the model
</span><span class="n">example_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="nf">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">example_input</span><span class="p">)</span>

<span class="c1"># Convert to Core ML
</span><span class="n">mlmodel</span> <span class="o">=</span> <span class="n">ct</span><span class="p">.</span><span class="nf">convert</span><span class="p">(</span>
    <span class="n">traced_model</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ct</span><span class="p">.</span><span class="nc">ImageType</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))],</span>
    <span class="n">convert_to</span><span class="o">=</span><span class="sh">"</span><span class="s">neuralnetwork</span><span class="sh">"</span>  <span class="c1"># or "mlprogram" for newer format
</span><span class="p">)</span>

<span class="c1"># Save
</span><span class="n">mlmodel</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sh">"</span><span class="s">model.mlmodel</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Optimization:</strong></p>
<ul>
  <li><strong>Quantization:</strong> Use <code class="language-plaintext highlighter-rouge">ct.compression.quantize_weights()</code>.</li>
  <li><strong>Pruning:</strong> Use <code class="language-plaintext highlighter-rouge">ct.compression.prune_weights()</code>.</li>
  <li><strong>Neural Engine:</strong> Optimize for Apple’s ANE (Neural Engine).</li>
</ul>

<h2 id="26-mobile-deployment-android-tensorflow-lite">26. Mobile Deployment: Android (TensorFlow Lite)</h2>

<p><strong>TensorFlow Lite</strong> is Google’s framework for mobile/edge ML.</p>

<p><strong>Workflow:</strong></p>
<ol>
  <li>Train model in TensorFlow/Keras.</li>
  <li>Convert to TFLite format (<code class="language-plaintext highlighter-rouge">.tflite</code>).</li>
  <li>Integrate into Android app.</li>
</ol>

<p><strong>Conversion:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># Load Keras model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span><span class="sh">'</span><span class="s">model.h5</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Convert to TFLite
</span><span class="n">converter</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">lite</span><span class="p">.</span><span class="n">TFLiteConverter</span><span class="p">.</span><span class="nf">from_keras_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">converter</span><span class="p">.</span><span class="n">optimizations</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">lite</span><span class="p">.</span><span class="n">Optimize</span><span class="p">.</span><span class="n">DEFAULT</span><span class="p">]</span>
<span class="n">tflite_model</span> <span class="o">=</span> <span class="n">converter</span><span class="p">.</span><span class="nf">convert</span><span class="p">()</span>

<span class="c1"># Save
</span><span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="sh">'</span><span class="s">model.tflite</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">wb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">tflite_model</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Optimization:</strong></p>
<ul>
  <li><strong>GPU Delegate:</strong> Use GPU for inference.</li>
  <li><strong>NNAPI Delegate:</strong> Use Android’s Neural Networks API.</li>
  <li><strong>Hexagon Delegate:</strong> Use Qualcomm’s DSP.</li>
</ul>

<h2 id="27-advanced-technique-sparse-tensor-cores-nvidia">27. Advanced Technique: Sparse Tensor Cores (NVIDIA)</h2>

<p><strong>NVIDIA Ampere</strong> (A100, RTX 3090) introduced <strong>Sparse Tensor Cores</strong> that accelerate 2:4 structured sparsity.</p>

<p><strong>2:4 Sparsity:</strong> In every 4 consecutive weights, at least 2 must be zero.</p>

<p><strong>Example:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original: [0.5, 0.3, 0.1, 0.2]
2:4 Sparse: [0.5, 0.3, 0.0, 0.0]  ✓ (2 zeros out of 4)
</code></pre></div></div>

<p><strong>Speedup:</strong> 2x faster than dense operations on Sparse Tensor Cores.</p>

<p><strong>Training with 2:4 Sparsity:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch.ao.pruning</span> <span class="kn">import</span> <span class="n">prune_to_structured_sparsity</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">MyModel</span><span class="p">()</span>
<span class="nf">prune_to_structured_sparsity</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sparsity_pattern</span><span class="o">=</span><span class="sh">"</span><span class="s">2:4</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Train normally
</span></code></pre></div></div>

<h2 id="28-deep-dive-huffman-coding-for-weight-compression">28. Deep Dive: Huffman Coding for Weight Compression</h2>

<p><strong>Idea:</strong> Use variable-length encoding for weights. Frequent weights get shorter codes.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Cluster</strong> weights into $K$ centroids (e.g., 256).</li>
  <li><strong>Count</strong> frequency of each centroid.</li>
  <li><strong>Build Huffman Tree:</strong> Assign shorter codes to frequent centroids.</li>
  <li><strong>Encode:</strong> Replace each weight with its Huffman code.</li>
</ol>

<p><strong>Compression Ratio:</strong></p>
<ul>
  <li>Fixed-length (8 bits): 8 bits/weight.</li>
  <li>Huffman: ~5-6 bits/weight (depending on distribution).</li>
</ul>

<p><strong>Trade-off:</strong> Decoding overhead during inference.</p>

<h2 id="29-production-case-study-mobilenet-deployment">29. Production Case Study: MobileNet Deployment</h2>

<p><strong>Scenario:</strong> Deploy MobileNetV2 for image classification on a smartphone.</p>

<p><strong>Baseline (FP32):</strong></p>
<ul>
  <li><strong>Size:</strong> 14MB.</li>
  <li><strong>Latency:</strong> 80ms (CPU).</li>
  <li><strong>Accuracy:</strong> 72% (ImageNet).</li>
</ul>

<p><strong>Optimization Pipeline:</strong></p>

<p><strong>Step 1: Quantization (INT8)</strong></p>
<ul>
  <li><strong>Size:</strong> 3.5MB (4x reduction).</li>
  <li><strong>Latency:</strong> 25ms (3x speedup).</li>
  <li><strong>Accuracy:</strong> 71.5% (0.5% drop).</li>
</ul>

<p><strong>Step 2: Pruning (50%)</strong></p>
<ul>
  <li><strong>Size:</strong> 1.75MB (2x reduction).</li>
  <li><strong>Latency:</strong> 15ms (1.6x speedup).</li>
  <li><strong>Accuracy:</strong> 70.8% (0.7% drop).</li>
</ul>

<p><strong>Step 3: Knowledge Distillation (MobileNetV2 → MobileNetV3-Small)</strong></p>
<ul>
  <li><strong>Size:</strong> 1.2MB.</li>
  <li><strong>Latency:</strong> 10ms.</li>
  <li><strong>Accuracy:</strong> 68% (4% drop from baseline).</li>
</ul>

<p><strong>Final Result:</strong></p>
<ul>
  <li><strong>Size:</strong> 1.2MB (12x compression).</li>
  <li><strong>Latency:</strong> 10ms (8x speedup).</li>
  <li><strong>Accuracy:</strong> 68% (acceptable for mobile use case).</li>
</ul>

<h2 id="30-advanced-technique-neural-ode-compression">30. Advanced Technique: Neural ODE Compression</h2>

<p><strong>Neural ODEs</strong> (Ordinary Differential Equations) model continuous transformations.</p>

<p><strong>Compression:</strong></p>
<ul>
  <li>Instead of storing weights for 100 layers, store the ODE parameters.</li>
  <li><strong>Benefit:</strong> Constant memory regardless of depth.</li>
  <li><strong>Trade-off:</strong> Slower inference (need to solve ODE).</li>
</ul>

<p><strong>Use Case:</strong> Compressing very deep networks (ResNet-1000).</p>

<h2 id="31-monitoring-compressed-models-in-production">31. Monitoring Compressed Models in Production</h2>

<p><strong>Metrics to Track:</strong></p>
<ol>
  <li><strong>Latency Distribution:</strong> P50, P95, P99. Alert if P95 &gt; SLA.</li>
  <li><strong>Accuracy Drift:</strong> Compare predictions with a “shadow” full-precision model.</li>
  <li><strong>Resource Usage:</strong> CPU, memory, battery drain (for mobile).</li>
  <li><strong>Error Analysis:</strong> Which classes have the highest error rate after compression?</li>
</ol>

<p><strong>A/B Testing:</strong></p>
<ul>
  <li><strong>Control:</strong> Full-precision model (10% of traffic).</li>
  <li><strong>Treatment:</strong> Compressed model (90% of traffic).</li>
  <li><strong>Metrics:</strong> Latency, accuracy, user engagement.</li>
</ul>

<p><strong>Rollback Strategy:</strong></p>
<ul>
  <li>If compressed model’s accuracy drops &gt; 2%, automatically rollback to full-precision.</li>
</ul>

<h2 id="32-interview-deep-dive-compression-trade-offs">32. Interview Deep Dive: Compression Trade-offs</h2>

<p><strong>Q: When would you use quantization vs pruning vs distillation?</strong></p>

<p><strong>A:</strong></p>
<ul>
  <li><strong>Quantization:</strong> When you need fast inference on CPUs/mobile. Works well for CNNs. Minimal accuracy drop.</li>
  <li><strong>Pruning:</strong> When you have specialized hardware (Sparse Tensor Cores) or can tolerate irregular sparsity. Best for very large models.</li>
  <li><strong>Distillation:</strong> When you can afford to retrain. Best for transferring knowledge from an ensemble to a single model. Works well for NLP.</li>
</ul>

<p><strong>Q: How do you choose the quantization precision (INT8 vs INT4)?</strong></p>

<p><strong>A:</strong></p>
<ul>
  <li><strong>INT8:</strong> Standard. 4x compression, &lt;1% accuracy drop for most models.</li>
  <li><strong>INT4:</strong> Aggressive. 8x compression, but 2-5% accuracy drop. Use only if latency is critical and accuracy drop is acceptable.</li>
  <li><strong>Mixed Precision:</strong> Use INT4 for less sensitive layers, INT8 for sensitive layers.</li>
</ul>

<h2 id="33-future-research-directions">33. Future Research Directions</h2>

<p><strong>1. Learned Compression:</strong></p>
<ul>
  <li>Use a neural network to learn the optimal compression strategy for each layer.</li>
  <li><strong>Example:</strong> AutoML for compression.</li>
</ul>

<p><strong>2. Post-Training Sparsification:</strong></p>
<ul>
  <li>Prune without retraining (like PTQ for quantization).</li>
  <li><strong>Challenge:</strong> Maintaining accuracy.</li>
</ul>

<p><strong>3. Hardware-Aware Compression:</strong></p>
<ul>
  <li>Compress specifically for the target hardware (e.g., iPhone 15 Pro’s A17 chip).</li>
  <li><strong>Benefit:</strong> Maximize performance on that specific device.</li>
</ul>

<h2 id="34-conclusion--best-practices">34. Conclusion &amp; Best Practices</h2>

<p><strong>Best Practices:</strong></p>
<ol>
  <li><strong>Start with Quantization:</strong> Easiest, fastest, minimal accuracy drop.</li>
  <li><strong>Combine Techniques:</strong> Quantization + Pruning + Distillation for maximum compression.</li>
  <li><strong>Profile First:</strong> Measure latency bottlenecks before optimizing.</li>
  <li><strong>Test on Target Device:</strong> Don’t rely on desktop benchmarks.</li>
  <li><strong>Monitor in Production:</strong> Track accuracy drift and latency.</li>
</ol>

<p><strong>Compression Checklist:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Benchmark baseline model (size, latency, accuracy)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Apply INT8 quantization (PTQ or QAT)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Measure accuracy drop (&lt;2% acceptable)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Apply structured pruning (30-50%)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Fine-tune pruned model</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Consider distillation if retraining is feasible</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Convert to target format (ONNX, TFLite, Core ML)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Test on target hardware</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Deploy with monitoring</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Set up A/B test</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Monitor and iterate</li>
</ul>

<p>The art of model compression is balancing the three-way trade-off: <strong>Size, Speed, Accuracy</strong>. There’s no one-size-fits-all solution. The best approach depends on your use case, hardware, and constraints. Mastering these techniques will make you indispensable in the era of edge AI.</p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#distillation" class="page__taxonomy-item p-category" rel="tag">distillation</a><span class="sep">, </span>
    
      <a href="/tags/#edge-ai" class="page__taxonomy-item p-category" rel="tag">edge-ai</a><span class="sep">, </span>
    
      <a href="/tags/#optimization" class="page__taxonomy-item p-category" rel="tag">optimization</a><span class="sep">, </span>
    
      <a href="/tags/#pruning" class="page__taxonomy-item p-category" rel="tag">pruning</a><span class="sep">, </span>
    
      <a href="/tags/#quantization" class="page__taxonomy-item p-category" rel="tag">quantization</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Model+Compression+Techniques%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0041-model-compression%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0041-model-compression%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0041-model-compression/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0040-distributed-training/" class="pagination--pager" title="Distributed Training Patterns">Previous</a>
    
    
      <a href="/ml-system-design/0042-feature-stores/" class="pagination--pager" title="Feature Stores">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
