<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Model Architecture Design - Arun Baby</title>
<meta name="description" content="Architecture is destiny. The difference between 50% accuracy and 90% accuracy is often just a skip connection.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Model Architecture Design">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0027-model-architecture-design/">


  <meta property="og:description" content="Architecture is destiny. The difference between 50% accuracy and 90% accuracy is often just a skip connection.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Model Architecture Design">
  <meta name="twitter:description" content="Architecture is destiny. The difference between 50% accuracy and 90% accuracy is often just a skip connection.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0027-model-architecture-design/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T15:12:33+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0027-model-architecture-design/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Model Architecture Design">
    <meta itemprop="description" content="Architecture is destiny. The difference between 50% accuracy and 90% accuracy is often just a skip connection.">
    <meta itemprop="datePublished" content="2025-12-29T15:12:33+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0027-model-architecture-design/" itemprop="url">Model Architecture Design
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem-statement">Problem Statement</a></li><li><a href="#understanding-the-requirements">Understanding the Requirements</a><ul><li><a href="#the-inductive-bias">The “Inductive Bias”</a></li></ul></li><li><a href="#high-level-architecture-the-building-blocks">High-Level Architecture: The Building Blocks</a><ul><li><a href="#1-the-stem">1. The Stem</a></li><li><a href="#2-the-backbone-the-body">2. The Backbone (The Body)</a></li><li><a href="#3-the-head">3. The Head</a></li></ul></li><li><a href="#the-evolution-of-architectures-a-historical-perspective">The Evolution of Architectures: A Historical Perspective</a><ul><li><a href="#1-the-dark-ages-pre-2012">1. The Dark Ages (Pre-2012)</a></li><li><a href="#2-the-alexnet-revolution-2012">2. The AlexNet Revolution (2012)</a></li><li><a href="#3-the-vgg-era-2014">3. The VGG Era (2014)</a></li><li><a href="#4-the-resnet-breakthrough-2015">4. The ResNet Breakthrough (2015)</a></li><li><a href="#5-the-transformer-invasion-2017-present">5. The Transformer Invasion (2017-Present)</a></li></ul></li><li><a href="#deep-dive-normalization-layers">Deep Dive: Normalization Layers</a><ul><li><a href="#1-batch-normalization-bn">1. Batch Normalization (BN)</a></li><li><a href="#2-layer-normalization-ln">2. Layer Normalization (LN)</a></li><li><a href="#3-group-normalization-gn">3. Group Normalization (GN)</a></li><li><a href="#4-instance-normalization-in">4. Instance Normalization (IN)</a></li></ul></li><li><a href="#deep-dive-activation-functions">Deep Dive: Activation Functions</a><ul><li><a href="#1-sigmoid--tanh">1. Sigmoid / Tanh</a></li><li><a href="#2-relu-rectified-linear-unit">2. ReLU (Rectified Linear Unit)</a></li><li><a href="#3-leaky-relu--prelu">3. Leaky ReLU / PReLU</a></li><li><a href="#4-gelu-gaussian-error-linear-unit">4. GeLU (Gaussian Error Linear Unit)</a></li><li><a href="#5-swish--silu">5. Swish / SiLU</a></li></ul></li><li><a href="#component-deep-dives">Component Deep-Dives</a><ul><li><a href="#1-convolutions-the-workhorse-of-vision">1. Convolutions: The Workhorse of Vision</a></li><li><a href="#2-attention-the-global-context">2. Attention: The Global Context</a></li><li><a href="#3-normalization-the-stabilizer">3. Normalization: The Stabilizer</a></li><li><a href="#4-activations-the-non-linearity">4. Activations: The Non-Linearity</a></li></ul></li><li><a href="#deep-dive-vision-transformers-vit">Deep Dive: Vision Transformers (ViT)</a><ul><li><a href="#1-patch-embedding">1. Patch Embedding</a></li><li><a href="#2-the-cls-token">2. The CLS Token</a></li><li><a href="#3-positional-embeddings">3. Positional Embeddings</a></li><li><a href="#4-inductive-bias-vs-data">4. Inductive Bias vs. Data</a></li></ul></li><li><a href="#deep-dive-mobilenet-and-efficient-architecture">Deep Dive: MobileNet and Efficient Architecture</a><ul><li><a href="#1-depthwise-separable-convolutions">1. Depthwise Separable Convolutions</a></li><li><a href="#2-inverted-residuals-mobilenetv2">2. Inverted Residuals (MobileNetV2)</a></li><li><a href="#3-squeeze-and-excitation-se">3. Squeeze-and-Excitation (SE)</a></li></ul></li><li><a href="#deep-dive-neural-architecture-search-nas">Deep Dive: Neural Architecture Search (NAS)</a><ul><li><a href="#1-reinforcement-learning-nasnet">1. Reinforcement Learning (NASNet)</a></li><li><a href="#2-evolutionary-algorithms-amoebanet">2. Evolutionary Algorithms (AmoebaNet)</a></li><li><a href="#3-differentiable-nas-darts">3. Differentiable NAS (DARTS)</a></li></ul></li><li><a href="#design-patterns-in-architecture">Design Patterns in Architecture</a><ul><li><a href="#1-residual-connections-skip-connections">1. Residual Connections (Skip Connections)</a></li><li><a href="#2-the-bottleneck-design">2. The Bottleneck Design</a></li><li><a href="#3-squeeze-and-excitation-se-1">3. Squeeze-and-Excitation (SE)</a></li></ul></li><li><a href="#implementation-building-a-modern-resnet-block">Implementation: Building a Modern ResNet Block</a></li><li><a href="#scaling-laws-the-physics-of-deep-learning">Scaling Laws: The Physics of Deep Learning</a></li><li><a href="#neural-architecture-search-nas">Neural Architecture Search (NAS)</a></li><li><a href="#case-study-efficientnet-compound-scaling">Case Study: EfficientNet (Compound Scaling)</a></li><li><a href="#deep-dive-distributed-training-strategies">Deep Dive: Distributed Training Strategies</a><ul><li><a href="#1-data-parallelism-ddp">1. Data Parallelism (DDP)</a></li><li><a href="#2-model-parallelism-tensor-parallelism">2. Model Parallelism (Tensor Parallelism)</a></li><li><a href="#3-pipeline-parallelism">3. Pipeline Parallelism</a></li><li><a href="#4-zero-zero-redundancy-optimizer">4. ZeRO (Zero Redundancy Optimizer)</a></li></ul></li><li><a href="#failure-modes-in-architecture-design">Failure Modes in Architecture Design</a></li><li><a href="#cost-analysis-flops-vs-latency">Cost Analysis: FLOPs vs. Latency</a></li><li><a href="#deep-dive-training-loop-implementation">Deep Dive: Training Loop Implementation</a></li><li><a href="#top-interview-questions">Top Interview Questions</a></li><li><a href="#deep-dive-regularization-techniques">Deep Dive: Regularization Techniques</a><ul><li><a href="#1-dropout-hinton-et-al-2012">1. Dropout (Hinton et al., 2012)</a></li><li><a href="#2-dropconnect-wan-et-al-2013">2. DropConnect (Wan et al., 2013)</a></li><li><a href="#3-stochastic-depth-huang-et-al-2016">3. Stochastic Depth (Huang et al., 2016)</a></li><li><a href="#4-label-smoothing">4. Label Smoothing</a></li></ul></li><li><a href="#deep-dive-optimizers-sgd-vs-adam">Deep Dive: Optimizers (SGD vs. Adam)</a><ul><li><a href="#1-sgd-with-momentum">1. SGD with Momentum</a></li><li><a href="#2-adam-adaptive-moment-estimation">2. Adam (Adaptive Moment Estimation)</a></li><li><a href="#3-adamw-adam-with-weight-decay">3. AdamW (Adam with Weight Decay)</a></li></ul></li><li><a href="#deep-dive-hardware-efficiency-the-memory-wall">Deep Dive: Hardware Efficiency (The Memory Wall)</a></li><li><a href="#further-reading">Further Reading</a></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Architecture is destiny. The difference between 50% accuracy and 90% accuracy is often just a skip connection.</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p>You are given a dataset and a task (e.g., “Classify these 1 million images” or “Translate this text”).
<strong>How do you design the Neural Network architecture?</strong>
Do you use a CNN? A Transformer? How deep? How wide? Which activation function? Which normalization?</p>

<p>This post explores the <strong>First Principles of Model Architecture Design</strong>. We aren’t just using <code class="language-plaintext highlighter-rouge">resnet50(pretrained=True)</code>. We are learning how to build <code class="language-plaintext highlighter-rouge">resnet50</code> from scratch, and why it was built that way.</p>

<h2 id="understanding-the-requirements">Understanding the Requirements</h2>

<p>Designing a model is an engineering trade-off between three variables:</p>
<ol>
  <li><strong>Capacity (Accuracy):</strong> Can the model learn the complex patterns in the data?</li>
  <li><strong>Compute (FLOPs/Latency):</strong> Can it run fast enough on the target hardware?</li>
  <li><strong>Memory (Parameters):</strong> Does it fit in VRAM?</li>
</ol>

<h3 id="the-inductive-bias">The “Inductive Bias”</h3>
<p>Every architecture makes assumptions about the data:</p>
<ul>
  <li><strong>Fully Connected (MLP):</strong> No assumptions. Every input relates to every other input. (Data inefficient).</li>
  <li><strong>CNN:</strong> Assumes <strong>Locality</strong> (pixels nearby matter) and <strong>Translation Invariance</strong> (a cat is a cat whether it’s in the top-left or bottom-right).</li>
  <li><strong>RNN:</strong> Assumes <strong>Sequentiality</strong> (time matters).</li>
  <li><strong>Transformer:</strong> Assumes <strong>Pairwise Relationships</strong> (Attention) matter most, regardless of distance.</li>
</ul>

<h2 id="high-level-architecture-the-building-blocks">High-Level Architecture: The Building Blocks</h2>

<p>A modern Deep Learning model is like a Lego castle built from a few fundamental blocks.</p>

<pre><code class="language-ascii">+---------------------------------------------------------------+
|                        The Model Head                         |
|             (Classifier / Regressor / Decoder)                |
+---------------------------------------------------------------+
                                ^
                                |
+---------------------------------------------------------------+
|                       The Backbone                            |
|               (Feature Extractor / Encoder)                   |
|                                                               |
|  +-------+    +-------+    +-------+    +-------+             |
|  | Block | -&gt; | Block | -&gt; | Block | -&gt; | Block |             |
|  +-------+    +-------+    +-------+    +-------+             |
+---------------------------------------------------------------+
                                ^
                                |
+---------------------------------------------------------------+
|                        The Stem                               |
|              (Initial Convolution / Embedding)                |
+---------------------------------------------------------------+
</code></pre>

<h3 id="1-the-stem">1. The Stem</h3>
<p>The entry point. It transforms raw data (pixels, text tokens) into the embedding space.</p>
<ul>
  <li><strong>Images:</strong> Usually a <code class="language-plaintext highlighter-rouge">7x7 Conv, stride 2</code> to reduce resolution early (ResNet).</li>
  <li><strong>Text:</strong> A Lookup Table (<code class="language-plaintext highlighter-rouge">nn.Embedding</code>) + Positional Encodings.</li>
</ul>

<h3 id="2-the-backbone-the-body">2. The Backbone (The Body)</h3>
<p>This is 90% of the compute. It consists of repeated <strong>Blocks</strong>.</p>
<ul>
  <li><strong>ResNet Block:</strong> <code class="language-plaintext highlighter-rouge">Conv -&gt; BN -&gt; ReLU -&gt; Conv -&gt; BN -&gt; Add Input</code>.</li>
  <li><strong>Transformer Block:</strong> <code class="language-plaintext highlighter-rouge">LayerNorm -&gt; Attention -&gt; Add -&gt; LayerNorm -&gt; MLP -&gt; Add</code>.</li>
</ul>

<h3 id="3-the-head">3. The Head</h3>
<p>The task-specific output.</p>
<ul>
  <li><strong>Classification:</strong> <code class="language-plaintext highlighter-rouge">GlobalAveragePooling -&gt; Linear -&gt; Softmax</code>.</li>
  <li><strong>Detection:</strong> <code class="language-plaintext highlighter-rouge">Conv</code> layers predicting Bounding Boxes.</li>
  <li><strong>Segmentation:</strong> Upsampling layers to restore resolution.</li>
</ul>

<h2 id="the-evolution-of-architectures-a-historical-perspective">The Evolution of Architectures: A Historical Perspective</h2>

<p>To understand <em>why</em> we use ResNets and Transformers today, we must understand the failures of the past.</p>

<h3 id="1-the-dark-ages-pre-2012">1. The Dark Ages (Pre-2012)</h3>
<p>Neural Networks were “Multi-Layer Perceptrons” (MLPs).</p>
<ul>
  <li><strong>Structure:</strong> Dense Matrix Multiplications.</li>
  <li><strong>Problem:</strong> No translation invariance. A cat in the top-left corner required different weights than a cat in the bottom-right.</li>
  <li><strong>Result:</strong> Couldn’t scale to images larger than 28x28 (MNIST).</li>
</ul>

<h3 id="2-the-alexnet-revolution-2012">2. The AlexNet Revolution (2012)</h3>
<p>Alex Krizhevsky used GPUs to train a deep CNN.</p>
<ul>
  <li><strong>Key Innovation:</strong> ReLU (instead of Sigmoid) to fix vanishing gradients. Dropout to fix overfitting.</li>
  <li><strong>Architecture:</strong> 5 Conv layers, 3 Dense layers.</li>
  <li><strong>Impact:</strong> Error rate on ImageNet dropped from 26% to 15%.</li>
</ul>

<h3 id="3-the-vgg-era-2014">3. The VGG Era (2014)</h3>
<p>“Simplicity is the ultimate sophistication.”</p>
<ul>
  <li><strong>Idea:</strong> Replace large kernels (11x11, 5x5) with stacks of 3x3 kernels.</li>
  <li><strong>Why?</strong> Two 3x3 layers have the same receptive field as one 5x5 layer but fewer parameters and more non-linearity.</li>
  <li><strong>Legacy:</strong> The “VGG Backbone” is still used in Transfer Learning.</li>
</ul>

<h3 id="4-the-resnet-breakthrough-2015">4. The ResNet Breakthrough (2015)</h3>
<p>“Deep networks are harder to train.”</p>
<ul>
  <li><strong>Problem:</strong> Adding layers made performance <em>worse</em> due to optimization difficulties (not overfitting).</li>
  <li><strong>Solution:</strong> Residual Connections (<code class="language-plaintext highlighter-rouge">x + F(x)</code>).</li>
  <li><strong>Result:</strong> We could train 100+ layer networks.</li>
</ul>

<h3 id="5-the-transformer-invasion-2017-present">5. The Transformer Invasion (2017-Present)</h3>
<p>“Attention is All You Need.”</p>
<ul>
  <li><strong>Shift:</strong> Inductive bias of “Locality” (CNNs) was replaced by “Global Correlation” (Attention).</li>
  <li><strong>Vision Transformers (ViT):</strong> Treat an image as a sequence of 16x16 patches.</li>
  <li><strong>Dominance:</strong> Transformers now rule NLP (GPT), Vision (ViT), and Speech (Conformer).</li>
</ul>

<h2 id="deep-dive-normalization-layers">Deep Dive: Normalization Layers</h2>

<p>Normalization is the unsung hero of Deep Learning. It smooths the loss landscape, allowing larger learning rates.</p>

<h3 id="1-batch-normalization-bn">1. Batch Normalization (BN)</h3>
<p>[ \hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \cdot \gamma + \beta ]</p>
<ul>
  <li><strong>Mechanism:</strong> Compute mean/var across the <strong>Batch (N)</strong> and <strong>Spatial (H, W)</strong> dimensions.</li>
  <li><strong>Training:</strong> Uses current batch stats.</li>
  <li><strong>Inference:</strong> Uses running average stats.</li>
  <li><strong>Pros:</strong> Fuses into Convolution (free at inference).</li>
  <li><strong>Cons:</strong>
    <ul>
      <li>Requires large batch size (&gt;32).</li>
      <li>Fails in RNNs (sequence length varies).</li>
      <li>Training/Inference discrepancy causes bugs.</li>
    </ul>
  </li>
</ul>

<h3 id="2-layer-normalization-ln">2. Layer Normalization (LN)</h3>
<ul>
  <li><strong>Mechanism:</strong> Compute mean/var across the <strong>Channel (C)</strong> dimension for a <em>single sample</em>.</li>
  <li><strong>Pros:</strong> Independent of batch size. Works great for RNNs/Transformers.</li>
  <li><strong>Cons:</strong> Cannot be fused. Slower inference.</li>
</ul>

<h3 id="3-group-normalization-gn">3. Group Normalization (GN)</h3>
<ul>
  <li><strong>Mechanism:</strong> Split channels into (G) groups. Normalize within each group.</li>
  <li><strong>Use Case:</strong> Object Detection (where batch size is small, e.g., 1 or 2).</li>
  <li><strong>Performance:</strong> Better than BN at small batch sizes, worse at large batch sizes.</li>
</ul>

<h3 id="4-instance-normalization-in">4. Instance Normalization (IN)</h3>
<ul>
  <li><strong>Mechanism:</strong> Normalize each channel independently.</li>
  <li><strong>Use Case:</strong> Style Transfer. It removes “contrast” information (style) while keeping content.</li>
</ul>

<h2 id="deep-dive-activation-functions">Deep Dive: Activation Functions</h2>

<p>The non-linearity is what gives NNs their power.</p>

<h3 id="1-sigmoid--tanh">1. Sigmoid / Tanh</h3>
<ul>
  <li><strong>Formula:</strong> (\sigma(x) = \frac{1}{1+e^{-x}})</li>
  <li><strong>Problem:</strong> <strong>Vanishing Gradient.</strong> For large (x), the gradient is 0. The network stops learning.</li>
  <li><strong>Status:</strong> Deprecated for hidden layers. Used only for output (Probability).</li>
</ul>

<h3 id="2-relu-rectified-linear-unit">2. ReLU (Rectified Linear Unit)</h3>
<ul>
  <li><strong>Formula:</strong> (\max(0, x))</li>
  <li><strong>Pros:</strong> Computationally free. No vanishing gradient for (x &gt; 0).</li>
  <li><strong>Cons:</strong> <strong>Dead ReLU.</strong> If (x &lt; 0) always, the neuron dies and never recovers.</li>
</ul>

<h3 id="3-leaky-relu--prelu">3. Leaky ReLU / PReLU</h3>
<ul>
  <li><strong>Formula:</strong> (\max(\alpha x, x)) where (\alpha \approx 0.01).</li>
  <li><strong>Fix:</strong> Allows a small gradient to flow when (x &lt; 0), reviving dead neurons.</li>
</ul>

<h3 id="4-gelu-gaussian-error-linear-unit">4. GeLU (Gaussian Error Linear Unit)</h3>
<ul>
  <li><strong>Formula:</strong> (x \cdot \Phi(x)) (approx (x \cdot \sigma(1.702x))).</li>
  <li><strong>Intuition:</strong> A smooth version of ReLU.</li>
  <li><strong>Why?</strong> The smoothness helps optimization in very deep Transformers (BERT, GPT).</li>
</ul>

<h3 id="5-swish--silu">5. Swish / SiLU</h3>
<ul>
  <li><strong>Formula:</strong> (x \cdot \sigma(x)).</li>
  <li><strong>Origin:</strong> Discovered by Google using Neural Architecture Search.</li>
  <li><strong>Properties:</strong> Non-monotonic. It dips slightly below 0 for negative values. This “self-gating” property helps information flow.</li>
</ul>

<h2 id="component-deep-dives">Component Deep-Dives</h2>

<h3 id="1-convolutions-the-workhorse-of-vision">1. Convolutions: The Workhorse of Vision</h3>
<p>Standard Convolutions are expensive: (O(K^2 \cdot C_{in} \cdot C_{out} \cdot H \cdot W)).</p>

<p><strong>Optimizations:</strong></p>
<ul>
  <li><strong>Depthwise Separable Conv (MobileNet):</strong>
    <ol>
      <li><strong>Depthwise:</strong> Spatial convolution per channel.</li>
      <li><strong>Pointwise:</strong> 1x1 convolution to mix channels.
        <ul>
          <li>Reduces parameters by ~9x.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li><strong>Dilated Conv (Atrous):</strong> Increases Receptive Field without reducing resolution. Great for Segmentation.</li>
</ul>

<h3 id="2-attention-the-global-context">2. Attention: The Global Context</h3>
<p>Self-Attention calculates the relationship between every pair of tokens.
[ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V ]</p>
<ul>
  <li><strong>Pros:</strong> Infinite Receptive Field.</li>
  <li><strong>Cons:</strong> (O(N^2)) complexity. Hard to scale to long sequences.</li>
</ul>

<h3 id="3-normalization-the-stabilizer">3. Normalization: The Stabilizer</h3>
<p>Normalization ensures the activations have mean 0 and variance 1. This prevents exploding/vanishing gradients.</p>
<ul>
  <li><strong>Batch Norm (BN):</strong> Normalize across the batch dimension.
    <ul>
      <li><em>Pros:</em> Fuses into Conv during inference (free!).</li>
      <li><em>Cons:</em> Fails with small batch sizes.</li>
    </ul>
  </li>
  <li><strong>Layer Norm (LN):</strong> Normalize across the channel dimension.
    <ul>
      <li><em>Pros:</em> Batch size independent. Standard for Transformers/RNNs.</li>
    </ul>
  </li>
  <li><strong>RMSNorm (Root Mean Square Norm):</strong> Like LN but skips mean subtraction. Faster. Used in LLaMA.</li>
</ul>

<h3 id="4-activations-the-non-linearity">4. Activations: The Non-Linearity</h3>
<ul>
  <li><strong>ReLU:</strong> <code class="language-plaintext highlighter-rouge">max(0, x)</code>. The classic. Fast. Dead ReLU problem.</li>
  <li><strong>LeakyReLU:</strong> <code class="language-plaintext highlighter-rouge">max(0.01x, x)</code>. Fixes dead ReLU.</li>
  <li><strong>GeLU (Gaussian Error Linear Unit):</strong> Smooth approximation of ReLU. Standard in BERT/GPT.</li>
  <li><strong>Swish (SiLU):</strong> <code class="language-plaintext highlighter-rouge">x * sigmoid(x)</code>. Discovered by NAS. Used in EfficientNet/LLaMA.</li>
</ul>

<h2 id="deep-dive-vision-transformers-vit">Deep Dive: Vision Transformers (ViT)</h2>

<p>The Transformer changed everything. But how do you feed an image (2D) into a model designed for text (1D)?</p>

<h3 id="1-patch-embedding">1. Patch Embedding</h3>
<ul>
  <li><strong>Concept:</strong> Break the image into fixed-size patches (e.g., 16x16 pixels).</li>
  <li><strong>Linear Projection:</strong> Flatten each patch (16x16x3 = 768) and map it to a vector of size (D).</li>
  <li><strong>Result:</strong> An image of 224x224 becomes a sequence of 196 tokens (14x14 patches).</li>
</ul>

<h3 id="2-the-cls-token">2. The CLS Token</h3>
<ul>
  <li><strong>Problem:</strong> In BERT, we use a special <code class="language-plaintext highlighter-rouge">[CLS]</code> token to aggregate sentence-level information.</li>
  <li><strong>ViT:</strong> We prepend a learnable <code class="language-plaintext highlighter-rouge">[CLS]</code> token to the patch sequence.</li>
  <li><strong>Output:</strong> The state of the <code class="language-plaintext highlighter-rouge">[CLS]</code> token at the final layer serves as the image representation.</li>
</ul>

<h3 id="3-positional-embeddings">3. Positional Embeddings</h3>
<ul>
  <li><strong>Problem:</strong> Self-Attention is permutation invariant. It doesn’t know that Patch 1 is next to Patch 2.</li>
  <li><strong>Solution:</strong> Add learnable position vectors to each patch embedding.</li>
  <li><strong>1D vs 2D:</strong> Surprisingly, standard 1D learnable embeddings work as well as 2D grid embeddings. The model learns the grid structure on its own.</li>
</ul>

<h3 id="4-inductive-bias-vs-data">4. Inductive Bias vs. Data</h3>
<ul>
  <li><strong>CNNs:</strong> Have strong inductive bias (Locality, Translation Invariance). They work well on small data.</li>
  <li><strong>ViT:</strong> Has weak inductive bias. It assumes nothing. It needs <strong>massive data</strong> (JFT-300M) to learn that “pixels nearby are related”.</li>
  <li><strong>DeiT (Data-efficient Image Transformers):</strong> Uses Distillation to train ViTs on ImageNet without extra data.</li>
</ul>

<h2 id="deep-dive-mobilenet-and-efficient-architecture">Deep Dive: MobileNet and Efficient Architecture</h2>

<p>Not everyone has an A100. How do we run models on phones?</p>

<h3 id="1-depthwise-separable-convolutions">1. Depthwise Separable Convolutions</h3>
<p>Standard Conv: (K \times K \times C_{in} \times C_{out}) parameters.
Depthwise Separable:</p>
<ol>
  <li><strong>Depthwise:</strong> (K \times K \times 1 \times C_{in}). (Spatial mixing).</li>
  <li><strong>Pointwise:</strong> (1 \times 1 \times C_{in} \times C_{out}). (Channel mixing).
<strong>Reduction:</strong> (\frac{1}{C_{out}} + \frac{1}{K^2}). For 3x3 kernels, it’s ~8-9x fewer FLOPs.</li>
</ol>

<h3 id="2-inverted-residuals-mobilenetv2">2. Inverted Residuals (MobileNetV2)</h3>
<ul>
  <li><strong>ResNet:</strong> Wide -&gt; Narrow -&gt; Wide. (Bottleneck).</li>
  <li><strong>MobileNetV2:</strong> Narrow -&gt; Wide -&gt; Narrow.</li>
  <li><strong>Why?</strong> We expand the low-dimensional manifold into high dimensions to apply non-linearity (ReLU), then project back.</li>
  <li><strong>Linear Bottlenecks:</strong> The last 1x1 projection has <strong>No ReLU</strong>. Why? ReLU destroys information in low dimensions.</li>
</ul>

<h3 id="3-squeeze-and-excitation-se">3. Squeeze-and-Excitation (SE)</h3>
<p>MobileNetV3 added SE blocks. They are cheap (parameters) but powerful.
They allow the model to say “This channel (e.g., ‘fur detector’) is important for this image, but that channel (‘wheel detector’) is not.”</p>

<h2 id="deep-dive-neural-architecture-search-nas">Deep Dive: Neural Architecture Search (NAS)</h2>

<p>Designing architectures by hand is tedious. Let’s automate it.</p>

<h3 id="1-reinforcement-learning-nasnet">1. Reinforcement Learning (NASNet)</h3>
<ul>
  <li><strong>Agent:</strong> An RNN controller.</li>
  <li><strong>Action:</strong> Generate a string describing a layer (e.g., “Conv 3x3, ReLU”).</li>
  <li><strong>Environment:</strong> Train the child network for 5 epochs.</li>
  <li><strong>Reward:</strong> Validation Accuracy.</li>
  <li><strong>Cost:</strong> 2000 GPU-days. (Expensive!).</li>
</ul>

<h3 id="2-evolutionary-algorithms-amoebanet">2. Evolutionary Algorithms (AmoebaNet)</h3>
<ul>
  <li><strong>Population:</strong> A set of architectures.</li>
  <li><strong>Mutation:</strong> Randomly change one operation (e.g., 3x3 -&gt; 5x5).</li>
  <li><strong>Selection:</strong> Train and keep the best. Kill the worst.</li>
  <li><strong>Result:</strong> AmoebaNet matched NASNet with less compute.</li>
</ul>

<h3 id="3-differentiable-nas-darts">3. Differentiable NAS (DARTS)</h3>
<ul>
  <li><strong>Relaxation:</strong> Instead of choosing <em>one</em> operation, compute a weighted sum of <em>all</em> operations.
[ \bar{o}(x) = \sum_{o \in \mathcal{O}} \frac{\exp(\alpha_o)}{\sum \exp(\alpha_{o’})} o(x) ]</li>
  <li><strong>Bilevel Optimization:</strong>
    <ol>
      <li>Update weights (w) to minimize Train Loss.</li>
      <li>Update architecture alphas (\alpha) to minimize Val Loss.</li>
    </ol>
  </li>
  <li><strong>Cost:</strong> 4 GPU-days.</li>
</ul>

<h2 id="design-patterns-in-architecture">Design Patterns in Architecture</h2>

<h3 id="1-residual-connections-skip-connections">1. Residual Connections (Skip Connections)</h3>
<p><strong>Problem:</strong> In deep networks (e.g., 20 layers), gradients vanish during backpropagation. The signal degrades.
<strong>Solution:</strong> Add the input to the output: <code class="language-plaintext highlighter-rouge">y = F(x) + x</code>.
<strong>Why it works:</strong> It creates a “gradient superhighway”. The gradient can flow unchanged through the <code class="language-plaintext highlighter-rouge">+ x</code> path. This allowed ResNet to go from 20 layers to 152 layers.</p>

<h3 id="2-the-bottleneck-design">2. The Bottleneck Design</h3>
<p><strong>Problem:</strong> 3x3 Convolutions on high-dimensional channels (e.g., 256) are expensive.
<strong>Solution:</strong></p>
<ol>
  <li><strong>1x1 Conv:</strong> Reduce channels (256 -&gt; 64).</li>
  <li><strong>3x3 Conv:</strong> Process spatial features on low channels (64).</li>
  <li><strong>1x1 Conv:</strong> Expand channels back (64 -&gt; 256).
<strong>Result:</strong> 10x fewer parameters for the same depth.</li>
</ol>

<h3 id="3-squeeze-and-excitation-se-1">3. Squeeze-and-Excitation (SE)</h3>
<p><strong>Idea:</strong> Not all channels are important. Let the network learn to weight them.</p>
<ol>
  <li><strong>Squeeze:</strong> Global Average Pooling to get a 1x1xC vector.</li>
  <li><strong>Excite:</strong> A small MLP learns a weight for each channel (sigmoid).</li>
  <li><strong>Scale:</strong> Multiply the original feature map by these weights.
<strong>Result:</strong> 1-2% accuracy boost for negligible compute.</li>
</ol>

<h2 id="implementation-building-a-modern-resnet-block">Implementation: Building a Modern ResNet Block</h2>

<p>Let’s implement a “Pre-Activation” ResNet block with Squeeze-and-Excitation in PyTorch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">SEBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">squeeze</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">excite</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span> <span class="o">//</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">excite</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>

<span class="k">class</span> <span class="nc">ResNetBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="c1"># Bottleneck design
</span>        <span class="n">mid_channels</span> <span class="o">=</span> <span class="n">out_channels</span> <span class="o">//</span> <span class="mi">4</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">,</span> <span class="n">mid_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                               <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">mid_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">se</span> <span class="o">=</span> <span class="nc">SEBlock</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        
        <span class="c1"># Shortcut handling (if dimensions change)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">bn3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="c1"># Apply SE Attention
</span>        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">se</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        
        <span class="n">out</span> <span class="o">+=</span> <span class="n">residual</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># Test
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">block</span> <span class="o">=</span> <span class="nc">ResNetBlock</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># torch.Size([2, 256, 32, 32])
</span></code></pre></div></div>

<h2 id="scaling-laws-the-physics-of-deep-learning">Scaling Laws: The Physics of Deep Learning</h2>

<p>In 2020, Kaplan et al. (OpenAI) and later Hoffmann et al. (DeepMind “Chinchilla”) discovered that model performance scales as a <strong>Power Law</strong> with respect to:</p>
<ol>
  <li><strong>N:</strong> Number of Parameters.</li>
  <li><strong>D:</strong> Dataset Size.</li>
  <li><strong>C:</strong> Compute (FLOPs).</li>
</ol>

<p>[ L(N) \propto \frac{1}{N^\alpha} ]</p>

<p><strong>Key Insight:</strong></p>
<ul>
  <li>If you double the model size, you need to double the data to train it optimally.</li>
  <li>Most models (like GPT-3) were <strong>undertrained</strong>. They were too big for the amount of data they saw.</li>
  <li><strong>Chinchilla Optimal:</strong> For a fixed compute budget, you should balance model size and data size equally.</li>
</ul>

<h2 id="neural-architecture-search-nas">Neural Architecture Search (NAS)</h2>

<p>Why design by hand when AI can design AI?</p>

<p><strong>1. Reinforcement Learning (RL):</strong></p>
<ul>
  <li>A “Controller” RNN generates an architecture string (e.g., “Conv 3x3 -&gt; MaxPool”).</li>
  <li>Train the child model for a few epochs. Get accuracy.</li>
  <li>Use accuracy as “Reward” to update the Controller.</li>
  <li><em>Example:</em> NASNet (Google).</li>
</ul>

<p><strong>2. Differentiable NAS (DARTS):</strong></p>
<ul>
  <li>Define a “Supergraph” containing all possible operations (Conv3x3, Conv5x5, MaxPool) on every edge.</li>
  <li>Assign a continuous weight (\alpha) to each operation.</li>
  <li>Train the weights via Gradient Descent.</li>
  <li>Prune the weak operations at the end.</li>
  <li><em>Pros:</em> Much faster than RL (GPU days vs GPU years).</li>
</ul>

<h2 id="case-study-efficientnet-compound-scaling">Case Study: EfficientNet (Compound Scaling)</h2>

<p>Before EfficientNet, people scaled models randomly.</p>
<ul>
  <li>“Let’s make it deeper!” (ResNet-152)</li>
  <li>“Let’s make it wider!” (WideResNet)</li>
  <li>“Let’s increase resolution!”</li>
</ul>

<p><strong>EfficientNet Insight:</strong>
Depth, Width, and Resolution are coupled.</p>
<ul>
  <li>If you make the image bigger, you need more layers (Depth) to increase the receptive field.</li>
  <li>If you make it deeper, you need more channels (Width) to capture fine-grained patterns.</li>
</ul>

<p><strong>Compound Scaling Method:</strong>
Scale all three dimensions uniformly using a coefficient (\phi):</p>
<ul>
  <li>Depth: (d = \alpha^\phi)</li>
  <li>Width: (w = \beta^\phi)</li>
  <li>Resolution: (r = \gamma^\phi)</li>
  <li>Constraint: (\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2)</li>
</ul>

<p>This principled approach produced a family of models (B0 to B7) that dominated the ImageNet leaderboard while being 10x smaller than competitors.</p>

<h2 id="deep-dive-distributed-training-strategies">Deep Dive: Distributed Training Strategies</h2>

<p>When your model is too big for one GPU (e.g., GPT-3 is 175B parameters, requiring 800GB VRAM), you need Distributed Training.</p>

<h3 id="1-data-parallelism-ddp">1. Data Parallelism (DDP)</h3>
<ul>
  <li><strong>Scenario:</strong> Model fits in one GPU, but Batch Size is small.</li>
  <li><strong>Mechanism:</strong> Replicate the model on 8 GPUs. Split the batch (e.g., 32 images -&gt; 4 per GPU).</li>
  <li><strong>Sync:</strong> Gradients are averaged across GPUs using <code class="language-plaintext highlighter-rouge">AllReduce</code> (Ring Algorithm).</li>
</ul>

<h3 id="2-model-parallelism-tensor-parallelism">2. Model Parallelism (Tensor Parallelism)</h3>
<ul>
  <li><strong>Scenario:</strong> Model layer is too wide for one GPU.</li>
  <li><strong>Mechanism:</strong> Split a single Matrix Multiplication across 2 GPUs.
    <ul>
      <li>GPU 1 computes the top half of the matrix.</li>
      <li>GPU 2 computes the bottom half.</li>
    </ul>
  </li>
  <li><strong>Sync:</strong> Requires high-bandwidth interconnect (NVLink).</li>
</ul>

<h3 id="3-pipeline-parallelism">3. Pipeline Parallelism</h3>
<ul>
  <li><strong>Scenario:</strong> Model is too deep.</li>
  <li><strong>Mechanism:</strong> Put Layers 1-10 on GPU 1, Layers 11-20 on GPU 2.</li>
  <li><strong>Issue:</strong> The “Bubble”. GPU 2 sits idle while GPU 1 works.</li>
  <li><strong>Fix:</strong> Micro-batches.</li>
</ul>

<h3 id="4-zero-zero-redundancy-optimizer">4. ZeRO (Zero Redundancy Optimizer)</h3>
<ul>
  <li><strong>Idea:</strong> Don’t replicate the Optimizer States and Gradients on every GPU. Shard them.</li>
  <li><strong>ZeRO-3:</strong> Shards the Model Parameters too. Allows training trillion-parameter models.</li>
</ul>

<h2 id="failure-modes-in-architecture-design">Failure Modes in Architecture Design</h2>

<ol>
  <li><strong>The Vanishing Gradient:</strong>
    <ul>
      <li><em>Symptom:</em> Loss doesn’t decrease.</li>
      <li><em>Cause:</em> Network too deep without Residual connections. Sigmoid/Tanh activations.</li>
      <li><em>Fix:</em> Use ResNets, ReLU, Batch Norm.</li>
    </ul>
  </li>
  <li><strong>The Information Bottleneck:</strong>
    <ul>
      <li><em>Symptom:</em> Poor performance on fine-grained tasks.</li>
      <li><em>Cause:</em> Downsampling too aggressively (Stride 2) too early.</li>
      <li><em>Fix:</em> Keep resolution high for longer. Use Dilated Convolutions.</li>
    </ul>
  </li>
  <li><strong>Over-Parameterization (Overfitting):</strong>
    <ul>
      <li><em>Symptom:</em> Training loss 0, Validation loss high.</li>
      <li><em>Cause:</em> Model too big for the dataset.</li>
      <li><em>Fix:</em> Dropout, Weight Decay, Data Augmentation, or smaller model.</li>
    </ul>
  </li>
</ol>

<h2 id="cost-analysis-flops-vs-latency">Cost Analysis: FLOPs vs. Latency</h2>

<p><strong>FLOPs (Floating Point Operations)</strong> is a theoretical metric.
<strong>Latency (ms)</strong> is what matters in production.</p>

<p>They are not always correlated!</p>
<ul>
  <li><strong>Depthwise Separable Convs</strong> have low FLOPs but high Latency on GPUs. Why? Because they are <strong>Memory Bound</strong>. They have low arithmetic intensity (compute/memory ratio).</li>
  <li><strong>Standard Convs</strong> are highly optimized by cuDNN and Tensor Cores.</li>
</ul>

<p><strong>Takeaway:</strong> Don’t just optimize FLOPs. Benchmark on the target hardware (T4, A100, Mobile CPU).</p>

<h2 id="deep-dive-training-loop-implementation">Deep Dive: Training Loop Implementation</h2>

<p>Designing the architecture is only half the battle. You need to train it.
Here is a standard PyTorch training loop for our ResNet.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># 1. Loss and Optimizer
</span>    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
    <span class="c1"># SGD with Momentum is standard for ResNets
</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
    <span class="c1"># Cosine Annealing Scheduler
</span>    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="nc">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
        <span class="n">running_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            
            <span class="c1"># Zero gradients
</span>            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            
            <span class="c1"># Forward
</span>            <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            
            <span class="c1"># Backward
</span>            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
            
            <span class="n">running_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="n">predicted</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">labels</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
            
        <span class="n">train_acc</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">: Loss </span><span class="si">{</span><span class="n">running_loss</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> | Acc </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># Validation
</span>        <span class="nf">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        
        <span class="n">scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            
            <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="n">predicted</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">labels</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
            
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Val Loss </span><span class="si">{</span><span class="n">val_loss</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">loader</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> | Val Acc </span><span class="si">{</span><span class="mf">100.</span><span class="o">*</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="top-interview-questions">Top Interview Questions</h2>

<p><strong>Q1: Why does ResNet work?</strong>
<em>Answer:</em></p>
<ol>
  <li><strong>Gradient Flow:</strong> The skip connection <code class="language-plaintext highlighter-rouge">x + F(x)</code> allows gradients to flow through the network without being multiplied by weight matrices at every layer. This prevents vanishing gradients.</li>
  <li><strong>Ensemble Hypothesis:</strong> A ResNet can be seen as an ensemble of many shallower networks. Dropping a layer in ResNet doesn’t kill performance, unlike in VGG.</li>
  <li><strong>Identity Mapping:</strong> It’s easier for the network to learn <code class="language-plaintext highlighter-rouge">F(x) = 0</code> (identity mapping) than to learn a specific transformation.</li>
</ol>

<p><strong>Q2: When should you use Layer Norm over Batch Norm?</strong>
<em>Answer:</em></p>
<ul>
  <li>Use <strong>Layer Norm</strong> for RNNs and Transformers (NLP/Speech). It works well when sequence lengths vary and batch sizes are small.</li>
  <li>Use <strong>Batch Norm</strong> for CNNs (Vision). It acts as a regularizer and speeds up convergence, but requires large fixed-size batches.</li>
</ul>

<p><strong>Q3: How do you calculate the number of parameters in a Convolutional Layer?</strong>
<em>Answer:</em>
[ \text{Params} = (K \times K \times C_{in} + 1) \times C_{out} ]
Where (K) is kernel size, (C_{in}) is input channels, (C_{out}) is output channels, and (+1) is for the bias.</p>

<p><strong>Q4: What is the “Receptive Field” and how do you increase it?</strong>
<em>Answer:</em>
The Receptive Field is the region of the input image that affects a specific neuron.
To increase it:</p>
<ol>
  <li>Add more layers (Depth).</li>
  <li>Use larger kernels (e.g., 7x7).</li>
  <li>Use <strong>Dilated Convolutions</strong> (Atrous).</li>
  <li>Use Pooling / Strided Convolutions (Downsampling).</li>
</ol>

<p><strong>Q5: Why do we use “He Initialization” (Kaiming Init) for ReLU networks?</strong>
<em>Answer:</em>
Xavier (Glorot) initialization assumes linear activations. ReLU is non-linear (half the activations are zeroed).
He Initialization scales the weights by (\sqrt{2/n}) instead of (\sqrt{1/n}) to maintain the variance of activations through the layers.</p>

<h2 id="deep-dive-regularization-techniques">Deep Dive: Regularization Techniques</h2>

<p>Deep networks are prone to overfitting. We need to stop them from memorizing the training data.</p>

<h3 id="1-dropout-hinton-et-al-2012">1. Dropout (Hinton et al., 2012)</h3>
<ul>
  <li><strong>Mechanism:</strong> Randomly zero out neurons during training with probability (p) (usually 0.5).</li>
  <li><strong>Effect:</strong> Prevents co-adaptation of features. Forces the network to learn redundant representations.</li>
  <li><strong>Inference:</strong> Scale weights by ((1-p)) or use Inverted Dropout during training.</li>
</ul>

<h3 id="2-dropconnect-wan-et-al-2013">2. DropConnect (Wan et al., 2013)</h3>
<ul>
  <li><strong>Mechanism:</strong> Instead of zeroing neurons (activations), zero out the <strong>weights</strong>.</li>
  <li><strong>Effect:</strong> A generalization of Dropout.</li>
</ul>

<h3 id="3-stochastic-depth-huang-et-al-2016">3. Stochastic Depth (Huang et al., 2016)</h3>
<ul>
  <li><strong>Mechanism:</strong> Randomly drop entire <strong>Residual Blocks</strong> during training.</li>
  <li><strong>Effect:</strong> Effectively trains an ensemble of networks of different depths. Crucial for training very deep ResNets (&gt;100 layers) and Vision Transformers.</li>
</ul>

<h3 id="4-label-smoothing">4. Label Smoothing</h3>
<ul>
  <li><strong>Mechanism:</strong> Instead of targeting <code class="language-plaintext highlighter-rouge">[0, 1, 0]</code>, target <code class="language-plaintext highlighter-rouge">[0.1, 0.8, 0.1]</code>.</li>
  <li><strong>Effect:</strong> Prevents the model from becoming over-confident. Calibrates probabilities.</li>
</ul>

<h2 id="deep-dive-optimizers-sgd-vs-adam">Deep Dive: Optimizers (SGD vs. Adam)</h2>

<p>Which optimizer should you use for your architecture?</p>

<h3 id="1-sgd-with-momentum">1. SGD with Momentum</h3>
<ul>
  <li><strong>Formula:</strong> (v_t = \mu v_{t-1} + g_t); (w_t = w_{t-1} - \eta v_t).</li>
  <li><strong>Best for:</strong> <strong>CNNs (ResNet, VGG)</strong>.</li>
  <li><strong>Why?</strong> It generalizes better. It finds flatter minima.</li>
</ul>

<h3 id="2-adam-adaptive-moment-estimation">2. Adam (Adaptive Moment Estimation)</h3>
<ul>
  <li><strong>Formula:</strong> Maintains per-parameter learning rates based on first and second moments of gradients.</li>
  <li><strong>Best for:</strong> <strong>Transformers (BERT, GPT, ViT)</strong> and <strong>RNNs</strong>.</li>
  <li><strong>Why?</strong> Transformers have very complex loss landscapes. SGD gets stuck. Adam navigates the curvature better.</li>
</ul>

<h3 id="3-adamw-adam-with-weight-decay">3. AdamW (Adam with Weight Decay)</h3>
<ul>
  <li><strong>Fix:</strong> Standard L2 regularization in Adam is broken. AdamW decouples weight decay from the gradient update.</li>
  <li><strong>Status:</strong> The default optimizer for all modern LLMs and ViTs.</li>
</ul>

<h2 id="deep-dive-hardware-efficiency-the-memory-wall">Deep Dive: Hardware Efficiency (The Memory Wall)</h2>

<p>Why is a 100M parameter model faster than a 50M parameter model sometimes?
Because of <strong>Arithmetic Intensity</strong>.</p>

<p>[ \text{Intensity} = \frac{\text{FLOPs}}{\text{Bytes Access}} ]</p>

<ul>
  <li><strong>Compute Bound:</strong> Layers like Conv2d (large channels) or Linear (large batch). The GPU cores are 100% utilized.</li>
  <li><strong>Memory Bound:</strong> Layers like Activation (ReLU), Normalization (BN), or Element-wise Add. The GPU cores are waiting for data from VRAM.</li>
</ul>

<p><strong>Optimization:</strong></p>
<ul>
  <li><strong>Operator Fusion:</strong> Fuse <code class="language-plaintext highlighter-rouge">Conv + BN + ReLU</code> into a single kernel. This reads data once, does 3 ops, and writes once.</li>
  <li><strong>FlashAttention:</strong> A hardware-aware attention algorithm that reduces HBM (High Bandwidth Memory) access by tiling the computation in SRAM (L1 Cache). It speeds up Transformers by 3-4x.</li>
</ul>

<h2 id="further-reading">Further Reading</h2>

<ol>
  <li><strong>ResNet:</strong> <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition (He et al., 2015)</a></li>
  <li><strong>Transformer:</strong> <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need (Vaswani et al., 2017)</a></li>
  <li><strong>EfficientNet:</strong> <a href="https://arxiv.org/abs/1905.11946">EfficientNet: Rethinking Model Scaling for CNNs (Tan &amp; Le, 2019)</a></li>
  <li><strong>ViT:</strong> <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words (Dosovitskiy et al., 2020)</a></li>
  <li><strong>Chinchilla:</strong> <a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models (Hoffmann et al., 2022)</a></li>
</ol>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li><strong>Inductive Bias:</strong> Choose the architecture that fits your data structure (CNN for images, Transformer for sets/sequences).</li>
  <li><strong>Residuals are King:</strong> You cannot train deep networks without skip connections.</li>
  <li><strong>Normalization is Queen:</strong> Batch Norm or Layer Norm is essential for convergence.</li>
  <li><strong>Scale Principledly:</strong> Use Compound Scaling (EfficientNet) or Chinchilla laws.</li>
  <li><strong>Don’t Reinvent:</strong> Start with a standard backbone (ResNet, ViT) and modify the Head. Only design a custom backbone if you have a very specific constraint.</li>
</ol>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0027-model-architecture-design/">arunbaby.com/ml-system-design/0027-model-architecture-design</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#architecture-search" class="page__taxonomy-item p-category" rel="tag">architecture-search</a><span class="sep">, </span>
    
      <a href="/tags/#cnn" class="page__taxonomy-item p-category" rel="tag">cnn</a><span class="sep">, </span>
    
      <a href="/tags/#nas" class="page__taxonomy-item p-category" rel="tag">nas</a><span class="sep">, </span>
    
      <a href="/tags/#neural-networks" class="page__taxonomy-item p-category" rel="tag">neural-networks</a><span class="sep">, </span>
    
      <a href="/tags/#transformers" class="page__taxonomy-item p-category" rel="tag">transformers</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#deep-learning" class="page__taxonomy-item p-category" rel="tag">deep-learning</a><span class="sep">, </span>
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0027-construct-binary-tree/" rel="permalink">Construct Binary Tree from Preorder and Inorder Traversal
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Given two arrays, can you rebuild the original tree? It’s like solving a jigsaw puzzle where the pieces are numbers.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0027-end-to-end-speech-model-design/" rel="permalink">End-to-End Speech Model Design
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Goodbye HMMs. Goodbye Phonemes. Goodbye Lexicons. We are teaching the machine to Listen, Attend, and Spell.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0027-api-integration-patterns/" rel="permalink">API Integration Patterns for AI Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Connecting the brain to the world’s nervous system.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Model+Architecture+Design%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0027-model-architecture-design%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0027-model-architecture-design%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0027-model-architecture-design/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0026-batch-processing-pipelines/" class="pagination--pager" title="Batch Processing Pipelines">Previous</a>
    
    
      <a href="/ml-system-design/0028-ranking-systems-at-scale/" class="pagination--pager" title="Ranking Systems at Scale">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
