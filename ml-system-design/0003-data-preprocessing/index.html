<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Data Preprocessing Pipeline Design - Arun Baby</title>
<meta name="description" content="How to build production-grade pipelines that clean, transform, and validate billions of data points before training.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Data Preprocessing Pipeline Design">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0003-data-preprocessing/">


  <meta property="og:description" content="How to build production-grade pipelines that clean, transform, and validate billions of data points before training.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Data Preprocessing Pipeline Design">
  <meta name="twitter:description" content="How to build production-grade pipelines that clean, transform, and validate billions of data points before training.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0003-data-preprocessing/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0003-data-preprocessing/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Data Preprocessing Pipeline Design">
    <meta itemprop="description" content="How to build production-grade pipelines that clean, transform, and validate billions of data points before training.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0003-data-preprocessing/" itemprop="url">Data Preprocessing Pipeline Design
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          28 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#problem-definition">Problem Definition</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li></ul></li><li><a href="#high-level-architecture">High-Level Architecture</a></li><li><a href="#component-1-data-validation">Component 1: Data Validation</a><ul><li><a href="#schema-validation">Schema Validation</a></li><li><a href="#statistical-validation">Statistical Validation</a></li></ul></li><li><a href="#component-2-data-cleaning">Component 2: Data Cleaning</a><ul><li><a href="#missing-value-handling">Missing Value Handling</a></li><li><a href="#outlier-detection--handling">Outlier Detection &amp; Handling</a></li><li><a href="#deduplication">Deduplication</a></li></ul></li><li><a href="#component-3-feature-engineering">Component 3: Feature Engineering</a><ul><li><a href="#numerical-transformations">Numerical Transformations</a></li><li><a href="#categorical-encoding">Categorical Encoding</a></li><li><a href="#temporal-features">Temporal Features</a></li></ul></li><li><a href="#component-4-pipeline-orchestration">Component 4: Pipeline Orchestration</a><ul><li><a href="#apache-beam-pipeline">Apache Beam Pipeline</a></li></ul></li><li><a href="#preventing-trainingserving-skew">Preventing Training/Serving Skew</a><ul><li><a href="#solution-1-unified-preprocessing-library">Solution 1: Unified Preprocessing Library</a></li><li><a href="#solution-2-feature-store">Solution 2: Feature Store</a></li></ul></li><li><a href="#monitoring--data-quality">Monitoring &amp; Data Quality</a></li><li><a href="#real-world-examples">Real-World Examples</a><ul><li><a href="#netflix-data-preprocessing-for-recommendations">Netflix: Data Preprocessing for Recommendations</a></li><li><a href="#uber-preprocessing-for-etas">Uber: Preprocessing for ETAs</a></li><li><a href="#google-search-ranking-data-pipeline">Google: Search Ranking Data Pipeline</a></li></ul></li><li><a href="#distributed-preprocessing-with-spark">Distributed Preprocessing with Spark</a><ul><li><a href="#spark-preprocessing-pipeline">Spark Preprocessing Pipeline</a></li></ul></li><li><a href="#advanced-feature-engineering-patterns">Advanced Feature Engineering Patterns</a><ul><li><a href="#1-time-series-features">1. Time-Series Features</a></li><li><a href="#2-interaction-features">2. Interaction Features</a></li><li><a href="#3-embedding-features">3. Embedding Features</a></li></ul></li><li><a href="#handling-data-drift">Handling Data Drift</a><ul><li><a href="#drift-detection">Drift Detection</a></li></ul></li><li><a href="#production-best-practices">Production Best Practices</a><ul><li><a href="#1-idempotency">1. Idempotency</a></li><li><a href="#2-data-versioning">2. Data Versioning</a></li><li><a href="#3-lineage-tracking">3. Lineage Tracking</a></li></ul></li><li><a href="#common-preprocessing-challenges--solutions">Common Preprocessing Challenges &amp; Solutions</a><ul><li><a href="#challenge-1-imbalanced-classes">Challenge 1: Imbalanced Classes</a></li><li><a href="#challenge-2-high-cardinality-categoricals">Challenge 2: High-Cardinality Categoricals</a></li><li><a href="#challenge-3-streaming-data-preprocessing">Challenge 3: Streaming Data Preprocessing</a></li><li><a href="#challenge-4-privacy--compliance-gdpr-ccpa">Challenge 4: Privacy &amp; Compliance (GDPR, CCPA)</a></li></ul></li><li><a href="#performance-optimization">Performance Optimization</a><ul><li><a href="#1-parallelize-transformations">1. Parallelize Transformations</a></li><li><a href="#2-use-efficient-data-formats">2. Use Efficient Data Formats</a></li><li><a href="#3-cache-intermediate-results">3. Cache Intermediate Results</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>How to build production-grade pipelines that clean, transform, and validate billions of data points before training.</strong></p>

<h2 id="introduction">Introduction</h2>

<p>Data preprocessing is the <strong>most time-consuming yet critical</strong> part of ML systems. Industry surveys show data scientists spend <strong>60-80% of their time</strong> on data preparation, cleaning, transforming, and validating data before training.</p>

<p><strong>Why it matters:</strong></p>
<ul>
  <li><strong>Garbage in, garbage out:</strong> Poor data quality → poor models</li>
  <li><strong>Scale:</strong> Process terabytes/petabytes efficiently</li>
  <li><strong>Repeatability:</strong> Same transformations in training &amp; serving</li>
  <li><strong>Monitoring:</strong> Detect data drift and quality issues</li>
</ul>

<p>This post covers end-to-end preprocessing pipeline design at scale.</p>

<p><strong>What you’ll learn:</strong></p>
<ul>
  <li>Architecture for scalable preprocessing</li>
  <li>Data cleaning and validation strategies</li>
  <li>Feature engineering pipelines</li>
  <li>Training/serving skew prevention</li>
  <li>Monitoring and data quality</li>
  <li>Real-world examples from top companies</li>
</ul>

<hr />

<h2 id="problem-definition">Problem Definition</h2>

<p>Design a scalable data preprocessing pipeline for a machine learning system.</p>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Data Ingestion</strong>
    <ul>
      <li>Ingest from multiple sources (databases, logs, streams)</li>
      <li>Support batch and streaming data</li>
      <li>Handle structured and unstructured data</li>
    </ul>
  </li>
  <li><strong>Data Cleaning</strong>
    <ul>
      <li>Handle missing values</li>
      <li>Remove duplicates</li>
      <li>Fix inconsistencies</li>
      <li>Outlier detection and handling</li>
    </ul>
  </li>
  <li><strong>Data Transformation</strong>
    <ul>
      <li>Normalization/standardization</li>
      <li>Encoding categorical variables</li>
      <li>Feature extraction</li>
      <li>Feature selection</li>
    </ul>
  </li>
  <li><strong>Data Validation</strong>
    <ul>
      <li>Schema validation</li>
      <li>Statistical validation</li>
      <li>Anomaly detection</li>
      <li>Data drift detection</li>
    </ul>
  </li>
  <li><strong>Feature Engineering</strong>
    <ul>
      <li>Create derived features</li>
      <li>Aggregations (time-based, user-based)</li>
      <li>Interaction features</li>
      <li>Embedding generation</li>
    </ul>
  </li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Scale</strong>
    <ul>
      <li>Process 1TB+ data/day</li>
      <li>Handle billions of records</li>
      <li>Support horizontal scaling</li>
    </ul>
  </li>
  <li><strong>Latency</strong>
    <ul>
      <li>Batch: Process daily data in &lt; 6 hours</li>
      <li>Streaming: &lt; 1 second latency for real-time features</li>
    </ul>
  </li>
  <li><strong>Reliability</strong>
    <ul>
      <li>99.9% pipeline success rate</li>
      <li>Automatic retries on failure</li>
      <li>Data lineage tracking</li>
    </ul>
  </li>
  <li><strong>Consistency</strong>
    <ul>
      <li>Same transformations in training and serving</li>
      <li>Versioned transformation logic</li>
      <li>Reproducible results</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="high-level-architecture">High-Level Architecture</h2>

<p><code class="language-plaintext highlighter-rouge">
┌─────────────────────────────────────────────────────────────┐
│ Data Sources │
├─────────────────────────────────────────────────────────────┤
│ Databases │ Event Logs │ File Storage │ APIs │
└──────┬──────┴──────┬──────┴───────┬─────────┴──────┬────────┘
 │ │ │ │
 └─────────────┼──────────────┼────────────────┘
 ↓ ↓
 ┌─────────────────────────────┐
 │ Data Ingestion Layer │
 │ (Kafka, Pub/Sub, Kinesis) │
 └──────────────┬──────────────┘
 ↓
 ┌─────────────────────────────┐
 │ Raw Data Storage │
 │ (Data Lake: S3/GCS) │
 └──────────────┬──────────────┘
 ↓
 ┌─────────────────────────────┐
 │ Preprocessing Pipeline │
 │ │
 │ ┌──────────────────────┐ │
 │ │ 1. Data Validation │ │
 │ └──────────────────────┘ │
 │ ┌──────────────────────┐ │
 │ │ 2. Data Cleaning │ │
 │ └──────────────────────┘ │
 │ ┌──────────────────────┐ │
 │ │ 3. Feature Extraction│ │
 │ └──────────────────────┘ │
 │ ┌──────────────────────┐ │
 │ │ 4. Transformation │ │
 │ └──────────────────────┘ │
 │ ┌──────────────────────┐ │
 │ │ 5. Quality Checks │ │
 │ └──────────────────────┘ │
 │ │
 │ (Spark/Beam/Airflow) │
 └──────────────┬──────────────┘
 ↓
 ┌─────────────────────────────┐
 │ Processed Data Storage │
 │ (Feature Store/DW) │
 └──────────────┬──────────────┘
 ↓
 ┌─────────────────────────────┐
 │ Model Training │
 │ &amp; Serving │
 └─────────────────────────────┘
</code></p>

<hr />

<h2 id="component-1-data-validation">Component 1: Data Validation</h2>

<p>Validate data quality and schema before processing.</p>

<h3 id="schema-validation">Schema Validation</h3>

<p>``python
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from enum import Enum
import pandas as pd</p>

<p>class DataType(Enum):
 INT = “int”
 FLOAT = “float”
 STRING = “string”
 TIMESTAMP = “timestamp”
 BOOLEAN = “boolean”</p>

<p>@dataclass
class FieldSchema:
 name: str
 dtype: DataType
 nullable: bool = True
 min_value: Optional[float] = None
 max_value: Optional[float] = None
 allowed_values: Optional[List[Any]] = None</p>

<p>class SchemaValidator:
 “””
 Validate data against expected schema</p>

<p>Use case: Ensure incoming data matches expectations
 “””</p>

<p>def <strong>init</strong>(self, schema: List[FieldSchema]):
 self.schema = {field.name: field for field in schema}</p>

<p>def validate(self, df: pd.DataFrame) -&gt; Dict[str, List[str]]:
 “””
 Validate DataFrame against schema</p>

<p>Returns:
 Dict of field_name → list of errors
 “””
 errors = {}</p>

<p># Check for missing columns
 expected_cols = set(self.schema.keys())
 actual_cols = set(df.columns)
 missing = expected_cols - actual_cols
 if missing:
 errors[‘_schema’] = [f”Missing columns: {missing}”]</p>

<p># Validate each field
 for field_name, field_schema in self.schema.items():
 if field_name not in df.columns:
 continue</p>

<p>field_errors = self._validate_field(df[field_name], field_schema)
 if field_errors:
 errors[field_name] = field_errors</p>

<p>return errors</p>

<p>def validate_record(self, record: Dict[str, Any]) -&gt; Dict[str, List[str]]:
 “””
 Validate a single record (dict) against schema
 “””
 df = pd.DataFrame([record])
 return self.validate(df)</p>

<p>def _validate_field(self, series: pd.Series, schema: FieldSchema) -&gt; List[str]:
 “"”Validate a single field”””
 errors = []</p>

<p># Check nulls
 if not schema.nullable and series.isnull().any():
 null_count = series.isnull().sum()
 errors.append(f”Found {null_count} null values (not allowed)”)</p>

<p># Check data type
 if schema.dtype == DataType.INT:
 if not pd.api.types.is_integer_dtype(series.dropna()):
 errors.append(“Expected integer type”)
 elif schema.dtype == DataType.FLOAT:
 if not pd.api.types.is_numeric_dtype(series.dropna()):
 errors.append(“Expected numeric type”)
 elif schema.dtype == DataType.STRING:
 if not pd.api.types.is_string_dtype(series.dropna()):
 errors.append(“Expected string type”)
 elif schema.dtype == DataType.BOOLEAN:
 if not pd.api.types.is_bool_dtype(series.dropna()):
 errors.append(“Expected boolean type”)
 elif schema.dtype == DataType.TIMESTAMP:
 if not pd.api.types.is_datetime64_any_dtype(series.dropna()):
 try:
 pd.to_datetime(series.dropna())
 except Exception:
 errors.append(“Expected timestamp/datetime type”)</p>

<p># Check value ranges
 if schema.min_value is not None:
 below_min = (series &lt; schema.min_value).sum()
 if below_min &gt; 0:
 errors.append(f”{below_min} values below minimum {schema.min_value}”)</p>

<p>if schema.max_value is not None:
 above_max = (series &gt; schema.max_value).sum()
 if above_max &gt; 0:
 errors.append(f”{above_max} values above maximum {schema.max_value}”)</p>

<p># Check allowed values
 if schema.allowed_values is not None:
 invalid = ~series.isin(schema.allowed_values)
 invalid_count = invalid.sum()
 if invalid_count &gt; 0:
 invalid_vals = series[invalid].unique()[:5]
 errors.append(
 f”{invalid_count} values not in allowed set. “
 f”Examples: {invalid_vals}”
 )</p>

<p>return errors</p>

<h1 id="usage">Usage</h1>
<p>user_schema = [
 FieldSchema(“user_id”, DataType.INT, nullable=False, min_value=0),
 FieldSchema(“age”, DataType.INT, nullable=True, min_value=0, max_value=120),
 FieldSchema(“country”, DataType.STRING, nullable=False, 
 allowed_values=[“US”, “UK”, “CA”, “AU”]),
 FieldSchema(“signup_date”, DataType.TIMESTAMP, nullable=False)
]</p>

<p>validator = SchemaValidator(user_schema)
errors = validator.validate(user_df)</p>

<p>if errors:
 print(“Validation errors found:”)
 for field, field_errors in errors.items():
 print(f” {field}: {field_errors}”)
``</p>

<h3 id="statistical-validation">Statistical Validation</h3>

<p>``python
import numpy as np
from scipy import stats</p>

<p>class StatisticalValidator:
 “””
 Detect statistical anomalies in data</p>

<p>Compare current batch against historical baseline
 “””</p>

<p>def <strong>init</strong>(self, baseline_stats: Dict[str, Dict]):
 “””
 Args:
 baseline_stats: Historical statistics per field
 {
 ‘age’: {‘mean’: 35.2, ‘std’: 12.5, ‘median’: 33},
 ‘price’: {‘mean’: 99.5, ‘std’: 25.0, ‘median’: 95}
 }
 “””
 self.baseline = baseline_stats</p>

<p>def validate(self, df: pd.DataFrame, threshold_sigma=3) -&gt; List[str]:
 “””
 Detect fields with distributions far from baseline</p>

<p>Returns:
 List of warnings
 “””
 warnings = []</p>

<p>for field, baseline in self.baseline.items():
 if field not in df.columns:
 continue</p>

<p>current = df[field].dropna()</p>

<p># Check mean shift
 current_mean = current.mean()
 expected_mean = baseline[‘mean’]
 expected_std = baseline[‘std’]</p>

<p>denom = expected_std if expected_std &gt; 1e-9 else 1e-9
 z_score = abs(current_mean - expected_mean) / denom</p>

<p>if z_score &gt; threshold_sigma:
 warnings.append(
 f”{field}: Mean shifted significantly “
 f”(current={current_mean:.2f}, “
 f”baseline={expected_mean:.2f}, “
 f”z-score={z_score:.2f})”
 )</p>

<p># Check distribution shift (KS test)
 baseline_samples = np.random.normal(
 baseline[‘mean’], 
 baseline[‘std’], 
 size=len(current)
 )</p>

<p>ks_stat, p_value = stats.ks_2samp(current, baseline_samples)</p>

<p>if p_value &lt; 0.01: # Significant difference
 warnings.append(
 f”{field}: Distribution changed “
 f”(KS statistic={ks_stat:.3f}, p={p_value:.3f})”
 )</p>

<p>return warnings
``</p>

<hr />

<h2 id="component-2-data-cleaning">Component 2: Data Cleaning</h2>

<p>Handle missing values, duplicates, and inconsistencies.</p>

<h3 id="missing-value-handling">Missing Value Handling</h3>

<p>``python
class MissingValueHandler:
 “””
 Handle missing values with different strategies
 “””</p>

<p>def <strong>init</strong>(self):
 self.imputers = {}</p>

<p>def fit(self, df: pd.DataFrame, strategies: Dict[str, str]):
 “””
 Fit imputation strategies</p>

<p>Args:
 strategies: {column: strategy}
 strategy options: ‘mean’, ‘median’, ‘mode’, ‘forward_fill’, ‘drop’
 “””
 for col, strategy in strategies.items():
 if col not in df.columns:
 continue</p>

<p>if strategy == ‘mean’:
 self.imputers[col] = df[col].mean()
 elif strategy == ‘median’:
 self.imputers[col] = df[col].median()
 elif strategy == ‘mode’:
 self.imputers[col] = df[col].mode()[0]
 # forward_fill and drop don’t need fitting</p>

<p>def transform(self, df: pd.DataFrame, strategies: Dict[str, str]) -&gt; pd.DataFrame:
 “"”Apply imputation”””
 df = df.copy()</p>

<p>for col, strategy in strategies.items():
 if col not in df.columns:
 continue</p>

<p>if strategy in [‘mean’, ‘median’, ‘mode’]:
 df[col].fillna(self.imputers[col], inplace=True)</p>

<p>elif strategy == ‘forward_fill’:
 df[col].fillna(method=’ffill’, inplace=True)</p>

<p>elif strategy == ‘backward_fill’:
 df[col].fillna(method=’bfill’, inplace=True)</p>

<p>elif strategy == ‘drop’:
 df.dropna(subset=[col], inplace=True)</p>

<p>elif strategy == ‘constant’:
 # Fill with a constant (e.g., 0, ‘Unknown’)
 fill_value = 0 if pd.api.types.is_numeric_dtype(df[col]) else ‘Unknown’
 df[col].fillna(fill_value, inplace=True)</p>

<p>return df
``</p>

<h3 id="outlier-detection--handling">Outlier Detection &amp; Handling</h3>

<p>``python
class OutlierHandler:
 “””
 Detect and handle outliers
 “””</p>

<p>def detect_outliers_iqr(self, series: pd.Series, multiplier=1.5):
 “””
 IQR method: values outside [Q1 - 1.5<em>IQR, Q3 + 1.5</em>IQR]
 “””
 Q1 = series.quantile(0.25)
 Q3 = series.quantile(0.75)
 IQR = Q3 - Q1</p>

<p>lower_bound = Q1 - multiplier * IQR
 upper_bound = Q3 + multiplier * IQR</p>

<table>
  <tbody>
    <tr>
      <td>outliers = (series &lt; lower_bound)</td>
      <td>(series &gt; upper_bound)</td>
    </tr>
  </tbody>
</table>

<p>return outliers</p>

<p>def detect_outliers_zscore(self, series: pd.Series, threshold=3):
 “””
 Z-score method: |z| &gt; threshold
 “””
 z_scores = np.abs(stats.zscore(series.dropna()))
 outliers = z_scores &gt; threshold</p>

<p>return outliers</p>

<p>def handle_outliers(self, df: pd.DataFrame, columns: List[str], method=’clip’):
 “””
 Handle outliers</p>

<p>Args:
 method: ‘clip’, ‘remove’, ‘cap’, ‘transform’
 “””
 df = df.copy()</p>

<p>for col in columns:
 outliers = self.detect_outliers_iqr(df[col])</p>

<p>if method == ‘clip’:
 # Clip to [Q1 - 1.5<em>IQR, Q3 + 1.5</em>IQR]
 Q1 = df[col].quantile(0.25)
 Q3 = df[col].quantile(0.75)
 IQR = Q3 - Q1
 lower = Q1 - 1.5 * IQR
 upper = Q3 + 1.5 * IQR
 df[col] = df[col].clip(lower, upper)</p>

<p>elif method == ‘remove’:
 # Remove outlier rows
 df = df[~outliers]</p>

<p>elif method == ‘cap’:
 # Cap at 99th percentile
 upper = df[col].quantile(0.99)
 df[col] = df[col].clip(upper=upper)</p>

<p>elif method == ‘transform’:
 # Log transform to reduce skew
 df[col] = np.log1p(df[col])</p>

<p>return df
``</p>

<h3 id="deduplication">Deduplication</h3>

<p>``python
class Deduplicator:
 “””
 Remove duplicate records
 “””</p>

<p>def deduplicate(
 self, 
 df: pd.DataFrame, 
 key_columns: List[str],
 keep=’last’,
 timestamp_col: Optional[str] = None
 ) -&gt; pd.DataFrame:
 “””
 Remove duplicates</p>

<p>Args:
 key_columns: Columns that define uniqueness
 keep: ‘first’, ‘last’, or False (remove all duplicates)
 timestamp_col: If provided, keep most recent
 “””
 if timestamp_col:
 # Sort by timestamp descending, then drop duplicates keeping first
 df = df.sort_values(timestamp_col, ascending=False)
 df = df.drop_duplicates(subset=key_columns, keep=’first’)
 else:
 df = df.drop_duplicates(subset=key_columns, keep=keep)</p>

<p>return df
``</p>

<hr />

<h2 id="component-3-feature-engineering">Component 3: Feature Engineering</h2>

<p>Transform raw data into ML-ready features.</p>

<h3 id="numerical-transformations">Numerical Transformations</h3>

<p>``python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler</p>

<p>class NumericalTransformer:
 “””
 Apply numerical transformations
 “””</p>

<p>def <strong>init</strong>(self):
 self.scalers = {}</p>

<p>def fit_transform(self, df: pd.DataFrame, transformations: Dict[str, str]):
 “””
 Apply transformations</p>

<p>transformations: {column: transformation_type}
 ‘standard’: StandardScaler (mean=0, std=1)
 ‘minmax’: MinMaxScaler (range [0, 1])
 ‘robust’: RobustScaler (use median, IQR - robust to outliers)
 ‘log’: Log transform
 ‘sqrt’: Square root transform
 “””
 df = df.copy()</p>

<p>for col, transform_type in transformations.items():
 if col not in df.columns:
 continue</p>

<p>if transform_type == ‘standard’:
 scaler = StandardScaler()
 df[col] = scaler.fit_transform(df[[col]])
 self.scalers[col] = scaler</p>

<p>elif transform_type == ‘minmax’:
 scaler = MinMaxScaler()
 df[col] = scaler.fit_transform(df[[col]])
 self.scalers[col] = scaler</p>

<p>elif transform_type == ‘robust’:
 scaler = RobustScaler()
 df[col] = scaler.fit_transform(df[[col]])
 self.scalers[col] = scaler</p>

<p>elif transform_type == ‘log’:
 df[col] = np.log1p(df[col]) # log(1 + x) to handle 0</p>

<p>elif transform_type == ‘sqrt’:
 df[col] = np.sqrt(df[col])</p>

<p>elif transform_type == ‘boxcox’:
 # Box-Cox transform (requires positive values)
 df[col], _ = stats.boxcox(df[col] + 1) # +1 to handle 0</p>

<p>return df
``</p>

<h3 id="categorical-encoding">Categorical Encoding</h3>

<p>``python
class CategoricalEncoder:
 “””
 Encode categorical variables
 “””</p>

<p>def <strong>init</strong>(self):
 self.encoders = {}</p>

<p>def fit_transform(self, df: pd.DataFrame, encodings: Dict[str, str]):
 “””
 Apply encodings</p>

<p>encodings: {column: encoding_type}
 ‘onehot’: One-hot encoding
 ‘label’: Label encoding (0, 1, 2, …)
 ‘target’: Target encoding (mean of target per category)
 ‘frequency’: Frequency encoding
 ‘ordinal’: Ordinal encoding with custom order
 “””
 df = df.copy()</p>

<p>for col, encoding_type in encodings.items():
 if col not in df.columns:
 continue</p>

<p>if encoding_type == ‘onehot’:
 # One-hot encoding
 dummies = pd.get_dummies(df[col], prefix=col)
 df = pd.concat([df, dummies], axis=1)
 df.drop(col, axis=1, inplace=True)
 self.encoders[col] = list(dummies.columns)</p>

<p>elif encoding_type == ‘label’:
 # Label encoding
 categories = df[col].unique()
 mapping = {cat: idx for idx, cat in enumerate(categories)}
 df[col] = df[col].map(mapping)
 self.encoders[col] = mapping</p>

<p>elif encoding_type == ‘frequency’:
 # Frequency encoding
 freq = df[col].value_counts(normalize=True)
 df[col] = df[col].map(freq)
 self.encoders[col] = freq</p>

<p>return df
``</p>

<h3 id="temporal-features">Temporal Features</h3>

<p>``python
class TemporalFeatureExtractor:
 “””
 Extract features from timestamps
 “””</p>

<p>def extract(self, df: pd.DataFrame, timestamp_col: str) -&gt; pd.DataFrame:
 “””
 Extract temporal features from timestamp column
 “””
 df = df.copy()
 df[timestamp_col] = pd.to_datetime(df[timestamp_col])</p>

<p># Basic temporal features
 df[f’{timestamp_col}_hour’] = df[timestamp_col].dt.hour
 df[f’{timestamp_col}_day_of_week’] = df[timestamp_col].dt.dayofweek
 df[f’{timestamp_col}_day_of_month’] = df[timestamp_col].dt.day
 df[f’{timestamp_col}_month’] = df[timestamp_col].dt.month
 df[f’{timestamp_col}_quarter’] = df[timestamp_col].dt.quarter
 df[f’{timestamp_col}_year’] = df[timestamp_col].dt.year</p>

<p># Derived features
 df[f’{timestamp_col}_is_weekend’] = df[f’{timestamp_col}_day_of_week’].isin([5, 6]).astype(int)
 df[f’{timestamp_col}_is_business_hours’] = df[f’{timestamp_col}_hour’].between(9, 17).astype(int)</p>

<p># Cyclical encoding (for periodic features like hour)
 df[f’{timestamp_col}_hour_sin’] = np.sin(2 * np.pi * df[f’{timestamp_col}_hour’] / 24)
 df[f’{timestamp_col}_hour_cos’] = np.cos(2 * np.pi * df[f’{timestamp_col}_hour’] / 24)</p>

<p>return df
``</p>

<hr />

<h2 id="component-4-pipeline-orchestration">Component 4: Pipeline Orchestration</h2>

<p>Orchestrate the entire preprocessing workflow.</p>

<h3 id="apache-beam-pipeline">Apache Beam Pipeline</h3>

<p>``python
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions</p>

<p>class PreprocessingPipeline:
 “””
 End-to-end preprocessing pipeline using Apache Beam</p>

<p>Handles:</p>
<ul>
  <li>Data validation</li>
  <li>Cleaning</li>
  <li>Feature engineering</li>
  <li>Quality checks
 “””</li>
</ul>

<p>def <strong>init</strong>(self, pipeline_options: PipelineOptions):
 self.options = pipeline_options</p>

<p>def run(self, input_path: str, output_path: str):
 “””
 Run preprocessing pipeline
 “””
 with beam.Pipeline(options=self.options) as pipeline:
 (
 pipeline
 | ‘Read Data’ » beam.io.ReadFromText(input_path)
 | ‘Parse JSON’ » beam.Map(json.loads)
 | ‘Validate Schema’ » beam.ParDo(ValidateSchemaFn())
 | ‘Clean Data’ » beam.ParDo(CleanDataFn())
 | ‘Extract Features’ » beam.ParDo(FeatureExtractionFn())
 | ‘Quality Check’ » beam.ParDo(QualityCheckFn())
 | ‘Write Output’ » beam.io.WriteToText(output_path)
 )</p>

<p>class ValidateSchemaFn(beam.DoFn):
 “"”Beam DoFn for schema validation”””</p>

<p>def process(self, element):
 # Lazily initialize schema validator (avoid re-creating per element)
 if not hasattr(self, ‘validator’):
 self.validator = SchemaValidator(get_schema())
 errors = self.validator.validate_record(element)</p>

<p>if errors:
 # Log to dead letter queue
 yield beam.pvalue.TaggedOutput(‘invalid’, (element, errors))
 else:
 yield element</p>

<p>class CleanDataFn(beam.DoFn):
 “"”Beam DoFn for data cleaning”””</p>

<p>def process(self, element):
 # Handle missing values
 element = handle_missing(element)</p>

<p># Handle outliers
 element = handle_outliers(element)</p>

<p># Remove duplicates (stateful processing)
 # …</p>

<p>yield element
``</p>

<hr />

<h2 id="preventing-trainingserving-skew">Preventing Training/Serving Skew</h2>

<p><strong>Critical problem:</strong> Different preprocessing in training vs serving leads to poor model performance.</p>

<h3 id="solution-1-unified-preprocessing-library">Solution 1: Unified Preprocessing Library</h3>

<p>``python
class PreprocessorV1:
 “””
 Versioned preprocessing logic</p>

<p>Same code used in training and serving
 “””</p>

<p>VERSION = “1.0.0”</p>

<p>def <strong>init</strong>(self, config: Dict):
 self.config = config
 self.fitted_params = {}</p>

<p>def fit(self, df: pd.DataFrame):
 “"”Fit on training data”””
 # Compute statistics needed for transform
 self.fitted_params[‘age_mean’] = df[‘age’].mean()
 self.fitted_params[‘price_scaler’] = MinMaxScaler().fit(df[[‘price’]])
 # …</p>

<p>def transform(self, df: pd.DataFrame) -&gt; pd.DataFrame:
 “"”Apply same transformations”””
 df = df.copy()</p>

<p># Use fitted parameters
 df[‘age_normalized’] = (df[‘age’] - self.fitted_params[‘age_mean’]) / 10
 df[‘price_scaled’] = self.fitted_params[‘price_scaler’].transform(df[[‘price’]])</p>

<p>return df</p>

<p>def save(self, path: str):
 “"”Save fitted preprocessor”””
 import pickle
 with open(path, ‘wb’) as f:
 pickle.dump(self, f)</p>

<p>@staticmethod
 def load(path: str):
 “"”Load fitted preprocessor”””
 import pickle
 with open(path, ‘rb’) as f:
 return pickle.load(f)</p>

<h1 id="training">Training</h1>
<p>preprocessor = PreprocessorV1(config)
preprocessor.fit(training_data)
preprocessor.save(‘models/preprocessor_v1.pkl’)
X_train = preprocessor.transform(training_data)</p>

<h1 id="serving">Serving</h1>
<p>preprocessor = PreprocessorV1.load(‘models/preprocessor_v1.pkl’)
X_serve = preprocessor.transform(serving_data)
``</p>

<h3 id="solution-2-feature-store">Solution 2: Feature Store</h3>

<p>Store pre-computed features, ensuring consistency.</p>

<p>``python
class FeatureStore:
 “””
 Centralized feature storage</p>

<p>Benefits:</p>
<ul>
  <li>Features computed once, used everywhere</li>
  <li>Versioned features</li>
  <li>Point-in-time correct joins
 “””</li>
</ul>

<p>def <strong>init</strong>(self, backend):
 self.backend = backend</p>

<p>def write_features(
 self, 
 entity_id: str,
 features: Dict[str, Any],
 timestamp: datetime,
 feature_set_name: str,
 version: str
 ):
 “””
 Write features for an entity
 “””
 key = f”{feature_set_name}:{version}:{entity_id}:{timestamp}”
 self.backend.write(key, features)</p>

<p>def read_features(
 self,
 entity_id: str,
 feature_set_name: str,
 version: str,
 as_of_timestamp: datetime
 ) -&gt; Dict[str, Any]:
 “””
 Read features as of a specific timestamp</p>

<p>Point-in-time correctness: Only use features available at inference time
 “””
 # Query features created before as_of_timestamp
 features = self.backend.read_point_in_time(
 entity_id,
 feature_set_name,
 version,
 as_of_timestamp
 )</p>

<p>return features
``</p>

<hr />

<h2 id="monitoring--data-quality">Monitoring &amp; Data Quality</h2>

<p>Track data quality metrics over time.</p>

<p>``python
from dataclasses import dataclass
from datetime import datetime</p>

<p>@dataclass
class DataQualityMetrics:
 “"”Metrics for a data batch”””
 timestamp: datetime
 total_records: int
 null_counts: Dict[str, int]
 duplicate_count: int
 schema_errors: int
 outlier_counts: Dict[str, int]
 statistical_warnings: List[str]</p>

<p>class DataQualityMonitor:
 “””
 Monitor data quality over time
 “””</p>

<p>def <strong>init</strong>(self, metrics_backend):
 self.backend = metrics_backend</p>

<p>def compute_metrics(self, df: pd.DataFrame) -&gt; DataQualityMetrics:
 “"”Compute quality metrics for a batch”””</p>

<p>metrics = DataQualityMetrics(
 timestamp=datetime.now(),
 total_records=len(df),
 null_counts={col: df[col].isnull().sum() for col in df.columns},
 duplicate_count=df.duplicated().sum(),
 schema_errors=0, # From validation
 outlier_counts={},
 statistical_warnings=[]
 )</p>

<p># Detect outliers
 outlier_handler = OutlierHandler()
 for col in df.select_dtypes(include=[np.number]).columns:
 outliers = outlier_handler.detect_outliers_iqr(df[col])
 metrics.outlier_counts[col] = outliers.sum()</p>

<p>return metrics</p>

<p>def log_metrics(self, metrics: DataQualityMetrics):
 “"”Log metrics to monitoring system”””
 self.backend.write(metrics)</p>

<p>def alert_on_anomalies(self, metrics: DataQualityMetrics):
 “"”Alert if metrics deviate significantly”””</p>

<p># Alert if &gt; 5% nulls in critical fields
 critical_fields = [‘user_id’, ‘timestamp’, ‘label’]
 for field in critical_fields:
 null_rate = metrics.null_counts.get(field, 0) / metrics.total_records
 if null_rate &gt; 0.05:
 self.send_alert(f”High null rate in {field}: {null_rate:.2%}”)</p>

<p># Alert if &gt; 10% duplicates
 dup_rate = metrics.duplicate_count / metrics.total_records
 if dup_rate &gt; 0.10:
 self.send_alert(f”High duplicate rate: {dup_rate:.2%}”)
``</p>

<hr />

<h2 id="real-world-examples">Real-World Examples</h2>

<h3 id="netflix-data-preprocessing-for-recommendations">Netflix: Data Preprocessing for Recommendations</h3>

<p><strong>Scale:</strong> Billions of viewing events/day</p>

<p><strong>Architecture:</strong>
``
Event Stream (Kafka)
 ↓
Flink/Spark Streaming
 ↓
Feature Engineering</p>
<ul>
  <li>User viewing history aggregations</li>
  <li>Time-based features</li>
  <li>Content embeddings
 ↓
Feature Store (Cassandra)
 ↓
Model Training &amp; Serving
``</li>
</ul>

<p><strong>Key techniques:</strong></p>
<ul>
  <li>Streaming aggregations (last 7 days views, etc.)</li>
  <li>Incremental updates to user profiles</li>
  <li>Point-in-time correct features</li>
</ul>

<h3 id="uber-preprocessing-for-etas">Uber: Preprocessing for ETAs</h3>

<p><strong>Challenge:</strong> Predict arrival times using GPS data</p>

<p><strong>Pipeline:</strong></p>
<ol>
  <li><strong>Map Matching:</strong> Snap GPS points to road network</li>
  <li><strong>Outlier Removal:</strong> Remove impossible speeds</li>
  <li><strong>Feature Extraction:</strong>
    <ul>
      <li>Time of day, day of week</li>
      <li>Traffic conditions</li>
      <li>Historical average speed</li>
    </ul>
  </li>
  <li><strong>Validation:</strong> Check for data drift</li>
</ol>

<p><strong>Latency:</strong> &lt; 100ms for real-time predictions</p>

<h3 id="google-search-ranking-data-pipeline">Google: Search Ranking Data Pipeline</h3>

<p><strong>Scale:</strong> Process billions of queries and web pages</p>

<p><strong>Preprocessing steps:</strong></p>
<ol>
  <li><strong>Query normalization:</strong> Lowercasing, tokenization, spelling correction</li>
  <li><strong>Feature extraction from documents:</strong>
    <ul>
      <li>PageRank scores</li>
      <li>Content embeddings (BERT)</li>
      <li>Click-through rate (CTR) features</li>
    </ul>
  </li>
  <li><strong>User context features:</strong>
    <ul>
      <li>Location</li>
      <li>Device type</li>
      <li>Search history embeddings</li>
    </ul>
  </li>
  <li><strong>Join multiple data sources:</strong>
    <ul>
      <li>User profile data</li>
      <li>Document metadata</li>
      <li>Real-time signals (freshness)</li>
    </ul>
  </li>
</ol>

<p><strong>Key insight:</strong> Distributed processing using MapReduce/Dataflow for petabyte-scale data.</p>

<hr />

<h2 id="distributed-preprocessing-with-spark">Distributed Preprocessing with Spark</h2>

<p>When data doesn’t fit on one machine, use distributed frameworks.</p>

<h3 id="spark-preprocessing-pipeline">Spark Preprocessing Pipeline</h3>

<p>``python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, mean, stddev, count
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml import Pipeline</p>

<p>class DistributedPreprocessor:
 “””
 Large-scale preprocessing using Apache Spark</p>

<p>Use case: Process 1TB+ data across cluster
 “””</p>

<p>def <strong>init</strong>(self):
 self.spark = SparkSession.builder <br />
 .appName(“MLPreprocessing”) <br />
 .getOrCreate()</p>

<p>def load_data(self, path: str, format=’parquet’):
 “"”Load data from distributed storage”””
 return self.spark.read.format(format).load(path)</p>

<p>def clean_data(self, df):
 “"”Distributed data cleaning”””</p>

<p># Remove nulls
 df = df.dropna(subset=[‘user_id’, ‘timestamp’])</p>

<p># Handle outliers (clip at 99th percentile)
 for col_name in [‘price’, ‘quantity’]:
 quantile_99 = df.approxQuantile(col_name, [0.99], 0.01)[0]
 df = df.withColumn(
 col_name,
 when(col(col_name) &gt; quantile_99, quantile_99).otherwise(col(col_name))
 )</p>

<p># Remove duplicates
 df = df.dropDuplicates([‘user_id’, ‘item_id’, ‘timestamp’])</p>

<p>return df</p>

<p>def feature_engineering(self, df):
 “"”Distributed feature engineering”””</p>

<p># Time-based features
 df = df.withColumn(‘hour’, hour(col(‘timestamp’)))
 df = df.withColumn(‘day_of_week’, dayofweek(col(‘timestamp’)))
 df = df.withColumn(‘is_weekend’, 
 when(col(‘day_of_week’).isin([1, 7]), 1).otherwise(0))</p>

<p># Aggregation features (window functions)
 from pyspark.sql.window import Window</p>

<p># User’s average purchase price (last 30 days)
 window_30d = Window.partitionBy(‘user_id’) <br />
 .orderBy(col(‘timestamp’).cast(‘long’)) <br />
 .rangeBetween(-30<em>24</em>3600, 0)</p>

<p>df = df.withColumn(‘user_avg_price_30d’, 
 avg(‘price’).over(window_30d))</p>

<p>return df</p>

<p>def normalize_features(self, df, numeric_cols):
 “"”Normalize numeric features”””</p>

<p># Assemble features into vector
 assembler = VectorAssembler(
 inputCols=numeric_cols,
 outputCol=’features_raw’
 )</p>

<p># Standard scaling
 scaler = StandardScaler(
 inputCol=’features_raw’,
 outputCol=’features_scaled’,
 withMean=True,
 withStd=True
 )</p>

<p># Create pipeline
 pipeline = Pipeline(stages=[assembler, scaler])</p>

<p># Fit and transform
 model = pipeline.fit(df)
 df = model.transform(df)</p>

<p>return df, model</p>

<p>def save_preprocessed(self, df, output_path, model_path):
 “"”Save preprocessed data and fitted model”””</p>

<p># Save data (partitioned for efficiency)
 df.write.mode(‘overwrite’) <br />
 .partitionBy(‘date’) <br />
 .parquet(output_path)</p>

<p># Save preprocessing model for serving
 # model.save(model_path)</p>

<h1 id="usage-1">Usage</h1>
<p>preprocessor = DistributedPreprocessor()
df = preprocessor.load_data(‘s3://bucket/raw_data/’)
df = preprocessor.clean_data(df)
df = preprocessor.feature_engineering(df)
df, model = preprocessor.normalize_features(df, [‘price’, ‘quantity’])
preprocessor.save_preprocessed(df, ‘s3://bucket/processed/’, ‘s3://bucket/models/’)
``</p>

<hr />

<h2 id="advanced-feature-engineering-patterns">Advanced Feature Engineering Patterns</h2>

<h3 id="1-time-series-features">1. Time-Series Features</h3>

<p>``python
class TimeSeriesFeatureExtractor:
 “””
 Extract features from time-series data</p>

<p>Use case: User engagement over time, sensor readings, stock prices
 “””</p>

<p>def extract_lag_features(self, df, value_col, lag_periods=[1, 7, 30]):
 “"”Create lagged features”””
 for lag in lag_periods:
 df[f’{value_col}<em>lag</em>{lag}’] = df.groupby(‘user_id’)[value_col].shift(lag)
 return df</p>

<p>def extract_rolling_statistics(self, df, value_col, windows=[7, 30]):
 “"”Rolling mean, std, min, max”””
 for window in windows:
 df[f’{value_col}<em>rolling_mean</em>{window}’] = <br />
 df.groupby(‘user_id’)[value_col].transform(
 lambda x: x.rolling(window, min_periods=1).mean()
 )
 df[f’{value_col}<em>rolling_std</em>{window}’] = <br />
 df.groupby(‘user_id’)[value_col].transform(
 lambda x: x.rolling(window, min_periods=1).std()
 )
 return df</p>

<p>def extract_trend_features(self, df, value_col):
 “””
 Trend: difference from moving average
 “””
 df[‘rolling_mean_7’] = df.groupby(‘user_id’)[value_col].transform(
 lambda x: x.rolling(7, min_periods=1).mean()
 )
 df[f’{value_col}_trend’] = df[value_col] - df[‘rolling_mean_7’]
 return df
``</p>

<h3 id="2-interaction-features">2. Interaction Features</h3>

<p>``python
class InteractionFeatureGenerator:
 “””
 Create interaction features between variables</p>

<p>Captures relationships not visible in individual features
 “””</p>

<p>def polynomial_features(self, df, cols, degree=2):
 “””
 Create polynomial features</p>

<p>Example: x, y → x, y, x², y², xy
 “””
 from sklearn.preprocessing import PolynomialFeatures</p>

<p>poly = PolynomialFeatures(degree=degree, include_bias=False)
 poly_features = poly.fit_transform(df[cols])</p>

<p>feature_names = poly.get_feature_names_out(cols)
 poly_df = pd.DataFrame(poly_features, columns=feature_names)</p>

<p>return pd.concat([df, poly_df], axis=1)</p>

<p>def ratio_features(self, df, numerator_cols, denominator_cols):
 “””
 Create ratio features</p>

<p>Example: revenue/cost, clicks/impressions (CTR)
 “””
 for num_col in numerator_cols:
 for den_col in denominator_cols:
 df[f’{num_col}<em>per</em>{den_col}’] = df[num_col] / (df[den_col] + 1e-9)
 return df</p>

<p>def categorical_interactions(self, df, cat_cols):
 “””
 Combine categorical variables</p>

<p>Example: city=’SF’, category=’Tech’ → ‘SF_Tech’
 “””
 if len(cat_cols) &gt;= 2:
 df[‘<em>‘.join(cat_cols)] = df[cat_cols].astype(str).agg(‘</em>‘.join, axis=1)
 return df
``</p>

<h3 id="3-embedding-features">3. Embedding Features</h3>

<p>``python
class EmbeddingFeatureGenerator:
 “””
 Generate embedding features from high-cardinality categoricals</p>

<p>Use case: user_id, item_id, text
 “””</p>

<p>def train_category_embeddings(self, df, category_col, embedding_dim=50):
 “””
 Train embeddings for categorical variable</p>

<p>Uses skip-gram approach: predict co-occurring categories
 “””
 from gensim.models import Word2Vec</p>

<p># Create sequences (e.g., user’s purchase history)
 sequences = df.groupby(‘user_id’)[category_col].apply(list).tolist()</p>

<p># Train Word2Vec
 model = Word2Vec(
 sentences=sequences,
 vector_size=embedding_dim,
 window=5,
 min_count=1,
 workers=4
 )</p>

<p># Get embeddings
 embeddings = {}
 for category in df[category_col].unique():
 if category in model.wv:
 embeddings[category] = model.wv[category]</p>

<p>return embeddings</p>

<p>def text_to_embeddings(self, df, text_col, model=’sentence-transformers’):
 “””
 Convert text to dense embeddings</p>

<p>Use pre-trained models (BERT, etc.)
 “””
 from sentence_transformers import SentenceTransformer</p>

<p>model = SentenceTransformer(‘all-MiniLM-L6-v2’)
 embeddings = model.encode(df[text_col].tolist())</p>

<p># Add as features
 for i in range(embeddings.shape[1]):
 df[f’{text_col}<em>emb</em>{i}’] = embeddings[:, i]</p>

<p>return df
``</p>

<hr />

<h2 id="handling-data-drift">Handling Data Drift</h2>

<p>Data distributions change over time - models degrade if not monitored.</p>

<h3 id="drift-detection">Drift Detection</h3>

<p>``python
from scipy.stats import ks_2samp, chi2_contingency</p>

<p>class DataDriftDetector:
 “””
 Detect when data distribution changes
 “””</p>

<p>def <strong>init</strong>(self, reference_data: pd.DataFrame):
 “””
 Args:
 reference_data: Historical “good” data (training distribution)
 “””
 self.reference = reference_data</p>

<p>def detect_numerical_drift(self, current_data: pd.DataFrame, col: str, threshold=0.05):
 “””
 Kolmogorov-Smirnov test for numerical columns</p>

<p>Returns:
 (drifted: bool, p_value: float)
 “””
 ref_values = self.reference[col].dropna()
 curr_values = current_data[col].dropna()</p>

<p>statistic, p_value = ks_2samp(ref_values, curr_values)</p>

<p>drifted = p_value &lt; threshold</p>

<p>return drifted, p_value</p>

<p>def detect_categorical_drift(self, current_data: pd.DataFrame, col: str, threshold=0.05):
 “””
 Chi-square test for categorical columns
 “””
 ref_dist = self.reference[col].value_counts(normalize=True)
 curr_dist = current_data[col].value_counts(normalize=True)</p>

<p># Align distributions
 all_categories = set(ref_dist.index) | set(curr_dist.index)
 ref_counts = [ref_dist.get(cat, 0) * len(self.reference) for cat in all_categories]
 curr_counts = [curr_dist.get(cat, 0) * len(current_data) for cat in all_categories]</p>

<p># Chi-square test
 contingency_table = [ref_counts, curr_counts]
 chi2, p_value, dof, expected = chi2_contingency(contingency_table)</p>

<p>drifted = p_value &lt; threshold</p>

<p>return drifted, p_value</p>

<p>def detect_all_drifts(self, current_data: pd.DataFrame):
 “””
 Check all columns for drift
 “””
 drifts = {}</p>

<p># Numerical columns
 for col in current_data.select_dtypes(include=[np.number]).columns:
 drifted, p_value = self.detect_numerical_drift(current_data, col)
 if drifted:
 drifts[col] = {‘type’: ‘numerical’, ‘p_value’: p_value}</p>

<p># Categorical columns
 for col in current_data.select_dtypes(include=[‘object’, ‘category’]).columns:
 drifted, p_value = self.detect_categorical_drift(current_data, col)
 if drifted:
 drifts[col] = {‘type’: ‘categorical’, ‘p_value’: p_value}</p>

<p>return drifts</p>

<h1 id="usage-2">Usage</h1>
<p>detector = DataDriftDetector(training_data)
drifts = detector.detect_all_drifts(current_production_data)</p>

<p>if drifts:
 print(“⚠️ Data drift detected in:”, drifts.keys())
 # Trigger retraining or alert
``</p>

<hr />

<h2 id="production-best-practices">Production Best Practices</h2>

<h3 id="1-idempotency">1. Idempotency</h3>

<p>Ensure pipeline can be re-run safely without side effects.</p>

<p>``python
class IdempotentPipeline:
 “””
 Pipeline that can be safely re-run
 “””</p>

<p>def process_batch(self, batch_id: str, input_path: str, output_path: str):
 “””
 Process a batch idempotently
 “””
 # Check if already processed
 if self.is_processed(batch_id):
 print(f”Batch {batch_id} already processed, skipping”)
 return</p>

<p># Process
 data = self.load(input_path)
 processed = self.transform(data)</p>

<p># Write with batch ID
 self.save_with_checksum(processed, output_path, batch_id)</p>

<p># Mark as complete
 self.mark_processed(batch_id)</p>

<p>def is_processed(self, batch_id: str) -&gt; bool:
 “"”Check if batch already processed”””
 # Query metadata store
 return self.metadata_store.exists(batch_id)</p>

<p>def mark_processed(self, batch_id: str):
 “"”Mark batch as processed”””
 self.metadata_store.write(batch_id, timestamp=datetime.now())
``</p>

<h3 id="2-data-versioning">2. Data Versioning</h3>

<p>Track versions of datasets and transformations.</p>

<p>``python
class VersionedDataset:
 “””
 Version datasets for reproducibility
 “””</p>

<p>def save(self, df: pd.DataFrame, name: str, version: str):
 “””
 Save versioned dataset</p>

<p>Path: s3://bucket/{name}/{version}/data.parquet
 “””
 path = f”s3://bucket/{name}/{version}/data.parquet”</p>

<p># Save data
 df.to_parquet(path)</p>

<p># Save metadata
 metadata = {
 ‘name’: name,
 ‘version’: version,
 ‘timestamp’: datetime.now().isoformat(),
 ‘num_rows’: len(df),
 ‘num_cols’: len(df.columns),
 ‘schema’: df.dtypes.to_dict(),
 ‘checksum’: self.compute_checksum(df)
 }</p>

<p>self.save_metadata(name, version, metadata)</p>

<p>def load(self, name: str, version: str) -&gt; pd.DataFrame:
 “"”Load specific version”””
 path = f”s3://bucket/{name}/{version}/data.parquet”
 return pd.read_parquet(path)
``</p>

<h3 id="3-lineage-tracking">3. Lineage Tracking</h3>

<p>Track data transformations for debugging and compliance.</p>

<p>``python
class LineageTracker:
 “””
 Track data lineage
 “””</p>

<p>def <strong>init</strong>(self):
 self.graph = {}</p>

<p>def record_transformation(
 self, 
 input_datasets: List[str],
 output_dataset: str,
 transformation_code: str,
 parameters: Dict
 ):
 “””
 Record a transformation
 “””
 self.graph[output_dataset] = {
 ‘inputs’: input_datasets,
 ‘transformation’: transformation_code,
 ‘parameters’: parameters,
 ‘timestamp’: datetime.now()
 }</p>

<p>def get_lineage(self, dataset: str) -&gt; Dict:
 “””
 Get full lineage of a dataset</p>

<p>Returns tree of upstream datasets and transformations
 “””
 if dataset not in self.graph:
 return {‘dataset’: dataset, ‘inputs’: []}</p>

<p>node = self.graph[dataset]</p>

<p>return {
 ‘dataset’: dataset,
 ‘transformation’: node[‘transformation’],
 ‘inputs’: [self.get_lineage(inp) for inp in node[‘inputs’]]
 }
``</p>

<hr />

<h2 id="common-preprocessing-challenges--solutions">Common Preprocessing Challenges &amp; Solutions</h2>

<h3 id="challenge-1-imbalanced-classes">Challenge 1: Imbalanced Classes</h3>

<p><strong>Problem:</strong> 95% of samples are class 0, 5% are class 1. Model always predicts class 0.</p>

<p><strong>Solutions:</strong></p>

<p>``python
class ImbalanceHandler:
 “””
 Handle class imbalance
 “””</p>

<p>def upsample_minority(self, df, target_col):
 “””
 Oversample minority class
 “””
 from sklearn.utils import resample</p>

<p># Separate majority and minority classes
 df_majority = df[df[target_col] == 0]
 df_minority = df[df[target_col] == 1]</p>

<p># Upsample minority class
 df_minority_upsampled = resample(
 df_minority,
 replace=True, # Sample with replacement
 n_samples=len(df_majority), # Match majority class size
 random_state=42
 )</p>

<p># Combine
 df_balanced = pd.concat([df_majority, df_minority_upsampled])</p>

<p>return df_balanced</p>

<p>def downsample_majority(self, df, target_col):
 “””
 Undersample majority class
 “””
 df_majority = df[df[target_col] == 0]
 df_minority = df[df[target_col] == 1]</p>

<p># Downsample majority class
 df_majority_downsampled = resample(
 df_majority,
 replace=False,
 n_samples=len(df_minority),
 random_state=42
 )</p>

<p>df_balanced = pd.concat([df_majority_downsampled, df_minority])</p>

<p>return df_balanced</p>

<p>def smote(self, X, y):
 “””
 Synthetic Minority Over-sampling Technique</p>

<p>Generate synthetic samples for minority class
 “””
 from imblearn.over_sampling import SMOTE</p>

<p>smote = SMOTE(random_state=42)
 X_resampled, y_resampled = smote.fit_resample(X, y)</p>

<p>return X_resampled, y_resampled
``</p>

<h3 id="challenge-2-high-cardinality-categoricals">Challenge 2: High-Cardinality Categoricals</h3>

<p><strong>Problem:</strong> User IDs have 10M unique values. One-hot encoding creates 10M columns.</p>

<p><strong>Solutions:</strong></p>

<p>``python
class HighCardinalityEncoder:
 “””
 Handle high-cardinality categorical features
 “””</p>

<p>def target_encoding(self, df, cat_col, target_col):
 “””
 Encode category by mean of target</p>

<p>Example:
 city=’SF’ → mean(target | city=’SF’) = 0.65
 city=’NY’ → mean(target | city=’NY’) = 0.52</p>

<p>Warning: Risk of overfitting. Use cross-validation encoding.
 “””
 # Compute target mean per category
 target_means = df.groupby(cat_col)[target_col].mean()</p>

<p># Map
 df[f’{cat_col}_target_enc’] = df[cat_col].map(target_means)</p>

<p>return df</p>

<p>def frequency_encoding(self, df, cat_col):
 “””
 Encode by frequency</p>

<p>Common categories → higher values
 “””
 freq = df[cat_col].value_counts(normalize=True)
 df[f’{cat_col}_freq’] = df[cat_col].map(freq)</p>

<p>return df</p>

<p>def hashing_trick(self, df, cat_col, n_features=100):
 “””
 Hash categories into fixed number of buckets</p>

<p>Pros: Fixed dimension
 Cons: Hash collisions
 “””
 from sklearn.feature_extraction import FeatureHasher</p>

<p>hasher = FeatureHasher(n_features=n_features, input_type=’string’)
 hashed = hasher.transform(df[[cat_col]].astype(str).values)</p>

<p># Convert to DataFrame
 hashed_df = pd.DataFrame(
 hashed.toarray(),
 columns=[f’{cat_col}<em>hash</em>{i}’ for i in range(n_features)]
 )</p>

<p>return pd.concat([df, hashed_df], axis=1)
``</p>

<h3 id="challenge-3-streaming-data-preprocessing">Challenge 3: Streaming Data Preprocessing</h3>

<p><strong>Problem:</strong> Need to preprocess real-time streams with low latency.</p>

<p><strong>Solution:</strong></p>

<p>``python
from kafka import KafkaConsumer, KafkaProducer
import json</p>

<p>class StreamingPreprocessor:
 “””
 Real-time preprocessing for streaming data
 “””</p>

<p>def <strong>init</strong>(self):
 self.consumer = KafkaConsumer(
 ‘raw_events’,
 bootstrap_servers=[‘localhost:9092’],
 value_deserializer=lambda m: json.loads(m.decode(‘utf-8’))
 )</p>

<p>self.producer = KafkaProducer(
 bootstrap_servers=[‘localhost:9092’],
 value_serializer=lambda v: json.dumps(v).encode(‘utf-8’)
 )</p>

<p># Load fitted preprocessor (from training)
 self.preprocessor = PreprocessorV1.load(‘models/preprocessor_v1.pkl’)</p>

<p>def process_stream(self):
 “””
 Process events in real-time
 “””
 for message in self.consumer:
 event = message.value</p>

<p># Preprocess
 processed = self.preprocess_event(event)</p>

<p># Validate
 if self.validate(processed):
 # Send to processed topic
 self.producer.send(‘processed_events’, processed)</p>

<p>def preprocess_event(self, event):
 “””
 Preprocess single event (must be fast!)
 “””
 # Convert to DataFrame
 df = pd.DataFrame([event])</p>

<p># Apply preprocessing
 df = self.preprocessor.transform(df)</p>

<p># Convert back to dict
 return df.to_dict(‘records’)[0]</p>

<p>def validate(self, event):
 “"”Quick validation”””
 required_fields = [‘user_id’, ‘timestamp’, ‘features’]
 return all(field in event for field in required_fields)
``</p>

<h3 id="challenge-4-privacy--compliance-gdpr-ccpa">Challenge 4: Privacy &amp; Compliance (GDPR, CCPA)</h3>

<p><strong>Problem:</strong> Need to handle PII (Personally Identifiable Information).</p>

<p><strong>Solutions:</strong></p>

<p>``python
import hashlib</p>

<p>class PrivacyPreserver:
 “””
 Handle PII in preprocessing
 “””</p>

<p>def anonymize_user_ids(self, df, id_col=’user_id’):
 “””
 Hash user IDs to anonymize
 “””
 df[f’{id_col}_anonymized’] = df[id_col].apply(
 lambda x: hashlib.sha256(str(x).encode()).hexdigest()
 )
 df.drop(id_col, axis=1, inplace=True)
 return df</p>

<p>def remove_pii(self, df, pii_cols=[‘email’, ‘phone’, ‘address’]):
 “””
 Remove PII columns
 “””
 df.drop(pii_cols, axis=1, inplace=True, errors=’ignore’)
 return df</p>

<p>def differential_privacy_noise(self, df, numeric_cols, epsilon=1.0):
 “””
 Add Laplacian noise for differential privacy</p>

<p>Args:
 epsilon: Privacy parameter (lower = more privacy, less utility)
 “””
 for col in numeric_cols:
 sensitivity = df[col].max() - df[col].min()
 noise_scale = sensitivity / epsilon</p>

<p>noise = np.random.laplace(0, noise_scale, size=len(df))
 df[col] = df[col] + noise</p>

<p>return df
``</p>

<hr />

<h2 id="performance-optimization">Performance Optimization</h2>

<h3 id="1-parallelize-transformations">1. Parallelize Transformations</h3>

<p>``python
from multiprocessing import Pool
import numpy as np</p>

<p>class ParallelPreprocessor:
 “””
 Parallelize preprocessing across CPU cores
 “””</p>

<p>def <strong>init</strong>(self, n_workers=4):
 self.n_workers = n_workers</p>

<p>def process_parallel(self, df, transform_fn):
 “””
 Apply transformation in parallel
 “””
 # Split dataframe into chunks
 chunks = np.array_split(df, self.n_workers)</p>

<p># Process chunks in parallel
 with Pool(self.n_workers) as pool:
 processed_chunks = pool.map(transform_fn, chunks)</p>

<p># Combine results
 return pd.concat(processed_chunks)
``</p>

<h3 id="2-use-efficient-data-formats">2. Use Efficient Data Formats</h3>

<p>``python</p>
<h1 id="bad-csv-slow-to-readwrite-no-compression">Bad: CSV (slow to read/write, no compression)</h1>
<p>df.to_csv(‘data.csv’) # 1 GB file, 60 seconds</p>

<h1 id="better-parquet-columnar-compressed">Better: Parquet (columnar, compressed)</h1>
<p>df.to_parquet(‘data.parquet’) # 200 MB file, 5 seconds</p>

<h1 id="best-for-streaming-avro-or-protocol-buffers">Best for streaming: Avro or Protocol Buffers</h1>
<p>``</p>

<h3 id="3-cache-intermediate-results">3. Cache Intermediate Results</h3>

<p>``python
class CachedPreprocessor:
 “””
 Cache preprocessing results
 “””</p>

<p>def <strong>init</strong>(self, cache_dir=’./cache’):
 self.cache_dir = cache_dir</p>

<p>def process_with_cache(self, df, batch_id):
 “””
 Check cache before processing
 “””
 cache_path = f”{self.cache_dir}/{batch_id}.parquet”</p>

<p>if os.path.exists(cache_path):
 print(f”Loading from cache: {batch_id}”)
 return pd.read_parquet(cache_path)</p>

<p># Process
 processed = self.preprocess(df)</p>

<p># Save to cache
 processed.to_parquet(cache_path)</p>

<p>return processed
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Data quality is critical</strong> - bad data → bad models 
✅ <strong>Schema validation</strong> catches errors early before expensive processing 
✅ <strong>Handle missing values</strong> with domain-appropriate strategies (mean/median/forward-fill) 
✅ <strong>Feature engineering</strong> is where domain knowledge creates value 
✅ <strong>Prevent training/serving skew</strong> with unified preprocessing code 
✅ <strong>Monitor data quality</strong> continuously - detect drift and anomalies 
✅ <strong>Use feature stores</strong> for consistency and reuse at scale 
✅ <strong>Distributed processing</strong> (Spark/Beam) required for large-scale data 
✅ <strong>Version datasets and transformations</strong> for reproducibility 
✅ <strong>Track data lineage</strong> for debugging and compliance 
✅ <strong>Handle class imbalance</strong> with resampling or SMOTE 
✅ <strong>Encode high-cardinality categoricals</strong> with target/frequency encoding or hashing 
✅ <strong>Optimize performance</strong> with parallel processing, efficient formats, caching</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0003-data-preprocessing/">arunbaby.com/ml-system-design/0003-data-preprocessing</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#data-preprocessing" class="page__taxonomy-item p-category" rel="tag">data-preprocessing</a><span class="sep">, </span>
    
      <a href="/tags/#data-quality" class="page__taxonomy-item p-category" rel="tag">data-quality</a><span class="sep">, </span>
    
      <a href="/tags/#feature-engineering" class="page__taxonomy-item p-category" rel="tag">feature-engineering</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0003-merge-sorted-lists/" rel="permalink">Merge Two Sorted Lists
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          29 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The pointer manipulation pattern that powers merge sort, data pipeline merging, and multi-source stream processing.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0003-audio-feature-extraction/" rel="permalink">Audio Feature Extraction for Speech ML
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          26 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How to transform raw audio waveforms into ML-ready features that capture speech characteristics for robust model training.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0003-prompt-engineering-for-agents/" rel="permalink">Prompt Engineering for Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Programming with English: The High-Level Language of 2024.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Data+Preprocessing+Pipeline+Design%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0003-data-preprocessing%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0003-data-preprocessing%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0003-data-preprocessing/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0002-classification-pipeline/" class="pagination--pager" title="Classification Pipeline Design">Previous</a>
    
    
      <a href="/ml-system-design/0004-ab-testing-systems/" class="pagination--pager" title="A/B Testing Systems for ML">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
