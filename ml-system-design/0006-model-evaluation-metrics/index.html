<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Model Evaluation Metrics - Arun Baby</title>
<meta name="description" content="How to measure if your ML model is actually good, choosing the right metrics is as important as building the model itself.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Model Evaluation Metrics">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0006-model-evaluation-metrics/">


  <meta property="og:description" content="How to measure if your ML model is actually good, choosing the right metrics is as important as building the model itself.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Model Evaluation Metrics">
  <meta name="twitter:description" content="How to measure if your ML model is actually good, choosing the right metrics is as important as building the model itself.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0006-model-evaluation-metrics/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:07:50+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0006-model-evaluation-metrics/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Model Evaluation Metrics">
    <meta itemprop="description" content="How to measure if your ML model is actually good, choosing the right metrics is as important as building the model itself.">
    <meta itemprop="datePublished" content="2025-12-31T10:07:50+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0006-model-evaluation-metrics/" itemprop="url">Model Evaluation Metrics
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#classification-metrics">Classification Metrics</a><ul><li><a href="#binary-classification">Binary Classification</a><ul><li><a href="#accuracy">Accuracy</a></li><li><a href="#precision">Precision</a></li><li><a href="#recall-sensitivity-true-positive-rate">Recall (Sensitivity, True Positive Rate)</a></li><li><a href="#f1-score">F1 Score</a></li><li><a href="#roc-curve--auc">ROC Curve &amp; AUC</a></li><li><a href="#precision-recall-curve">Precision-Recall Curve</a></li></ul></li><li><a href="#multi-class-classification">Multi-Class Classification</a></li></ul></li><li><a href="#regression-metrics">Regression Metrics</a><ul><li><a href="#mean-squared-error-mse">Mean Squared Error (MSE)</a></li><li><a href="#root-mean-squared-error-rmse">Root Mean Squared Error (RMSE)</a></li><li><a href="#mean-absolute-error-mae">Mean Absolute Error (MAE)</a></li><li><a href="#r-score-coefficient-of-determination">R² Score (Coefficient of Determination)</a></li><li><a href="#mean-absolute-percentage-error-mape">Mean Absolute Percentage Error (MAPE)</a></li></ul></li><li><a href="#ranking-metrics">Ranking Metrics</a><ul><li><a href="#normalized-discounted-cumulative-gain-ndcg">Normalized Discounted Cumulative Gain (NDCG)</a></li><li><a href="#mean-average-precision-map">Mean Average Precision (MAP)</a></li><li><a href="#mean-reciprocal-rank-mrr">Mean Reciprocal Rank (MRR)</a></li></ul></li><li><a href="#choosing-the-right-metric">Choosing the Right Metric</a><ul><li><a href="#decision-framework">Decision Framework</a></li></ul></li><li><a href="#production-monitoring">Production Monitoring</a><ul><li><a href="#metric-tracking-system">Metric Tracking System</a></li></ul></li><li><a href="#model-calibration">Model Calibration</a><ul><li><a href="#calibration-plot">Calibration Plot</a></li><li><a href="#calibrating-models">Calibrating Models</a></li></ul></li><li><a href="#threshold-tuning">Threshold Tuning</a><ul><li><a href="#finding-optimal-threshold">Finding Optimal Threshold</a></li><li><a href="#threshold-selection-strategies">Threshold Selection Strategies</a></li></ul></li><li><a href="#handling-imbalanced-datasets">Handling Imbalanced Datasets</a><ul><li><a href="#why-standard-metrics-fail">Why Standard Metrics Fail</a></li><li><a href="#better-metrics-for-imbalanced-data">Better Metrics for Imbalanced Data</a></li><li><a href="#sampling-strategies">Sampling Strategies</a></li></ul></li><li><a href="#aligning-ml-metrics-with-business-kpis">Aligning ML Metrics with Business KPIs</a><ul><li><a href="#example-1-e-commerce-recommendation-system">Example 1: E-commerce Recommendation System</a></li><li><a href="#example-2-content-moderation">Example 2: Content Moderation</a></li></ul></li><li><a href="#common-pitfalls">Common Pitfalls</a><ul><li><a href="#pitfall-1-data-leakage-in-evaluation">Pitfall 1: Data Leakage in Evaluation</a></li><li><a href="#pitfall-2-using-wrong-metric-for-problem">Pitfall 2: Using Wrong Metric for Problem</a></li><li><a href="#pitfall-3-ignoring-confidence-intervals">Pitfall 3: Ignoring Confidence Intervals</a></li><li><a href="#pitfall-4-overfitting-to-validation-set">Pitfall 4: Overfitting to Validation Set</a></li></ul></li><li><a href="#connection-to-speech-systems">Connection to Speech Systems</a><ul><li><a href="#tts-quality-metrics">TTS Quality Metrics</a></li><li><a href="#asr-error-metrics">ASR Error Metrics</a></li><li><a href="#speaker-verification">Speaker Verification</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>How to measure if your ML model is actually good, choosing the right metrics is as important as building the model itself.</strong></p>

<h2 id="introduction">Introduction</h2>

<p><strong>Model evaluation metrics</strong> are quantitative measures of model performance. Choosing the wrong metric can lead to models that optimize for the wrong objective.</p>

<p><strong>Why metrics matter:</strong></p>
<ul>
  <li><strong>Define success:</strong> What does “good” mean for your model?</li>
  <li><strong>Compare models:</strong> Which of 10 models should you deploy?</li>
  <li><strong>Monitor production:</strong> Detect when model degrades</li>
  <li><strong>Align with business:</strong> ML metrics must connect to business KPIs</li>
</ul>

<p><strong>What you’ll learn:</strong></p>
<ul>
  <li>Classification metrics (accuracy, precision, recall, F1, ROC-AUC)</li>
  <li>Regression metrics (MSE, MAE, R²)</li>
  <li>Ranking metrics (NDCG, MAP, MRR)</li>
  <li>Choosing the right metric for your problem</li>
  <li>Production monitoring strategies</li>
</ul>

<hr />

<h2 id="classification-metrics">Classification Metrics</h2>

<h3 id="binary-classification">Binary Classification</h3>

<p><strong>Confusion Matrix:</strong> Foundation of all classification metrics.</p>

<p>``
 Predicted
 Pos Neg
Actual Pos TP FN
 Neg FP TN</p>

<p>TP: True Positive - Correctly predicted positive
TN: True Negative - Correctly predicted negative
FP: False Positive - Incorrectly predicted positive (Type I error)
FN: False Negative - Incorrectly predicted negative (Type II error)
``</p>

<h4 id="accuracy">Accuracy</h4>

<p><code class="language-plaintext highlighter-rouge">
Accuracy = (TP + TN) / (TP + TN + FP + FN)
</code></p>

<p><strong>When to use:</strong> Balanced datasets 
<strong>When NOT to use:</strong> Imbalanced datasets</p>

<p><strong>Example:</strong>
``python
from sklearn.metrics import accuracy_score</p>

<p>y_true = [1, 0, 1, 1, 0, 1, 0, 0]
y_pred = [1, 0, 1, 0, 0, 1, 0, 1]</p>

<p>accuracy = accuracy_score(y_true, y_pred)
print(f”Accuracy: {accuracy:.2%}”) # 75.00%
``</p>

<p><strong>Accuracy Paradox:</strong>
``python</p>
<h1 id="dataset-95-negative-5-positive-highly-imbalanced">Dataset: 95% negative, 5% positive (highly imbalanced)</h1>
<h1 id="model-always-predicts-negative--95-accurate">Model always predicts negative → 95% accurate!</h1>
<h1 id="but-useless-for-detecting-positive-class">But useless for detecting positive class</h1>
<p>``</p>

<h4 id="precision">Precision</h4>

<p><code class="language-plaintext highlighter-rouge">
Precision = TP / (TP + FP)
</code></p>

<p><strong>Interpretation:</strong> Of all positive predictions, how many were actually positive?</p>

<p><strong>When to use:</strong> Cost of false positives is high 
<strong>Example:</strong> Email spam detection (don’t mark legitimate emails as spam)</p>

<h4 id="recall-sensitivity-true-positive-rate">Recall (Sensitivity, True Positive Rate)</h4>

<p><code class="language-plaintext highlighter-rouge">
Recall = TP / (TP + FN)
</code></p>

<p><strong>Interpretation:</strong> Of all actual positives, how many did we detect?</p>

<p><strong>When to use:</strong> Cost of false negatives is high 
<strong>Example:</strong> Cancer detection (don’t miss actual cases)</p>

<h4 id="f1-score">F1 Score</h4>

<p><code class="language-plaintext highlighter-rouge">
F1 = 2 * (Precision * Recall) / (Precision + Recall)
</code></p>

<p><strong>Interpretation:</strong> Harmonic mean of precision and recall</p>

<p><strong>When to use:</strong> Need balance between precision and recall</p>

<p><strong>Implementation:</strong>
``python
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix</p>

<p>y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1]
y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]</p>

<h1 id="compute-metrics">Compute metrics</h1>
<p>precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)</p>

<p>print(f”Precision: {precision:.2%}”)
print(f”Recall: {recall:.2%}”)
print(f”F1 Score: {f1:.2%}”)</p>

<h1 id="confusion-matrix">Confusion matrix</h1>
<p>cm = confusion_matrix(y_true, y_pred)
print(f”Confusion Matrix:\n{cm}”)
``</p>

<h4 id="roc-curve--auc">ROC Curve &amp; AUC</h4>

<p><strong>ROC (Receiver Operating Characteristic):</strong> Plot of True Positive Rate vs False Positive Rate at different thresholds.</p>

<p>``python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import numpy as np</p>

<h1 id="predicted-probabilities">Predicted probabilities</h1>
<p>y_true = [0, 0, 1, 1, 0, 1, 0, 1]
y_scores = [0.1, 0.4, 0.35, 0.8, 0.2, 0.9, 0.3, 0.7]</p>

<h1 id="compute-roc-curve">Compute ROC curve</h1>
<p>fpr, tpr, thresholds = roc_curve(y_true, y_scores)</p>

<h1 id="compute-auc">Compute AUC</h1>
<p>auc = roc_auc_score(y_true, y_scores)</p>

<h1 id="plot">Plot</h1>
<p>plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f’ROC Curve (AUC = {auc:.3f})’)
plt.plot([0, 1], [0, 1], ‘k–’, label=’Random Classifier’)
plt.xlabel(‘False Positive Rate’)
plt.ylabel(‘True Positive Rate’)
plt.title(‘ROC Curve’)
plt.legend()
plt.show()</p>

<p>print(f”AUC: {auc:.3f}”)
``</p>

<p><strong>AUC Interpretation:</strong></p>
<ul>
  <li>1.0: Perfect classifier</li>
  <li>0.5: Random classifier</li>
  <li>&lt; 0.5: Worse than random (inverted predictions)</li>
</ul>

<p><strong>When to use AUC:</strong> When you want threshold-independent performance measure</p>

<h4 id="precision-recall-curve">Precision-Recall Curve</h4>

<p>Better than ROC for imbalanced datasets.</p>

<p>``python
from sklearn.metrics import precision_recall_curve, average_precision_score
import numpy as np</p>

<h1 id="compute-precision-recall-curve">Compute precision-recall curve</h1>
<p>precision, recall, thresholds = precision_recall_curve(y_true, y_scores)</p>

<h1 id="average-precision">Average precision</h1>
<p>avg_precision = average_precision_score(y_true, y_scores)</p>

<h1 id="plot-1">Plot</h1>
<p>plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label=f’PR Curve (AP = {avg_precision:.3f})’)
plt.xlabel(‘Recall’)
plt.ylabel(‘Precision’)
plt.title(‘Precision-Recall Curve’)
plt.legend()
plt.show()
``</p>

<hr />

<h3 id="multi-class-classification">Multi-Class Classification</h3>

<p><strong>Macro vs Micro Averaging:</strong></p>

<p>``python
from sklearn.metrics import classification_report</p>

<p>y_true = [0, 1, 2, 0, 1, 2, 0, 1, 2]
y_pred = [0, 2, 1, 0, 1, 2, 0, 2, 2]</p>

<h1 id="classification-report">Classification report</h1>
<p>report = classification_report(y_true, y_pred, target_names=[‘Class A’, ‘Class B’, ‘Class C’])
print(report)
``</p>

<p><strong>Macro Average:</strong> Average of per-class metrics (treats all classes equally) 
<strong>Micro Average:</strong> Aggregate TP, FP, FN across all classes (favors frequent classes) 
<strong>Weighted Average:</strong> Weighted by class frequency</p>

<p><strong>When to use which:</strong></p>
<ul>
  <li><strong>Macro:</strong> All classes equally important</li>
  <li><strong>Micro:</strong> Overall performance across all predictions</li>
  <li><strong>Weighted:</strong> Account for class imbalance</li>
</ul>

<hr />

<h2 id="regression-metrics">Regression Metrics</h2>

<h3 id="mean-squared-error-mse">Mean Squared Error (MSE)</h3>

<p><code class="language-plaintext highlighter-rouge">
MSE = (1/n) * Σ(y_true - y_pred)²
</code></p>

<p><strong>Properties:</strong></p>
<ul>
  <li>Penalizes large errors heavily (squared term)</li>
  <li>Always non-negative</li>
  <li>Same units as y²</li>
</ul>

<p>``python
from sklearn.metrics import mean_squared_error
import numpy as np</p>

<p>y_true = [3.0, -0.5, 2.0, 7.0]
y_pred = [2.5, 0.0, 2.0, 8.0]</p>

<p>mse = mean_squared_error(y_true, y_pred)
print(f”MSE: {mse:.4f}”)
``</p>

<h3 id="root-mean-squared-error-rmse">Root Mean Squared Error (RMSE)</h3>

<p><code class="language-plaintext highlighter-rouge">
RMSE = √MSE
</code></p>

<p><strong>Properties:</strong></p>
<ul>
  <li>Same units as y (interpretable)</li>
  <li>Sensitive to outliers</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">python
rmse = np.sqrt(mse)
print(f"RMSE: {rmse:.4f}")
</code></p>

<h3 id="mean-absolute-error-mae">Mean Absolute Error (MAE)</h3>

<p><code class="language-plaintext highlighter-rouge">
MAE = (1/n) * Σ|y_true - y_pred|
</code></p>

<p><strong>Properties:</strong></p>
<ul>
  <li>Linear penalty (all errors weighted equally)</li>
  <li>More robust to outliers than MSE</li>
  <li>Same units as y</li>
</ul>

<p>``python
from sklearn.metrics import mean_absolute_error</p>

<p>mae = mean_absolute_error(y_true, y_pred)
print(f”MAE: {mae:.4f}”)
``</p>

<p><strong>MSE vs MAE:</strong></p>
<ul>
  <li>Use <strong>MSE</strong> when large errors are especially bad</li>
  <li>Use <strong>MAE</strong> when all errors have equal weight</li>
</ul>

<h3 id="r-score-coefficient-of-determination">R² Score (Coefficient of Determination)</h3>

<p>``
R² = 1 - (SS_res / SS_tot)</p>

<p>where:
 SS_res = Σ(y_true - y_pred)² (residual sum of squares)
 SS_tot = Σ(y_true - y_mean)² (total sum of squares)
``</p>

<p><strong>Interpretation:</strong></p>
<ul>
  <li>1.0: Perfect predictions</li>
  <li>0.0: Model performs as well as predicting mean</li>
  <li>&lt; 0.0: Model worse than predicting mean</li>
</ul>

<p>``python
from sklearn.metrics import r2_score
import numpy as np</p>

<p>r2 = r2_score(y_true, y_pred)
print(f”R²: {r2:.4f}”)
``</p>

<h3 id="mean-absolute-percentage-error-mape">Mean Absolute Percentage Error (MAPE)</h3>

<p><code class="language-plaintext highlighter-rouge">
MAPE = (100/n) * Σ|((y_true - y_pred) / y_true)|
</code></p>

<p><strong>When to use:</strong> When relative error matters more than absolute error</p>

<p><strong>Caveat:</strong> Undefined when y_true = 0</p>

<p>``python
def mean_absolute_percentage_error(y_true, y_pred):
 “””
 MAPE implementation</p>

<p>Warning: Undefined when y_true contains zeros
 “””
 y_true, y_pred = np.array(y_true), np.array(y_pred)</p>

<p># Avoid division by zero
 non_zero_mask = y_true != 0</p>

<p>if not np.any(non_zero_mask):
 return np.inf</p>

<p>return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100</p>

<p>y_true = [100, 200, 150, 300]
y_pred = [110, 190, 160, 280]</p>

<p>mape = mean_absolute_percentage_error(y_true, y_pred)
print(f”MAPE: {mape:.2f}%”)
``</p>

<hr />

<h2 id="ranking-metrics">Ranking Metrics</h2>

<p>For recommendation systems, search engines, etc.</p>

<h3 id="normalized-discounted-cumulative-gain-ndcg">Normalized Discounted Cumulative Gain (NDCG)</h3>

<p>Measures quality of ranking where position matters.</p>

<p>``python
from sklearn.metrics import ndcg_score</p>

<h1 id="relevance-scores-for-each-item-higher--more-relevant">Relevance scores for each item (higher = more relevant)</h1>
<h1 id="order-matters-first-item-is-ranked-first-etc">Order matters: first item is ranked first, etc.</h1>
<p>y_true = [[3, 2, 3, 0, 1, 2]] # True relevance
y_pred = [[2.8, 1.9, 2.5, 0.1, 1.2, 1.8]] # Predicted scores</p>

<h1 id="ndcgk-for-different-k-values">NDCG@k for different k values</h1>
<p>for k in [3, 5, None]: # None means all items
 ndcg = ndcg_score(y_true, y_pred, k=k)
 label = f”NDCG@{k if k else ‘all’}”
 print(f”{label}: {ndcg:.4f}”)
``</p>

<p><strong>Interpretation:</strong></p>
<ul>
  <li>1.0: Perfect ranking</li>
  <li>0.0: Worst possible ranking</li>
</ul>

<p><strong>When to use:</strong> Position-aware ranking (search, recommendations)</p>

<h3 id="mean-average-precision-map">Mean Average Precision (MAP)</h3>

<p>``python
def average_precision(y_true, y_scores):
 “””
 Compute Average Precision</p>

<p>Args:
 y_true: Binary relevance (1 = relevant, 0 = not relevant)
 y_scores: Predicted scores</p>

<p>Returns:
 Average precision
 “””
 # Sort by scores (descending)
 sorted_indices = np.argsort(y_scores)[::-1]
 y_true_sorted = np.array(y_true)[sorted_indices]</p>

<p># Compute precision at each relevant item
 precisions = []
 num_relevant = 0</p>

<p>for i, is_relevant in enumerate(y_true_sorted, 1):
 if is_relevant:
 num_relevant += 1
 precision_at_i = num_relevant / i
 precisions.append(precision_at_i)</p>

<p>if not precisions:
 return 0.0</p>

<p>return np.mean(precisions)</p>

<h1 id="example">Example</h1>
<p>y_true = [1, 0, 1, 0, 1, 0]
y_scores = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4]</p>

<p>ap = average_precision(y_true, y_scores)
print(f”Average Precision: {ap:.4f}”)
``</p>

<h3 id="mean-reciprocal-rank-mrr">Mean Reciprocal Rank (MRR)</h3>

<p>Measures where the first relevant item appears.</p>

<p>``
MRR = (1/|Q|) * Σ(1 / rank_i)</p>

<p>where rank_i is the rank of first relevant item for query i
``</p>

<p>``python
def mean_reciprocal_rank(y_true_queries, y_pred_queries):
 “””
 Compute MRR across multiple queries</p>

<p>Args:
 y_true_queries: List of relevance lists (one per query)
 y_pred_queries: List of score lists (one per query)</p>

<p>Returns:
 MRR score
 “””
 reciprocal_ranks = []</p>

<p>for y_true, y_scores in zip(y_true_queries, y_pred_queries):
 # Sort by scores
 sorted_indices = np.argsort(y_scores)[::-1]
 y_true_sorted = np.array(y_true)[sorted_indices]</p>

<p># Find first relevant item
 for rank, is_relevant in enumerate(y_true_sorted, 1):
 if is_relevant:
 reciprocal_ranks.append(1.0 / rank)
 break
 else:
 # No relevant item found
 reciprocal_ranks.append(0.0)</p>

<p>return np.mean(reciprocal_ranks)</p>

<h1 id="example-3-queries">Example: 3 queries</h1>
<p>y_true_queries = [
 [0, 1, 0, 1], # Query 1: first relevant at position 2
 [1, 0, 0, 0], # Query 2: first relevant at position 1
 [0, 0, 1, 0], # Query 3: first relevant at position 3
]</p>

<p>y_pred_queries = [
 [0.2, 0.8, 0.3, 0.9],
 [0.9, 0.1, 0.2, 0.3],
 [0.1, 0.2, 0.9, 0.3],
]</p>

<p>mrr = mean_reciprocal_rank(y_true_queries, y_pred_queries)
print(f”MRR: {mrr:.4f}”)
``</p>

<hr />

<h2 id="choosing-the-right-metric">Choosing the Right Metric</h2>

<h3 id="decision-framework">Decision Framework</h3>

<p>``python
class MetricSelector:
 “””
 Help choose appropriate metric based on problem characteristics
 “””</p>

<p>def recommend_metric(
 self,
 task_type: str,
 class_balance: str = ‘balanced’,
 business_priority: str = None
 ) -&gt; list[str]:
 “””
 Recommend metrics based on problem characteristics</p>

<p>Args:
 task_type: ‘binary_classification’, ‘multiclass’, ‘regression’, ‘ranking’
 class_balance: ‘balanced’, ‘imbalanced’
 business_priority: ‘precision’, ‘recall’, ‘both’, None</p>

<p>Returns:
 List of recommended metrics
 “””
 recommendations = []</p>

<p>if task_type == ‘binary_classification’:
 if class_balance == ‘balanced’:
 recommendations.append(‘Accuracy’)
 recommendations.append(‘ROC-AUC’)
 else:
 recommendations.append(‘Precision-Recall AUC’)
 recommendations.append(‘F1 Score’)</p>

<p>if business_priority == ‘precision’:
 recommendations.append(‘Precision (optimize threshold)’)
 elif business_priority == ‘recall’:
 recommendations.append(‘Recall (optimize threshold)’)
 elif business_priority == ‘both’:
 recommendations.append(‘F1 Score’)</p>

<p>elif task_type == ‘multiclass’:
 recommendations.append(‘Macro F1 (if classes equally important)’)
 recommendations.append(‘Weighted F1 (if accounting for imbalance)’)
 recommendations.append(‘Confusion Matrix (for detailed analysis)’)</p>

<p>elif task_type == ‘regression’:
 recommendations.append(‘RMSE (if penalizing large errors)’)
 recommendations.append(‘MAE (if robust to outliers)’)
 recommendations.append(‘R² (for explained variance)’)</p>

<p>elif task_type == ‘ranking’:
 recommendations.append(‘NDCG (for position-aware ranking)’)
 recommendations.append(‘MAP (for information retrieval)’)
 recommendations.append(‘MRR (for first relevant item)’)</p>

<p>return recommendations</p>

<h1 id="usage">Usage</h1>
<p>selector = MetricSelector()</p>

<h1 id="example-1-fraud-detection-imbalanced-recall-critical">Example 1: Fraud detection (imbalanced, recall critical)</h1>
<p>metrics = selector.recommend_metric(
 task_type=’binary_classification’,
 class_balance=’imbalanced’,
 business_priority=’recall’
)
print(“Fraud detection metrics:”, metrics)</p>

<h1 id="example-2-search-ranking">Example 2: Search ranking</h1>
<p>metrics = selector.recommend_metric(
 task_type=’ranking’
)
print(“Search ranking metrics:”, metrics)
``</p>

<hr />

<h2 id="production-monitoring">Production Monitoring</h2>

<h3 id="metric-tracking-system">Metric Tracking System</h3>

<p>``python
import time
from collections import deque
from typing import Dict, List</p>

<p>class MetricTracker:
 “””
 Track metrics over time in production</p>

<p>Use case: Monitor model performance degradation
 “””</p>

<p>def <strong>init</strong>(self, window_size=1000):
 self.window_size = window_size</p>

<p># Sliding windows for predictions and actuals
 self.predictions = deque(maxlen=window_size)
 self.actuals = deque(maxlen=window_size)
 self.timestamps = deque(maxlen=window_size)</p>

<p># Historical metrics
 self.metric_history = {
 ‘accuracy’: [],
 ‘precision’: [],
 ‘recall’: [],
 ‘f1’: [],
 ‘timestamp’: []
 }</p>

<p>def log_prediction(self, y_true, y_pred):
 “””
 Log a prediction and its actual outcome
 “””
 self.predictions.append(y_pred)
 self.actuals.append(y_true)
 self.timestamps.append(time.time())</p>

<p>def compute_current_metrics(self) -&gt; Dict:
 “””
 Compute metrics over current window
 “””
 if len(self.predictions) &lt; 10:
 return {}</p>

<p>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score</p>

<p>try:
 metrics = {
 ‘accuracy’: accuracy_score(self.actuals, self.predictions),
 ‘precision’: precision_score(self.actuals, self.predictions, zero_division=0),
 ‘recall’: recall_score(self.actuals, self.predictions, zero_division=0),
 ‘f1’: f1_score(self.actuals, self.predictions, zero_division=0),
 ‘sample_count’: len(self.predictions)
 }</p>

<p># Save to history
 for metric_name, value in metrics.items():
 if metric_name != ‘sample_count’:
 self.metric_history[metric_name].append(value)</p>

<p>self.metric_history[‘timestamp’].append(time.time())</p>

<p>return metrics</p>

<p>except Exception as e:
 print(f”Error computing metrics: {e}”)
 return {}</p>

<p>def detect_degradation(self, baseline_metric: str = ‘f1’, threshold: float = 0.05) -&gt; bool:
 “””
 Detect if model performance has degraded</p>

<p>Args:
 baseline_metric: Metric to monitor
 threshold: Alert if metric drops by this much from baseline</p>

<p>Returns:
 True if degradation detected
 “””
 history = self.metric_history.get(baseline_metric, [])</p>

<p>if len(history) &lt; 10:
 return False</p>

<p># Compare recent average to baseline (first 10% of history)
 baseline_size = max(10, len(history) // 10)
 baseline_avg = np.mean(history[:baseline_size])
 recent_avg = np.mean(history[-baseline_size:])</p>

<p>degradation = baseline_avg - recent_avg</p>

<p>return degradation &gt; threshold</p>

<h1 id="usage-1">Usage</h1>
<p>tracker = MetricTracker(window_size=1000)</p>

<h1 id="simulate-predictions-over-time">Simulate predictions over time</h1>
<p>for i in range(1500):
 # Simulate ground truth and prediction
 y_true = np.random.choice([0, 1], p=[0.7, 0.3])</p>

<p># Simulate model getting worse over time
 accuracy_degradation = min(0.1, i / 10000)
 if np.random.random() &lt; (0.8 - accuracy_degradation):
 y_pred = y_true
 else:
 y_pred = 1 - y_true</p>

<p>tracker.log_prediction(y_true, y_pred)</p>

<p># Compute metrics every 100 predictions
 if i % 100 == 0 and i &gt; 0:
 metrics = tracker.compute_current_metrics()
 if metrics:
 print(f”Step {i}: F1 = {metrics[‘f1’]:.3f}”)</p>

<p>if tracker.detect_degradation():
 print(f”⚠️ WARNING: Model degradation detected at step {i}”)
``</p>

<hr />

<h2 id="model-calibration">Model Calibration</h2>

<p><strong>Calibration:</strong> How well predicted probabilities match actual outcomes.</p>

<p><strong>Example of poor calibration:</strong>
``python</p>
<h1 id="model-predicts-80-probability-for-100-samples">Model predicts 80% probability for 100 samples</h1>
<h1 id="only-40-of-them-are-actually-positive">Only 40 of them are actually positive</h1>
<h1 id="model-is-overconfident-80-predicted-vs-40-actual">Model is overconfident! (80% predicted vs 40% actual)</h1>
<p>``</p>

<h3 id="calibration-plot">Calibration Plot</h3>

<p>``python
from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt</p>

<p>def plot_calibration_curve(y_true, y_prob, n_bins=10):
 “””
 Plot calibration curve</p>

<p>A well-calibrated model’s curve follows the diagonal
 “””
 prob_true, prob_pred = calibration_curve(
 y_true,
 y_prob,
 n_bins=n_bins,
 strategy=’uniform’
 )</p>

<p>plt.figure(figsize=(8, 6))
 plt.plot(prob_pred, prob_true, marker=’o’, label=’Model’)
 plt.plot([0, 1], [0, 1], ‘k–’, label=’Perfect Calibration’)
 plt.xlabel(‘Mean Predicted Probability’)
 plt.ylabel(‘Fraction of Positives’)
 plt.title(‘Calibration Plot’)
 plt.legend()
 plt.grid(True)
 plt.show()</p>

<h1 id="example-1">Example</h1>
<p>y_true = [0, 1, 1, 0, 1, 0, 1, 1, 0, 1] * 10 # 100 samples
y_prob = [0.2, 0.7, 0.8, 0.3, 0.9, 0.1, 0.6, 0.85, 0.15, 0.75] * 10</p>

<p>plot_calibration_curve(y_true, y_prob)
``</p>

<h3 id="calibrating-models">Calibrating Models</h3>

<p>Some models (e.g., SVMs, tree ensembles) output poorly calibrated probabilities.</p>

<p>``python
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import RandomForestClassifier</p>

<h1 id="train-base-model">Train base model</h1>
<p>base_model = RandomForestClassifier()
base_model.fit(X_train, y_train)</p>

<h1 id="calibrate-predictions">Calibrate predictions</h1>
<p>calibrated_model = CalibratedClassifierCV(
 base_model,
 method=’sigmoid’, # or ‘isotonic’
 cv=5
)
calibrated_model.fit(X_train, y_train)</p>

<h1 id="now-probabilities-are-better-calibrated">Now probabilities are better calibrated</h1>
<p>y_prob_calibrated = calibrated_model.predict_proba(X_test)[:, 1]
``</p>

<p><strong>Calibration methods:</strong></p>
<ul>
  <li><strong>Platt scaling (sigmoid):</strong> Fits logistic regression on predictions</li>
  <li><strong>Isotonic regression:</strong> Non-parametric, more flexible but needs more data</li>
</ul>

<hr />

<h2 id="threshold-tuning">Threshold Tuning</h2>

<p>Classification models output probabilities. Choosing the decision threshold impacts precision/recall trade-off.</p>

<h3 id="finding-optimal-threshold">Finding Optimal Threshold</h3>

<p>``python
import numpy as np
from sklearn.metrics import precision_recall_curve, f1_score</p>

<p>def find_optimal_threshold(y_true, y_prob, metric=’f1’):
 “””
 Find threshold that maximizes a metric</p>

<p>Args:
 y_true: True labels
 y_prob: Predicted probabilities
 metric: ‘f1’, ‘precision’, ‘recall’, or custom function</p>

<p>Returns:
 optimal_threshold, best_score
 “””
 if metric == ‘f1’:
 # Compute F1 at different thresholds
 precision, recall, thresholds = precision_recall_curve(y_true, y_prob)
 f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)</p>

<p>best_idx = np.argmax(f1_scores)
 return thresholds[best_idx] if best_idx &lt; len(thresholds) else 0.5, f1_scores[best_idx]</p>

<p>elif metric == ‘precision’:
 precision, recall, thresholds = precision_recall_curve(y_true, y_prob)
 # Find threshold for minimum acceptable recall (e.g., 0.8)
 min_recall = 0.8
 valid_idx = recall &gt;= min_recall
 if not any(valid_idx):
 return None, 0
 best_idx = np.argmax(precision[valid_idx])
 return thresholds[valid_idx][best_idx], precision[valid_idx][best_idx]</p>

<p>elif metric == ‘recall’:
 precision, recall, thresholds = precision_recall_curve(y_true, y_prob)
 # Find threshold for minimum acceptable precision (e.g., 0.9)
 min_precision = 0.9
 valid_idx = precision &gt;= min_precision
 if not any(valid_idx):
 return None, 0
 best_idx = np.argmax(recall[valid_idx])
 return thresholds[valid_idx][best_idx], recall[valid_idx][best_idx]</p>

<h1 id="example-2">Example</h1>
<p>y_true = np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 1])
y_prob = np.array([0.2, 0.7, 0.8, 0.3, 0.9, 0.1, 0.6, 0.85, 0.15, 0.75])</p>

<p>optimal_threshold, best_f1 = find_optimal_threshold(y_true, y_prob, metric=’f1’)
print(f”Optimal threshold: {optimal_threshold:.3f}, Best F1: {best_f1:.3f}”)
``</p>

<h3 id="threshold-selection-strategies">Threshold Selection Strategies</h3>

<p><strong>1. Maximize F1 Score</strong></p>
<ul>
  <li>Balanced precision and recall</li>
  <li>Good default choice</li>
</ul>

<p><strong>2. Business-Driven</strong>
``python</p>
<h1 id="example-fraud-detection">Example: Fraud detection</h1>
<h1 id="false-negative-missed-fraud-costs-500">False negative (missed fraud) costs $500</h1>
<h1 id="false-positive-declined-legit-transaction-costs-10">False positive (declined legit transaction) costs $10</h1>

<p>def business_value_threshold(y_true, y_prob, fn_cost=500, fp_cost=10):
 “””
 Find threshold that maximizes business value
 “””
 best_threshold = 0.5
 best_value = float(‘-inf’)</p>

<p>for threshold in np.arange(0.1, 0.9, 0.01):
 y_pred = (y_prob &gt;= threshold).astype(int)</p>

<p># Compute confusion matrix
 tn = ((y_true == 0) &amp; (y_pred == 0)).sum()
 fp = ((y_true == 0) &amp; (y_pred == 1)).sum()
 fn = ((y_true == 1) &amp; (y_pred == 0)).sum()
 tp = ((y_true == 1) &amp; (y_pred == 1)).sum()</p>

<p># Business value = savings from catching fraud - cost of false alarms
 value = tp * fn_cost - fp * fp_cost</p>

<p>if value &gt; best_value:
 best_value = value
 best_threshold = threshold</p>

<p>return best_threshold, best_value</p>

<p>threshold, value = business_value_threshold(y_true, y_prob)
print(f”Best threshold: {threshold:.2f}, Business value: ${value:.2f}”)
``</p>

<p><strong>3. Operating Point Selection</strong>
``python</p>
<h1 id="healthcare-prioritize-recall-dont-miss-diseases">Healthcare: Prioritize recall (don’t miss diseases)</h1>
<h1 id="set-minimum-recall--095-maximize-precision-subject-to-that">Set minimum recall = 0.95, maximize precision subject to that</h1>

<p>def threshold_for_min_recall(y_true, y_prob, min_recall=0.95):
 “"”Find threshold that achieves minimum recall while maximizing precision”””
 precision, recall, thresholds = precision_recall_curve(y_true, y_prob)</p>

<p>valid_indices = recall &gt;= min_recall
 if not any(valid_indices):
 return None</p>

<p>best_precision_idx = np.argmax(precision[valid_indices])
 threshold_idx = np.where(valid_indices)[0][best_precision_idx]</p>

<p>return thresholds[threshold_idx] if threshold_idx &lt; len(thresholds) else 0.0
``</p>

<hr />

<h2 id="handling-imbalanced-datasets">Handling Imbalanced Datasets</h2>

<h3 id="why-standard-metrics-fail">Why Standard Metrics Fail</h3>

<p>``python</p>
<h1 id="dataset-99-negative-1-positive">Dataset: 99% negative, 1% positive</h1>
<p>y_true = [0] * 990 + [1] * 10
y_pred_dummy = [0] * 1000 # Always predict negative</p>

<p>from sklearn.metrics import accuracy_score, precision_score, recall_score</p>

<p>print(f”Accuracy: {accuracy_score(y_true, y_pred_dummy):.1%}”) # 99%!
print(f”Precision: {precision_score(y_true, y_pred_dummy, zero_division=0):.1%}”) # Undefined (0/0)
print(f”Recall: {recall_score(y_true, y_pred_dummy):.1%}”) # 0%
``</p>

<p><strong>Accuracy is 99%</strong> but model is useless!</p>

<h3 id="better-metrics-for-imbalanced-data">Better Metrics for Imbalanced Data</h3>

<p><strong>1. Precision-Recall AUC</strong></p>

<p>Better than ROC-AUC for imbalanced data because it doesn’t include TN (which dominates in imbalanced datasets).</p>

<p>``python
from sklearn.metrics import average_precision_score</p>

<p>ap = average_precision_score(y_true, y_scores)
print(f”Average Precision: {ap:.3f}”)
``</p>

<p><strong>2. Cohen’s Kappa</strong></p>

<p>Measures agreement between predicted and actual, adjusted for chance.</p>

<p>``python
from sklearn.metrics import cohen_kappa_score</p>

<p>kappa = cohen_kappa_score(y_true, y_pred)
print(f”Cohen’s Kappa: {kappa:.3f}”)</p>

<h1 id="interpretation">Interpretation:</h1>
<h1 id="-0-no-agreement">&lt; 0: No agreement</h1>
<h1 id="0-020-slight">0-0.20: Slight</h1>
<h1 id="021-040-fair">0.21-0.40: Fair</h1>
<h1 id="041-060-moderate">0.41-0.60: Moderate</h1>
<h1 id="061-080-substantial">0.61-0.80: Substantial</h1>
<h1 id="081-10-almost-perfect">0.81-1.0: Almost perfect</h1>
<p>``</p>

<p><strong>3. Matthews Correlation Coefficient (MCC)</strong></p>

<p>Takes all four confusion matrix values into account. Ranges from -1 to +1.</p>

<p>``python
from sklearn.metrics import matthews_corrcoef</p>

<p>mcc = matthews_corrcoef(y_true, y_pred)
print(f”MCC: {mcc:.3f}”)</p>

<h1 id="interpretation-1">Interpretation:</h1>
<h1 id="1-perfect-prediction">+1: Perfect prediction</h1>
<h1 id="0-random-prediction">0: Random prediction</h1>
<h1 id="-1-perfect-inverse-prediction">-1: Perfect inverse prediction</h1>
<p>``</p>

<p><strong>4. Class-Weighted Metrics</strong></p>

<p>``python
from sklearn.metrics import fbeta_score</p>

<h1 id="emphasize-recall-beta--1-for-imbalanced-positive-class">Emphasize recall (beta &gt; 1) for imbalanced positive class</h1>
<p>f2 = fbeta_score(y_true, y_pred, beta=2) # Recall weighted 2x more than precision
print(f”F2 Score: {f2:.3f}”)
``</p>

<h3 id="sampling-strategies">Sampling Strategies</h3>

<p>``python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline</p>

<h1 id="combine-over-sampling-and-under-sampling">Combine over-sampling and under-sampling</h1>
<p>pipeline = ImbPipeline([
 (‘oversample’, SMOTE(sampling_strategy=0.5)), # Increase minority to 50% of majority
 (‘undersample’, RandomUnderSampler(sampling_strategy=1.0)) # Balance classes
])</p>

<p>X_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)
``</p>

<hr />

<h2 id="aligning-ml-metrics-with-business-kpis">Aligning ML Metrics with Business KPIs</h2>

<h3 id="example-1-e-commerce-recommendation-system">Example 1: E-commerce Recommendation System</h3>

<p><strong>ML Metrics:</strong></p>
<ul>
  <li>Precision@10: 0.65</li>
  <li>Recall@10: 0.45</li>
  <li>NDCG@10: 0.72</li>
</ul>

<p><strong>Business KPIs:</strong></p>
<ul>
  <li>Click-through rate (CTR): 3.5%</li>
  <li>Conversion rate: 1.2%</li>
  <li>Revenue per user: $45</li>
</ul>

<p><strong>Alignment:</strong>
``python
class BusinessMetricTracker:
 “””
 Track both ML and business metrics</p>

<p>Use case: Connect model performance to business impact
 “””</p>

<p>def <strong>init</strong>(self):
 self.ml_metrics = {}
 self.business_metrics = {}
 self.correlations = {}</p>

<p>def log_session(
 self,
 ml_metrics: dict,
 business_metrics: dict
 ):
 “"”Log metrics for a user session”””
 for metric, value in ml_metrics.items():
 if metric not in self.ml_metrics:
 self.ml_metrics[metric] = []
 self.ml_metrics[metric].append(value)</p>

<p>for metric, value in business_metrics.items():
 if metric not in self.business_metrics:
 self.business_metrics[metric] = []
 self.business_metrics[metric].append(value)</p>

<p>def compute_correlations(self):
 “"”Compute correlation between ML and business metrics”””
 import numpy as np
 from scipy.stats import pearsonr</p>

<p>for ml_metric in self.ml_metrics:
 for biz_metric in self.business_metrics:
 ml_values = np.array(self.ml_metrics[ml_metric])
 biz_values = np.array(self.business_metrics[biz_metric])</p>

<p>if len(ml_values) == len(biz_values):
 corr, p_value = pearsonr(ml_values, biz_values)
 self.correlations[(ml_metric, biz_metric)] = {
 ‘correlation’: corr,
 ‘p_value’: p_value
 }</p>

<p>return self.correlations</p>

<h1 id="usage-2">Usage</h1>
<p>tracker = BusinessMetricTracker()</p>

<h1 id="log-multiple-sessions">Log multiple sessions</h1>
<p>for _ in range(100):
 tracker.log_session(
 ml_metrics={‘precision’: np.random.uniform(0.6, 0.7)},
 business_metrics={‘ctr’: np.random.uniform(0.03, 0.04)}
 )</p>

<p>correlations = tracker.compute_correlations()
print(“ML Metric ↔ Business KPI Correlations:”)
for (ml, biz), stats in correlations.items():
 print(f”{ml} ↔ {biz}: r={stats[‘correlation’]:.3f}, p={stats[‘p_value’]:.3f}”)
``</p>

<h3 id="example-2-content-moderation">Example 2: Content Moderation</h3>

<p><strong>ML Metrics:</strong></p>
<ul>
  <li>Precision: 0.92 (92% of flagged content is actually bad)</li>
  <li>Recall: 0.78 (catch 78% of bad content)</li>
</ul>

<p><strong>Business KPIs:</strong></p>
<ul>
  <li>User reports: How many users still report bad content?</li>
  <li>User retention: Are false positives causing users to leave?</li>
  <li>Moderator workload: Hours spent reviewing flagged content</li>
</ul>

<p><strong>Trade-off:</strong></p>
<ul>
  <li>High recall → More bad content caught → Fewer user reports ✓</li>
  <li>But also → More false positives → Higher moderator workload ✗</li>
</ul>

<p>``python
def estimate_moderator_cost(precision, recall, daily_content, hourly_rate=50):
 “””
 Estimate cost of content moderation</p>

<p>Args:
 precision: Model precision
 recall: Model recall
 daily_content: Number of content items per day
 hourly_rate: Cost per moderator hour</p>

<p>Returns:
 Daily moderation cost
 “””
 # Assume 1% of content is actually bad
 bad_content = daily_content * 0.01</p>

<p># Content flagged by model
 flagged = (bad_content * recall) / precision</p>

<p># Time to review (assume 30 seconds per item)
 review_hours = (flagged * 30) / 3600</p>

<p># Cost
 cost = review_hours * hourly_rate</p>

<p>return cost, review_hours</p>

<h1 id="compare-different-models">Compare different models</h1>
<p>models = [
 {‘name’: ‘Conservative’, ‘precision’: 0.95, ‘recall’: 0.70},
 {‘name’: ‘Balanced’, ‘precision’: 0.90, ‘recall’: 0.80},
 {‘name’: ‘Aggressive’, ‘precision’: 0.85, ‘recall’: 0.90}
]</p>

<p>for model in models:
 cost, hours = estimate_moderator_cost(
 model[‘precision’],
 model[‘recall’],
 daily_content=100000
 )
 print(f”{model[‘name’]}: ${cost:.2f}/day, {hours:.1f} hours/day”)
``</p>

<hr />

<h2 id="common-pitfalls">Common Pitfalls</h2>

<h3 id="pitfall-1-data-leakage-in-evaluation">Pitfall 1: Data Leakage in Evaluation</h3>

<p>``python</p>
<h1 id="wrong-fit-preprocessing-on-entire-dataset">WRONG: Fit preprocessing on entire dataset</h1>
<p>from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split</p>

<p>scaler = StandardScaler()
X_scaled = scaler.fit_transform(X) # Leakage! Test data info leaks into training</p>

<p>X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)</p>

<h1 id="correct-fit-only-on-training-data">CORRECT: Fit only on training data</h1>
<p>X_train, X_test, y_train, y_test = train_test_split(X, y)</p>

<p>scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train) # Fit on train only
X_test_scaled = scaler.transform(X_test) # Transform test
``</p>

<h3 id="pitfall-2-using-wrong-metric-for-problem">Pitfall 2: Using Wrong Metric for Problem</h3>

<p>``python</p>
<h1 id="wrong-using-accuracy-for-imbalanced-fraud-detection">Wrong: Using accuracy for imbalanced fraud detection</h1>
<h1 id="fraud-rate-01-model-always-predicts-not-fraud">Fraud rate: 0.1%, model always predicts “not fraud”</h1>
<h1 id="accuracy-999--misleading">Accuracy: 99.9% ✓ (misleading!)</h1>
<h1 id="recall-0--useless">Recall: 0% ✗ (useless!)</h1>

<h1 id="right-use-precision-recall-f1-or-pr-auc">Right: Use precision-recall, F1, or PR-AUC</h1>
<p>``</p>

<h3 id="pitfall-3-ignoring-confidence-intervals">Pitfall 3: Ignoring Confidence Intervals</h3>

<p>``python</p>
<h1 id="model-a-accuracy--852">Model A: Accuracy = 85.2%</h1>
<h1 id="model-b-accuracy--855">Model B: Accuracy = 85.5%</h1>

<h1 id="is-b-really-better-need-confidence-intervals">Is B really better? Need confidence intervals!</h1>

<p>from scipy import stats</p>

<p>def accuracy_confidence_interval(y_true, y_pred, confidence=0.95):
 “"”Compute confidence interval for accuracy”””
 n = len(y_true)
 y_true = np.array(y_true)
 y_pred = np.array(y_pred)
 accuracy = (y_true == y_pred).sum() / n</p>

<p># Wilson score interval
 z = stats.norm.ppf((1 + confidence) / 2)
 denominator = 1 + z<strong>2 / n
 center = (accuracy + z</strong>2 / (2*n)) / denominator
 margin = z * np.sqrt(accuracy * (1 - accuracy) / n + z<strong>2 / (4 * n</strong>2)) / denominator</p>

<p>return center - margin, center + margin</p>

<p>import numpy as np</p>

<h1 id="example-toy-predictions-for-illustration">Example toy predictions for illustration</h1>
<p>y_true_a = np.random.randint(0, 2, size=1000)
y_pred_a = np.random.randint(0, 2, size=1000)
y_true_b = np.random.randint(0, 2, size=1000)
y_pred_b = np.random.randint(0, 2, size=1000)</p>

<p>ci_a = accuracy_confidence_interval(y_true_a, y_pred_a)
acc_a = (y_true_a == y_pred_a).mean() * 100
print(f”Model A: {acc_a:.1f}% [{ci_a[0]<em>100:.1f}%, {ci_a[1]</em>100:.1f}%]”)</p>

<p>ci_b = accuracy_confidence_interval(y_true_b, y_pred_b)
acc_b = (y_true_b == y_pred_b).mean() * 100
print(f”Model B: {acc_b:.1f}% [{ci_b[0]<em>100:.1f}%, {ci_b[1]</em>100:.1f}%]”)</p>

<h1 id="if-intervals-overlap-significantly-difference-may-not-be-meaningful">If intervals overlap significantly, difference may not be meaningful</h1>
<p>``</p>

<h3 id="pitfall-4-overfitting-to-validation-set">Pitfall 4: Overfitting to Validation Set</h3>

<p>``python</p>
<h1 id="wrong-repeatedly-tuning-on-same-validation-set">WRONG: Repeatedly tuning on same validation set</h1>
<p>for _ in range(100): # Many iterations
 model = train_model(X_train, y_train, hyperparams)
 val_score = evaluate(model, X_val, y_val)
 hyperparams = adjust_based_on_score(val_score) # Overfitting to val!</p>

<h1 id="correct-use-nested-cross-validation-or-holdout-test-set">CORRECT: Use nested cross-validation or holdout test set</h1>
<p>X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2)</p>

<h1 id="tune-on-train_full-with-inner-cv">Tune on train_full (with inner CV)</h1>
<p>best_model = grid_search_cv(X_train_full, y_train_full)</p>

<h1 id="evaluate-once-on-test-set">Evaluate ONCE on test set</h1>
<p>final_score = evaluate(best_model, X_test, y_test)
``</p>

<hr />

<h2 id="connection-to-speech-systems">Connection to Speech Systems</h2>

<p>Model evaluation principles apply directly to speech/audio ML systems:</p>

<h3 id="tts-quality-metrics">TTS Quality Metrics</h3>

<p><strong>Objective Metrics:</strong></p>
<ul>
  <li><strong>Mel Cepstral Distortion (MCD):</strong> Similar to MSE for regression</li>
  <li><strong>F0 RMSE:</strong> Pitch prediction error</li>
  <li><strong>Duration Accuracy:</strong> Similar to classification metrics for boundary detection</li>
</ul>

<p><strong>Subjective Metrics:</strong></p>
<ul>
  <li><strong>Mean Opinion Score (MOS):</strong> Like human evaluation for content moderation</li>
  <li><strong>Must have confidence intervals:</strong> Just like accuracy CIs above</li>
</ul>

<h3 id="asr-error-metrics">ASR Error Metrics</h3>

<p><strong>Word Error Rate (WER):</strong>
``
WER = (S + D + I) / N</p>

<p>S: Substitutions
D: Deletions
I: Insertions
N: Total words in reference
``</p>

<p>Similar to precision/recall trade-off:</p>
<ul>
  <li>High substitutions → Low precision (predicting wrong words)</li>
  <li>High deletions → Low recall (missing words)</li>
</ul>

<h3 id="speaker-verification">Speaker Verification</h3>

<p>Uses same binary classification metrics:</p>
<ul>
  <li><strong>EER (Equal Error Rate):</strong> Point where FPR = FNR</li>
  <li><strong>DCF (Detection Cost Function):</strong> Business-driven threshold (like threshold tuning above)</li>
</ul>

<p>``python
def compute_eer(y_true, y_scores):
 “””
 Compute Equal Error Rate for speaker verification</p>

<p>Similar to finding optimal threshold
 “””
 from sklearn.metrics import roc_curve</p>

<p>fpr, tpr, thresholds = roc_curve(y_true, y_scores)
 fnr = 1 - tpr</p>

<p># Find where FPR ≈ FNR
 eer_idx = np.argmin(np.abs(fpr - fnr))
 eer = (fpr[eer_idx] + fnr[eer_idx]) / 2</p>

<p>return eer, thresholds[eer_idx]</p>

<h1 id="example-speaker-verification-scores">Example: Speaker verification scores</h1>
<p>y_true = [1, 1, 1, 0, 0, 0, 1, 1, 0, 0]
y_scores = [0.9, 0.85, 0.7, 0.4, 0.3, 0.2, 0.8, 0.75, 0.5, 0.35]</p>

<p>eer, eer_threshold = compute_eer(y_true, y_scores)
print(f”EER: {eer:.2%} at threshold {eer_threshold:.3f}”)
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>No single best metric</strong> - choice depends on problem and business context 
✅ <strong>Accuracy misleading</strong> for imbalanced datasets - use precision/recall/F1 
✅ <strong>ROC-AUC</strong> good for threshold-independent evaluation 
✅ <strong>Precision-Recall</strong> better than ROC for imbalanced data 
✅ <strong>Regression metrics</strong> - MSE for outlier sensitivity, MAE for robustness 
✅ <strong>Ranking metrics</strong> - NDCG for position-aware, MRR for first relevant item 
✅ <strong>Production monitoring</strong> - track metrics over time to detect degradation 
✅ <strong>Align with business</strong> - metrics must connect to business KPIs</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0006-model-evaluation-metrics/">arunbaby.com/ml-system-design/0006-model-evaluation-metrics</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#evaluation" class="page__taxonomy-item p-category" rel="tag">evaluation</a><span class="sep">, </span>
    
      <a href="/tags/#metrics" class="page__taxonomy-item p-category" rel="tag">metrics</a><span class="sep">, </span>
    
      <a href="/tags/#model-performance" class="page__taxonomy-item p-category" rel="tag">model-performance</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0006-climbing-stairs/" rel="permalink">Climbing Stairs
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          25 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The Fibonacci problem in disguise, teaching the fundamental transition from recursion to dynamic programming to space optimization.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0006-text-to-speech-basics/" rel="permalink">Text-to-Speech (TTS) System Fundamentals
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">From text to natural speech: understanding modern neural TTS architectures that power Alexa, Google Assistant, and Siri.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0006-agent-frameworks-landscape/" rel="permalink">Agent Frameworks Landscape
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“To Framework or Not to Framework? Navigating the Agent Ecosystem.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Model+Evaluation+Metrics%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0006-model-evaluation-metrics%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0006-model-evaluation-metrics%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0006-model-evaluation-metrics/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0005-batch-realtime-inference/" class="pagination--pager" title="Batch vs Real-Time Inference">Previous</a>
    
    
      <a href="/ml-system-design/0007-feature-engineering/" class="pagination--pager" title="Feature Engineering at Scale">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
