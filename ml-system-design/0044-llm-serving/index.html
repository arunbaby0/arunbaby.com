<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>LLM Serving Infrastructure - Arun Baby</title>
<meta name="description" content="“Serving models that think at human scale.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="LLM Serving Infrastructure">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0044-llm-serving/">


  <meta property="og:description" content="“Serving models that think at human scale.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="LLM Serving Infrastructure">
  <meta name="twitter:description" content="“Serving models that think at human scale.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0044-llm-serving/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-20T15:48:47+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0044-llm-serving/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="LLM Serving Infrastructure">
    <meta itemprop="description" content="“Serving models that think at human scale.”">
    <meta itemprop="datePublished" content="2025-12-20T15:48:47+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0044-llm-serving/" itemprop="url">LLM Serving Infrastructure
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction-the-llm-serving-challenge">1. Introduction: The LLM Serving Challenge</a></li><li><a href="#2-llm-inference-basics">2. LLM Inference Basics</a><ul><li><a href="#21-autoregressive-generation">2.1. Autoregressive Generation</a></li><li><a href="#22-memory-bottleneck">2.2. Memory Bottleneck</a></li></ul></li><li><a href="#3-key-performance-metrics">3. Key Performance Metrics</a></li><li><a href="#4-serving-architecture">4. Serving Architecture</a><ul><li><a href="#41-basic-architecture">4.1. Basic Architecture</a></li><li><a href="#42-components">4.2. Components</a></li></ul></li><li><a href="#5-optimization-techniques">5. Optimization Techniques</a><ul><li><a href="#51-continuous-batching">5.1. Continuous Batching</a></li><li><a href="#52-pagedattention-vllm">5.2. PagedAttention (vLLM)</a></li><li><a href="#53-speculative-decoding">5.3. Speculative Decoding</a></li><li><a href="#54-quantization">5.4. Quantization</a></li><li><a href="#55-tensor-parallelism">5.5. Tensor Parallelism</a></li><li><a href="#56-pipeline-parallelism">5.6. Pipeline Parallelism</a></li></ul></li><li><a href="#6-popular-llm-serving-frameworks">6. Popular LLM Serving Frameworks</a><ul><li><a href="#61-vllm">6.1. vLLM</a></li><li><a href="#62-tensorrt-llm-nvidia">6.2. TensorRT-LLM (NVIDIA)</a></li><li><a href="#63-text-generation-inference-tgi">6.3. Text Generation Inference (TGI)</a></li><li><a href="#64-ollama">6.4. Ollama</a></li><li><a href="#65-onnx-runtime">6.5. ONNX Runtime</a></li></ul></li><li><a href="#7-system-design-chatgpt-like-service">7. System Design: ChatGPT-like Service</a><ul><li><a href="#71-architecture">7.1. Architecture</a></li><li><a href="#72-request-flow">7.2. Request Flow</a></li><li><a href="#73-scaling-strategy">7.3. Scaling Strategy</a></li></ul></li><li><a href="#8-caching-strategies">8. Caching Strategies</a><ul><li><a href="#81-prompt-caching">8.1. Prompt Caching</a></li><li><a href="#82-semantic-caching">8.2. Semantic Caching</a></li><li><a href="#83-response-caching">8.3. Response Caching</a></li></ul></li><li><a href="#9-production-monitoring">9. Production Monitoring</a></li><li><a href="#10-cost-optimization">10. Cost Optimization</a></li><li><a href="#11-interview-questions">11. Interview Questions</a></li><li><a href="#12-common-pitfalls">12. Common Pitfalls</a></li><li><a href="#13-future-trends">13. Future Trends</a></li><li><a href="#14-conclusion">14. Conclusion</a></li><li><a href="#15-deep-dive-kv-cache-optimization">15. Deep Dive: KV Cache Optimization</a></li><li><a href="#16-deep-dive-mixture-of-experts-moe">16. Deep Dive: Mixture of Experts (MoE)</a></li><li><a href="#17-production-case-study-openai-chatgpt">17. Production Case Study: OpenAI ChatGPT</a></li><li><a href="#18-production-case-study-anthropic-claude">18. Production Case Study: Anthropic Claude</a></li><li><a href="#19-advanced-prefill-decode-disaggregation">19. Advanced: Prefill-Decode Disaggregation</a></li><li><a href="#20-advanced-request-scheduling">20. Advanced: Request Scheduling</a></li><li><a href="#21-benchmarking-llm-serving">21. Benchmarking LLM Serving</a></li><li><a href="#22-multi-model-serving">22. Multi-Model Serving</a></li><li><a href="#23-disaster-recovery">23. Disaster Recovery</a></li><li><a href="#24-mastery-checklist">24. Mastery Checklist</a></li><li><a href="#25-conclusion">25. Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Serving models that think at human scale.”</strong></p>

<h2 id="1-introduction-the-llm-serving-challenge">1. Introduction: The LLM Serving Challenge</h2>

<p>Large Language Models (LLMs) like GPT-4, Claude, and LLaMA present unique serving challenges:</p>

<ul>
  <li><strong>Size:</strong> 7B to 175B+ parameters (tens to hundreds of GBs).</li>
  <li><strong>Latency:</strong> Users expect real-time responses (&lt;1s for first token).</li>
  <li><strong>Cost:</strong> GPU compute is expensive ($2-10/hour per A100).</li>
  <li><strong>Scale:</strong> Millions of concurrent users.</li>
</ul>

<p><strong>Goal:</strong> Serve LLMs efficiently, balancing latency, throughput, and cost.</p>

<h2 id="2-llm-inference-basics">2. LLM Inference Basics</h2>

<h3 id="21-autoregressive-generation">2.1. Autoregressive Generation</h3>

<p>LLMs generate text token by token:</p>
<ol>
  <li><strong>Prefill:</strong> Process the input prompt (can be parallelized).</li>
  <li><strong>Decode:</strong> Generate one token at a time (sequential).</li>
</ol>

<p><strong>Latency Components:</strong></p>
<ul>
  <li><strong>Time to First Token (TTFT):</strong> Time to generate the first output token.</li>
  <li><strong>Time Per Output Token (TPOT):</strong> Time to generate each subsequent token.</li>
  <li><strong>Total Latency:</strong> TTFT + (num_tokens - 1) × TPOT.</li>
</ul>

<h3 id="22-memory-bottleneck">2.2. Memory Bottleneck</h3>

<p><strong>KV Cache:</strong> During generation, we cache the key-value pairs from previous tokens.</p>

<p><strong>Memory Usage:</strong></p>
<ul>
  <li>Model weights: 2 bytes per parameter (FP16).</li>
  <li>KV Cache: 2 × num_layers × hidden_size × 2 bytes per token.</li>
</ul>

<p><strong>Example (LLaMA-7B):</strong></p>
<ul>
  <li>Weights: 7B × 2 = 14GB.</li>
  <li>KV Cache (4096 tokens): 32 layers × 4096 dim × 4096 tokens × 2 × 2 = 4GB.</li>
  <li><strong>Total:</strong> ~18GB per request (with long context).</li>
</ul>

<h2 id="3-key-performance-metrics">3. Key Performance Metrics</h2>

<p><strong>Latency:</strong></p>
<ul>
  <li><strong>TTFT:</strong> Time to first token (&lt;500ms typical).</li>
  <li><strong>TPOT:</strong> Time per output token (&lt;50ms typical).</li>
</ul>

<p><strong>Throughput:</strong></p>
<ul>
  <li><strong>Tokens/second:</strong> How many tokens generated per second.</li>
  <li><strong>Requests/second:</strong> How many concurrent requests served.</li>
</ul>

<p><strong>Efficiency:</strong></p>
<ul>
  <li><strong>GPU Utilization:</strong> % of GPU compute used.</li>
  <li><strong>Cost per Token:</strong> $/1000 tokens.</li>
</ul>

<h2 id="4-serving-architecture">4. Serving Architecture</h2>

<h3 id="41-basic-architecture">4.1. Basic Architecture</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                    ┌─────────────────────┐
                    │   Load Balancer     │
                    │   (HAProxy/Nginx)   │
                    └─────────┬───────────┘
                              │
         ┌────────────────────┴────────────────────┐
         │                    │                    │
┌────────▼────────┐  ┌────────▼────────┐  ┌────────▼────────┐
│   GPU Node 1    │  │   GPU Node 2    │  │   GPU Node N    │
│  (4x A100 80GB) │  │  (4x A100 80GB) │  │  (4x A100 80GB) │
└─────────────────┘  └─────────────────┘  └─────────────────┘
</code></pre></div></div>

<h3 id="42-components">4.2. Components</h3>

<p><strong>1. Load Balancer:</strong></p>
<ul>
  <li>Distributes requests across GPU nodes.</li>
  <li>Health checks, sticky sessions (for streaming).</li>
</ul>

<p><strong>2. Inference Server:</strong></p>
<ul>
  <li>Runs the LLM.</li>
  <li>Examples: vLLM, TensorRT-LLM, TGI (Text Generation Inference).</li>
</ul>

<p><strong>3. Request Queue:</strong></p>
<ul>
  <li>Buffers requests during peak load.</li>
  <li>Prioritization (premium users first).</li>
</ul>

<p><strong>4. Caching Layer:</strong></p>
<ul>
  <li>Cache common prompts and responses.</li>
  <li>Reduces compute for repeated queries.</li>
</ul>

<h2 id="5-optimization-techniques">5. Optimization Techniques</h2>

<h3 id="51-continuous-batching">5.1. Continuous Batching</h3>

<p><strong>Problem:</strong> Static batching waits for all requests in a batch to finish before processing new ones.</p>

<p><strong>Solution:</strong> <strong>Continuous batching</strong> allows new requests to join the batch as soon as old ones finish.</p>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Higher GPU utilization.</li>
  <li>Lower latency for short requests.</li>
  <li>2-10x throughput improvement.</li>
</ul>

<h3 id="52-pagedattention-vllm">5.2. PagedAttention (vLLM)</h3>

<p><strong>Problem:</strong> KV cache memory is fragmented, leading to inefficient memory usage.</p>

<p><strong>Solution:</strong> PagedAttention manages KV cache like OS virtual memory:</p>
<ul>
  <li>Divide KV cache into fixed-size blocks.</li>
  <li>Allocate blocks on demand.</li>
  <li>Share blocks for common prefixes (e.g., system prompts).</li>
</ul>

<p><strong>Benefits:</strong></p>
<ul>
  <li>2-4x more concurrent requests.</li>
  <li>Efficient memory utilization.</li>
</ul>

<h3 id="53-speculative-decoding">5.3. Speculative Decoding</h3>

<p><strong>Problem:</strong> Autoregressive decoding is inherently sequential.</p>

<p><strong>Solution:</strong> Use a smaller, faster “draft” model to generate candidate tokens. Verify with the large model in parallel.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Draft model generates N tokens.</li>
  <li>Large model verifies (can be parallelized).</li>
  <li>Accept verified tokens, reject or correct others.</li>
</ol>

<p><strong>Benefits:</strong></p>
<ul>
  <li>2-3x speedup for when draft model matches well.</li>
</ul>

<h3 id="54-quantization">5.4. Quantization</h3>

<p><strong>Reduce memory and compute by using lower precision:</strong></p>
<ul>
  <li><strong>FP16:</strong> Standard (2 bytes/param).</li>
  <li><strong>INT8:</strong> 4x reduction (1 byte/param), minimal quality loss.</li>
  <li><strong>INT4:</strong> 8x reduction, some quality loss.</li>
  <li><strong>GPTQ/AWQ:</strong> Post-training quantization methods.</li>
</ul>

<p><strong>Example (LLaMA-70B):</strong></p>
<ul>
  <li>FP16: 140GB (2x A100 80GB).</li>
  <li>INT4: 35GB (1x A100 80GB).</li>
</ul>

<h3 id="55-tensor-parallelism">5.5. Tensor Parallelism</h3>

<p><strong>Split model across multiple GPUs:</strong></p>
<ul>
  <li>Each GPU holds a portion of each layer.</li>
  <li>Communicate intermediate results.</li>
</ul>

<p><strong>Use Case:</strong> Models too large for single GPU.</p>

<h3 id="56-pipeline-parallelism">5.6. Pipeline Parallelism</h3>

<p><strong>Split model by layers across GPUs:</strong></p>
<ul>
  <li>GPU 1: Layers 1-10.</li>
  <li>GPU 2: Layers 11-20.</li>
  <li>GPU 3: Layers 21-30.</li>
</ul>

<p><strong>Issue:</strong> Bubble time (GPUs waiting for each other).</p>

<h2 id="6-popular-llm-serving-frameworks">6. Popular LLM Serving Frameworks</h2>

<h3 id="61-vllm">6.1. vLLM</h3>

<p><strong>Features:</strong></p>
<ul>
  <li>PagedAttention for memory efficiency.</li>
  <li>Continuous batching.</li>
  <li>OpenAI-compatible API.</li>
  <li>High throughput.</li>
</ul>

<p><strong>Usage:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">vllm</span> <span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">meta-llama/Llama-2-7b-hf</span><span class="sh">"</span><span class="p">)</span>
<span class="n">sampling_params</span> <span class="o">=</span> <span class="nc">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">([</span><span class="sh">"</span><span class="s">What is AI?</span><span class="sh">"</span><span class="p">],</span> <span class="n">sampling_params</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="62-tensorrt-llm-nvidia">6.2. TensorRT-LLM (NVIDIA)</h3>

<p><strong>Features:</strong></p>
<ul>
  <li>NVIDIA-optimized kernels.</li>
  <li>INT8/INT4 quantization.</li>
  <li>Tensor parallelism.</li>
  <li>Flash Attention.</li>
</ul>

<p><strong>Best for:</strong> Maximum performance on NVIDIA GPUs.</p>

<h3 id="63-text-generation-inference-tgi">6.3. Text Generation Inference (TGI)</h3>

<p><strong>Features:</strong></p>
<ul>
  <li>By Hugging Face.</li>
  <li>Docker-based deployment.</li>
  <li>Supports many models.</li>
  <li>Streaming support.</li>
</ul>

<h3 id="64-ollama">6.4. Ollama</h3>

<p><strong>Features:</strong></p>
<ul>
  <li>Simple local deployment.</li>
  <li>Bundled models.</li>
  <li>Good for development/testing.</li>
</ul>

<h3 id="65-onnx-runtime">6.5. ONNX Runtime</h3>

<p><strong>Features:</strong></p>
<ul>
  <li>Cross-platform.</li>
  <li>CPU and GPU support.</li>
  <li>Quantization support.</li>
</ul>

<h2 id="7-system-design-chatgpt-like-service">7. System Design: ChatGPT-like Service</h2>

<p><strong>Requirements:</strong></p>
<ul>
  <li>Handle 1M+ requests/day.</li>
  <li>TTFT &lt; 500ms.</li>
  <li>Support streaming responses.</li>
  <li>Handle long conversations (context &gt; 4K tokens).</li>
</ul>

<h3 id="71-architecture">7.1. Architecture</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                    ┌─────────────────────┐
                    │   API Gateway       │
                    │   (Rate Limiting)   │
                    └─────────┬───────────┘
                              │
                    ┌─────────▼───────────┐
                    │   Request Router    │
                    │   (Model Selection) │
                    └─────────┬───────────┘
                              │
         ┌────────────────────┴────────────────────┐
         │                                         │
┌────────▼────────┐                      ┌────────▼────────┐
│   Inference     │                      │   Inference     │
│   Cluster A     │                      │   Cluster B     │
│   (GPT-4)       │                      │   (GPT-3.5)     │
└────────┬────────┘                      └────────┬────────┘
         │                                         │
         └────────────────────┬────────────────────┘
                              │
                    ┌─────────▼───────────┐
                    │   Streaming         │
                    │   Response Server   │
                    └─────────────────────┘
</code></pre></div></div>

<h3 id="72-request-flow">7.2. Request Flow</h3>

<ol>
  <li><strong>Authentication:</strong> Verify API key, check rate limits.</li>
  <li><strong>Routing:</strong> Route to appropriate model cluster.</li>
  <li><strong>Queueing:</strong> Add to request queue with priority.</li>
  <li><strong>Inference:</strong> Process with continuous batching.</li>
  <li><strong>Streaming:</strong> Stream tokens back to client.</li>
</ol>

<h3 id="73-scaling-strategy">7.3. Scaling Strategy</h3>

<p><strong>Horizontal Scaling:</strong></p>
<ul>
  <li>Add more GPU nodes for throughput.</li>
  <li>Use auto-scaling based on queue depth.</li>
</ul>

<p><strong>Vertical Scaling:</strong></p>
<ul>
  <li>Use larger GPUs (A100 → H100).</li>
  <li>Use tensor parallelism for larger models.</li>
</ul>

<h2 id="8-caching-strategies">8. Caching Strategies</h2>

<h3 id="81-prompt-caching">8.1. Prompt Caching</h3>

<p><strong>Cache common prompts</strong> (system messages, instructions).</p>

<p><strong>Implementation:</strong></p>
<ul>
  <li>Hash the prompt prefix.</li>
  <li>Store KV cache for the prefix.</li>
  <li>Reuse for new requests with same prefix.</li>
</ul>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Lower TTFT for repeated prompts.</li>
  <li>Reduced compute.</li>
</ul>

<h3 id="82-semantic-caching">8.2. Semantic Caching</h3>

<p><strong>Cache based on semantic similarity:</strong></p>
<ul>
  <li>Embed the query.</li>
  <li>Find similar cached queries.</li>
  <li>Return cached response if similarity &gt; threshold.</li>
</ul>

<p><strong>Tools:</strong> GPTCache, LangChain caching.</p>

<h3 id="83-response-caching">8.3. Response Caching</h3>

<p><strong>Cache full responses for common queries:</strong></p>
<ul>
  <li>Hash the full input.</li>
  <li>Store the response.</li>
  <li>Return cached response for exact matches.</li>
</ul>

<h2 id="9-production-monitoring">9. Production Monitoring</h2>

<p><strong>Metrics to Track:</strong></p>
<ol>
  <li><strong>Latency:</strong> TTFT, TPOT, P50/P95/P99.</li>
  <li><strong>Throughput:</strong> Requests/second, tokens/second.</li>
  <li><strong>GPU Utilization:</strong> Compute and memory.</li>
  <li><strong>Queue Depth:</strong> Requests waiting.</li>
  <li><strong>Error Rate:</strong> Failed requests.</li>
</ol>

<p><strong>Dashboards:</strong></p>
<ul>
  <li>Grafana with Prometheus.</li>
  <li>Custom dashboards in Datadog.</li>
</ul>

<p><strong>Alerting:</strong></p>
<ul>
  <li>TTFT P99 &gt; 1s.</li>
  <li>GPU utilization &lt; 50% (underutilization).</li>
  <li>Error rate &gt; 1%.</li>
</ul>

<h2 id="10-cost-optimization">10. Cost Optimization</h2>

<p><strong>Cost Drivers:</strong></p>
<ul>
  <li>GPU hours (major cost).</li>
  <li>Network egress (streaming).</li>
  <li>Storage (model weights, logs).</li>
</ul>

<p><strong>Optimization Strategies:</strong></p>
<ol>
  <li><strong>Quantization:</strong> Fit larger models on fewer GPUs.</li>
  <li><strong>Batching:</strong> Higher throughput = lower cost/request.</li>
  <li><strong>Caching:</strong> Avoid redundant computation.</li>
  <li><strong>Spot Instances:</strong> Use for non-critical workloads.</li>
  <li><strong>Auto-Scaling:</strong> Scale down during off-peak hours.</li>
</ol>

<p><strong>Example Cost Analysis:</strong></p>
<ul>
  <li>A100 80GB: ~$2/hour.</li>
  <li>LLaMA-7B: 1000 tokens/second.</li>
  <li>Cost: $0.002 per 1000 tokens.</li>
</ul>

<h2 id="11-interview-questions">11. Interview Questions</h2>

<ol>
  <li><strong>Explain continuous batching.</strong> Why is it better than static batching?</li>
  <li><strong>What is PagedAttention?</strong> How does it improve memory efficiency?</li>
  <li><strong>How do you handle long contexts (&gt;4K tokens)?</strong></li>
  <li><strong>Design a ChatGPT-like service.</strong> What are the key components?</li>
  <li><strong>Compare vLLM, TensorRT-LLM, and TGI.</strong> When would you use each?</li>
</ol>

<h2 id="12-common-pitfalls">12. Common Pitfalls</h2>

<ul>
  <li><strong>Underestimating Memory:</strong> KV cache grows with sequence length.</li>
  <li><strong>Ignoring Cold Start:</strong> First request is slow (model loading).</li>
  <li><strong>Not Using Streaming:</strong> Users perceive lower latency with streaming.</li>
  <li><strong>Over-Provisioning:</strong> GPU idle time is wasted money.</li>
  <li><strong>Ignoring Quantization:</strong> FP16 may be overkill for many applications.</li>
</ul>

<h2 id="13-future-trends">13. Future Trends</h2>

<p><strong>1. Specialized Hardware:</strong></p>
<ul>
  <li>NVIDIA H100, AMD MI300.</li>
  <li>Google TPU v5.</li>
  <li>Custom ASICs (Groq, Cerebras).</li>
</ul>

<p><strong>2. Efficient Architectures:</strong></p>
<ul>
  <li>Mixture of Experts (MoE).</li>
  <li>State Space Models (Mamba).</li>
  <li>Sparse Attention.</li>
</ul>

<p><strong>3. On-Device LLMs:</strong></p>
<ul>
  <li>Quantized models on mobile.</li>
  <li>Apple Intelligence, Gemini Nano.</li>
</ul>

<p><strong>4. Disaggregated Serving:</strong></p>
<ul>
  <li>Separate prefill and decode phases.</li>
  <li>Optimize each independently.</li>
</ul>

<h2 id="14-conclusion">14. Conclusion</h2>

<p>LLM serving is a rapidly evolving field. The key challenges are:</p>
<ul>
  <li><strong>Memory:</strong> KV cache is the bottleneck.</li>
  <li><strong>Latency:</strong> Users expect real-time responses.</li>
  <li><strong>Cost:</strong> GPU compute is expensive.</li>
</ul>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Continuous Batching:</strong> Essential for throughput.</li>
  <li><strong>PagedAttention:</strong> Efficient memory management.</li>
  <li><strong>Quantization:</strong> Reduce memory and cost.</li>
  <li><strong>Caching:</strong> Avoid redundant computation.</li>
  <li><strong>Monitoring:</strong> Track latency, throughput, and utilization.</li>
</ul>

<p>As LLMs become ubiquitous, efficient serving will become a critical skill. The techniques here are the foundation for building production LLM systems at scale.</p>

<h2 id="15-deep-dive-kv-cache-optimization">15. Deep Dive: KV Cache Optimization</h2>

<p><strong>Challenge:</strong> KV cache grows linearly with sequence length.</p>

<p><strong>For LLaMA-7B, 4K context:</strong></p>
<ul>
  <li>32 layers × 32 heads × 128 dim × 4K tokens × 2 (K+V) × 2 bytes = 4GB per request.</li>
</ul>

<p><strong>Optimization Strategies:</strong></p>

<p><strong>1. Multi-Query Attention (MQA):</strong></p>
<ul>
  <li>Share K and V heads across attention heads.</li>
  <li>Reduction: 8x (if 8 Q heads share 1 KV head).</li>
</ul>

<p><strong>2. Grouped-Query Attention (GQA):</strong></p>
<ul>
  <li>Compromise between MHA and MQA.</li>
  <li>LLaMA 2 70B uses 8 KV heads for 64 Q heads.</li>
</ul>

<p><strong>3. KV Cache Compression:</strong></p>
<ul>
  <li>Quantize KV cache to INT8.</li>
  <li>2x reduction, minimal quality loss.</li>
</ul>

<p><strong>4. Sliding Window Attention:</strong></p>
<ul>
  <li>Only attend to last N tokens.</li>
  <li>Mistral uses 4K sliding window.</li>
</ul>

<p><strong>5. Sparse Attention:</strong></p>
<ul>
  <li>Only attend to important tokens.</li>
  <li>Block-sparse or learned patterns.</li>
</ul>

<h2 id="16-deep-dive-mixture-of-experts-moe">16. Deep Dive: Mixture of Experts (MoE)</h2>

<p><strong>Idea:</strong> Not all parameters are active for each token. Route tokens to specialized experts.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Multiple FFN “experts” per layer.</li>
  <li>Router selects top-K experts per token.</li>
  <li>Only K experts’ parameters are active.</li>
</ul>

<p><strong>Benefits:</strong></p>
<ul>
  <li>More parameters with similar compute.</li>
  <li>Mixtral-8x7B: 8 experts, uses 2 per token.</li>
</ul>

<p><strong>Serving Challenges:</strong></p>
<ul>
  <li>Expert weights may be on different GPUs.</li>
  <li>All-to-all communication for routing.</li>
  <li>Load balancing across experts.</li>
</ul>

<h2 id="17-production-case-study-openai-chatgpt">17. Production Case Study: OpenAI ChatGPT</h2>

<p><strong>Scale:</strong></p>
<ul>
  <li>Millions of requests per day.</li>
  <li>Multiple model sizes (GPT-3.5, GPT-4).</li>
</ul>

<p><strong>Architecture (Estimated):</strong></p>
<ul>
  <li>Azure infrastructure.</li>
  <li>A100 clusters with tensor parallelism.</li>
  <li>Custom inference stack.</li>
</ul>

<p><strong>Optimizations:</strong></p>
<ul>
  <li>Speculative decoding for GPT-4.</li>
  <li>Heavy prompt caching.</li>
  <li>Priority queuing for Plus subscribers.</li>
</ul>

<h2 id="18-production-case-study-anthropic-claude">18. Production Case Study: Anthropic Claude</h2>

<p><strong>Innovations:</strong></p>
<ul>
  <li>Constitutional AI (safety filtering).</li>
  <li>Long context (100K+ tokens).</li>
</ul>

<p><strong>Serving Challenges:</strong></p>
<ul>
  <li>100K context = massive KV cache.</li>
  <li>Requires memory optimization.</li>
</ul>

<p><strong>Solutions (Likely):</strong></p>
<ul>
  <li>Sliding window + summary caching.</li>
  <li>Efficient attention patterns.</li>
</ul>

<h2 id="19-advanced-prefill-decode-disaggregation">19. Advanced: Prefill-Decode Disaggregation</h2>

<p><strong>Observation:</strong> Prefill and decode have different compute profiles.</p>
<ul>
  <li><strong>Prefill:</strong> Compute-bound (parallel).</li>
  <li><strong>Decode:</strong> Memory-bound (sequential).</li>
</ul>

<p><strong>Solution:</strong> Use different hardware for each phase.</p>
<ul>
  <li><strong>Prefill:</strong> High-compute GPUs (H100).</li>
  <li><strong>Decode:</strong> Memory-optimized GPUs (A10G).</li>
</ul>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Better resource utilization.</li>
  <li>Lower cost per token.</li>
</ul>

<h2 id="20-advanced-request-scheduling">20. Advanced: Request Scheduling</h2>

<p><strong>Challenge:</strong> Different requests have different priorities and lengths.</p>

<p><strong>Strategies:</strong></p>
<ol>
  <li><strong>FIFO:</strong> Simple, but long requests block short ones.</li>
  <li><strong>Shortest Job First:</strong> Better latency, but starvation risk.</li>
  <li><strong>Priority Queuing:</strong> Premium users first.</li>
  <li><strong>Fair Scheduling:</strong> Balance latency across users.</li>
</ol>

<p><strong>Implementation:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RequestScheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">queues</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">premium</span><span class="sh">'</span><span class="p">:</span> <span class="p">[],</span>
            <span class="sh">'</span><span class="s">standard</span><span class="sh">'</span><span class="p">:</span> <span class="p">[],</span>
            <span class="sh">'</span><span class="s">batch</span><span class="sh">'</span><span class="p">:</span> <span class="p">[]</span>
        <span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">add_request</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">priority</span><span class="o">=</span><span class="sh">'</span><span class="s">standard</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">queues</span><span class="p">[</span><span class="n">priority</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">get_next_request</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">priority</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">premium</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">standard</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">batch</span><span class="sh">'</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">queues</span><span class="p">[</span><span class="n">priority</span><span class="p">]:</span>
                <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">queues</span><span class="p">[</span><span class="n">priority</span><span class="p">].</span><span class="nf">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">None</span>
</code></pre></div></div>

<h2 id="21-benchmarking-llm-serving">21. Benchmarking LLM Serving</h2>

<p><strong>Metrics:</strong></p>
<ul>
  <li><strong>Tokens/second/GPU:</strong> Throughput efficiency.</li>
  <li><strong>TTFT P50/P99:</strong> Latency for first token.</li>
  <li><strong>TPOT:</strong> Time per output token.</li>
  <li><strong>Maximum Concurrent Requests:</strong> Scalability.</li>
</ul>

<p><strong>Benchmark Tools:</strong></p>
<ul>
  <li><strong>LLMPerf:</strong> Open-source benchmark suite.</li>
  <li><strong>vLLM Benchmark:</strong> Built-in benchmarking.</li>
  <li><strong>Custom Load Testing:</strong> Locust, k6.</li>
</ul>

<p><strong>Example Benchmark (LLaMA-7B on A100 80GB):</strong>
| Framework | Tokens/sec | TTFT (ms) | Max Concurrent |
|———–|————|———–|—————-|
| vLLM | 2000 | 50 | 100 |
| TensorRT-LLM | 2500 | 40 | 120 |
| TGI | 1500 | 80 | 80 |</p>

<h2 id="22-multi-model-serving">22. Multi-Model Serving</h2>

<p><strong>Scenario:</strong> Serve multiple models on the same infrastructure.</p>

<p><strong>Approaches:</strong></p>
<ol>
  <li><strong>Dedicated Clusters:</strong> Each model on separate GPUs.</li>
  <li><strong>Time-Slicing:</strong> Switch models on same GPU.</li>
  <li><strong>Memory Sharing:</strong> Load multiple models if memory allows.</li>
</ol>

<p><strong>Trade-offs:</strong></p>
<ul>
  <li>Dedicated: Simple, no interference, but expensive.</li>
  <li>Time-Slicing: Flexible, but model loading overhead.</li>
  <li>Memory Sharing: Efficient, but complex management.</li>
</ul>

<h2 id="23-disaster-recovery">23. Disaster Recovery</h2>

<p><strong>Scenarios:</strong></p>
<ul>
  <li>GPU failure.</li>
  <li>Network partition.</li>
  <li>Model corruption.</li>
</ul>

<p><strong>Strategies:</strong></p>
<ol>
  <li><strong>Redundancy:</strong> Multiple replicas of each model.</li>
  <li><strong>Fallback:</strong> Route to smaller model if primary fails.</li>
  <li><strong>Health Checks:</strong> Quick detection and recovery.</li>
  <li><strong>Load Shedding:</strong> Reject requests gracefully under overload.</li>
</ol>

<h2 id="24-mastery-checklist">24. Mastery Checklist</h2>

<p><strong>Mastery Checklist:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Explain TTFT vs TPOT</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement continuous batching</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Understand PagedAttention</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Deploy vLLM for production</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement prompt caching</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Configure quantization (INT8/INT4)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Set up monitoring dashboards</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Estimate costs per 1000 tokens</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Design ChatGPT-like architecture</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Handle long context (&gt;32K tokens)</li>
</ul>

<h2 id="25-conclusion">25. Conclusion</h2>

<p>LLM serving is one of the most challenging and rewarding areas in ML infrastructure. The techniques covered here—continuous batching, PagedAttention, quantization, and caching—are the building blocks of production LLM systems.</p>

<p>As models get larger and contexts get longer, these optimizations become even more critical. The engineers who master LLM serving will be essential to the AI infrastructure of the future.</p>

<p><strong>The key insight:</strong> LLM inference is memory-bound, not compute-bound. Optimize for memory, and throughput will follow.</p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#inference" class="page__taxonomy-item p-category" rel="tag">inference</a><span class="sep">, </span>
    
      <a href="/tags/#infrastructure" class="page__taxonomy-item p-category" rel="tag">infrastructure</a><span class="sep">, </span>
    
      <a href="/tags/#llm" class="page__taxonomy-item p-category" rel="tag">llm</a><span class="sep">, </span>
    
      <a href="/tags/#optimization" class="page__taxonomy-item p-category" rel="tag">optimization</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=LLM+Serving+Infrastructure%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0044-llm-serving%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0044-llm-serving%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0044-llm-serving/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0043-vector-databases/" class="pagination--pager" title="Vector Databases">Previous</a>
    
    
      <a href="/ml-system-design/0045-rag-systems/" class="pagination--pager" title="RAG Systems">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
