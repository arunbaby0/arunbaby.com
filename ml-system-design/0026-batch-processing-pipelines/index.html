<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Batch Processing Pipelines - Arun Baby</title>
<meta name="description" content="Not everything needs to be real-time. Sometimes, “tomorrow morning” is fast enough.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Batch Processing Pipelines">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0026-batch-processing-pipelines/">


  <meta property="og:description" content="Not everything needs to be real-time. Sometimes, “tomorrow morning” is fast enough.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Batch Processing Pipelines">
  <meta name="twitter:description" content="Not everything needs to be real-time. Sometimes, “tomorrow morning” is fast enough.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0026-batch-processing-pipelines/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-20T15:48:47+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0026-batch-processing-pipelines/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Batch Processing Pipelines">
    <meta itemprop="description" content="Not everything needs to be real-time. Sometimes, “tomorrow morning” is fast enough.">
    <meta itemprop="datePublished" content="2025-12-20T15:48:47+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0026-batch-processing-pipelines/" itemprop="url">Batch Processing Pipelines
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#the-case-for-batch">The Case for Batch</a></li><li><a href="#architecture-the-modern-data-stack">Architecture: The Modern Data Stack</a></li><li><a href="#high-level-architecture-the-modern-data-stack">High-Level Architecture: The Modern Data Stack</a></li><li><a href="#deep-dive-apache-airflow-orchestration">Deep Dive: Apache Airflow (Orchestration)</a></li><li><a href="#deep-dive-apache-spark-processing">Deep Dive: Apache Spark (Processing)</a></li><li><a href="#system-design-retraining-pipeline">System Design: Retraining Pipeline</a></li><li><a href="#engineering-the-small-files-problem">Engineering: The “Small Files” Problem</a></li><li><a href="#deep-dive-lambda-vs-kappa-architecture">Deep Dive: Lambda vs. Kappa Architecture</a></li><li><a href="#engineering-file-formats-parquet-vs-avro-vs-orc">Engineering: File Formats (Parquet vs. Avro vs. ORC)</a></li><li><a href="#deep-dive-partitioning-and-bucketing">Deep Dive: Partitioning and Bucketing</a></li><li><a href="#system-design-handling-late-data">System Design: Handling Late Data</a></li><li><a href="#deep-dive-idempotency">Deep Dive: Idempotency</a></li><li><a href="#engineering-data-quality-with-great-expectations">Engineering: Data Quality with Great Expectations</a></li><li><a href="#appendix-b-workflow-orchestration-wars">Appendix B: Workflow Orchestration Wars</a></li><li><a href="#appendix-c-interview-questions">Appendix C: Interview Questions</a></li><li><a href="#deep-dive-spark-catalyst-optimizer">Deep Dive: Spark Catalyst Optimizer</a></li><li><a href="#deep-dive-the-shuffle-timsort">Deep Dive: The Shuffle (Timsort)</a></li><li><a href="#system-design-data-lineage-openlineage">System Design: Data Lineage (OpenLineage)</a></li><li><a href="#engineering-cicd-for-data-pipelines">Engineering: CI/CD for Data Pipelines</a></li><li><a href="#finops-autoscaling-strategies">FinOps: Autoscaling Strategies</a></li><li><a href="#case-study-netflix-data-mesh">Case Study: Netflix Data Mesh</a></li><li><a href="#appendix-d-the-small-files-problem-revisited">Appendix D: The “Small Files” Problem (Revisited)</a></li><li><a href="#appendix-e-advanced-interview-questions">Appendix E: Advanced Interview Questions</a></li><li><a href="#deep-dive-bloom-filters-probabilistic-data-structures">Deep Dive: Bloom Filters (Probabilistic Data Structures)</a></li><li><a href="#deep-dive-hyperloglog-cardinality-estimation">Deep Dive: HyperLogLog (Cardinality Estimation)</a></li><li><a href="#deep-dive-count-min-sketch-frequency-estimation">Deep Dive: Count-Min Sketch (Frequency Estimation)</a></li><li><a href="#engineering-handling-pii-with-hashingsalting">Engineering: Handling PII with Hashing/Salting</a></li><li><a href="#case-study-ubers-michelangelo-batch-training">Case Study: Uber’s Michelangelo (Batch Training)</a></li><li><a href="#appendix-f-the-thundering-herd-problem">Appendix F: The “Thundering Herd” Problem</a></li><li><a href="#conclusion">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Not everything needs to be real-time. Sometimes, “tomorrow morning” is fast enough.</strong></p>

<h2 id="the-case-for-batch">The Case for Batch</h2>

<p>In the age of Real-Time Streaming (Kafka, Flink), Batch Processing feels archaic.
But 90% of ML workloads are still Batch.</p>
<ul>
  <li><strong>Training:</strong> You train on a <em>dataset</em> (Batch), not a stream.</li>
  <li><strong>Reporting:</strong> “Daily Active Users” is a batch metric.</li>
  <li><strong>Backfilling:</strong> Re-computing history requires batch.</li>
</ul>

<p><strong>Batch vs. Stream:</strong></p>
<ul>
  <li><strong>Batch:</strong> High Throughput, High Latency. (Process 1TB in 1 hour).</li>
  <li><strong>Stream:</strong> Low Throughput, Low Latency. (Process 1 event in 10ms).</li>
</ul>

<h2 id="architecture-the-modern-data-stack">Architecture: The Modern Data Stack</h2>

<ol>
  <li><strong>Ingestion:</strong> Fivetran / Airbyte. Pull data from Postgres/Salesforce into the Warehouse.</li>
  <li><strong>Storage:</strong> Data Lake (S3/GCS) or Data Warehouse (Snowflake/BigQuery).</li>
  <li><strong>Transformation:</strong> dbt (SQL) or Spark (Python).</li>
  <li><strong>Orchestration:</strong> Airflow / Dagster / Prefect.</li>
</ol>

<h2 id="high-level-architecture-the-modern-data-stack">High-Level Architecture: The Modern Data Stack</h2>

<pre><code class="language-ascii">+-----------+     +------------+     +-------------+     +-------------+
|  Sources  | --&gt; | Ingestion  | --&gt; |  Data Lake  | --&gt; |  Warehouse  |
+-----------+     +------------+     +-------------+     +-------------+
(Postgres)        (Fivetran)         (S3 / GCS)          (Snowflake)
                                                               |
                                                               v
+-----------+     +------------+     +-------------+     +-------------+
| Dashboard | &lt;-- | Serving    | &lt;-- | Transform   | &lt;-- | Orchestrator|
+-----------+     +------------+     +-------------+     +-------------+
(Tableau)         (Redis/API)        (dbt / Spark)       (Airflow)
</code></pre>

<h2 id="deep-dive-apache-airflow-orchestration">Deep Dive: Apache Airflow (Orchestration)</h2>

<p>Airflow allows you to define pipelines as code (Python).
<strong>DAG (Directed Acyclic Graph):</strong> A collection of tasks with dependencies.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="n">airflow.operators.python</span> <span class="kn">import</span> <span class="n">PythonOperator</span>
<span class="kn">from</span> <span class="n">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="k">def</span> <span class="nf">extract</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Extracting data from S3...</span><span class="sh">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">transform</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Running Spark Job...</span><span class="sh">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Loading into Feature Store...</span><span class="sh">"</span><span class="p">)</span>

<span class="k">with</span> <span class="nc">DAG</span><span class="p">(</span><span class="sh">"</span><span class="s">daily_training_pipeline</span><span class="sh">"</span><span class="p">,</span> <span class="n">start_date</span><span class="o">=</span><span class="nf">datetime</span><span class="p">(</span><span class="mi">2023</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="nc">PythonOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="sh">"</span><span class="s">extract</span><span class="sh">"</span><span class="p">,</span> <span class="n">python_callable</span><span class="o">=</span><span class="n">extract</span><span class="p">)</span>
    <span class="n">t2</span> <span class="o">=</span> <span class="nc">PythonOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="sh">"</span><span class="s">transform</span><span class="sh">"</span><span class="p">,</span> <span class="n">python_callable</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">t3</span> <span class="o">=</span> <span class="nc">PythonOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="sh">"</span><span class="s">load</span><span class="sh">"</span><span class="p">,</span> <span class="n">python_callable</span><span class="o">=</span><span class="n">load</span><span class="p">)</span>

    <span class="n">t1</span> <span class="o">&gt;&gt;</span> <span class="n">t2</span> <span class="o">&gt;&gt;</span> <span class="n">t3</span> <span class="c1"># Define dependencies
</span></code></pre></div></div>

<p><strong>Key Concepts:</strong></p>
<ul>
  <li><strong>Scheduler:</strong> Monitors time and triggers DAGs.</li>
  <li><strong>Executor:</strong> Runs the tasks (Local, Celery, Kubernetes).</li>
  <li><strong>Backfill:</strong> Rerunning the DAG for past dates (e.g., “Run for all of 2022”).</li>
</ul>

<h2 id="deep-dive-apache-spark-processing">Deep Dive: Apache Spark (Processing)</h2>

<p>When data doesn’t fit in RAM (Pandas), you need Spark.
Spark is a <strong>Distributed Computing Engine</strong>.</p>

<p><strong>RDD (Resilient Distributed Dataset):</strong></p>
<ul>
  <li><strong>Distributed:</strong> Data is split into partitions across nodes.</li>
  <li><strong>Resilient:</strong> If a node fails, Spark rebuilds the partition using the lineage graph.</li>
  <li><strong>Lazy Evaluation:</strong> Transformations (<code class="language-plaintext highlighter-rouge">map</code>, <code class="language-plaintext highlighter-rouge">filter</code>) are not executed until an Action (<code class="language-plaintext highlighter-rouge">count</code>, <code class="language-plaintext highlighter-rouge">save</code>) is called.</li>
</ul>

<p><strong>PySpark Example:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">FeatureEng</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="c1"># Read 1TB of logs
</span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">json</span><span class="p">(</span><span class="sh">"</span><span class="s">s3://bucket/logs/*.json</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Group By User and Count Clicks
</span><span class="n">user_features</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">groupBy</span><span class="p">(</span><span class="sh">"</span><span class="s">user_id</span><span class="sh">"</span><span class="p">).</span><span class="nf">count</span><span class="p">()</span>

<span class="c1"># Write to Parquet
</span><span class="n">user_features</span><span class="p">.</span><span class="n">write</span><span class="p">.</span><span class="nf">parquet</span><span class="p">(</span><span class="sh">"</span><span class="s">s3://bucket/features/</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="system-design-retraining-pipeline">System Design: Retraining Pipeline</h2>

<p><strong>Scenario:</strong> Design a system to retrain the Recommendation Model every week.</p>

<p><strong>Components:</strong></p>
<ol>
  <li><strong>Trigger:</strong> Airflow DAG runs every Sunday at 00:00.</li>
  <li><strong>Data Prep:</strong> Spark job reads last 30 days of clicks from Data Lake. Joins with User Table. Saves <code class="language-plaintext highlighter-rouge">training_data.parquet</code>.</li>
  <li><strong>Validation:</strong> Great Expectations checks for nulls/outliers.</li>
  <li><strong>Training:</strong> SageMaker Training Job launches a GPU instance. Loads parquet. Trains XGBoost. Saves <code class="language-plaintext highlighter-rouge">model.tar.gz</code>.</li>
  <li><strong>Evaluation:</strong> Load model. Predict on Holdout Set. If <code class="language-plaintext highlighter-rouge">AUC &gt; 0.8</code>, tag as <code class="language-plaintext highlighter-rouge">Production</code>.</li>
  <li><strong>Deployment:</strong> Update the SageMaker Endpoint to point to the new model artifact.</li>
</ol>

<h2 id="engineering-the-small-files-problem">Engineering: The “Small Files” Problem</h2>

<p>A common mistake in Data Lakes: Saving millions of tiny files (1KB each).
<strong>Why is it bad?</strong></p>
<ul>
  <li><strong>S3 API Costs:</strong> You pay per PUT/GET request.</li>
  <li><strong>Spark Slowness:</strong> Listing millions of files takes forever. Opening a file has overhead.</li>
</ul>

<p><strong>Solution:</strong> Compaction.
Run a nightly job to merge small files into larger files (128MB - 1GB).
<code class="language-plaintext highlighter-rouge">df.repartition(10).write...</code></p>

<h2 id="deep-dive-lambda-vs-kappa-architecture">Deep Dive: Lambda vs. Kappa Architecture</h2>

<p>How do you combine Batch (Accuracy) and Stream (Speed)?</p>

<p><strong>1. Lambda Architecture (The Old Way):</strong></p>
<ul>
  <li><strong>Speed Layer:</strong> Kafka + Flink. Provides low-latency, approximate results.</li>
  <li><strong>Batch Layer:</strong> Hadoop/Spark. Provides high-latency, accurate results.</li>
  <li><strong>Serving Layer:</strong> Merges the two.</li>
  <li><strong>Pros:</strong> Robust. If Stream fails, Batch fixes it.</li>
  <li><strong>Cons:</strong> <strong>Maintenance Nightmare.</strong> You write logic twice (Java for Flink, Python for Spark).</li>
</ul>

<p><strong>2. Kappa Architecture (The New Way):</strong></p>
<ul>
  <li><strong>Everything is a Stream.</strong></li>
  <li>Batch is just a stream of bounded data.</li>
  <li>Use <strong>Flink</strong> for both.</li>
  <li><strong>Pros:</strong> Single codebase.</li>
  <li><strong>Cons:</strong> Reprocessing history is harder (requires replaying the Kafka topic).</li>
</ul>

<h2 id="engineering-file-formats-parquet-vs-avro-vs-orc">Engineering: File Formats (Parquet vs. Avro vs. ORC)</h2>

<p>CSV/JSON are terrible for Big Data (Slow parsing, no schema, large size).</p>

<p><strong>1. Parquet (Columnar):</strong></p>
<ul>
  <li><strong>Best for:</strong> Analytics (OLAP). “Select average(age) from users”.</li>
  <li><strong>Why:</strong> It only reads the “age” column from disk. Skips the rest.</li>
  <li><strong>Compression:</strong> Snappy/Gzip. Very efficient.</li>
</ul>

<p><strong>2. Avro (Row-based):</strong></p>
<ul>
  <li><strong>Best for:</strong> Write-heavy workloads, Kafka messages.</li>
  <li><strong>Why:</strong> Schema evolution is first-class. Good for appending data.</li>
</ul>

<p><strong>3. ORC (Optimized Row Columnar):</strong></p>
<ul>
  <li><strong>Best for:</strong> Hive/Presto. Similar to Parquet but optimized for the Hadoop ecosystem.</li>
</ul>

<h2 id="deep-dive-partitioning-and-bucketing">Deep Dive: Partitioning and Bucketing</h2>

<p>How do you make <code class="language-plaintext highlighter-rouge">SELECT * FROM logs WHERE date = '2023-01-01'</code> fast?</p>

<p><strong>1. Partitioning:</strong></p>
<ul>
  <li>Organize folders by key.</li>
  <li><code class="language-plaintext highlighter-rouge">s3://bucket/logs/date=2023-01-01/part-001.parquet</code></li>
  <li>Spark automatically “prunes” partitions. It only scans the relevant folder.</li>
  <li><strong>Warning:</strong> Don’t partition by high-cardinality columns (UserID). You’ll get millions of tiny folders.</li>
</ul>

<p><strong>2. Bucketing:</strong></p>
<ul>
  <li>Hash the key to <code class="language-plaintext highlighter-rouge">N</code> buckets.</li>
  <li><code class="language-plaintext highlighter-rouge">hash(user_id) % 100</code>.</li>
  <li>Useful for <strong>Joins</strong>. If two tables are bucketed by <code class="language-plaintext highlighter-rouge">user_id</code>, the join is a “Sort-Merge Join” (no shuffle needed).</li>
</ul>

<h2 id="system-design-handling-late-data">System Design: Handling Late Data</h2>

<p>In Batch, “Daily” doesn’t mean “Midnight to Midnight”.
Data arrives late (mobile device offline).</p>

<p><strong>Strategies:</strong></p>
<ol>
  <li><strong>Watermark:</strong> Wait for X hours (e.g., process “Yesterday” at 2 AM today).</li>
  <li><strong>Lookback:</strong> When processing “Today”, also re-process “Yesterday” to catch late arrivals.</li>
  <li><strong>Delta Lake / Hudi:</strong> These “Lakehouse” formats allow <strong>Upserts</strong>. You can update yesterday’s partition without rewriting the whole table.</li>
</ol>

<h2 id="deep-dive-idempotency">Deep Dive: Idempotency</h2>

<p><strong>Definition:</strong> <code class="language-plaintext highlighter-rouge">f(f(x)) = f(x)</code>. Running the job twice produces the same result.
<strong>Why:</strong> Airflow <em>will</em> retry your job if it fails.</p>

<p><strong>Anti-Pattern:</strong>
<code class="language-plaintext highlighter-rouge">INSERT INTO table SELECT ...</code>
(Running twice duplicates data).</p>

<p><strong>Pattern:</strong>
<code class="language-plaintext highlighter-rouge">INSERT OVERWRITE table PARTITION (date='2023-01-01') SELECT ...</code>
(Running twice overwrites the partition).</p>

<h2 id="engineering-data-quality-with-great-expectations">Engineering: Data Quality with Great Expectations</h2>

<p><strong>The Nightmare:</strong>
The upstream team changes <code class="language-plaintext highlighter-rouge">age</code> from “Years” (Int) to “Birthdate” (String).
Your Spark job crashes at 3 AM.</p>

<p><strong>Solution: Circuit Breakers.</strong>
Use <strong>Great Expectations</strong> to validate data <em>before</em> processing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">great_expectations</span> <span class="k">as</span> <span class="n">ge</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">ge</span><span class="p">.</span><span class="nf">read_parquet</span><span class="p">(</span><span class="sh">"</span><span class="s">s3://bucket/input/</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Define expectations
</span><span class="n">df</span><span class="p">.</span><span class="nf">expect_column_values_to_be_not_null</span><span class="p">(</span><span class="sh">"</span><span class="s">user_id</span><span class="sh">"</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">expect_column_values_to_be_between</span><span class="p">(</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">expect_table_row_count_to_be_between</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">100000</span><span class="p">)</span>

<span class="c1"># Validate
</span><span class="n">results</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">validate</span><span class="p">()</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">results</span><span class="p">[</span><span class="sh">"</span><span class="s">success</span><span class="sh">"</span><span class="p">]:</span>
    <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">"</span><span class="s">Data Quality Check Failed!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="appendix-b-workflow-orchestration-wars">Appendix B: Workflow Orchestration Wars</h2>

<p><strong>Airflow:</strong></p>
<ul>
  <li><strong>Pros:</strong> Industry standard, huge community, Python.</li>
  <li><strong>Cons:</strong> Scheduling latency, complex setup, “The Scheduler Loop”.</li>
</ul>

<p><strong>Prefect:</strong></p>
<ul>
  <li><strong>Pros:</strong> “Negative Engineering” (handles retries/failures elegantly), hybrid execution model.</li>
  <li><strong>Cons:</strong> Smaller ecosystem.</li>
</ul>

<p><strong>Dagster:</strong></p>
<ul>
  <li><strong>Pros:</strong> Data-aware (knows about assets, not just tasks), strong typing.</li>
  <li><strong>Cons:</strong> Steep learning curve.</li>
</ul>

<h2 id="appendix-c-interview-questions">Appendix C: Interview Questions</h2>

<ol>
  <li>
    <p><strong>Q:</strong> “What is the difference between Transformation and Action in Spark?”
<strong>A:</strong> Transformations (Map, Filter) are lazy (build the DAG). Actions (Count, Collect) trigger execution.</p>
  </li>
  <li><strong>Q:</strong> “How do you handle Skewed Data in a Join?”
<strong>A:</strong>
    <ul>
      <li><strong>Salting:</strong> Add a random number (salt) to the skewed key to split it.</li>
      <li><strong>Broadcast Join:</strong> If one table is small, broadcast it to all nodes.</li>
    </ul>
  </li>
  <li><strong>Q:</strong> “Explain the difference between Data Warehouse and Data Lake.”
<strong>A:</strong>
    <ul>
      <li><strong>Warehouse (Snowflake):</strong> Structured, Schema-on-Write, SQL, Expensive.</li>
      <li><strong>Lake (S3):</strong> Unstructured/Semi-structured, Schema-on-Read, Files, Cheap.</li>
      <li><strong>Lakehouse:</strong> Best of both (ACID on S3).</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-spark-catalyst-optimizer">Deep Dive: Spark Catalyst Optimizer</h2>

<p>Why is Spark SQL faster than raw RDDs? <strong>Catalyst.</strong>
It’s an extensible query optimizer.</p>

<p><strong>Phases:</strong></p>
<ol>
  <li><strong>Analysis:</strong> Resolve column names (<code class="language-plaintext highlighter-rouge">SELECT name FROM users</code>).</li>
  <li><strong>Logical Optimization:</strong>
    <ul>
      <li><strong>Predicate Pushdown:</strong> Move <code class="language-plaintext highlighter-rouge">FILTER</code> before <code class="language-plaintext highlighter-rouge">JOIN</code>.</li>
      <li><strong>Column Pruning:</strong> Read only used columns.</li>
    </ul>
  </li>
  <li><strong>Physical Planning:</strong> Choose join strategy (Broadcast vs. Sort-Merge).</li>
  <li><strong>Code Generation:</strong> Generate Java bytecode on the fly (Whole-Stage Code Gen).</li>
</ol>

<h2 id="deep-dive-the-shuffle-timsort">Deep Dive: The Shuffle (Timsort)</h2>

<p>The bottleneck of any distributed system is the <strong>Shuffle</strong>.
Moving data from Mapper to Reducer over the network.</p>

<p><strong>Sort-Based Shuffle:</strong>
Spark sorts data on the Mapper side before sending it.
It uses <strong>Timsort</strong> (Hybrid of Merge Sort and Insertion Sort).</p>
<ul>
  <li><strong>Complexity:</strong> (O(N \log N)).</li>
  <li><strong>Memory:</strong> Efficient. Spills to disk if RAM is full.</li>
</ul>

<p><strong>Tuning:</strong>
<code class="language-plaintext highlighter-rouge">spark.sql.shuffle.partitions</code> (Default 200).</p>
<ul>
  <li>Too low: OOM (Out of Memory).</li>
  <li>Too high: Too many small files/tasks.</li>
</ul>

<h2 id="system-design-data-lineage-openlineage">System Design: Data Lineage (OpenLineage)</h2>

<p><strong>Problem:</strong> “The revenue number is wrong. Where did it come from?”
<strong>Solution:</strong> Data Lineage.</p>

<p><strong>OpenLineage:</strong> Standard spec for lineage.</p>
<ul>
  <li><strong>Job:</strong> “Daily Revenue ETL”.</li>
  <li><strong>Input:</strong> <code class="language-plaintext highlighter-rouge">s3://bucket/orders</code>.</li>
  <li><strong>Output:</strong> <code class="language-plaintext highlighter-rouge">s3://bucket/revenue</code>.</li>
</ul>

<p><strong>Marquez:</strong> An Open Source lineage server.
It visualizes the graph: <code class="language-plaintext highlighter-rouge">Postgres -&gt; Spark -&gt; S3 -&gt; Snowflake -&gt; Tableau</code>.
If the dashboard breaks, you trace it back to the source.</p>

<h2 id="engineering-cicd-for-data-pipelines">Engineering: CI/CD for Data Pipelines</h2>

<p>Software Engineers have CI/CD. Data Engineers usually test in production. <strong>Don’t.</strong></p>

<p><strong>Pipeline:</strong></p>
<ol>
  <li><strong>Unit Test:</strong> Test individual Python functions (PyTest).</li>
  <li><strong>Integration Test:</strong> Run the DAG on a small sample dataset (Dockerized Airflow).</li>
  <li><strong>Staging:</strong> Deploy to Staging environment. Run on full data (or 10%).</li>
  <li><strong>Production:</strong> Deploy.</li>
</ol>

<p><strong>Tools:</strong></p>
<ul>
  <li><strong>DataOps:</strong> The philosophy of applying DevOps to Data.</li>
  <li><strong>dbt test:</strong> Validates SQL logic.</li>
</ul>

<h2 id="finops-autoscaling-strategies">FinOps: Autoscaling Strategies</h2>

<p>Batch jobs are bursty. You need 100 nodes for 1 hour, then 0.</p>

<p><strong>1. Cluster Autoscaling:</strong></p>
<ul>
  <li>If pending tasks &gt; 0, add nodes.</li>
  <li>If CPU utilization &lt; 50%, remove nodes.</li>
</ul>

<p><strong>2. Spot Instances:</strong></p>
<ul>
  <li>Use AWS Spot Instances (90% cheaper).</li>
  <li><strong>Risk:</strong> AWS can reclaim them with 2-minute warning.</li>
  <li><strong>Mitigation:</strong> Spark is fault-tolerant. If a node dies, the driver reschedules the task on another node.</li>
</ul>

<h2 id="case-study-netflix-data-mesh">Case Study: Netflix Data Mesh</h2>

<p>Netflix moved from a Monolithic Data Lake to a <strong>Data Mesh</strong>.
<strong>Principles:</strong></p>
<ol>
  <li><strong>Domain-Oriented Ownership:</strong> The “Content Team” owns the “Content Data Product”.</li>
  <li><strong>Data as a Product:</strong> Data must have SLAs, Documentation, and Quality Checks.</li>
  <li><strong>Self-Serve Infrastructure:</strong> Platform team provides the tools (Spark/Airflow), Domain teams build the pipelines.</li>
  <li><strong>Federated Governance:</strong> Global standards (GDPR), local implementation.</li>
</ol>

<h2 id="appendix-d-the-small-files-problem-revisited">Appendix D: The “Small Files” Problem (Revisited)</h2>

<p><strong>Compaction Strategies:</strong></p>
<ol>
  <li><strong>Coalesce:</strong> <code class="language-plaintext highlighter-rouge">df.coalesce(10)</code>. Moves data to fewer partitions. No shuffle.</li>
  <li><strong>Repartition:</strong> <code class="language-plaintext highlighter-rouge">df.repartition(10)</code>. Full shuffle. Balances data perfectly.</li>
  <li><strong>Bin-Packing:</strong> Combine small files into a single task during reading (<code class="language-plaintext highlighter-rouge">spark.sql.files.maxPartitionBytes</code>).</li>
</ol>

<h2 id="appendix-e-advanced-interview-questions">Appendix E: Advanced Interview Questions</h2>

<ol>
  <li>
    <p><strong>Q:</strong> “What is the difference between <code class="language-plaintext highlighter-rouge">repartition()</code> and <code class="language-plaintext highlighter-rouge">coalesce()</code>?”
<strong>A:</strong> <code class="language-plaintext highlighter-rouge">repartition</code> does a full shuffle (network I/O). <code class="language-plaintext highlighter-rouge">coalesce</code> just merges local partitions (no shuffle). Use <code class="language-plaintext highlighter-rouge">coalesce</code> to reduce file count.</p>
  </li>
  <li><strong>Q:</strong> “How do you handle a Hot Key in a Join?”
<strong>A:</strong> If “Justin Bieber” has 100M clicks, the node processing him will OOM.
    <ul>
      <li><strong>Solution:</strong> Salt the key. <code class="language-plaintext highlighter-rouge">key = key + random(1, 100)</code>. Explode the other table 100 times.</li>
    </ul>
  </li>
  <li><strong>Q:</strong> “What is a Broadcast Variable?”
<strong>A:</strong> A read-only variable cached on every machine. Used to send a small lookup table (Country Codes) to all workers to avoid a Shuffle Join.</li>
</ol>

<h2 id="deep-dive-bloom-filters-probabilistic-data-structures">Deep Dive: Bloom Filters (Probabilistic Data Structures)</h2>

<p><strong>Problem:</strong> You have 1 Billion URLs. You want to check if a new URL is already in the set.
<strong>Naive:</strong> Store all URLs in a HashSet. (Requires 100GB RAM).
<strong>Solution:</strong> Bloom Filter. (Requires 1GB RAM).</p>

<p><strong>Mechanism:</strong></p>
<ol>
  <li>Bit array of size <code class="language-plaintext highlighter-rouge">M</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">K</code> hash functions.</li>
  <li><strong>Add(item):</strong> Hash item <code class="language-plaintext highlighter-rouge">K</code> times. Set bits at those indices to 1.</li>
  <li><strong>Check(item):</strong> Hash item <code class="language-plaintext highlighter-rouge">K</code> times. If all bits are 1, return “Maybe Present”. If any bit is 0, return “Definitely Not Present”.</li>
</ol>

<p><strong>False Positives:</strong> Possible.
<strong>False Negatives:</strong> Impossible.</p>

<h2 id="deep-dive-hyperloglog-cardinality-estimation">Deep Dive: HyperLogLog (Cardinality Estimation)</h2>

<p><strong>Problem:</strong> Count unique visitors (DAU) for Facebook (2 Billion users).
<strong>Naive:</strong> <code class="language-plaintext highlighter-rouge">SELECT COUNT(DISTINCT user_id)</code>. Requires storing all IDs. Slow.
<strong>Solution:</strong> HyperLogLog (HLL).</p>

<p><strong>Mechanism:</strong></p>
<ol>
  <li>Hash the user ID.</li>
  <li>Count the number of leading zeros in the binary hash.</li>
  <li>If you see a hash with 10 leading zeros, you probably saw <code class="language-plaintext highlighter-rouge">2^10</code> items.</li>
  <li>Average this across many buckets (Harmonic Mean).</li>
</ol>

<p><strong>Accuracy:</strong> 99% accuracy using only 1.5KB of memory.</p>

<h2 id="deep-dive-count-min-sketch-frequency-estimation">Deep Dive: Count-Min Sketch (Frequency Estimation)</h2>

<p><strong>Problem:</strong> Find the “Top 10” most popular songs.
<strong>Solution:</strong> Count-Min Sketch.</p>

<p><strong>Mechanism:</strong></p>
<ol>
  <li>2D Array <code class="language-plaintext highlighter-rouge">[Depth][Width]</code>.</li>
  <li><strong>Add(item):</strong> Hash item <code class="language-plaintext highlighter-rouge">Depth</code> times. Increment the counter at <code class="language-plaintext highlighter-rouge">[d][hash(item)]</code>.</li>
  <li><strong>Query(item):</strong> Return <code class="language-plaintext highlighter-rouge">min(counters)</code> for that item.</li>
</ol>

<p><strong>Why Min?</strong> Because collisions only <em>increase</em> the count. The minimum is the closest to the truth.</p>

<h2 id="engineering-handling-pii-with-hashingsalting">Engineering: Handling PII with Hashing/Salting</h2>

<p><strong>Requirement:</strong> GDPR “Right to be Forgotten”.
<strong>Problem:</strong> If you delete a user from the DB, their ID is still in the logs/backups.</p>

<p><strong>Solution: Crypto-Shredding.</strong></p>
<ol>
  <li>Don’t store <code class="language-plaintext highlighter-rouge">user_id</code>. Store <code class="language-plaintext highlighter-rouge">HMAC(user_id, key)</code>.</li>
  <li>Store the <code class="language-plaintext highlighter-rouge">key</code> in a separate Key Management Service (KMS).</li>
  <li>To “forget” a user, delete their <code class="language-plaintext highlighter-rouge">key</code>.</li>
  <li>Now the logs contain garbage that can never be decrypted.</li>
</ol>

<h2 id="case-study-ubers-michelangelo-batch-training">Case Study: Uber’s Michelangelo (Batch Training)</h2>

<p>Uber built an internal ML-as-a-Service platform.
<strong>Workflow:</strong></p>
<ol>
  <li><strong>Feature Store:</strong> Hive tables containing pre-computed features (<code class="language-plaintext highlighter-rouge">avg_ride_cost_7d</code>).</li>
  <li><strong>Selection:</strong> Data Scientist selects features in UI.</li>
  <li><strong>Join:</strong> Spark job joins features with labels (Point-in-Time correct).</li>
  <li><strong>Train:</strong> Distributed XGBoost / Horovod (Deep Learning).</li>
  <li><strong>Model Store:</strong> Versioned artifact saved to S3.</li>
  <li><strong>Serving:</strong> Model deployed to a Docker container.</li>
</ol>

<h2 id="appendix-f-the-thundering-herd-problem">Appendix F: The “Thundering Herd” Problem</h2>

<p><strong>Scenario:</strong> 10,000 Airflow tasks are scheduled for 00:00.
<strong>Result:</strong> The Scheduler crashes. The Database CPU spikes to 100%.</p>

<p><strong>Solution:</strong></p>
<ol>
  <li><strong>Jitter:</strong> Add a random delay (0-60s) to the start time.</li>
  <li><strong>Pools:</strong> Limit concurrency. <code class="language-plaintext highlighter-rouge">pool='heavy_sql', slots=10</code>.</li>
  <li><strong>Sensor Deferral:</strong> Use <code class="language-plaintext highlighter-rouge">SmartSensor</code> (Async) instead of blocking threads.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>Batch processing is the workhorse of ML.
While Real-Time is sexy, Batch is reliable, replayable, and easy to debug.
<em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#airflow" class="page__taxonomy-item p-category" rel="tag">airflow</a><span class="sep">, </span>
    
      <a href="/tags/#data-lake" class="page__taxonomy-item p-category" rel="tag">data-lake</a><span class="sep">, </span>
    
      <a href="/tags/#etl" class="page__taxonomy-item p-category" rel="tag">etl</a><span class="sep">, </span>
    
      <a href="/tags/#mapreduce" class="page__taxonomy-item p-category" rel="tag">mapreduce</a><span class="sep">, </span>
    
      <a href="/tags/#spark" class="page__taxonomy-item p-category" rel="tag">spark</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#data-engineering" class="page__taxonomy-item p-category" rel="tag">data-engineering</a><span class="sep">, </span>
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0026-level-order-traversal/" rel="permalink">Binary Tree Level Order Traversal
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How do you print a corporate hierarchy level by level? CEO first, then VPs, then Managers…
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0026-batch-speech-processing/" rel="permalink">Batch Speech Processing
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Real-time ASR is hard. Offline ASR is big.
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Batch+Processing+Pipelines%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0026-batch-processing-pipelines%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0026-batch-processing-pipelines%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0026-batch-processing-pipelines/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0025-model-monitoring-systems/" class="pagination--pager" title="Model Monitoring Systems">Previous</a>
    
    
      <a href="/ml-system-design/0027-model-architecture-design/" class="pagination--pager" title="Model Architecture Design">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
