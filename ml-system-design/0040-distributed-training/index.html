<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Distributed Training Patterns - Arun Baby</title>
<meta name="description" content="“Scaling from one GPU to thousands.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Distributed Training Patterns">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0040-distributed-training/">


  <meta property="og:description" content="“Scaling from one GPU to thousands.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Distributed Training Patterns">
  <meta name="twitter:description" content="“Scaling from one GPU to thousands.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0040-distributed-training/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-14T09:38:49+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0040-distributed-training/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Distributed Training Patterns">
    <meta itemprop="description" content="“Scaling from one GPU to thousands.”">
    <meta itemprop="datePublished" content="2025-12-14T09:38:49+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0040-distributed-training/" itemprop="url">Distributed Training Patterns
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          20 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-the-need-for-scale">1. The Need for Scale</a></li><li><a href="#2-taxonomy-of-parallelism">2. Taxonomy of Parallelism</a></li><li><a href="#3-data-parallelism-dp">3. Data Parallelism (DP)</a><ul><li><a href="#31-parameter-server-ps-architecture">3.1. Parameter Server (PS) Architecture</a></li><li><a href="#32-ring-all-reduce">3.2. Ring All-Reduce</a></li></ul></li><li><a href="#4-model-parallelism-mp">4. Model Parallelism (MP)</a><ul><li><a href="#41-pipeline-parallelism-pp">4.1. Pipeline Parallelism (PP)</a></li><li><a href="#42-tensor-parallelism-tp">4.2. Tensor Parallelism (TP)</a></li></ul></li><li><a href="#5-zero-zero-redundancy-optimizer">5. ZeRO (Zero Redundancy Optimizer)</a></li><li><a href="#6-system-design-designing-a-training-cluster">6. System Design: Designing a Training Cluster</a><ul><li><a href="#61-hardware">6.1. Hardware</a></li><li><a href="#62-network-topology">6.2. Network Topology</a></li><li><a href="#63-fault-tolerance">6.3. Fault Tolerance</a></li></ul></li><li><a href="#7-communication-primitives-nccl">7. Communication Primitives (NCCL)</a></li><li><a href="#8-deep-dive-gradient-accumulation">8. Deep Dive: Gradient Accumulation</a></li><li><a href="#9-case-study-training-gpt-3">9. Case Study: Training GPT-3</a></li><li><a href="#10-deep-dive-collective-communication-ring-all-reduce">10. Deep Dive: Collective Communication (Ring All-Reduce)</a></li><li><a href="#11-deep-dive-zero-zero-redundancy-optimizer-details">11. Deep Dive: ZeRO (Zero Redundancy Optimizer) Details</a></li><li><a href="#12-deep-dive-pipeline-parallelism-1f1b">12. Deep Dive: Pipeline Parallelism (1F1B)</a></li><li><a href="#13-deep-dive-tensor-parallelism-megatron-lm">13. Deep Dive: Tensor Parallelism (Megatron-LM)</a></li><li><a href="#15-deep-dive-network-topologies-for-ai-clusters">15. Deep Dive: Network Topologies for AI Clusters</a></li><li><a href="#16-deep-dive-framework-internals-pytorch-ddp">16. Deep Dive: Framework Internals (PyTorch DDP)</a></li><li><a href="#17-case-study-llama-2-training-infrastructure">17. Case Study: LLaMA 2 Training Infrastructure</a></li><li><a href="#18-deep-dive-asynchronous-vs-synchronous-sgd">18. Deep Dive: Asynchronous vs Synchronous SGD</a></li><li><a href="#19-interview-questions">19. Interview Questions</a></li><li><a href="#20-deep-dive-gradient-checkpointing-activation-recomputation">20. Deep Dive: Gradient Checkpointing (Activation Recomputation)</a></li><li><a href="#21-deep-dive-mixed-precision-training-fp16bf16">21. Deep Dive: Mixed Precision Training (FP16/BF16)</a></li><li><a href="#22-deep-dive-flashattention-memory-efficient-attention">22. Deep Dive: FlashAttention (Memory-Efficient Attention)</a></li><li><a href="#23-deep-dive-fsdp-fully-sharded-data-parallel">23. Deep Dive: FSDP (Fully Sharded Data Parallel)</a></li><li><a href="#24-production-deployment-monitoring--observability">24. Production Deployment: Monitoring &amp; Observability</a></li><li><a href="#25-production-deployment-cost-optimization">25. Production Deployment: Cost Optimization</a></li><li><a href="#27-deep-dive-bandwidth-analysis--bottleneck-detection">27. Deep Dive: Bandwidth Analysis &amp; Bottleneck Detection</a></li><li><a href="#28-deep-dive-debugging-distributed-training">28. Deep Dive: Debugging Distributed Training</a></li><li><a href="#29-advanced-topic-sequence-parallelism">29. Advanced Topic: Sequence Parallelism</a></li><li><a href="#30-advanced-topic-expert-parallelism-moe">30. Advanced Topic: Expert Parallelism (MoE)</a></li><li><a href="#31-summary--best-practices">31. Summary &amp; Best Practices</a></li><li><a href="#32-common-pitfalls">32. Common Pitfalls</a></li><li><a href="#33-real-world-deployment-kubernetes-for-ml">33. Real-World Deployment: Kubernetes for ML</a></li><li><a href="#34-conclusion">34. Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Scaling from one GPU to thousands.”</strong></p>

<h2 id="1-the-need-for-scale">1. The Need for Scale</h2>

<p>Modern Deep Learning models are massive.</p>
<ul>
  <li><strong>GPT-3:</strong> 175 Billion parameters.</li>
  <li><strong>PaLM:</strong> 540 Billion parameters.</li>
  <li><strong>Training Data:</strong> Trillions of tokens.</li>
</ul>

<p>A single NVIDIA A100 GPU (80GB VRAM) cannot hold these models, let alone train them in a reasonable time.
To train these models, we must distribute the computation across hundreds or thousands of GPUs.</p>

<h2 id="2-taxonomy-of-parallelism">2. Taxonomy of Parallelism</h2>

<p>There are three main dimensions of parallelism in Deep Learning:</p>

<ol>
  <li><strong>Data Parallelism:</strong> Split the <em>data</em> across devices. Replicate the <em>model</em>.</li>
  <li><strong>Model Parallelism:</strong> Split the <em>model</em> across devices. Replicate the <em>data</em> (or split it).
    <ul>
      <li><strong>Pipeline Parallelism:</strong> Split layers across devices (Inter-layer).</li>
      <li><strong>Tensor Parallelism:</strong> Split individual operations (matrices) across devices (Intra-layer).</li>
    </ul>
  </li>
  <li><strong>3D Parallelism:</strong> Combining all the above.</li>
</ol>

<h2 id="3-data-parallelism-dp">3. Data Parallelism (DP)</h2>

<p>This is the simplest and most common form.</p>

<p><strong>Mechanism:</strong></p>
<ol>
  <li>Copy the entire model to every GPU (Worker).</li>
  <li>Split the global batch into mini-batches. Assign one mini-batch to each GPU.</li>
  <li><strong>Forward Pass:</strong> Each GPU computes predictions and loss for its mini-batch.</li>
  <li><strong>Backward Pass:</strong> Each GPU computes gradients w.r.t. its local data.</li>
  <li><strong>Synchronization:</strong> Gradients are aggregated (averaged) across all GPUs.</li>
  <li><strong>Update:</strong> All GPUs update their model weights with the averaged gradients.</li>
</ol>

<h3 id="31-parameter-server-ps-architecture">3.1. Parameter Server (PS) Architecture</h3>
<ul>
  <li><strong>Workers:</strong> Compute gradients.</li>
  <li><strong>Parameter Servers:</strong> Store global weights.</li>
  <li><strong>Flow:</strong> Workers pull weights -&gt; Compute gradients -&gt; Push gradients to PS -&gt; PS updates weights.</li>
  <li><strong>Bottleneck:</strong> Network bandwidth at the PS.</li>
</ul>

<h3 id="32-ring-all-reduce">3.2. Ring All-Reduce</h3>
<p>Used in <strong>DistributedDataParallel (DDP)</strong> (PyTorch) and <strong>Horovod</strong>.</p>
<ul>
  <li>No central server.</li>
  <li>GPUs are arranged in a logical ring.</li>
  <li>Each GPU sends data to its neighbor and receives from the other neighbor.</li>
  <li><strong>Scatter-Reduce:</strong> Chunk gradients and reduce them as they pass around the ring.</li>
  <li><strong>All-Gather:</strong> Gather the reduced chunks back to all GPUs.</li>
  <li><strong>Bandwidth Optimal:</strong> Bandwidth usage is constant regardless of the number of GPUs.</li>
</ul>

<h2 id="4-model-parallelism-mp">4. Model Parallelism (MP)</h2>

<p>When the model doesn’t fit in one GPU’s memory.</p>

<h3 id="41-pipeline-parallelism-pp">4.1. Pipeline Parallelism (PP)</h3>
<p>Split the model layers into stages.</p>
<ul>
  <li>GPU 1: Layers 1-10</li>
  <li>GPU 2: Layers 11-20</li>
  <li>…</li>
  <li><strong>Naive Approach:</strong> GPU 2 waits for GPU 1. Huge “bubble” (idle time).</li>
  <li><strong>GPipe / PipeDream:</strong> Split the mini-batch into <strong>micro-batches</strong>. Pipeline the execution of micro-batches to fill the bubbles.</li>
</ul>

<h3 id="42-tensor-parallelism-tp">4.2. Tensor Parallelism (TP)</h3>
<p>Split the tensors (matrices) themselves.</p>
<ul>
  <li><strong>Example:</strong> Matrix Multiplication $Y = X \cdot A$.</li>
  <li>Split $A$ into columns $A_1, A_2$.</li>
  <li>GPU 1 computes $Y_1 = X \cdot A_1$.</li>
  <li>GPU 2 computes $Y_2 = X \cdot A_2$.</li>
  <li>Concatenate $Y = [Y_1, Y_2]$.</li>
  <li>Requires high-bandwidth interconnect (NVLink) because synchronization happens <em>per layer</em>. Used in <strong>Megatron-LM</strong>.</li>
</ul>

<h2 id="5-zero-zero-redundancy-optimizer">5. ZeRO (Zero Redundancy Optimizer)</h2>

<p>Standard Data Parallelism replicates the <strong>Optimizer States</strong>, <strong>Gradients</strong>, and <strong>Parameters</strong> on every GPU. This is wasteful.</p>

<p><strong>ZeRO-DP (DeepSpeed):</strong></p>
<ul>
  <li><strong>ZeRO-1:</strong> Shard Optimizer States. (4x memory reduction).</li>
  <li><strong>ZeRO-2:</strong> Shard Gradients. (8x memory reduction).</li>
  <li><strong>ZeRO-3:</strong> Shard Parameters. (Memory usage scales linearly with number of GPUs).</li>
</ul>

<p>With ZeRO-3, parameters are fetched on-demand just before the forward/backward pass of a layer and released immediately after. It effectively allows training trillion-parameter models on limited GPU memory.</p>

<h2 id="6-system-design-designing-a-training-cluster">6. System Design: Designing a Training Cluster</h2>

<p><strong>Scenario:</strong> Build a cluster to train a 100B parameter model.</p>

<h3 id="61-hardware">6.1. Hardware</h3>
<ul>
  <li><strong>Compute:</strong> NVIDIA H100 or A100 GPUs.</li>
  <li><strong>Interconnect:</strong>
    <ul>
      <li><strong>Intra-node:</strong> NVLink / NVSwitch (900 GB/s). Crucial for Tensor Parallelism.</li>
      <li><strong>Inter-node:</strong> InfiniBand or RoCE (RDMA over Converged Ethernet) (400 Gbps). Crucial for Data/Pipeline Parallelism.</li>
    </ul>
  </li>
  <li><strong>Storage:</strong> High-performance parallel file system (Lustre, GPUDirect Storage) to feed data at line rate.</li>
</ul>

<h3 id="62-network-topology">6.2. Network Topology</h3>
<ul>
  <li><strong>Rail-optimized:</strong> All GPU-0s across nodes communicate, all GPU-1s communicate, etc.</li>
  <li><strong>Non-Blocking Fat Tree:</strong> Ensures full bisection bandwidth.</li>
</ul>

<h3 id="63-fault-tolerance">6.3. Fault Tolerance</h3>
<ul>
  <li><strong>Checkpointing:</strong> Save model state frequently to S3/HDFS.</li>
  <li><strong>Elastic Training:</strong> If a node fails, the job should pause, reconfigure (remove bad node), and resume from the last checkpoint automatically (e.g., PyTorch Elastic / TorchRun).</li>
</ul>

<h2 id="7-communication-primitives-nccl">7. Communication Primitives (NCCL)</h2>

<p>Understanding the underlying collective operations is key.</p>
<ul>
  <li><strong>Broadcast:</strong> One sends to all.</li>
  <li><strong>Scatter:</strong> One splits data and sends parts to all.</li>
  <li><strong>Gather:</strong> All send to one.</li>
  <li><strong>All-Gather:</strong> Everyone gathers data from everyone.</li>
  <li><strong>Reduce:</strong> Aggregate data to one (Sum, Min, Max).</li>
  <li><strong>All-Reduce:</strong> Aggregate data and distribute result to all. (The workhorse of DP).</li>
</ul>

<h2 id="8-deep-dive-gradient-accumulation">8. Deep Dive: Gradient Accumulation</h2>

<p>If you can’t fit a large enough batch size for convergence (e.g., batch size 32) even with parallelism:</p>
<ul>
  <li>Run forward/backward for batch size 1.</li>
  <li>Don’t update weights.</li>
  <li>Accumulate gradients.</li>
  <li>Repeat 32 times.</li>
  <li>Update weights.</li>
  <li>Simulates a larger batch size at the cost of compute time (no extra memory).</li>
</ul>

<h2 id="9-case-study-training-gpt-3">9. Case Study: Training GPT-3</h2>

<ul>
  <li><strong>Architecture:</strong> Transformer Decoder.</li>
  <li><strong>Parallelism:</strong>
    <ul>
      <li><strong>Tensor Parallelism:</strong> Within each node (8 GPUs).</li>
      <li><strong>Pipeline Parallelism:</strong> Across nodes.</li>
      <li><strong>Data Parallelism:</strong> Across replicas of the pipeline.</li>
    </ul>
  </li>
  <li><strong>Infrastructure:</strong> Microsoft Azure AI Supercomputer (10,000+ GPUs).</li>
  <li><strong>Challenges:</strong> Stragglers (slow nodes), Silent Data Corruption (bit flips), Loss Spikes.</li>
</ul>

<h2 id="10-deep-dive-collective-communication-ring-all-reduce">10. Deep Dive: Collective Communication (Ring All-Reduce)</h2>

<p>The efficiency of Data Parallelism hinges on the <strong>All-Reduce</strong> operation.
<strong>Goal:</strong> Every GPU starts with a gradient vector $G_i$. Every GPU ends with the sum $\sum G_i$.</p>

<p><strong>Naive Approach:</strong></p>
<ul>
  <li>All GPUs send their gradients to GPU 0 (Gather).</li>
  <li>GPU 0 sums them.</li>
  <li>GPU 0 sends the sum back to all GPUs (Broadcast).</li>
  <li><strong>Bottleneck:</strong> GPU 0’s bandwidth. Time scales linearly with $N$ (number of GPUs).</li>
</ul>

<p><strong>Ring All-Reduce:</strong></p>
<ul>
  <li><strong>Topology:</strong> GPU 0 -&gt; GPU 1 -&gt; … -&gt; GPU N-1 -&gt; GPU 0.</li>
  <li><strong>Step 1: Scatter-Reduce:</strong>
    <ul>
      <li>Split the gradient vector into $N$ chunks.</li>
      <li>In each step, GPU $i$ sends a chunk to GPU $i+1$ and receives a chunk from GPU $i-1$.</li>
      <li>It adds the received chunk to its local chunk and passes it on.</li>
      <li>After $N-1$ steps, each GPU holds a fully summed chunk of the gradient vector (different chunk for each GPU).</li>
    </ul>
  </li>
  <li><strong>Step 2: All-Gather:</strong>
    <ul>
      <li>Each GPU sends its fully summed chunk to the next neighbor.</li>
      <li>After $N-1$ steps, all GPUs have all the fully summed chunks.</li>
    </ul>
  </li>
  <li><strong>Complexity:</strong>
    <ul>
      <li>Data transmitted per GPU: $2(N-1) \frac{K}{N} \approx 2K$.</li>
      <li><strong>Crucially:</strong> Independent of $N$. This allows scaling to thousands of GPUs.</li>
    </ul>
  </li>
</ul>

<h2 id="11-deep-dive-zero-zero-redundancy-optimizer-details">11. Deep Dive: ZeRO (Zero Redundancy Optimizer) Details</h2>

<p>Let’s break down the memory savings for a model with $\Psi$ parameters.
Using Mixed Precision (fp16/bf16) and Adam Optimizer.</p>

<p><strong>Baseline (Standard DDP):</strong></p>
<ul>
  <li><strong>Parameters (fp16):</strong> $2\Psi$ bytes.</li>
  <li><strong>Gradients (fp16):</strong> $2\Psi$ bytes.</li>
  <li><strong>Optimizer States (fp32):</strong>
    <ul>
      <li>Master Weights: $4\Psi$ bytes.</li>
      <li>Momentum: $4\Psi$ bytes.</li>
      <li>Variance: $4\Psi$ bytes.</li>
      <li>Total Opt States: $12\Psi$ bytes.</li>
    </ul>
  </li>
  <li><strong>Total per GPU:</strong> $16\Psi$ bytes.</li>
</ul>

<p><strong>ZeRO-1 (Shard Optimizer States):</strong></p>
<ul>
  <li>Partition the $12\Psi$ optimizer states across $N$ GPUs.</li>
  <li>Each GPU stores $\frac{12\Psi}{N}$.</li>
  <li><strong>Total per GPU:</strong> $4\Psi + \frac{12\Psi}{N}$.</li>
  <li><strong>Savings:</strong> ~4x reduction.</li>
</ul>

<p><strong>ZeRO-2 (Shard Gradients):</strong></p>
<ul>
  <li>Partition the $2\Psi$ gradients as well.</li>
  <li>Each GPU stores $\frac{2\Psi + 12\Psi}{N}$.</li>
  <li><strong>Total per GPU:</strong> $2\Psi + \frac{14\Psi}{N}$.</li>
  <li><strong>Savings:</strong> ~8x reduction.</li>
</ul>

<p><strong>ZeRO-3 (Shard Parameters):</strong></p>
<ul>
  <li>Partition the $2\Psi$ parameters too.</li>
  <li><strong>Total per GPU:</strong> $\frac{16\Psi}{N}$.</li>
  <li><strong>Savings:</strong> Linear reduction with $N$.</li>
  <li><strong>Trade-off:</strong> Increased communication. Parameters must be broadcasted (all-gather) before each layer’s forward/backward pass and discarded immediately.</li>
</ul>

<h2 id="12-deep-dive-pipeline-parallelism-1f1b">12. Deep Dive: Pipeline Parallelism (1F1B)</h2>

<p><strong>GPipe:</strong></p>
<ul>
  <li>Split batch into $M$ micro-batches.</li>
  <li>Run all $M$ forward passes.</li>
  <li>Run all $M$ backward passes.</li>
  <li><strong>Memory Issue:</strong> Need to store activations for all $M$ micro-batches until the backward pass starts.</li>
</ul>

<p><strong>1F1B (One Forward One Backward) - PipeDream:</strong></p>
<ul>
  <li>Schedule: Forward 1, Forward 2, …, Backward 1, Forward 3, Backward 2…</li>
  <li>As soon as the first micro-batch finishes its forward pass at the last stage, it starts its backward pass.</li>
  <li>This frees up activation memory much earlier.</li>
  <li><strong>Bubble:</strong> The idle time at the start and end of the pipeline.
    <ul>
      <li>Bubble fraction $\approx \frac{P-1}{M}$, where $P$ is pipeline stages.</li>
      <li>To minimize bubble, we need $M \gg P$.</li>
    </ul>
  </li>
</ul>

<h2 id="13-deep-dive-tensor-parallelism-megatron-lm">13. Deep Dive: Tensor Parallelism (Megatron-LM)</h2>

<p>How do we split a Transformer Layer across GPUs?</p>

<p><strong>MLP Layer ($A \rightarrow GeLU \rightarrow B$):</strong></p>
<ul>
  <li>$Y = GeLU(X A)$.</li>
  <li>Split $A$ by <strong>columns</strong> ($A_1, A_2$).</li>
  <li>Each GPU computes $Y_i = GeLU(X A_i)$.</li>
  <li>Next matrix $B$ is split by <strong>rows</strong> ($B_1, B_2$).</li>
  <li>Compute $Z = [Y_1, Y_2] \cdot \begin{bmatrix} B_1 \ B_2 \end{bmatrix} = Y_1 B_1 + Y_2 B_2$.</li>
  <li><strong>All-Reduce:</strong> Sum the results $Y_1 B_1 + Y_2 B_2$ across GPUs.</li>
  <li><strong>Benefit:</strong> Only one All-Reduce needed per MLP block.</li>
</ul>

<p><strong>Self-Attention Layer:</strong></p>
<ul>
  <li>Split Query ($Q$), Key ($K$), Value ($V$) weight matrices by <strong>columns</strong> (split heads).</li>
  <li>Each GPU computes attention for a subset of heads.</li>
  <li>Output projection matrix $O$ is split by <strong>rows</strong>.</li>
  <li><strong>All-Reduce:</strong> Sum the outputs of the projection.</li>
</ul>

<h2 id="15-deep-dive-network-topologies-for-ai-clusters">15. Deep Dive: Network Topologies for AI Clusters</h2>

<p>The physical wiring of the cluster determines the maximum possible bandwidth and latency.</p>

<p><strong>1. Fat Tree (Clos Network):</strong></p>
<ul>
  <li><strong>Structure:</strong> A tree where links near the root have higher bandwidth (fatter) than links near the leaves.</li>
  <li><strong>Non-Blocking:</strong> Guarantees that any node can communicate with any other node at full line rate (if the switch capacity allows).</li>
  <li><strong>Pros:</strong> Predictable performance, easy to route.</li>
  <li><strong>Cons:</strong> Expensive (lots of switches and cables).</li>
</ul>

<p><strong>2. Torus / Mesh:</strong></p>
<ul>
  <li><strong>Structure:</strong> Grid-like connection. 2D Torus connects neighbors in X and Y (wrapping around). 3D Torus adds Z.</li>
  <li><strong>Pros:</strong> Cheaper (fewer switches, direct node-to-node links). Good for local communication (stencil patterns).</li>
  <li><strong>Cons:</strong> Higher latency for distant nodes (multi-hop).</li>
</ul>

<p><strong>3. Dragonfly:</strong></p>
<ul>
  <li><strong>Structure:</strong> Groups of routers fully connected within the group, and groups are fully connected to other groups.</li>
  <li><strong>Pros:</strong> Low diameter (max 3 hops for any pair), highly scalable.</li>
  <li><strong>Cons:</strong> Complex routing (needs adaptive routing to avoid congestion).</li>
</ul>

<p><strong>NVIDIA SuperPOD:</strong> Uses a non-blocking Fat Tree with InfiniBand HDR/NDR to ensure 800 Gbps per GPU.</p>

<h2 id="16-deep-dive-framework-internals-pytorch-ddp">16. Deep Dive: Framework Internals (PyTorch DDP)</h2>

<p>How does <code class="language-plaintext highlighter-rouge">DistributedDataParallel</code> actually work?</p>

<p><strong>1. Bucketing:</strong></p>
<ul>
  <li>Gradients are small (e.g., bias vector). Sending millions of tiny packets kills performance (latency overhead).</li>
  <li>DDP groups parameters into <strong>Buckets</strong> (e.g., 25MB).</li>
  <li>It waits for all gradients in a bucket to be computed, then triggers one All-Reduce for the entire bucket.</li>
</ul>

<p><strong>2. Gradient Hooks:</strong></p>
<ul>
  <li>DDP registers autograd hooks on every parameter.</li>
  <li>When a gradient is ready, the hook fires.</li>
  <li>The hook copies the gradient into the bucket buffer.</li>
</ul>

<p><strong>3. Overlap (Compute-Comm):</strong></p>
<ul>
  <li>While the backward pass is computing gradients for layer $L-1$, the All-Reduce for layer $L$ (already in bucket) is running asynchronously on the network card.</li>
  <li><strong>Goal:</strong> Hide communication time behind computation time.</li>
</ul>

<h2 id="17-case-study-llama-2-training-infrastructure">17. Case Study: LLaMA 2 Training Infrastructure</h2>

<p><strong>Meta’s Research SuperCluster (RSC):</strong></p>
<ul>
  <li><strong>GPUs:</strong> 16,000 NVIDIA A100 (80GB).</li>
  <li><strong>Interconnect:</strong> InfiniBand 200 Gbps (Fat Tree).</li>
  <li><strong>Storage:</strong> 175 PB of Pure Storage FlashBlade.</li>
  <li><strong>Optimization:</strong>
    <ul>
      <li><strong>xFormers:</strong> Optimized Attention kernels (FlashAttention).</li>
      <li><strong>Checkpointing:</strong> Saved to distributed storage every few hours.</li>
      <li><strong>Silent Data Corruption:</strong> Detected via loss spikes. If detected, roll back to previous checkpoint and skip the bad batch.</li>
    </ul>
  </li>
</ul>

<p><strong>Training Stability:</strong></p>
<ul>
  <li><strong>Loss Spikes:</strong> Often caused by “bad” data or numerical instability (fp16 overflow).</li>
  <li><strong>Fix:</strong> Gradient Clipping (norm 1.0), Weight Decay decoupling (AdamW), and skipping batches with NaN gradients.</li>
</ul>

<h2 id="18-deep-dive-asynchronous-vs-synchronous-sgd">18. Deep Dive: Asynchronous vs Synchronous SGD</h2>

<p><strong>Synchronous SGD (Standard):</strong></p>
<ul>
  <li>Wait for ALL workers to finish.</li>
  <li>Update = Average of all.</li>
  <li><strong>Pros:</strong> Mathematically equivalent to large-batch SGD. Converges well.</li>
  <li><strong>Cons:</strong> Straggler problem (fastest worker waits for slowest).</li>
</ul>

<p><strong>Asynchronous SGD (Hogwild!):</strong></p>
<ul>
  <li>Workers push gradients to PS whenever they are done.</li>
  <li>PS updates weights immediately.</li>
  <li>Workers pull new weights.</li>
  <li><strong>Pros:</strong> No waiting. High hardware utilization.</li>
  <li><strong>Cons:</strong> <strong>Stale Gradients</strong>. Worker computes gradient on $W_t$, but by the time it pushes, the global weights are $W_{t+10}$. The gradient is “stale” and points in the wrong direction.</li>
  <li><strong>Solution:</strong> Rarely used now. Sync SGD with backup workers (ignore slowest 5%) is preferred.</li>
</ul>

<h2 id="19-interview-questions">19. Interview Questions</h2>

<ol>
  <li><strong>Explain All-Reduce.</strong> How does Ring All-Reduce work? What is its complexity? ($2(N-1) \frac{K}{N}$).</li>
  <li><strong>Data Parallelism vs. Model Parallelism.</strong> When to use which?</li>
  <li><strong>What is the “Stale Gradient” problem?</strong> In Asynchronous SGD, workers might compute gradients on old weights. How does Synchronous SGD fix this?</li>
  <li><strong>How does ZeRO-3 work?</strong> How does it handle communication overhead? (Prefetching).</li>
  <li><strong>Calculate Memory Footprint.</strong> For a model with $P$ parameters, using Adam optimizer and mixed precision.
    <ul>
      <li>Parameters: $2P$ bytes (fp16).</li>
      <li>Gradients: $2P$ bytes (fp16).</li>
      <li>Optimizer States: $12P$ bytes (fp32 copy of params, momentum, variance).</li>
      <li>Total: $16P$ bytes. For 1B params, ~16GB.</li>
    </ul>
  </li>
</ol>

<h2 id="20-deep-dive-gradient-checkpointing-activation-recomputation">20. Deep Dive: Gradient Checkpointing (Activation Recomputation)</h2>

<p><strong>Problem:</strong> During backprop, we need activations from the forward pass. For a 100-layer model, storing all activations requires massive memory.
<strong>Solution:</strong> Trade compute for memory.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Forward Pass:</strong> Only save activations at <strong>checkpoints</strong> (e.g., every 10 layers). Discard intermediate activations.</li>
  <li><strong>Backward Pass:</strong> When we need activations for layer 15:
    <ul>
      <li>Re-run forward from checkpoint 10 to layer 15.</li>
      <li>Compute gradients.</li>
      <li>Discard the recomputed activations.</li>
    </ul>
  </li>
</ol>

<p><strong>Memory Savings:</strong></p>
<ul>
  <li>Without checkpointing: $O(N)$ memory for $N$ layers.</li>
  <li>With checkpointing every $\sqrt{N}$ layers: $O(\sqrt{N})$ memory.</li>
  <li><strong>Cost:</strong> $\sqrt{N}$ extra forward passes (33% compute overhead for Transformers).</li>
</ul>

<p><strong>PyTorch Implementation:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.utils.checkpoint</span> <span class="kn">import</span> <span class="n">checkpoint</span>

<span class="k">class</span> <span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Checkpoint expensive blocks
</span>        <span class="n">x</span> <span class="o">=</span> <span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">checkpoint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<h2 id="21-deep-dive-mixed-precision-training-fp16bf16">21. Deep Dive: Mixed Precision Training (FP16/BF16)</h2>

<p><strong>Motivation:</strong> FP32 (32-bit floats) are slow and memory-hungry. FP16 (16-bit) is 2x faster and uses half the memory.
<strong>Challenge:</strong> FP16 has limited range ($6 \times 10^{-8}$ to $65504$). Gradients can underflow (become zero) or overflow.</p>

<p><strong>Solution: Mixed Precision (NVIDIA Apex / PyTorch AMP):</strong></p>
<ol>
  <li><strong>Master Weights:</strong> Keep FP32 copy of weights.</li>
  <li><strong>Forward/Backward:</strong> Use FP16 for matrix multiplications (fast).</li>
  <li><strong>Loss Scaling:</strong> Multiply loss by a large number (e.g., 1024) before backprop. This shifts gradients into FP16’s representable range.</li>
  <li><strong>Unscale Gradients:</strong> Divide gradients by the scale factor before updating FP32 master weights.</li>
  <li><strong>Update:</strong> Update FP32 weights, then copy to FP16 for next iteration.</li>
</ol>

<p><strong>BFloat16 (BF16):</strong></p>
<ul>
  <li>Same exponent range as FP32 (8 bits), but only 7 bits for mantissa.</li>
  <li><strong>Advantage:</strong> No loss scaling needed. Easier to use.</li>
  <li><strong>Disadvantage:</strong> Lower precision than FP16 for small numbers.</li>
  <li><strong>Used in:</strong> Google TPUs, AMD MI250, NVIDIA H100.</li>
</ul>

<h2 id="22-deep-dive-flashattention-memory-efficient-attention">22. Deep Dive: FlashAttention (Memory-Efficient Attention)</h2>

<p>Standard Attention: $O(N^2)$ memory for the attention matrix.
For $N=4096$ tokens, this is 16M elements (64MB in FP16).</p>

<p><strong>FlashAttention (Dao et al., 2022):</strong></p>
<ul>
  <li><strong>Idea:</strong> Never materialize the full $N \times N$ attention matrix.</li>
  <li><strong>Algorithm:</strong>
    <ol>
      <li>Tile $Q$, $K$, $V$ into blocks that fit in SRAM (on-chip cache).</li>
      <li>Compute attention for each block.</li>
      <li>Use online softmax (incremental computation) to avoid storing intermediate results in HBM (slow GPU memory).</li>
    </ol>
  </li>
  <li><strong>Result:</strong> 2-4x speedup, enables training with 64K context length.</li>
</ul>

<p><strong>Impact on Distributed Training:</strong></p>
<ul>
  <li>Reduces activation memory, allowing larger batch sizes or longer sequences.</li>
  <li>Critical for LLaMA, GPT-4, Claude.</li>
</ul>

<h2 id="23-deep-dive-fsdp-fully-sharded-data-parallel">23. Deep Dive: FSDP (Fully Sharded Data Parallel)</h2>

<p><strong>PyTorch FSDP</strong> is Meta’s implementation of ZeRO-3.
<strong>Key Features:</strong></p>
<ol>
  <li><strong>Auto-Wrapping:</strong> Automatically wraps model layers for sharding.</li>
  <li><strong>CPU Offloading:</strong> Can offload parameters to CPU RAM when not in use (train 13B model on 1x A100).</li>
  <li><strong>Mixed Precision:</strong> Native support for BF16.</li>
</ol>

<p><strong>Usage:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">MyTransformer</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">FSDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
             <span class="n">auto_wrap_policy</span><span class="o">=</span><span class="n">transformer_auto_wrap_policy</span><span class="p">,</span>
             <span class="n">mixed_precision</span><span class="o">=</span><span class="n">bf16_policy</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>When to use FSDP vs DDP:</strong></p>
<ul>
  <li><strong>DDP:</strong> Model fits in one GPU. Simple, fast.</li>
  <li><strong>FSDP:</strong> Model doesn’t fit. Need memory efficiency.</li>
</ul>

<h2 id="24-production-deployment-monitoring--observability">24. Production Deployment: Monitoring &amp; Observability</h2>

<p>Training a model for weeks/months requires robust monitoring.</p>

<p><strong>Metrics to Track:</strong></p>
<ol>
  <li><strong>Loss Curves:</strong> Train/Val loss per step. Detect divergence early.</li>
  <li><strong>Gradient Norms:</strong> Sudden spikes indicate instability.</li>
  <li><strong>GPU Utilization:</strong> Should be &gt;90%. If low, data loading is the bottleneck.</li>
  <li><strong>Communication Time:</strong> Time spent in All-Reduce. Should be &lt;10% of step time.</li>
  <li><strong>Throughput:</strong> Tokens/second. Track degradation over time (memory leaks, stragglers).</li>
</ol>

<p><strong>Tools:</strong></p>
<ul>
  <li><strong>TensorBoard / Weights &amp; Biases:</strong> Visualize metrics.</li>
  <li><strong>NVIDIA DCGM (Data Center GPU Manager):</strong> Monitor GPU health (temperature, power, ECC errors).</li>
  <li><strong>Prometheus + Grafana:</strong> Cluster-wide metrics.</li>
</ul>

<p><strong>Alerting:</strong></p>
<ul>
  <li><strong>Loss NaN:</strong> Immediate rollback.</li>
  <li><strong>GPU Failure:</strong> Auto-restart with elastic training.</li>
  <li><strong>Slow Node:</strong> Blacklist and redistribute work.</li>
</ul>

<h2 id="25-production-deployment-cost-optimization">25. Production Deployment: Cost Optimization</h2>

<p>Training GPT-3 cost ~$5M. How do we reduce this?</p>

<p><strong>1. Spot Instances:</strong></p>
<ul>
  <li>Use AWS/GCP spot instances (70% cheaper).</li>
  <li><strong>Risk:</strong> Can be preempted.</li>
  <li><strong>Mitigation:</strong> Frequent checkpointing + elastic training.</li>
</ul>

<p><strong>2. Gradient Accumulation:</strong></p>
<ul>
  <li>Simulate large batch size without needing more GPUs.</li>
  <li>Example: 8 GPUs, batch size 4 per GPU, accumulate 8 steps = effective batch size 256.</li>
</ul>

<p><strong>3. Selective Precision:</strong></p>
<ul>
  <li>Use FP16 for most layers, FP32 for sensitive layers (LayerNorm, Softmax).</li>
</ul>

<p><strong>4. Data Loading Optimization:</strong></p>
<ul>
  <li><strong>Prefetching:</strong> Load next batch while GPU is computing.</li>
  <li><strong>Compression:</strong> Store data in compressed format (Parquet, Arrow).</li>
  <li><strong>Sharding:</strong> Shard dataset across nodes to avoid network bottleneck.</li>
</ul>

<h2 id="27-deep-dive-bandwidth-analysis--bottleneck-detection">27. Deep Dive: Bandwidth Analysis &amp; Bottleneck Detection</h2>

<p><strong>Theoretical Peak Performance:</strong>
For a model with $P$ parameters and batch size $B$:</p>
<ul>
  <li><strong>Compute:</strong> $2 \times P \times B$ FLOPs per forward pass (matrix multiplications).</li>
  <li><strong>Memory Bandwidth:</strong> $2 \times P$ bytes to load weights (fp16).</li>
</ul>

<p><strong>Roofline Model:</strong></p>
<ul>
  <li><strong>Arithmetic Intensity (AI):</strong> FLOPs / Bytes.</li>
  <li>For Transformers: $AI \approx \frac{2PB}{2P} = B$ (batch size).</li>
  <li><strong>A100 GPU:</strong>
    <ul>
      <li>Peak Compute: 312 TFLOPS (fp16).</li>
      <li>Peak Bandwidth: 2 TB/s.</li>
      <li><strong>Ridge Point:</strong> $AI = \frac{312}{2000} = 0.156$ FLOPs/Byte.</li>
    </ul>
  </li>
  <li>If $B &lt; 0.156$, we are <strong>memory-bound</strong>. If $B &gt; 0.156$, we are <strong>compute-bound</strong>.</li>
</ul>

<p><strong>Implication:</strong></p>
<ul>
  <li>Small batch sizes (B=1, inference) are memory-bound. Need to optimize data loading.</li>
  <li>Large batch sizes (B=256, training) are compute-bound. Need to optimize kernels (FlashAttention).</li>
</ul>

<h2 id="28-deep-dive-debugging-distributed-training">28. Deep Dive: Debugging Distributed Training</h2>

<p><strong>Common Issues:</strong></p>

<p><strong>1. Hanging (Deadlock):</strong></p>
<ul>
  <li><strong>Symptom:</strong> Training freezes. No error message.</li>
  <li><strong>Cause:</strong> One GPU is waiting for All-Reduce, but another GPU crashed before reaching it.</li>
  <li><strong>Debug:</strong> Set <code class="language-plaintext highlighter-rouge">NCCL_DEBUG=INFO</code>. Check which GPU is stuck.</li>
  <li><strong>Fix:</strong> Add timeouts to collective operations.</li>
</ul>

<p><strong>2. Loss Divergence:</strong></p>
<ul>
  <li><strong>Symptom:</strong> Loss becomes NaN after a few steps.</li>
  <li><strong>Cause:</strong> Gradient explosion, bad data, or numerical instability.</li>
  <li><strong>Debug:</strong>
    <ul>
      <li>Log gradient norms per layer.</li>
      <li>Check for NaN/Inf in activations.</li>
      <li>Reduce learning rate.</li>
    </ul>
  </li>
  <li><strong>Fix:</strong> Gradient clipping, mixed precision with loss scaling.</li>
</ul>

<p><strong>3. Slow Training:</strong></p>
<ul>
  <li><strong>Symptom:</strong> Throughput is 50% of expected.</li>
  <li><strong>Cause:</strong> Communication overhead, data loading bottleneck, or stragglers.</li>
  <li><strong>Debug:</strong>
    <ul>
      <li>Profile with <code class="language-plaintext highlighter-rouge">torch.profiler</code> or NVIDIA Nsight.</li>
      <li>Check GPU utilization (<code class="language-plaintext highlighter-rouge">nvidia-smi dmon</code>).</li>
      <li>Measure communication time vs compute time.</li>
    </ul>
  </li>
  <li><strong>Fix:</strong>
    <ul>
      <li>Increase batch size (reduce communication frequency).</li>
      <li>Use faster interconnect (InfiniBand).</li>
      <li>Optimize data loading (more workers, prefetching).</li>
    </ul>
  </li>
</ul>

<h2 id="29-advanced-topic-sequence-parallelism">29. Advanced Topic: Sequence Parallelism</h2>

<p>For very long sequences (e.g., 100K tokens), even the sequence dimension doesn’t fit in memory.
<strong>Sequence Parallelism (Megatron-LM):</strong></p>
<ul>
  <li>Split the sequence across GPUs along the time dimension.</li>
  <li>Each GPU processes a chunk of the sequence.</li>
  <li><strong>Challenge:</strong> Self-Attention requires the full sequence. Need to gather all chunks for attention, then scatter back.</li>
  <li><strong>Optimization:</strong> Overlap communication with computation (pipeline the gather/scatter).</li>
</ul>

<h2 id="30-advanced-topic-expert-parallelism-moe">30. Advanced Topic: Expert Parallelism (MoE)</h2>

<p><strong>Mixture of Experts (MoE):</strong></p>
<ul>
  <li>Replace the MLP layer with $N$ expert MLPs.</li>
  <li>A router (gating network) decides which expert(s) to use for each token.</li>
  <li><strong>Benefit:</strong> Increase model capacity without increasing compute (only 1-2 experts are active per token).</li>
</ul>

<p><strong>Expert Parallelism:</strong></p>
<ul>
  <li>Place each expert on a different GPU.</li>
  <li>Tokens are routed to the appropriate GPU.</li>
  <li><strong>Challenge:</strong> Load imbalance (some experts get more tokens).</li>
  <li><strong>Solution:</strong> Auxiliary loss to encourage balanced routing.</li>
</ul>

<p><strong>Example: Switch Transformer (Google):</strong></p>
<ul>
  <li>1.6 Trillion parameters.</li>
  <li>128 experts per layer.</li>
  <li>Trained with expert parallelism + data parallelism.</li>
</ul>

<h2 id="31-summary--best-practices">31. Summary &amp; Best Practices</h2>

<p><strong>Choosing the Right Parallelism Strategy:</strong></p>
<ul>
  <li><strong>Model fits in 1 GPU:</strong> Use DDP (Data Parallelism).</li>
  <li><strong>Model fits in 1 node (8 GPUs):</strong> Use Tensor Parallelism (Megatron).</li>
  <li><strong>Model doesn’t fit in 1 node:</strong> Use Pipeline Parallelism + Tensor Parallelism.</li>
  <li><strong>Model is HUGE (&gt;100B):</strong> Use 3D Parallelism (DP + TP + PP) or FSDP/ZeRO-3.</li>
</ul>

<p><strong>Optimization Checklist:</strong></p>
<ol>
  <li>✅ Mixed Precision (BF16).</li>
  <li>✅ Gradient Checkpointing.</li>
  <li>✅ FlashAttention.</li>
  <li>✅ Fused Kernels (AdamW, LayerNorm).</li>
  <li>✅ Gradient Accumulation (if batch size is limited).</li>
  <li>✅ Data Loading (prefetch, multiple workers).</li>
  <li>✅ Profiling (find bottlenecks).</li>
</ol>

<h2 id="32-common-pitfalls">32. Common Pitfalls</h2>

<ul>
  <li><strong>OOM (Out of Memory):</strong> Not using gradient checkpointing (activation recomputation) or mixed precision.</li>
  <li><strong>Communication Overhead:</strong> Using Ethernet instead of InfiniBand for large models.</li>
  <li><strong>Uneven Load Balancing:</strong> In Pipeline Parallelism, if layers have different compute costs, some GPUs wait.</li>
  <li><strong>Batch Norm:</strong> Standard Batch Norm only sees the local batch. Need <strong>SyncBatchNorm</strong> to compute statistics across all GPUs.</li>
  <li><strong>Random Seed:</strong> Forgetting to set different random seeds per worker for data shuffling (all workers see same data).</li>
  <li><strong>Learning Rate Scaling:</strong> When increasing batch size from 256 to 2048, need to scale LR proportionally (Linear Scaling Rule).</li>
</ul>

<h2 id="33-real-world-deployment-kubernetes-for-ml">33. Real-World Deployment: Kubernetes for ML</h2>

<p><strong>Challenges:</strong></p>
<ul>
  <li>GPUs are expensive. Need efficient scheduling.</li>
  <li>Jobs can fail. Need auto-restart.</li>
  <li>Multi-tenancy. Need isolation.</li>
</ul>

<p><strong>Kubeflow:</strong></p>
<ul>
  <li>Kubernetes-native ML platform.</li>
  <li><strong>Components:</strong>
    <ul>
      <li><strong>TFJob / PyTorchJob:</strong> Operators for distributed training.</li>
      <li><strong>Katib:</strong> Hyperparameter tuning.</li>
      <li><strong>KFServing:</strong> Model serving.</li>
    </ul>
  </li>
</ul>

<p><strong>Example PyTorchJob:</strong></p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">kubeflow.org/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PyTorchJob</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">gpt-training</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">pytorchReplicaSpecs</span><span class="pi">:</span>
    <span class="na">Master</span><span class="pi">:</span>
      <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
      <span class="na">template</span><span class="pi">:</span>
        <span class="na">spec</span><span class="pi">:</span>
          <span class="na">containers</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pytorch</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">pytorch/pytorch:2.0</span>
            <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">python"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">train.py"</span><span class="pi">]</span>
            <span class="na">resources</span><span class="pi">:</span>
              <span class="na">limits</span><span class="pi">:</span>
                <span class="na">nvidia.com/gpu</span><span class="pi">:</span> <span class="m">8</span>
    <span class="na">Worker</span><span class="pi">:</span>
      <span class="na">replicas</span><span class="pi">:</span> <span class="m">15</span>
      <span class="na">template</span><span class="pi">:</span>
        <span class="na">spec</span><span class="pi">:</span>
          <span class="na">containers</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pytorch</span>
            <span class="na">image</span><span class="pi">:</span> <span class="s">pytorch/pytorch:2.0</span>
            <span class="na">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">python"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">train.py"</span><span class="pi">]</span>
            <span class="na">resources</span><span class="pi">:</span>
              <span class="na">limits</span><span class="pi">:</span>
                <span class="na">nvidia.com/gpu</span><span class="pi">:</span> <span class="m">8</span>
</code></pre></div></div>

<h2 id="34-conclusion">34. Conclusion</h2>

<p>Distributed training is the backbone of modern AI. From GPT to Stable Diffusion, every large model relies on these techniques.
<strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Start Simple:</strong> Use DDP for models that fit in one GPU.</li>
  <li><strong>Scale Gradually:</strong> Add Tensor/Pipeline Parallelism as needed.</li>
  <li><strong>Optimize Aggressively:</strong> Mixed precision, gradient checkpointing, FlashAttention are non-negotiable.</li>
  <li><strong>Monitor Everything:</strong> Loss, gradients, GPU utilization, communication time.</li>
  <li><strong>Expect Failures:</strong> Checkpointing and elastic training are essential.</li>
</ul>

<p>The future of AI depends on our ability to train ever-larger models efficiently. Mastering distributed training is no longer optional—it’s a core skill for ML engineers.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#deep-learning" class="page__taxonomy-item p-category" rel="tag">deep-learning</a><span class="sep">, </span>
    
      <a href="/tags/#distributed-systems" class="page__taxonomy-item p-category" rel="tag">distributed-systems</a><span class="sep">, </span>
    
      <a href="/tags/#gpu" class="page__taxonomy-item p-category" rel="tag">gpu</a><span class="sep">, </span>
    
      <a href="/tags/#parallelism" class="page__taxonomy-item p-category" rel="tag">parallelism</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Distributed+Training+Patterns%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0040-distributed-training%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0040-distributed-training%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0040-distributed-training/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0039-model-interpretability/" class="pagination--pager" title="Model Interpretability and Explainability (XAI)">Previous</a>
    
    
      <a href="/ml-system-design/0041-model-compression/" class="pagination--pager" title="Model Compression Techniques">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
