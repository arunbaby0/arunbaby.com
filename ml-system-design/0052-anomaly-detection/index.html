<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Anomaly Detection - Arun Baby</title>
<meta name="description" content="“Anomaly detection is trapping rain water for metrics: find the boundaries of ‘normal’ and measure what overflows.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Anomaly Detection">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0052-anomaly-detection/">


  <meta property="og:description" content="“Anomaly detection is trapping rain water for metrics: find the boundaries of ‘normal’ and measure what overflows.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Anomaly Detection">
  <meta name="twitter:description" content="“Anomaly detection is trapping rain water for metrics: find the boundaries of ‘normal’ and measure what overflows.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0052-anomaly-detection/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T13:14:56+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0052-anomaly-detection/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Anomaly Detection">
    <meta itemprop="description" content="“Anomaly detection is trapping rain water for metrics: find the boundaries of ‘normal’ and measure what overflows.”">
    <meta itemprop="datePublished" content="2025-12-29T13:14:56+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0052-anomaly-detection/" itemprop="url">Anomaly Detection
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          21 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-problem-statement">1. Problem Statement</a></li><li><a href="#2-understanding-the-requirements">2. Understanding the Requirements</a><ul><li><a href="#21-functional-requirements">2.1 Functional requirements</a></li><li><a href="#22-non-functional-requirements">2.2 Non-functional requirements</a></li></ul></li><li><a href="#3-high-level-architecture">3. High-Level Architecture</a><ul><li><a href="#31-diagram">3.1 Diagram</a></li><li><a href="#32-core-idea">3.2 Core idea</a></li></ul></li><li><a href="#4-component-deep-dives">4. Component Deep-Dives</a><ul><li><a href="#41-data-ingestion-and-schema-discipline">4.1 Data ingestion and schema discipline</a></li><li><a href="#42-feature-engineering-for-anomaly-detection">4.2 Feature engineering for anomaly detection</a></li><li><a href="#43-detectors-rules-statistics-ml">4.3 Detectors: rules, statistics, ML</a></li><li><a href="#44-alerting-and-routing">4.4 Alerting and routing</a></li><li><a href="#45-explanation-why-did-this-fire">4.5 Explanation (why did this fire?)</a></li><li><a href="#46-thresholding-is-a-product-decision-not-just-statistics">4.6 Thresholding is a product decision (not just statistics)</a></li><li><a href="#47-root-cause-analysis-rca-requires-drill-down-not-just-detection">4.7 Root cause analysis (RCA) requires drill-down, not just detection</a></li><li><a href="#48-handling-seasonality-and-holidays-the-hard-baseline-problem">4.8 Handling seasonality and holidays (the hard baseline problem)</a></li></ul></li><li><a href="#5-data-flow-streaming--batch">5. Data Flow (Streaming + Batch)</a><ul><li><a href="#51-streaming-path">5.1 Streaming path</a></li><li><a href="#52-batch-path">5.2 Batch path</a></li></ul></li><li><a href="#6-implementation-core-logic-in-python">6. Implementation (Core Logic in Python)</a><ul><li><a href="#61-robust-z-score-median--mad">6.1 Robust z-score (median + MAD)</a></li><li><a href="#62-ewma-control-chart">6.2 EWMA control chart</a></li><li><a href="#63-fusing-detectors-rule--stats">6.3 Fusing detectors (rule + stats)</a></li><li><a href="#64-multivariate-anomaly-detection-when-one-metric-isnt-enough">6.4 Multivariate anomaly detection (when one metric isn’t enough)</a></li></ul></li><li><a href="#7-scaling-strategies">7. Scaling Strategies</a><ul><li><a href="#71-cardinality-management-the-silent-killer">7.1 Cardinality management (the silent killer)</a></li><li><a href="#72-state-management-in-streaming">7.2 State management in streaming</a></li><li><a href="#73-multi-tenancy">7.3 Multi-tenancy</a></li><li><a href="#74-change-log-correlation-bridging-detection-to-action">7.4 Change-log correlation (bridging detection to action)</a></li><li><a href="#75-collective-anomalies-the-ones-point-detectors-miss">7.5 Collective anomalies (the ones point detectors miss)</a></li></ul></li><li><a href="#8-monitoring--metrics-for-the-anomaly-system-itself">8. Monitoring &amp; Metrics (for the anomaly system itself)</a><ul><li><a href="#81-evaluating-anomaly-detection-without-perfect-labels">8.1 Evaluating anomaly detection (without perfect labels)</a></li><li><a href="#82-impact-gating-avoid-paging-on-harmless-anomalies">8.2 “Impact gating” (avoid paging on harmless anomalies)</a></li></ul></li><li><a href="#9-failure-modes-and-mitigations">9. Failure Modes (and Mitigations)</a><ul><li><a href="#91-alert-fatigue">9.1 Alert fatigue</a></li><li><a href="#92-seasonality-false-positives">9.2 Seasonality false positives</a></li><li><a href="#93-missing-data-vs-true-zero">9.3 Missing data vs true zero</a></li><li><a href="#94-feedback-loops-and-gaming">9.4 Feedback loops and gaming</a></li><li><a href="#95-high-cardinality-explosions">9.5 High-cardinality explosions</a></li><li><a href="#96-anomaly-isnt-always-bad-launches-look-like-incidents">9.6 “Anomaly” isn’t always “bad” (launches look like incidents)</a></li></ul></li><li><a href="#10-real-world-case-study">10. Real-World Case Study</a><ul><li><a href="#netflixstreaming-quality">Netflix/Streaming quality</a></li><li><a href="#101-a-second-case-study-data-pipeline-anomaly-in-ml-training">10.1 A second case study: data pipeline anomaly in ML training</a></li><li><a href="#102-an-rca-first-ui-pattern">10.2 An “RCA-first” UI pattern</a></li></ul></li><li><a href="#11-cost-analysis">11. Cost Analysis</a></li><li><a href="#12-key-takeaways">12. Key Takeaways</a><ul><li><a href="#121-appendix-detector-selection-cheat-sheet">12.1 Appendix: detector selection cheat sheet</a></li><li><a href="#122-appendix-an-alerting-playbook-how-to-keep-trust">12.2 Appendix: an alerting playbook (how to keep trust)</a></li><li><a href="#123-appendix-baseline-modeling-options-from-simplest-to-strongest">12.3 Appendix: baseline modeling options (from simplest to strongest)</a></li><li><a href="#124-appendix-a-minimal-config-contract-for-detectors">12.4 Appendix: a minimal “config contract” for detectors</a></li><li><a href="#125-appendix-anomaly-vs-drift-vs-outlier-how-teams-get-confused">12.5 Appendix: anomaly vs drift vs outlier (how teams get confused)</a></li><li><a href="#126-appendix-an-on-call-runbook-for-anomaly-alerts">12.6 Appendix: an on-call runbook for anomaly alerts</a></li><li><a href="#127-appendix-testing-detectors-safely-before-production">12.7 Appendix: testing detectors safely before production</a></li><li><a href="#128-appendix-cost-and-scaling-intuition-why-platforms-exist">12.8 Appendix: cost and scaling intuition (why platforms exist)</a></li><li><a href="#129-appendix-multi-tenancy-and-fairness-an-under-discussed-risk">12.9 Appendix: multi-tenancy and fairness (an under-discussed risk)</a></li><li><a href="#1210-appendix-a-minimal-anomaly-triage-packet-what-every-alert-should-carry">12.10 Appendix: a minimal anomaly “triage packet” (what every alert should carry)</a></li></ul></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Anomaly detection is trapping rain water for metrics: find the boundaries of ‘normal’ and measure what overflows.”</strong></p>

<h2 id="1-problem-statement">1. Problem Statement</h2>

<p>In production ML and data systems, anomalies are the early warning signals of:</p>
<ul>
  <li>data quality regressions</li>
  <li>pipeline failures</li>
  <li>model performance drift</li>
  <li>fraud and abuse</li>
  <li>infrastructure incidents</li>
</ul>

<p>The goal is to design an <strong>anomaly detection system</strong> that:</p>
<ul>
  <li>ingests high-volume metrics/events</li>
  <li>detects meaningful anomalies quickly</li>
  <li>keeps false positives low (alert fatigue is real)</li>
  <li>supports investigation and root cause analysis</li>
  <li>is robust to seasonality and shifting baselines</li>
</ul>

<p>Today’s shared theme is <strong>pattern recognition</strong>:
in DSA, we detect the “boundary pattern” that determines trapped water.
In anomaly detection, we detect deviations from the learned “normal pattern”.</p>

<hr />

<h2 id="2-understanding-the-requirements">2. Understanding the Requirements</h2>

<h3 id="21-functional-requirements">2.1 Functional requirements</h3>

<ul>
  <li><strong>Inputs</strong>
    <ul>
      <li>time-series metrics (QPS, latency, error rates)</li>
      <li>categorical counts (events by user type, country)</li>
      <li>distributions (histograms, sketches)</li>
      <li>model signals (loss, accuracy proxies, embedding stats)</li>
    </ul>
  </li>
  <li><strong>Outputs</strong>
    <ul>
      <li>anomaly alerts with severity</li>
      <li>explanation context (top contributing dimensions)</li>
      <li>links to dashboards/runbooks</li>
      <li>ability to suppress/acknowledge</li>
    </ul>
  </li>
</ul>

<h3 id="22-non-functional-requirements">2.2 Non-functional requirements</h3>

<ul>
  <li><strong>Low latency</strong>: detect within minutes (or seconds for fraud/infra).</li>
  <li><strong>Scalability</strong>: multi-tenant at 1M events/sec.</li>
  <li><strong>Precision</strong>: too many alerts kill trust.</li>
  <li><strong>Recall</strong>: missing important anomalies is costly.</li>
  <li><strong>Robustness</strong>: handle missing data, late arrivals, backfills.</li>
  <li><strong>Interpretability</strong>: support “why did this alert fire?”.</li>
  <li><strong>Governance</strong>: alert ownership, runbooks, on-call routing.</li>
</ul>

<hr />

<h2 id="3-high-level-architecture">3. High-Level Architecture</h2>

<h3 id="31-diagram">3.1 Diagram</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   Producers (services / models / pipelines)
            |
            v
     +-------------+        +-------------------+
     | Kafka/PubSub| -----&gt; | Stream Processor  |
     +-------------+        | (Flink/Spark)     |
            |               +---------+---------+
            |                         |
            |                         v
            |               +-------------------+
            |               | Feature Builder   |
            |               | (windows, joins)  |
            |               +---------+---------+
            |                         |
            v                         v
 +-------------------+      +-------------------+
 | TS Store          |      | Detector Service  |
 | (Prom/TSDB)       |      | rules + ML        |
 +---------+---------+      +---------+---------+
           |                         |
           v                         v
 +-------------------+      +-------------------+
 | Dashboards        |      | Alert Router      |
 | (Grafana)         |      | (Pager/Slack)     |
 +-------------------+      +---------+---------+
                                     |
                                     v
                           +-------------------+
                           | Case Mgmt / RCA   |
                           | (tickets, notes)  |
                           +-------------------+
</code></pre></div></div>

<h3 id="32-core-idea">3.2 Core idea</h3>

<p>You generally run a <strong>two-layer detector</strong>:</p>
<ul>
  <li><strong>fast rules</strong> for clear anomalies (error rate &gt; X, no events for Y minutes)</li>
  <li><strong>statistical/ML detectors</strong> for subtle drift (seasonality-aware, multivariate)</li>
</ul>

<p>Rules are cheap and interpretable.
ML handles complex patterns, but must be deployed carefully.</p>

<hr />

<h2 id="4-component-deep-dives">4. Component Deep-Dives</h2>

<h3 id="41-data-ingestion-and-schema-discipline">4.1 Data ingestion and schema discipline</h3>

<p>The most common failure is not “bad model”, it’s bad data:</p>
<ul>
  <li>missing metrics due to instrumentation bugs</li>
  <li>unit changes (ms → s)</li>
  <li>counter resets</li>
  <li>duplicate events</li>
</ul>

<p>So your system needs:</p>
<ul>
  <li>schema registry (event contracts)</li>
  <li>validation (ranges, types, monotonicity for counters)</li>
  <li>backfill handling (late arrivals)</li>
</ul>

<h3 id="42-feature-engineering-for-anomaly-detection">4.2 Feature engineering for anomaly detection</h3>

<p>Features depend on signal type:</p>

<p><strong>Time series</strong></p>
<ul>
  <li>moving averages, EWMA</li>
  <li>seasonal decomposition (daily/weekly)</li>
  <li>derivative and acceleration (change rates)</li>
</ul>

<p><strong>Categorical slices</strong></p>
<ul>
  <li>per-dimension rates (country, device)</li>
  <li>top-k contributors (heavy hitters)</li>
  <li>ratio metrics (errors / requests)</li>
</ul>

<p><strong>Distributions</strong></p>
<ul>
  <li>quantiles (p50/p95/p99)</li>
  <li>histogram distance metrics (KL divergence, Wasserstein)</li>
</ul>

<p>This is the “pattern recognition” layer: translate raw signals into something that makes deviations obvious.</p>

<h3 id="43-detectors-rules-statistics-ml">4.3 Detectors: rules, statistics, ML</h3>

<p>Common detector families:</p>

<ul>
  <li><strong>Rules</strong>
    <ul>
      <li>simple thresholds, rate-of-change, “no data” checks</li>
      <li>best for: clear SLO violations, missing pipelines</li>
    </ul>
  </li>
  <li><strong>Statistical</strong>
    <ul>
      <li>z-score with robust estimators (median/MAD)</li>
      <li>EWMA control charts</li>
      <li>seasonal baselines (Holt-Winters)</li>
      <li>best for: stable metrics with known periodicity</li>
    </ul>
  </li>
  <li><strong>ML / Unsupervised</strong>
    <ul>
      <li>Isolation Forest, One-Class SVM</li>
      <li>autoencoders</li>
      <li>clustering distance to centroids</li>
      <li>best for: high-dimensional signals, unknown patterns</li>
    </ul>
  </li>
</ul>

<p>The important production truth:</p>
<blockquote>
  <p>You don’t “pick one model”. You build a system that can run multiple detectors and fuse them.</p>
</blockquote>

<h3 id="44-alerting-and-routing">4.4 Alerting and routing</h3>

<p>Anomaly detection is useless without action.
You need:</p>
<ul>
  <li>deduplication (same alert grouped)</li>
  <li>suppression (maintenance windows)</li>
  <li>routing by ownership (service/team)</li>
  <li>escalation policies and runbooks</li>
</ul>

<h3 id="45-explanation-why-did-this-fire">4.5 Explanation (why did this fire?)</h3>

<p>For slice-based anomalies, a powerful technique is “top contributors”:</p>
<ul>
  <li>overall error rate rose</li>
  <li>80% of delta is from <code class="language-plaintext highlighter-rouge">country=IN</code> and <code class="language-plaintext highlighter-rouge">device=android_low_end</code></li>
</ul>

<p>This reduces MTTR dramatically.</p>

<h3 id="46-thresholding-is-a-product-decision-not-just-statistics">4.6 Thresholding is a product decision (not just statistics)</h3>

<p>A detector outputs a score. You still need to decide:</p>
<ul>
  <li>when do we page a human?</li>
  <li>when do we log-only?</li>
  <li>when do we auto-mitigate?</li>
</ul>

<p>A practical severity ladder:</p>
<ul>
  <li><strong>S0 (log-only)</strong>: weak signal, used for trend analysis</li>
  <li><strong>S1 (notify)</strong>: Slack alert, no pager</li>
  <li><strong>S2 (page)</strong>: likely user impact or SLO breach</li>
  <li><strong>S3 (auto-mitigate + page)</strong>: safety-critical systems</li>
</ul>

<p>How to set thresholds:</p>
<ul>
  <li>start with conservative paging thresholds (avoid alert fatigue)</li>
  <li>use historical replay to estimate expected alert volume</li>
  <li>adjust per metric and per service criticality</li>
</ul>

<p>The uncomfortable truth:</p>
<blockquote>
  <p>“Correct” thresholds depend on team capacity, business impact, and tolerance for noise.</p>
</blockquote>

<h3 id="47-root-cause-analysis-rca-requires-drill-down-not-just-detection">4.7 Root cause analysis (RCA) requires drill-down, not just detection</h3>

<p>Detection answers: “something is wrong”.
RCA answers: “what is wrong, where, and why”.</p>

<p>A common production pattern:</p>
<ol>
  <li>detect anomaly on an aggregate metric (e.g., error_rate)</li>
  <li>compute deltas by dimension to find top contributors:
    <ul>
      <li>region, cluster, device, app_version, request_type</li>
    </ul>
  </li>
  <li>present ranked suspects with evidence</li>
</ol>

<p>This is essentially “explainability via decomposition”.
It often beats fancy ML models in reducing MTTR.</p>

<h3 id="48-handling-seasonality-and-holidays-the-hard-baseline-problem">4.8 Handling seasonality and holidays (the hard baseline problem)</h3>

<p>Many metrics have:</p>
<ul>
  <li>daily cycles (traffic peaks)</li>
  <li>weekly cycles (weekday vs weekend)</li>
  <li>event-driven spikes (launches, holidays)</li>
</ul>

<p>If you ignore seasonality, you’ll page every day at 9am.</p>

<p>Baseline strategies (in increasing complexity):</p>
<ul>
  <li>compare to “same time yesterday / last week”</li>
  <li>rolling window median and MAD</li>
  <li>Holt-Winters / Prophet-like seasonal models</li>
  <li>learned baselines per segment</li>
</ul>

<p>For production, the best choice is often:</p>
<ul>
  <li>simple seasonal comparisons + robust statistics</li>
  <li>plus explicit suppression windows for known events (launch windows)</li>
</ul>

<hr />

<h2 id="5-data-flow-streaming--batch">5. Data Flow (Streaming + Batch)</h2>

<h3 id="51-streaming-path">5.1 Streaming path</h3>
<ul>
  <li>ingest events into Kafka</li>
  <li>compute rolling windows (1m, 5m, 1h)</li>
  <li>compute features (rates, deltas)</li>
  <li>run rule-based detectors for immediate signals</li>
  <li>run lightweight statistical detectors (EWMA)</li>
  <li>emit alerts</li>
</ul>

<h3 id="52-batch-path">5.2 Batch path</h3>
<ul>
  <li>nightly backfills, seasonality model fitting</li>
  <li>compute baselines per metric and per segment</li>
  <li>retrain unsupervised models if used</li>
  <li>store parameters in a registry</li>
</ul>

<p>This hybrid is important because:</p>
<ul>
  <li>streaming is low-latency but limited context</li>
  <li>batch has rich history but higher latency</li>
</ul>

<hr />

<h2 id="6-implementation-core-logic-in-python">6. Implementation (Core Logic in Python)</h2>

<p>The goal here is to show “detector primitives” that can be composed. In production you’d wrap these in streaming jobs and persistent state.</p>

<h3 id="61-robust-z-score-median--mad">6.1 Robust z-score (median + MAD)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="k">def</span> <span class="nf">robust_zscore</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Robust z-score using median and MAD (median absolute deviation).
    Less sensitive to outliers than mean/std.
    </span><span class="sh">"""</span>
    <span class="n">med</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">median</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">mad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">median</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">med</span><span class="p">))</span> <span class="o">+</span> <span class="mf">1e-9</span>
    <span class="k">return</span> <span class="mf">0.6745</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">med</span><span class="p">)</span> <span class="o">/</span> <span class="n">mad</span>


<span class="k">def</span> <span class="nf">detect_anomaly_robust_z</span><span class="p">(</span><span class="n">x_window</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">=</span> <span class="nf">robust_zscore</span><span class="p">(</span><span class="n">x_window</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">abs</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;=</span> <span class="n">threshold</span>
</code></pre></div></div>

<h3 id="62-ewma-control-chart">6.2 EWMA control chart</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ewma</span><span class="p">(</span><span class="n">series</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">series</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">series</span><span class="p">)):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">series</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">out</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">detect_ewma_shift</span><span class="p">(</span><span class="n">series</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Simple EWMA shift detector: compare last value to EWMA +/- k * std.
    (In production, use rolling std and seasonality-aware baselines.)
    </span><span class="sh">"""</span>
    <span class="n">smoothed</span> <span class="o">=</span> <span class="nf">ewma</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">smoothed</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">smoothed</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="mf">1e-9</span>
    <span class="k">return</span> <span class="nf">abs</span><span class="p">(</span><span class="n">smoothed</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">k</span> <span class="o">*</span> <span class="n">sigma</span>
</code></pre></div></div>

<h3 id="63-fusing-detectors-rule--stats">6.3 Fusing detectors (rule + stats)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">DetectionResult</span><span class="p">:</span>
    <span class="n">is_anomaly</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">reason</span><span class="p">:</span> <span class="nb">str</span>


<span class="k">def</span> <span class="nf">fused_detector</span><span class="p">(</span><span class="n">series</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DetectionResult</span><span class="p">:</span>
    <span class="c1"># Rule: “no data” or impossible values
</span>    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nf">isnan</span><span class="p">(</span><span class="n">series</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">return</span> <span class="nc">DetectionResult</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="sh">"</span><span class="s">missing_data</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">series</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="nc">DetectionResult</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="sh">"</span><span class="s">negative_value</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Statistical: robust z-score
</span>    <span class="k">if</span> <span class="nf">detect_anomaly_robust_z</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">4.0</span><span class="p">):</span>
        <span class="k">return</span> <span class="nc">DetectionResult</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="sh">"</span><span class="s">robust_zscore</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Statistical: EWMA shift
</span>    <span class="k">if</span> <span class="nf">detect_ewma_shift</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">3.0</span><span class="p">):</span>
        <span class="k">return</span> <span class="nc">DetectionResult</span><span class="p">(</span><span class="bp">True</span><span class="p">,</span> <span class="sh">"</span><span class="s">ewma_shift</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">return</span> <span class="nc">DetectionResult</span><span class="p">(</span><span class="bp">False</span><span class="p">,</span> <span class="sh">"</span><span class="s">normal</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This illustrates a production principle:
<strong>anomaly detection is a system of small components</strong>, not a single model.</p>

<h3 id="64-multivariate-anomaly-detection-when-one-metric-isnt-enough">6.4 Multivariate anomaly detection (when one metric isn’t enough)</h3>

<p>Many real anomalies only show up when you look at multiple signals together:</p>
<ul>
  <li>latency rises while QPS is stable (infra saturation)</li>
  <li>error rate rises only for one endpoint (partial outage)</li>
  <li>embedding stats shift while overall accuracy proxy is stable (silent drift)</li>
</ul>

<p>Multivariate approaches:</p>
<ul>
  <li>Isolation Forest / One-Class SVM on feature vectors</li>
  <li>reconstruction error from autoencoders</li>
  <li>clustering distance to normal centroids</li>
</ul>

<p>Here’s a small Isolation Forest example (conceptual; production needs careful feature scaling and segment baselines):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">IsolationForest</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="k">def</span> <span class="nf">fit_isolation_forest</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">IsolationForest</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    X shape: [n_samples, n_features]
    Features could include: latency_p95, error_rate, qps, cpu, mem, etc.
    </span><span class="sh">"""</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">IsolationForest</span><span class="p">(</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">contamination</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">detect_multivariate</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">IsolationForest</span><span class="p">,</span> <span class="n">x_latest</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">score_threshold</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="c1"># sklearn: higher score = more normal; lower = more anomalous
</span>    <span class="n">score</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">score_samples</span><span class="p">(</span><span class="n">x_latest</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">score</span> <span class="o">&lt;</span> <span class="n">score_threshold</span>
</code></pre></div></div>

<p>Operational note:</p>
<ul>
  <li>multivariate models are powerful but harder to explain</li>
  <li>pair them with decomposition-style explanations (top contributor slices) for RCA</li>
  <li>gate paging on both “model score” and “impact metrics” (e.g., SLO risk)</li>
</ul>

<hr />

<h2 id="7-scaling-strategies">7. Scaling Strategies</h2>

<h3 id="71-cardinality-management-the-silent-killer">7.1 Cardinality management (the silent killer)</h3>

<p>If you alert on <code class="language-plaintext highlighter-rouge">metric x country x device x app_version</code>, cardinality explodes.
Strategies:</p>
<ul>
  <li>alert only on top-K segments (heavy hitters)</li>
  <li>hierarchical detection:
    <ul>
      <li>detect anomaly globally</li>
      <li>then drill down into segments to find top contributors</li>
    </ul>
  </li>
  <li>sketching (Count-Min Sketch for counts, t-digest/KLL for quantiles)</li>
</ul>

<h3 id="72-state-management-in-streaming">7.2 State management in streaming</h3>

<p>Detectors need history:</p>
<ul>
  <li>rolling windows</li>
  <li>seasonal baselines</li>
  <li>suppression state</li>
</ul>

<p>Store state in:</p>
<ul>
  <li>stream processor state (RocksDB in Flink)</li>
  <li>or external state stores (Redis, Cassandra)</li>
</ul>

<h3 id="73-multi-tenancy">7.3 Multi-tenancy</h3>

<p>Different teams want different sensitivity.
You need:</p>
<ul>
  <li>per-metric configs</li>
  <li>per-tenant budgets (alert quotas)</li>
  <li>isolation (one tenant’s high-cardinality metrics shouldn’t DOS the system)</li>
</ul>

<h3 id="74-change-log-correlation-bridging-detection-to-action">7.4 Change-log correlation (bridging detection to action)</h3>

<p>The fastest way to reduce MTTR is to correlate anomalies with “what changed”:</p>
<ul>
  <li>deploys</li>
  <li>feature flags</li>
  <li>config pushes</li>
  <li>schema changes</li>
  <li>infrastructure events (autoscaling, failovers)</li>
</ul>

<p>A pragmatic design:</p>
<ul>
  <li>ingest change events into the same stream as metrics</li>
  <li>attach a “recent changes” panel to every alert</li>
  <li>compute correlation candidates (“this alert fired within 10 minutes of deploy X in region Y”)</li>
</ul>

<p>This does not require fancy causality to be useful.
In practice, showing the top 3 recent changes near the anomaly often saves hours.</p>

<h3 id="75-collective-anomalies-the-ones-point-detectors-miss">7.5 Collective anomalies (the ones point detectors miss)</h3>

<p>Not all anomalies are single spikes. Common anomaly types:</p>
<ul>
  <li><strong>point anomaly</strong>: one bad point</li>
  <li><strong>contextual anomaly</strong>: normal value at the wrong time (seasonality)</li>
  <li><strong>collective anomaly</strong>: a sustained subtle shift (e.g., p95 latency +5% for 6 hours)</li>
</ul>

<p>Collective anomalies are the hardest in production because:</p>
<ul>
  <li>they are “small enough” to evade thresholds</li>
  <li>but “long enough” to cause real user impact</li>
</ul>

<p>Detectors for collective anomalies:</p>
<ul>
  <li>EWMA / CUSUM-style change detection</li>
  <li>burn-rate based alerting (SLO-focused)</li>
  <li>rolling baseline comparisons (same time last week)</li>
</ul>

<hr />

<h2 id="8-monitoring--metrics-for-the-anomaly-system-itself">8. Monitoring &amp; Metrics (for the anomaly system itself)</h2>

<p>You need to monitor your monitor:</p>
<ul>
  <li>alert volume per tenant</li>
  <li>false positive rate proxies (how many alerts are auto-acked)</li>
  <li>time-to-ack, time-to-resolve</li>
  <li>detector latency and backlog</li>
  <li>“missing data” alert rates (often instrumentation failures)</li>
</ul>

<p>And you need a feedback loop:
when humans label an alert as “noise”, it should improve the system.</p>

<h3 id="81-evaluating-anomaly-detection-without-perfect-labels">8.1 Evaluating anomaly detection (without perfect labels)</h3>

<p>Unlike classification, anomaly detection often lacks ground truth.
Practical evaluation strategies:</p>

<ul>
  <li><strong>Historical replay</strong>
    <ul>
      <li>replay a week/month of metrics</li>
      <li>compare alerts to known incidents and change logs</li>
    </ul>
  </li>
  <li><strong>Synthetic injections</strong>
    <ul>
      <li>inject controlled anomalies (drop QPS, spike errors, shift distributions)</li>
      <li>ensure detectors catch them with acceptable delay</li>
    </ul>
  </li>
  <li><strong>Proxy labels</strong>
    <ul>
      <li>incident tickets, rollbacks, on-call notes</li>
      <li>not perfect, but useful for measuring recall on “real pain”</li>
    </ul>
  </li>
  <li><strong>Human-in-the-loop tuning</strong>
    <ul>
      <li>collect feedback (noise vs real)</li>
      <li>tune thresholds and suppression policies</li>
    </ul>
  </li>
</ul>

<p>You typically optimize for:</p>
<ul>
  <li>high precision for paging alerts</li>
  <li>high recall for “notify” alerts (lower severity)</li>
</ul>

<h3 id="82-impact-gating-avoid-paging-on-harmless-anomalies">8.2 “Impact gating” (avoid paging on harmless anomalies)</h3>

<p>A classic production trick:
don’t page just because a detector score is high.
Page when:</p>
<ul>
  <li>anomaly score is high <strong>and</strong></li>
  <li>impact metrics indicate user risk</li>
</ul>

<p>Examples:</p>
<ul>
  <li>error rate anomaly + SLO burn rate increasing</li>
  <li>latency anomaly + p99 above a hard threshold</li>
</ul>

<p>This reduces alert fatigue while keeping critical incidents caught.</p>

<hr />

<h2 id="9-failure-modes-and-mitigations">9. Failure Modes (and Mitigations)</h2>

<h3 id="91-alert-fatigue">9.1 Alert fatigue</h3>
<p>Mitigation:</p>
<ul>
  <li>dedupe + grouping</li>
  <li>severity levels</li>
  <li>alert budgets and rate limits</li>
  <li>better routing + suppression windows</li>
</ul>

<h3 id="92-seasonality-false-positives">9.2 Seasonality false positives</h3>
<p>Mitigation:</p>
<ul>
  <li>baseline models (daily/weekly)</li>
  <li>compare to same time last week</li>
  <li>use robust estimators</li>
</ul>

<h3 id="93-missing-data-vs-true-zero">9.3 Missing data vs true zero</h3>
<p>Mitigation:</p>
<ul>
  <li>explicit “heartbeat” signals</li>
  <li>separate pipelines for “metric missing” vs “value is zero”</li>
</ul>

<h3 id="94-feedback-loops-and-gaming">9.4 Feedback loops and gaming</h3>
<p>If teams learn to “silence alerts”, your system loses trust.
Mitigation:</p>
<ul>
  <li>governance: who can suppress, for how long</li>
  <li>audit logs</li>
  <li>periodic review of suppressed alerts</li>
</ul>

<h3 id="95-high-cardinality-explosions">9.5 High-cardinality explosions</h3>

<p>A common production outage pattern:</p>
<ul>
  <li>someone adds a new label/dimension (e.g., <code class="language-plaintext highlighter-rouge">user_id</code>)</li>
  <li>cardinality explodes</li>
  <li>storage and stream processing costs spike</li>
  <li>detectors become meaningless (each series has too few points)</li>
</ul>

<p>Mitigations:</p>
<ul>
  <li>enforce label allowlists/denylists</li>
  <li>use hierarchical detection (alert on aggregate first, then drill down)</li>
  <li>cap top-k dimensions and use heavy-hitter sketches</li>
  <li>charge back cost to teams for uncontrolled cardinality (governance matters)</li>
</ul>

<h3 id="96-anomaly-isnt-always-bad-launches-look-like-incidents">9.6 “Anomaly” isn’t always “bad” (launches look like incidents)</h3>

<p>Product launches and marketing events create legitimate shifts.
If the system treats every spike as an incident, it loses credibility.</p>

<p>Mitigations:</p>
<ul>
  <li>change-log correlation: if a known launch happened, adjust baseline</li>
  <li>planned maintenance windows (suppression)</li>
  <li>“expected anomaly” annotations (Grafana-style)</li>
</ul>

<p>This is another reason anomaly detection is a system: you need metadata and governance, not just math.</p>

<hr />

<h2 id="10-real-world-case-study">10. Real-World Case Study</h2>

<h3 id="netflixstreaming-quality">Netflix/Streaming quality</h3>
<p>Netflix-like systems track:</p>
<ul>
  <li>playback start failures</li>
  <li>buffering ratio</li>
  <li>CDN health</li>
</ul>

<p>Anomalies matter because:</p>
<ul>
  <li>small regressions affect millions of users quickly</li>
  <li>seasonality exists (prime time)</li>
</ul>

<p>Common pattern:</p>
<ul>
  <li>global anomaly triggers</li>
  <li>drill-down identifies region/device/CDN nodes as top contributors</li>
  <li>routing goes to the owning team (CDN vs player vs backend)</li>
</ul>

<h3 id="101-a-second-case-study-data-pipeline-anomaly-in-ml-training">10.1 A second case study: data pipeline anomaly in ML training</h3>

<p>Consider a feature pipeline that writes training data daily.
One day, an upstream service changes a field from “milliseconds” to “seconds”.</p>

<p>What happens:</p>
<ul>
  <li>models still train (no crash)</li>
  <li>loss might even look “okay”</li>
  <li>but downstream predictions degrade subtly</li>
</ul>

<p>Anomaly detection can catch this early with:</p>
<ul>
  <li><strong>schema validation</strong>: units and ranges</li>
  <li><strong>distribution shift</strong>: feature histograms vs baseline</li>
  <li><strong>training metrics drift</strong>: gradient norms, embedding stats</li>
</ul>

<p>The key lesson:
the most expensive ML failures are often “silent correctness failures”.
You need anomaly detection not just for infra metrics, but for data and model health signals.</p>

<h3 id="102-an-rca-first-ui-pattern">10.2 An “RCA-first” UI pattern</h3>

<p>When an alert fires, show:</p>
<ul>
  <li>the metric that triggered</li>
  <li>the baseline comparison (last week, seasonal)</li>
  <li>the top contributing dimensions (country/device/version)</li>
  <li>links to relevant dashboards and runbooks</li>
  <li>recent change log correlation (deploys, config changes, schema changes)</li>
</ul>

<p>This turns anomaly detection from “pager noise” into a decision system.</p>

<hr />

<h2 id="11-cost-analysis">11. Cost Analysis</h2>

<p>Cost drivers:</p>
<ul>
  <li>stream processing state (memory/disk)</li>
  <li>high-cardinality storage</li>
  <li>alert routing and case management integration</li>
</ul>

<p>Cost levers:</p>
<ul>
  <li>limit cardinality and use sketches</li>
  <li>run heavy ML detectors only when cheaper detectors suspect an anomaly</li>
  <li>compress histories and store only what’s needed for RCA windows</li>
</ul>

<hr />

<h2 id="12-key-takeaways">12. Key Takeaways</h2>

<ol>
  <li><strong>Anomaly detection is a system, not a model</strong>: ingestion, features, detectors, alerting, RCA.</li>
  <li><strong>Boundaries and invariants matter</strong>: define “normal” and detect boundary violations, like two-pointer reasoning.</li>
  <li><strong>Operational success = low false positives + good explanations</strong>: otherwise no one trusts alerts.</li>
</ol>

<h3 id="121-appendix-detector-selection-cheat-sheet">12.1 Appendix: detector selection cheat sheet</h3>

<p>Use this as a quick mapping from problem → detector:</p>

<table>
  <thead>
    <tr>
      <th>Signal type</th>
      <th>Typical anomaly</th>
      <th>Best first detector</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Counters (QPS, errors)</td>
      <td>drop to zero, spike</td>
      <td>rules + burn-rate</td>
      <td>also detect missing vs true zero</td>
    </tr>
    <tr>
      <td>Latency percentiles</td>
      <td>sustained shift</td>
      <td>EWMA / baseline compare</td>
      <td>seasonality is common</td>
    </tr>
    <tr>
      <td>High-dimensional telemetry</td>
      <td>weird combinations</td>
      <td>Isolation Forest / AE</td>
      <td>require careful explanation</td>
    </tr>
    <tr>
      <td>Categorical slices</td>
      <td>one segment dominates</td>
      <td>hierarchical drill-down</td>
      <td>“top contributors” is key</td>
    </tr>
    <tr>
      <td>Data features (ML)</td>
      <td>distribution shift</td>
      <td>histogram distance + rules</td>
      <td>unit changes are common</td>
    </tr>
  </tbody>
</table>

<p>Rule of thumb:</p>
<ul>
  <li>page on rules + high-impact metrics</li>
  <li>use ML scores as supporting evidence or for “notify” tier alerts</li>
</ul>

<h3 id="122-appendix-an-alerting-playbook-how-to-keep-trust">12.2 Appendix: an alerting playbook (how to keep trust)</h3>

<p>If your anomaly system is noisy, it dies. A practical playbook:</p>

<ul>
  <li><strong>Start narrow</strong>
    <ul>
      <li>detect a small set of critical metrics</li>
      <li>keep paging volume low</li>
    </ul>
  </li>
  <li><strong>Add explainability</strong>
    <ul>
      <li>top contributors</li>
      <li>baseline comparisons</li>
      <li>recent change log correlation</li>
    </ul>
  </li>
  <li><strong>Use severity tiers</strong>
    <ul>
      <li>paging only for high-confidence/high-impact signals</li>
      <li>Slack-only for exploratory detectors</li>
    </ul>
  </li>
  <li><strong>Build feedback</strong>
    <ul>
      <li>allow “noise” labeling</li>
      <li>review suppressed alerts periodically</li>
    </ul>
  </li>
</ul>

<p>This is the same “pattern recognition” lesson as the DSA problem:
don’t react to every fluctuation; define the boundary of “actionable abnormal”.</p>

<h3 id="123-appendix-baseline-modeling-options-from-simplest-to-strongest">12.3 Appendix: baseline modeling options (from simplest to strongest)</h3>

<p>When teams say “anomaly detection”, they often mean “baseline modeling”.
Here’s a pragmatic ladder:</p>

<ul>
  <li><strong>Static thresholds</strong>
    <ul>
      <li>best for: hard limits (error_rate must be &lt; 1%)</li>
      <li>worst for: seasonal metrics</li>
    </ul>
  </li>
  <li><strong>Rolling robust baseline (median/MAD)</strong>
    <ul>
      <li>best for: noisy but mostly stationary signals</li>
      <li>robust to spikes and outliers</li>
    </ul>
  </li>
  <li><strong>Seasonal baseline (same time last week)</strong>
    <ul>
      <li>best for: strong weekly patterns</li>
      <li>easy to explain and cheap to run</li>
    </ul>
  </li>
  <li><strong>Holt-Winters / ETS</strong>
    <ul>
      <li>best for: smooth seasonality + trend</li>
      <li>good interpretability</li>
    </ul>
  </li>
  <li><strong>Learned baselines (per segment)</strong>
    <ul>
      <li>best for: complex multi-segment products</li>
      <li>requires strong governance to avoid accidental bias</li>
    </ul>
  </li>
</ul>

<p>Production tip:
start with seasonal comparisons + robust statistics.
Only move to heavier models when simpler baselines fail.</p>

<h3 id="124-appendix-a-minimal-config-contract-for-detectors">12.4 Appendix: a minimal “config contract” for detectors</h3>

<p>To run this system safely at scale, you need a configuration contract:</p>
<ul>
  <li>metric name</li>
  <li>aggregation window (1m/5m/1h)</li>
  <li>baseline method</li>
  <li>sensitivity / threshold</li>
  <li>severity tier (log/notify/page)</li>
  <li>routing owner (team/on-call)</li>
  <li>suppression windows (maintenance)</li>
</ul>

<p>This turns anomaly detection into an operable platform:
teams can onboard new signals without changing code, and you can audit changes.</p>

<h3 id="125-appendix-anomaly-vs-drift-vs-outlier-how-teams-get-confused">12.5 Appendix: anomaly vs drift vs outlier (how teams get confused)</h3>

<p>These terms are often used interchangeably, but they’re different:</p>

<ul>
  <li><strong>Outlier</strong>: a single unusual point relative to its neighbors.
    <ul>
      <li>Example: one batch has a huge spike in null values.</li>
    </ul>
  </li>
  <li><strong>Anomaly</strong>: a point or segment that violates an expected pattern.
    <ul>
      <li>Example: error rate spike at 2am that is not part of normal seasonality.</li>
    </ul>
  </li>
  <li><strong>Drift</strong>: the underlying distribution changes over time.
    <ul>
      <li>Example: feature distribution slowly shifts after a product change.</li>
    </ul>
  </li>
</ul>

<p>Operationally:</p>
<ul>
  <li>outliers are often “data quality” issues</li>
  <li>anomalies are “incident” candidates</li>
  <li>drift is “model health” and requires different response (retraining, feature changes)</li>
</ul>

<p>The best platforms support all three, but keep the response paths distinct:</p>
<ul>
  <li>anomalies page humans</li>
  <li>drift triggers investigation and model iteration</li>
  <li>outliers often trigger data validation and pipeline fixes</li>
</ul>

<h3 id="126-appendix-an-on-call-runbook-for-anomaly-alerts">12.6 Appendix: an on-call runbook for anomaly alerts</h3>

<p>When an alert pages a human, the system should also provide a runbook-style checklist. A practical sequence:</p>

<ol>
  <li><strong>Confirm impact</strong>
    <ul>
      <li>are user-facing SLOs burning?</li>
      <li>is this confined to a segment (region/device) or global?</li>
    </ul>
  </li>
  <li><strong>Check data validity</strong>
    <ul>
      <li>missing data vs real zeros</li>
      <li>instrumentation changes (new labels, new units)</li>
    </ul>
  </li>
  <li><strong>Correlate with recent changes</strong>
    <ul>
      <li>deploys, feature flags, config changes</li>
      <li>schema changes in pipelines</li>
    </ul>
  </li>
  <li><strong>Drill down</strong>
    <ul>
      <li>top contributors by dimension</li>
      <li>compare against historical baselines (same time last week)</li>
    </ul>
  </li>
  <li><strong>Mitigate</strong>
    <ul>
      <li>rollback suspect deploys</li>
      <li>fail over traffic if it’s infra-related</li>
      <li>suppress alerts only with an explicit maintenance annotation</li>
    </ul>
  </li>
</ol>

<p>This checklist turns “anomaly detection” into operational reality: consistent response beats clever scoring.</p>

<h3 id="127-appendix-testing-detectors-safely-before-production">12.7 Appendix: testing detectors safely before production</h3>

<p>Before enabling paging:</p>
<ul>
  <li>run in shadow mode (log-only) for 1–2 weeks</li>
  <li>replay historical incidents and ensure alerts would have fired</li>
  <li>inject synthetic anomalies (spikes, drops, sustained shifts)</li>
  <li>check alert volume budgets per team</li>
</ul>

<p>If you do this, you avoid the most common failure:
shipping a detector that immediately pages everyone and loses trust forever.</p>

<h3 id="128-appendix-cost-and-scaling-intuition-why-platforms-exist">12.8 Appendix: cost and scaling intuition (why platforms exist)</h3>

<p>Teams often start anomaly detection as ad-hoc scripts per service.
At small scale, that works. At org scale, it collapses due to:</p>
<ul>
  <li>duplicated logic (everyone re-implements EWMA differently)</li>
  <li>inconsistent thresholds and alert policies</li>
  <li>uncontrolled cardinality costs</li>
  <li>lack of auditability (“who changed this threshold?”)</li>
</ul>

<p>That’s why anomaly detection becomes a platform:</p>
<ul>
  <li>shared ingestion and schema validation</li>
  <li>shared baseline modeling primitives</li>
  <li>shared routing/suppression/governance</li>
  <li>shared RCA UI patterns (top contributors, change-log correlation)</li>
</ul>

<p>If you can explain this evolution clearly in interviews, it signals strong “systems taste”.</p>

<h3 id="129-appendix-multi-tenancy-and-fairness-an-under-discussed-risk">12.9 Appendix: multi-tenancy and fairness (an under-discussed risk)</h3>

<p>Anomaly systems can create “fairness” issues at the org level:</p>
<ul>
  <li>teams with noisier metrics consume disproportionate paging attention</li>
  <li>teams with low traffic get ignored because their signals are sparse</li>
  <li>some regions/devices get under-monitored because baselines are not segment-aware</li>
</ul>

<p>Mitigations:</p>
<ul>
  <li>per-tenant alert budgets (rate limits and quotas)</li>
  <li>segment-aware baselines for critical dimensions (region/device/app version)</li>
  <li>“coverage” metrics: which segments have enough data to monitor reliably</li>
  <li>explicit ownership: every alert route must map to a team/runbook</li>
</ul>

<p>This is the same lesson as other large platforms: governance is part of the technical design.</p>

<h3 id="1210-appendix-a-minimal-anomaly-triage-packet-what-every-alert-should-carry">12.10 Appendix: a minimal anomaly “triage packet” (what every alert should carry)</h3>

<p>To make alerts actionable, every alert should include a small, standardized packet:</p>

<ul>
  <li><strong>What</strong>: metric name, current value, baseline value, anomaly score</li>
  <li><strong>When</strong>: start time, duration, detection time</li>
  <li><strong>Where</strong>: top affected segments (region/device/version) with contribution deltas</li>
  <li><strong>Impact</strong>: SLO burn rate / user-impact proxy</li>
  <li><strong>Why likely</strong>: top recent change events correlated (deploy/flag/config/schema)</li>
  <li><strong>Next</strong>: runbook link + owning team + mitigation suggestions (rollback, failover)</li>
</ul>

<p>This single design choice reduces MTTR more than most detector tweaks, because it turns “math output” into “operational context”.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0052-anomaly-detection/">arunbaby.com/ml-system-design/0052-anomaly-detection</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#alerting" class="page__taxonomy-item p-category" rel="tag">alerting</a><span class="sep">, </span>
    
      <a href="/tags/#anomaly-detection" class="page__taxonomy-item p-category" rel="tag">anomaly-detection</a><span class="sep">, </span>
    
      <a href="/tags/#mlops" class="page__taxonomy-item p-category" rel="tag">mlops</a><span class="sep">, </span>
    
      <a href="/tags/#monitoring" class="page__taxonomy-item p-category" rel="tag">monitoring</a><span class="sep">, </span>
    
      <a href="/tags/#streaming" class="page__taxonomy-item p-category" rel="tag">streaming</a><span class="sep">, </span>
    
      <a href="/tags/#time-series" class="page__taxonomy-item p-category" rel="tag">time-series</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0052-trapping-rain-water/" rel="permalink">Trapping Rain Water
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Water doesn’t care about every bar—only the highest walls to the left and right.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0052-anomaly-detection/" rel="permalink">Anomaly Detection
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          21 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Anomaly detection is trapping rain water for metrics: find the boundaries of ‘normal’ and measure what overflows.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0052-speech-anomaly-detection/" rel="permalink">Speech Anomaly Detection
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“If ASR is the brain, anomaly detection is the nervous system—it tells you when the audio reality changed.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0052-long-context-agent-strategies/" rel="permalink">Long-Context Agent Strategies
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Long context isn’t ‘more tokens’—it’s a strategy for keeping the right boundaries of information.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Anomaly+Detection%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0052-anomaly-detection%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0052-anomaly-detection%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0052-anomaly-detection/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0051-federated-learning/" class="pagination--pager" title="Federated Learning">Previous</a>
    
    
      <a href="/ml-system-design/0053-data-validation/" class="pagination--pager" title="Data Validation">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
