<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>A/B Testing Systems for ML - Arun Baby</title>
<meta name="description" content="How to design experimentation platforms that enable rapid iteration while maintaining statistical rigor at scale.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="A/B Testing Systems for ML">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0004-ab-testing-systems/">


  <meta property="og:description" content="How to design experimentation platforms that enable rapid iteration while maintaining statistical rigor at scale.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="A/B Testing Systems for ML">
  <meta name="twitter:description" content="How to design experimentation platforms that enable rapid iteration while maintaining statistical rigor at scale.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0004-ab-testing-systems/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0004-ab-testing-systems/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="A/B Testing Systems for ML">
    <meta itemprop="description" content="How to design experimentation platforms that enable rapid iteration while maintaining statistical rigor at scale.">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0004-ab-testing-systems/" itemprop="url">A/B Testing Systems for ML
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          28 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#problem-definition">Problem Definition</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li></ul></li><li><a href="#high-level-architecture">High-Level Architecture</a></li><li><a href="#component-1-experiment-assignment">Component 1: Experiment Assignment</a><ul><li><a href="#deterministic-assignment-via-hashing">Deterministic Assignment via Hashing</a></li><li><a href="#why-hashing-works">Why Hashing Works</a></li><li><a href="#handling-traffic-allocation">Handling Traffic Allocation</a></li></ul></li><li><a href="#component-2-metrics-tracking">Component 2: Metrics Tracking</a><ul><li><a href="#event-logging">Event Logging</a></li><li><a href="#metrics-aggregation">Metrics Aggregation</a></li></ul></li><li><a href="#component-3-statistical-analysis">Component 3: Statistical Analysis</a><ul><li><a href="#t-test-for-continuous-metrics">T-Test for Continuous Metrics</a></li><li><a href="#chi-square-test-for-binary-metrics">Chi-Square Test for Binary Metrics</a></li></ul></li><li><a href="#sample-size-calculation--power-analysis">Sample Size Calculation &amp; Power Analysis</a><ul><li><a href="#power-analysis">Power Analysis</a></li></ul></li><li><a href="#guardrail-metrics">Guardrail Metrics</a><ul><li><a href="#implementing-guardrails">Implementing Guardrails</a></li></ul></li><li><a href="#real-world-examples">Real-World Examples</a><ul><li><a href="#netflix-experimentation-at-scale">Netflix: Experimentation at Scale</a></li><li><a href="#google-large-scale-testing">Google: Large-scale Testing</a></li></ul></li><li><a href="#advanced-topics">Advanced Topics</a><ul><li><a href="#sequential-testing--early-stopping">Sequential Testing &amp; Early Stopping</a></li><li><a href="#multi-armed-bandits">Multi-Armed Bandits</a></li><li><a href="#variance-reduction-cuped">Variance Reduction: CUPED</a></li><li><a href="#stratified-sampling">Stratified Sampling</a></li></ul></li><li><a href="#multiple-testing-correction">Multiple Testing Correction</a><ul><li><a href="#bonferroni-correction">Bonferroni Correction</a></li><li><a href="#false-discovery-rate-fdr---benjamini-hochberg">False Discovery Rate (FDR) - Benjamini-Hochberg</a></li></ul></li><li><a href="#layered-experiments">Layered Experiments</a></li><li><a href="#airbnbs-experiment-framework">Airbnbâ€™s Experiment Framework</a></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>How to design experimentation platforms that enable rapid iteration while maintaining statistical rigor at scale.</strong></p>

<h2 id="introduction">Introduction</h2>

<p>A/B testing is the <strong>backbone of data-driven decision making</strong> in ML systems. Every major tech company runs thousands of experiments simultaneously to:</p>

<ul>
  <li>Test new model versions</li>
  <li>Validate product changes</li>
  <li>Optimize user experience</li>
  <li>Measure feature impact</li>
</ul>

<p><strong>Why it matters:</strong></p>
<ul>
  <li><strong>Validate improvements:</strong> Ensure new models actually perform better</li>
  <li><strong>Reduce risk:</strong> Test changes on small cohorts before full rollout</li>
  <li><strong>Quantify impact:</strong> Measure precise effect size, not just gut feeling</li>
  <li><strong>Enable velocity:</strong> Run multiple experiments in parallel</li>
</ul>

<p><strong>What youâ€™ll learn:</strong></p>
<ul>
  <li>A/B testing architecture for ML systems</li>
  <li>Statistical foundations (hypothesis testing, power analysis)</li>
  <li>Experiment assignment and randomization</li>
  <li>Metrics tracking and analysis</li>
  <li>Guardrail metrics and quality assurance</li>
  <li>Real-world examples from tech giants</li>
</ul>

<hr />

<h2 id="problem-definition">Problem Definition</h2>

<p>Design an A/B testing platform for ML systems.</p>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Experiment Setup</strong>
    <ul>
      <li>Create experiments with control/treatment variants</li>
      <li>Define success metrics and guardrails</li>
      <li>Set experiment parameters (duration, traffic allocation)</li>
      <li>Support multi-variant testing (A/B/C/D)</li>
    </ul>
  </li>
  <li><strong>User Assignment</strong>
    <ul>
      <li>Randomly assign users to variants</li>
      <li>Ensure consistency (same user always sees same variant)</li>
      <li>Support layered experiments</li>
      <li>Handle new vs returning users</li>
    </ul>
  </li>
  <li><strong>Metrics Tracking</strong>
    <ul>
      <li>Log user actions and outcomes</li>
      <li>Compute experiment metrics in real-time</li>
      <li>Track both primary and secondary metrics</li>
      <li>Monitor guardrail metrics</li>
    </ul>
  </li>
  <li><strong>Statistical Analysis</strong>
    <ul>
      <li>Calculate statistical significance</li>
      <li>Compute confidence intervals</li>
      <li>Detect early wins/losses</li>
      <li>Generate experiment reports</li>
    </ul>
  </li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Scale</strong>
    <ul>
      <li>Handle 10M+ users</li>
      <li>Support 100+ concurrent experiments</li>
      <li>Process billions of events/day</li>
    </ul>
  </li>
  <li><strong>Latency</strong>
    <ul>
      <li>Assignment: &lt; 10ms</li>
      <li>Metrics updates: Near real-time (&lt; 1 minute lag)</li>
    </ul>
  </li>
  <li><strong>Reliability</strong>
    <ul>
      <li>99.9% uptime</li>
      <li>No data loss</li>
      <li>Audit trail for all experiments</li>
    </ul>
  </li>
  <li><strong>Statistical Rigor</strong>
    <ul>
      <li>Type I error (false positive) &lt; 5%</li>
      <li>Sufficient statistical power (80%+)</li>
      <li>Multiple testing corrections</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="high-level-architecture">High-Level Architecture</h2>

<p>``
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Experimentation Platform â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Experiment Configuration Service â”‚ â”‚
â”‚ â”‚ - Create experiments â”‚ â”‚
â”‚ â”‚ - Define metrics â”‚ â”‚
â”‚ â”‚ - Set parameters â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â†“ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Assignment Service â”‚ â”‚
â”‚ â”‚ - Hash user_id â†’ variant â”‚ â”‚
â”‚ â”‚ - Consistent assignment â”‚ â”‚
â”‚ â”‚ - Cache assignments â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â†“ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Event Logging Service â”‚ â”‚
â”‚ â”‚ - Log user actions â”‚ â”‚
â”‚ â”‚ - Track outcomes â”‚ â”‚
â”‚ â”‚ - Stream to analytics â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â†“ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Metrics Aggregation Service â”‚ â”‚
â”‚ â”‚ - Compute experiment metrics â”‚ â”‚
â”‚ â”‚ - Real-time dashboards â”‚ â”‚
â”‚ â”‚ - Statistical tests â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â†“ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Analysis &amp; Reporting Service â”‚ â”‚
â”‚ â”‚ - Statistical significance â”‚ â”‚
â”‚ â”‚ - Confidence intervals â”‚ â”‚
â”‚ â”‚ - Decision recommendations â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</p>

<p>Data Flow:
User Request â†’ Assignment â†’ Show Variant â†’ Log Events â†’ Aggregate Metrics â†’ Analyze
``</p>

<hr />

<h2 id="component-1-experiment-assignment">Component 1: Experiment Assignment</h2>

<p>Assign users to experiment variants consistently and randomly.</p>

<h3 id="deterministic-assignment-via-hashing">Deterministic Assignment via Hashing</h3>

<p>``python
import hashlib
from typing import List, Dict</p>

<p>class ExperimentAssigner:
 â€œâ€â€
 Assign users to experiment variants</p>

<p>Requirements:</p>
<ul>
  <li>Deterministic: Same user_id â†’ same variant</li>
  <li>Random: Uniform distribution across variants</li>
  <li>Independent: Different experiments use different hash seeds
 â€œâ€â€</li>
</ul>

<p>def <strong>init</strong>(self):
 self.experiments = {} # experiment_id â†’ config</p>

<p>def create_experiment(
 self,
 experiment_id: str,
 variants: List[str],
 traffic_allocation: float = 1.0
 ):
 â€œâ€â€
 Create new experiment</p>

<p>Args:
 experiment_id: Unique experiment identifier
 variants: List of variant names (e.g., [â€˜controlâ€™, â€˜treatmentâ€™])
 traffic_allocation: Fraction of users to include (0.0 to 1.0)
 â€œâ€â€
 self.experiments[experiment_id] = {
 â€˜variantsâ€™: variants,
 â€˜traffic_allocationâ€™: traffic_allocation,
 â€˜num_variantsâ€™: len(variants)
 }</p>

<p>def assign_variant(self, user_id: str, experiment_id: str) -&gt; str:
 â€œâ€â€
 Assign user to variant</p>

<p>Uses consistent hashing for deterministic assignment</p>

<p>Returns:
 Variant name or None if user not in experiment
 â€œâ€â€
 if experiment_id not in self.experiments:
 raise ValueError(fâ€Experiment {experiment_id} not foundâ€)</p>

<p>config = self.experiments[experiment_id]</p>

<p># Hash user_id + experiment_id
 hash_input = fâ€{user_id}:{experiment_id}â€.encode(â€˜utf-8â€™)
 hash_value = int(hashlib.md5(hash_input).hexdigest(), 16)</p>

<p># Map to [0, 1]
 normalized = (hash_value % 10000) / 10000.0</p>

<p># Check if user is in experiment (traffic allocation)
 if normalized &gt;= config[â€˜traffic_allocationâ€™]:
 return None # User not in experiment</p>

<p># Assign to variant
 # Re-normalize to [0, 1] within allocated traffic
 variant_hash = normalized / config[â€˜traffic_allocationâ€™]
 variant_idx = int(variant_hash * config[â€˜num_variantsâ€™])</p>

<p>return config[â€˜variantsâ€™][variant_idx]</p>

<h1 id="usage">Usage</h1>
<p>assigner = ExperimentAssigner()</p>

<h1 id="create-experiment-50-control-50-treatment-100-of-users">Create experiment: 50% control, 50% treatment, 100% of users</h1>
<p>assigner.create_experiment(
 experiment_id=â€™model_v2_testâ€™,
 variants=[â€˜controlâ€™, â€˜treatmentâ€™],
 traffic_allocation=1.0
)</p>

<h1 id="assign-users">Assign users</h1>
<p>user_1_variant = assigner.assign_variant(â€˜user_123â€™, â€˜model_v2_testâ€™)
print(fâ€User 123 assigned to: {user_1_variant}â€)</p>

<h1 id="same-user-always-gets-same-variant">Same user always gets same variant</h1>
<p>assert assigner.assign_variant(â€˜user_123â€™, â€˜model_v2_testâ€™) == user_1_variant
``</p>

<h3 id="why-hashing-works">Why Hashing Works</h3>

<p><strong>Properties of MD5/SHA hashing:</strong></p>
<ol>
  <li><strong>Deterministic:</strong> Same input â†’ same output</li>
  <li><strong>Uniform:</strong> Output uniformly distributed</li>
  <li><strong>Independent:</strong> Different inputs â†’ uncorrelated outputs</li>
</ol>

<p><strong>Key insight:</strong> Hash(user_id + experiment_id) acts as a random number generator with a fixed seed per user-experiment pair.</p>

<h3 id="handling-traffic-allocation">Handling Traffic Allocation</h3>

<p>``python
def assign_with_traffic_split(self, user_id: str, experiment_id: str) -&gt; str:
 â€œâ€â€
 Assign with partial traffic allocation</p>

<p>Example: 10% of users in experiment</p>
<ul>
  <li>Hash to [0, 1]</li>
  <li>If &lt; 0.10 â†’ assign to variant</li>
  <li>Else â†’ not in experiment
 â€œâ€â€
 config = self.experiments[experiment_id]</li>
</ul>

<p># Hash
 hash_input = fâ€{user_id}:{experiment_id}â€.encode(â€˜utf-8â€™)
 hash_value = int(hashlib.md5(hash_input).hexdigest(), 16)
 normalized = (hash_value % 10000) / 10000.0</p>

<p># Traffic allocation check
 if normalized &gt;= config[â€˜traffic_allocationâ€™]:
 return None</p>

<p># Within traffic, assign to variant
 # Scale normalized to [0, traffic_allocation] â†’ [0, 1]
 variant_hash = normalized / config[â€˜traffic_allocationâ€™]
 variant_idx = int(variant_hash * config[â€˜num_variantsâ€™])</p>

<p>return config[â€˜variantsâ€™][variant_idx]
``</p>

<hr />

<h2 id="component-2-metrics-tracking">Component 2: Metrics Tracking</h2>

<p>Track user actions and compute experiment metrics.</p>

<h3 id="event-logging">Event Logging</h3>

<p>``python
from dataclasses import dataclass
from datetime import datetime
from typing import Optional
import json</p>

<p>@dataclass
class ExperimentEvent:
 â€œ"â€Single experiment eventâ€â€â€
 user_id: str
 experiment_id: str
 variant: str
 event_type: str # e.g., â€˜impressionâ€™, â€˜clickâ€™, â€˜purchaseâ€™
 timestamp: datetime
 metadata: dict = None</p>

<p>def to_dict(self):
 return {
 â€˜user_idâ€™: self.user_id,
 â€˜experiment_idâ€™: self.experiment_id,
 â€˜variantâ€™: self.variant,
 â€˜event_typeâ€™: self.event_type,
 â€˜timestampâ€™: self.timestamp.isoformat(),
 â€˜metadataâ€™: self.metadata or {}
 }</p>

<p>class EventLogger:
 â€œâ€â€
 Log experiment events</p>

<p>In production: Stream to Kafka/Kinesis â†’ Data warehouse
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, output_file=â€™experiment_events.jsonlâ€™):
 self.output_file = output_file</p>

<p>def log_event(self, event: ExperimentEvent):
 â€œâ€â€
 Log single event</p>

<p>In production: Send to message queue
 â€œâ€â€
 with open(self.output_file, â€˜aâ€™) as f:
 f.write(json.dumps(event.to_dict()) + â€˜\nâ€™)</p>

<p>def log_assignment(self, user_id: str, experiment_id: str, variant: str):
 â€œ"â€Log when user is assigned to variantâ€â€â€
 event = ExperimentEvent(
 user_id=user_id,
 experiment_id=experiment_id,
 variant=variant,
 event_type=â€™assignmentâ€™,
 timestamp=datetime.now()
 )
 self.log_event(event)</p>

<p>def log_metric_event(
 self,
 user_id: str,
 experiment_id: str,
 variant: str,
 metric_name: str,
 metric_value: float
 ):
 â€œ"â€Log metric event (e.g., click, purchase)â€â€â€
 event = ExperimentEvent(
 user_id=user_id,
 experiment_id=experiment_id,
 variant=variant,
 event_type=metric_name,
 timestamp=datetime.now(),
 metadata={â€˜valueâ€™: metric_value}
 )
 self.log_event(event)</p>

<h1 id="usage-1">Usage</h1>
<p>logger = EventLogger()</p>

<h1 id="log-assignment">Log assignment</h1>
<p>logger.log_assignment(â€˜user_123â€™, â€˜model_v2_testâ€™, â€˜treatmentâ€™)</p>

<h1 id="log-click">Log click</h1>
<p>logger.log_metric_event(â€˜user_123â€™, â€˜model_v2_testâ€™, â€˜treatmentâ€™, â€˜clickâ€™, 1.0)</p>

<h1 id="log-purchase">Log purchase</h1>
<p>logger.log_metric_event(â€˜user_123â€™, â€˜model_v2_testâ€™, â€˜treatmentâ€™, â€˜purchaseâ€™, 49.99)
``</p>

<h3 id="metrics-aggregation">Metrics Aggregation</h3>

<p>``python
from collections import defaultdict
import pandas as pd</p>

<p>class MetricsAggregator:
 â€œâ€â€
 Aggregate experiment metrics from events</p>

<p>Computes per-variant statistics
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.variant_stats = defaultdict(lambda: defaultdict(list))</p>

<p>def add_event(self, variant: str, metric_name: str, value: float):
 â€œ"â€Add metric value for variantâ€â€â€
 self.variant_stats[variant][metric_name].append(value)</p>

<p>def compute_metrics(self, experiment_id: str) -&gt; pd.DataFrame:
 â€œâ€â€
 Compute aggregated metrics per variant</p>

<p>Returns DataFrame with columns:</p>
<ul>
  <li>variant</li>
  <li>metric</li>
  <li>count</li>
  <li>mean</li>
  <li>std</li>
  <li>sum
 â€œâ€â€
 results = []</li>
</ul>

<p>for variant, metrics in self.variant_stats.items():
 for metric_name, values in metrics.items():
 import numpy as np</p>

<p>results.append({
 â€˜variantâ€™: variant,
 â€˜metricâ€™: metric_name,
 â€˜countâ€™: len(values),
 â€˜meanâ€™: np.mean(values),
 â€˜stdâ€™: np.std(values),
 â€˜sumâ€™: np.sum(values),
 â€˜minâ€™: np.min(values),
 â€˜maxâ€™: np.max(values)
 })</p>

<p>return pd.DataFrame(results)</p>

<h1 id="usage-2">Usage</h1>
<p>aggregator = MetricsAggregator()</p>

<h1 id="simulate-events">Simulate events</h1>
<p>aggregator.add_event(â€˜controlâ€™, â€˜ctrâ€™, 0.05)
aggregator.add_event(â€˜controlâ€™, â€˜ctrâ€™, 0.04)
aggregator.add_event(â€˜treatmentâ€™, â€˜ctrâ€™, 0.06)
aggregator.add_event(â€˜treatmentâ€™, â€˜ctrâ€™, 0.07)</p>

<p>metrics_df = aggregator.compute_metrics(â€˜model_v2_testâ€™)
print(metrics_df)
``</p>

<hr />

<h2 id="component-3-statistical-analysis">Component 3: Statistical Analysis</h2>

<p>Determine if observed differences are statistically significant.</p>

<h3 id="t-test-for-continuous-metrics">T-Test for Continuous Metrics</h3>

<p>``python
from scipy import stats
import numpy as np</p>

<p>class StatisticalAnalyzer:
 â€œâ€â€
 Perform statistical tests on experiment data
 â€œâ€â€</p>

<p>def t_test(
 self,
 control_values: List[float],
 treatment_values: List[float],
 alpha: float = 0.05
 ) -&gt; dict:
 â€œâ€â€
 Two-sample t-test</p>

<p>H0: mean(treatment) = mean(control)
 H1: mean(treatment) â‰  mean(control)</p>

<p>Args:
 control_values: Metric values from control group
 treatment_values: Metric values from treatment group
 alpha: Significance level (default 0.05)</p>

<p>Returns:
 Dictionary with test results
 â€œâ€â€
 control = np.array(control_values)
 treatment = np.array(treatment_values)</p>

<p># Perform t-test
 t_statistic, p_value = stats.ttest_ind(treatment, control)</p>

<p># Compute effect size (Cohenâ€™s d)
 pooled_std = np.sqrt(
 ((len(control) - 1) * np.var(control, ddof=1) +
 (len(treatment) - 1) * np.var(treatment, ddof=1)) /
 (len(control) + len(treatment) - 2)
 )</p>

<p>cohens_d = (np.mean(treatment) - np.mean(control)) / pooled_std if pooled_std &gt; 0 else 0</p>

<p># Confidence interval for difference
 se = pooled_std * np.sqrt(1/len(control) + 1/len(treatment))
 df = len(control) + len(treatment) - 2
 t_critical = stats.t.ppf(1 - alpha/2, df)</p>

<p>mean_diff = np.mean(treatment) - np.mean(control)
 ci_lower = mean_diff - t_critical * se
 ci_upper = mean_diff + t_critical * se</p>

<p># Relative lift
 relative_lift = (np.mean(treatment) / np.mean(control) - 1) * 100 if np.mean(control) &gt; 0 else 0</p>

<p>return {
 â€˜control_meanâ€™: np.mean(control),
 â€˜treatment_meanâ€™: np.mean(treatment),
 â€˜absolute_diffâ€™: mean_diff,
 â€˜relative_lift_pctâ€™: relative_lift,
 â€˜t_statisticâ€™: t_statistic,
 â€˜p_valueâ€™: p_value,
 â€˜is_significantâ€™: p_value &lt; alpha,
 â€˜confidence_intervalâ€™: (ci_lower, ci_upper),
 â€˜cohens_dâ€™: cohens_d,
 â€˜sample_size_controlâ€™: len(control),
 â€˜sample_size_treatmentâ€™: len(treatment)
 }</p>

<p>def chi_square_test(
 self,
 control_successes: int,
 control_total: int,
 treatment_successes: int,
 treatment_total: int,
 alpha: float = 0.05
 ) -&gt; dict:
 â€œâ€â€
 Chi-square test for proportions (e.g., CTR, conversion rate)
 â€œâ€â€
 # Construct contingency table
 contingency = np.array([
 [treatment_successes, treatment_total - treatment_successes],
 [control_successes, control_total - control_successes]
 ])</p>

<p># Chi-square test
 chi2, p_value, dof, expected = stats.chi2_contingency(contingency)</p>

<p># Rates
 control_rate = control_successes / control_total if control_total &gt; 0 else 0
 treatment_rate = treatment_successes / treatment_total if treatment_total &gt; 0 else 0</p>

<p># Relative lift
 relative_lift = (treatment_rate / control_rate - 1) * 100 if control_rate &gt; 0 else 0</p>

<p># CI for difference in proportions (Wald)
 p1 = treatment_rate
 p2 = control_rate
 se = np.sqrt(
 (p1 * (1 - p1) / max(treatment_total, 1)) +
 (p2 * (1 - p2) / max(control_total, 1))
 )
 z_critical = stats.norm.ppf(1 - alpha / 2)
 diff = p1 - p2
 ci_lower = diff - z_critical * se
 ci_upper = diff + z_critical * se</p>

<p>return {
 â€˜control_rateâ€™: control_rate,
 â€˜treatment_rateâ€™: treatment_rate,
 â€˜absolute_diffâ€™: diff,
 â€˜relative_lift_pctâ€™: relative_lift,
 â€˜chi2_statisticâ€™: chi2,
 â€˜p_valueâ€™: p_value,
 â€˜is_significantâ€™: p_value &lt; alpha,
 â€˜confidence_intervalâ€™: (ci_lower, ci_upper),
 â€˜sample_size_controlâ€™: control_total,
 â€˜sample_size_treatmentâ€™: treatment_total
 }</p>

<h1 id="usage-3">Usage</h1>
<p>analyzer = StatisticalAnalyzer()</p>

<h1 id="simulate-metric-data-eg-session-duration-in-seconds">Simulate metric data (e.g., session duration in seconds)</h1>
<p>control_sessions = np.random.normal(120, 30, size=1000) # mean=120s, std=30s
treatment_sessions = np.random.normal(125, 30, size=1000) # mean=125s (5s improvement)</p>

<p>result = analyzer.t_test(control_sessions, treatment_sessions)</p>

<p>print(fâ€Control mean: {result[â€˜control_meanâ€™]:.2f}â€)
print(fâ€Treatment mean: {result[â€˜treatment_meanâ€™]:.2f}â€)
print(fâ€Relative lift: {result[â€˜relative_lift_pctâ€™]:.2f}%â€)
print(fâ€P-value: {result[â€˜p_valueâ€™]:.4f}â€)
print(fâ€Significant: {result[â€˜is_significantâ€™]}â€)
print(fâ€95% CI: [{result[â€˜confidence_intervalâ€™][0]:.2f}, {result[â€˜confidence_intervalâ€™][1]:.2f}]â€)
``</p>

<h3 id="chi-square-test-for-binary-metrics">Chi-Square Test for Binary Metrics</h3>

<p>``python
def chi_square_test(
 self,
 control_successes: int,
 control_total: int,
 treatment_successes: int,
 treatment_total: int,
 alpha: float = 0.05
) -&gt; dict:
 â€œâ€â€
 Chi-square test for proportions (e.g., CTR, conversion rate)</p>

<p>H0: p(treatment) = p(control)
 H1: p(treatment) â‰  p(control)
 â€œâ€â€
 # Construct contingency table
 contingency = np.array([
 [treatment_successes, treatment_total - treatment_successes],
 [control_successes, control_total - control_successes]
 ])</p>

<p># Chi-square test
 chi2, p_value, dof, expected = stats.chi2_contingency(contingency)</p>

<p># Compute rates
 control_rate = control_successes / control_total if control_total &gt; 0 else 0
 treatment_rate = treatment_successes / treatment_total if treatment_total &gt; 0 else 0</p>

<p># Relative lift
 relative_lift = (treatment_rate / control_rate - 1) * 100 if control_rate &gt; 0 else 0</p>

<p># Confidence interval for difference in proportions
 p1 = treatment_rate
 p2 = control_rate</p>

<p>se = np.sqrt(p1<em>(1-p1)/treatment_total + p2</em>(1-p2)/control_total)
 z_critical = stats.norm.ppf(1 - alpha/2)</p>

<p>diff = p1 - p2
 ci_lower = diff - z_critical * se
 ci_upper = diff + z_critical * se</p>

<p>return {
 â€˜control_rateâ€™: control_rate,
 â€˜treatment_rateâ€™: treatment_rate,
 â€˜absolute_diffâ€™: diff,
 â€˜relative_lift_pctâ€™: relative_lift,
 â€˜chi2_statisticâ€™: chi2,
 â€˜p_valueâ€™: p_value,
 â€˜is_significantâ€™: p_value &lt; alpha,
 â€˜confidence_intervalâ€™: (ci_lower, ci_upper),
 â€˜sample_size_controlâ€™: control_total,
 â€˜sample_size_treatmentâ€™: treatment_total
 }</p>

<h1 id="add-to-statisticalanalyzer-class">Add to StatisticalAnalyzer class</h1>

<h1 id="usage-4">Usage</h1>
<h1 id="example-click-through-rate-test">Example: Click-through rate test</h1>
<p>control_clicks = 450
control_impressions = 10000
treatment_clicks = 520
treatment_impressions = 10000</p>

<p>result = analyzer.chi_square_test(
 control_clicks, control_impressions,
 treatment_clicks, treatment_impressions
)</p>

<p>print(fâ€Control CTR: {result[â€˜control_rateâ€™]<em>100:.2f}%â€)
print(fâ€Treatment CTR: {result[â€˜treatment_rateâ€™]</em>100:.2f}%â€)
print(fâ€Relative lift: {result[â€˜relative_lift_pctâ€™]:.2f}%â€)
print(fâ€P-value: {result[â€˜p_valueâ€™]:.4f}â€)
print(fâ€Significant: {result[â€˜is_significantâ€™]}â€)
``</p>

<hr />

<h2 id="sample-size-calculation--power-analysis">Sample Size Calculation &amp; Power Analysis</h2>

<p>Determine required sample size before running experiment.</p>

<h3 id="power-analysis">Power Analysis</h3>

<p>``python
from scipy.stats import norm</p>

<p>class PowerAnalysis:
 â€œâ€â€
 Calculate required sample size for experiments
 â€œâ€â€</p>

<p>def sample_size_for_proportions(
 self,
 baseline_rate: float,
 mde: float, # Minimum Detectable Effect
 alpha: float = 0.05,
 power: float = 0.80
 ) -&gt; int:
 â€œâ€â€
 Calculate sample size needed to detect effect on proportion</p>

<p>Args:
 baseline_rate: Current conversion rate (e.g., 0.05 for 5%)
 mde: Minimum relative effect to detect (e.g., 0.10 for 10% improvement)
 alpha: Significance level (Type I error rate)
 power: Statistical power (1 - Type II error rate)</p>

<p>Returns:
 Required sample size per variant
 â€œâ€â€
 # Target rate after improvement
 target_rate = baseline_rate * (1 + mde)</p>

<p># Z-scores
 z_alpha = norm.ppf(1 - alpha/2) # Two-tailed
 z_beta = norm.ppf(power)</p>

<p># Pooled proportion under H0
 p_avg = (baseline_rate + target_rate) / 2</p>

<p># Sample size formula
 numerator = (z_alpha * np.sqrt(2 * p_avg * (1 - p_avg)) +
 z_beta * np.sqrt(baseline_rate * (1 - baseline_rate) +
 target_rate * (1 - target_rate))) ** 2</p>

<p>denominator = (target_rate - baseline_rate) ** 2</p>

<p>n = numerator / denominator</p>

<p>return int(np.ceil(n))</p>

<p>def sample_size_for_means(
 self,
 baseline_mean: float,
 baseline_std: float,
 mde: float,
 alpha: float = 0.05,
 power: float = 0.80
 ) -&gt; int:
 â€œâ€â€
 Calculate sample size for continuous metric</p>

<p>Args:
 baseline_mean: Current mean value
 baseline_std: Standard deviation
 mde: Minimum relative effect (e.g., 0.05 for 5% improvement)
 alpha: Significance level
 power: Statistical power</p>

<p>Returns:
 Required sample size per variant
 â€œâ€â€
 target_mean = baseline_mean * (1 + mde)
 effect_size = abs(target_mean - baseline_mean) / baseline_std</p>

<p>z_alpha = norm.ppf(1 - alpha/2)
 z_beta = norm.ppf(power)</p>

<p>n = 2 * ((z_alpha + z_beta) / effect_size) ** 2</p>

<p>return int(np.ceil(n))</p>

<p>def experiment_duration(
 self,
 required_sample_size: int,
 daily_users: int,
 traffic_allocation: float = 0.5
 ) -&gt; int:
 â€œâ€â€
 Calculate experiment duration in days</p>

<p>Args:
 required_sample_size: Sample size per variant
 daily_users: Daily active users
 traffic_allocation: Fraction of users in experiment</p>

<p>Returns:
 Duration in days
 â€œâ€â€
 users_per_day = daily_users * traffic_allocation
 days = required_sample_size / users_per_day</p>

<p>return int(np.ceil(days))</p>

<h1 id="usage-5">Usage</h1>
<p>power = PowerAnalysis()</p>

<h1 id="example-ctr-improvement-test">Example: CTR improvement test</h1>
<p>current_ctr = 0.05 # 5% baseline
mde = 0.10 # Want to detect 10% relative improvement (5% â†’ 5.5%)</p>

<p>sample_size = power.sample_size_for_proportions(
 baseline_rate=current_ctr,
 mde=mde,
 alpha=0.05,
 power=0.80
)</p>

<p>print(fâ€Required sample size per variant: {sample_size:,}â€)</p>

<h1 id="if-we-have-100k-daily-users-and-allocate-50-to-experiment">If we have 100K daily users and allocate 50% to experiment</h1>
<p>duration = power.experiment_duration(
 required_sample_size=sample_size,
 daily_users=100000,
 traffic_allocation=0.5
)</p>

<p>print(fâ€Experiment duration: {duration} daysâ€)
``</p>

<hr />

<h2 id="guardrail-metrics">Guardrail Metrics</h2>

<p>Ensure experiments donâ€™t harm key business metrics.</p>

<h3 id="implementing-guardrails">Implementing Guardrails</h3>

<p>``python
class GuardrailChecker:
 â€œâ€â€
 Monitor guardrail metrics during experiments</p>

<p>Guardrails: Metrics that must not degrade
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.guardrails = {}</p>

<p>def define_guardrail(
 self,
 metric_name: str,
 threshold_type: str, # â€˜relativeâ€™ or â€˜absoluteâ€™
 threshold_value: float,
 direction: str # â€˜decreaseâ€™ or â€˜increaseâ€™
 ):
 â€œâ€â€
 Define a guardrail metric</p>

<p>Example:</p>
<ul>
  <li>Revenue must not decrease by more than 2%</li>
  <li>Error rate must not increase by more than 0.5 percentage points
 â€œâ€â€
 self.guardrails[metric_name] = {
 â€˜threshold_typeâ€™: threshold_type,
 â€˜threshold_valueâ€™: threshold_value,
 â€˜directionâ€™: direction
 }</li>
</ul>

<p>def check_guardrails(
 self,
 control_metrics: dict,
 treatment_metrics: dict
 ) -&gt; dict:
 â€œâ€â€
 Check if treatment violates guardrails</p>

<p>Returns:
 Dictionary of guardrail violations
 â€œâ€â€
 violations = {}</p>

<p>for metric_name, guardrail in self.guardrails.items():
 control_value = control_metrics.get(metric_name)
 treatment_value = treatment_metrics.get(metric_name)</p>

<p>if control_value is None or treatment_value is None:
 continue</p>

<p># Calculate change
 if guardrail[â€˜threshold_typeâ€™] == â€˜relativeâ€™:
 change = (treatment_value / control_value - 1) * 100
 else: # absolute
 change = treatment_value - control_value</p>

<p># Check violation
 violated = False</p>

<p>if guardrail[â€˜directionâ€™] == â€˜decreaseâ€™:
 # Metric should not decrease beyond threshold
 if change &lt; -guardrail[â€˜threshold_valueâ€™]:
 violated = True
 else: # increase
 # Metric should not increase beyond threshold
 if change &gt; guardrail[â€˜threshold_valueâ€™]:
 violated = True</p>

<p>if violated:
 violations[metric_name] = {
 â€˜controlâ€™: control_value,
 â€˜treatmentâ€™: treatment_value,
 â€˜changeâ€™: change,
 â€˜thresholdâ€™: guardrail[â€˜threshold_valueâ€™],
 â€˜typeâ€™: guardrail[â€˜threshold_typeâ€™]
 }</p>

<p>return violations</p>

<h1 id="usage-6">Usage</h1>
<p>guardrails = GuardrailChecker()</p>

<h1 id="define-guardrails">Define guardrails</h1>
<p>guardrails.define_guardrail(
 metric_name=â€™revenue_per_userâ€™,
 threshold_type=â€™relativeâ€™,
 threshold_value=2.0, # Cannot decrease by more than 2%
 direction=â€™decreaseâ€™
)</p>

<p>guardrails.define_guardrail(
 metric_name=â€™error_rateâ€™,
 threshold_type=â€™absoluteâ€™,
 threshold_value=0.5, # Cannot increase by more than 0.5 percentage points
 direction=â€™increaseâ€™
)</p>

<h1 id="check-guardrails">Check guardrails</h1>
<p>control_metrics = {
 â€˜revenue_per_userâ€™: 10.0,
 â€˜error_rateâ€™: 1.0
}</p>

<p>treatment_metrics = {
 â€˜revenue_per_userâ€™: 9.5, # 5% decrease - violates guardrail!
 â€˜error_rateâ€™: 1.2 # 0.2pp increase - OK
}</p>

<p>violations = guardrails.check_guardrails(control_metrics, treatment_metrics)</p>

<p>if violations:
 print(â€œâš ï¸ Guardrail violations detected:â€)
 for metric, details in violations.items():
 print(fâ€ {metric}: {details[â€˜changeâ€™]:.2f}% change (threshold: {details[â€˜thresholdâ€™]}%)â€)
else:
 print(â€œâœ… All guardrails passedâ€)
``</p>

<hr />

<h2 id="real-world-examples">Real-World Examples</h2>

<h3 id="netflix-experimentation-at-scale">Netflix: Experimentation at Scale</h3>

<p><strong>Scale:</strong></p>
<ul>
  <li>1000+ experiments running concurrently</li>
  <li>200M+ users worldwide</li>
  <li>Multiple metrics per experiment</li>
</ul>

<p><strong>Key innovations:</strong></p>
<ul>
  <li><strong>Quasi-experimentation:</strong> Use observational data when randomization not possible</li>
  <li><strong>Interleaving:</strong> Test ranking algorithms by mixing results</li>
  <li><strong>Heterogeneous treatment effects:</strong> Analyze impact per user segment</li>
</ul>

<p><strong>Example metric:</strong></p>
<ul>
  <li><strong>Stream starts per member:</strong> How many shows/movies a user starts watching</li>
  <li><strong>Effective catalog size:</strong> Number of unique titles watched (diversity metric)</li>
</ul>

<h3 id="google-large-scale-testing">Google: Large-scale Testing</h3>

<p><strong>Scale:</strong></p>
<ul>
  <li>10,000+ experiments per year</li>
  <li>1B+ users</li>
  <li>Experiments across Search, Ads, YouTube, etc.</li>
</ul>

<p><strong>Methodology:</strong></p>
<ul>
  <li><strong>Layered experiments:</strong> Run multiple experiments on same users (orthogonal layers)</li>
  <li><strong>Ramping:</strong> Gradually increase traffic allocation</li>
  <li><strong>Long-running holdouts:</strong> Keep small % in old version to measure long-term effects</li>
</ul>

<p><strong>Example:</strong>
Testing new ranking algorithm in Google Search:</p>
<ul>
  <li><strong>Primary metric:</strong> Click-through rate on top results</li>
  <li><strong>Guardrails:</strong> Ad revenue, latency, user satisfaction</li>
  <li><strong>Duration:</strong> 2-4 weeks</li>
  <li><strong>Traffic:</strong> Start at 1%, ramp to 50%</li>
</ul>

<hr />

<h2 id="advanced-topics">Advanced Topics</h2>

<h3 id="sequential-testing--early-stopping">Sequential Testing &amp; Early Stopping</h3>

<p>Stop experiments early when results are conclusive.</p>

<p>``python
import math
from scipy.stats import norm</p>

<p>class SequentialTesting:
 â€œâ€â€
 Sequential probability ratio test (SPRT)</p>

<p>Allows stopping experiment early while controlling error rates
 â€œâ€â€</p>

<p>def <strong>init</strong>(
 self,
 alpha=0.05,
 beta=0.20,
 mde=0.05 # Minimum detectable effect
 ):
 self.alpha = alpha # Type I error rate
 self.beta = beta # Type II error rate (1 - power)
 self.mde = mde</p>

<p># Calculate log-likelihood ratio bounds
 self.upper_bound = math.log((1 - beta) / alpha)
 self.lower_bound = math.log(beta / (1 - alpha))</p>

<p>def should_stop(
 self,
 control_successes: int,
 control_total: int,
 treatment_successes: int,
 treatment_total: int
 ) -&gt; dict:
 â€œâ€â€
 Check if experiment can be stopped</p>

<p>Returns:
 {
 â€˜decisionâ€™: â€˜continueâ€™ | â€˜stop_treatment_winsâ€™ | â€˜stop_control_winsâ€™,
 â€˜log_likelihood_ratioâ€™: float
 }
 â€œâ€â€
 # Compute rates
 p_control = control_successes / control_total if control_total &gt; 0 else 0
 p_treatment = treatment_successes / treatment_total if treatment_total &gt; 0 else 0</p>

<p># Avoid edge cases
 p_control = max(min(p_control, 0.9999), 0.0001)
 p_treatment = max(min(p_treatment, 0.9999), 0.0001)</p>

<p># Log-likelihood ratio
 # H1: treatment is better by mde
 # H0: treatment = control</p>

<p>p_h1 = p_control * (1 + self.mde)</p>

<p>llr = 0</p>

<p># Contribution from treatment group
 llr += treatment_successes * math.log(p_h1 / p_control)
 llr += (treatment_total - treatment_successes) * math.log((1 - p_h1) / (1 - p_control))</p>

<p># Decision
 if llr &gt;= self.upper_bound:
 return {â€˜decisionâ€™: â€˜stop_treatment_winsâ€™, â€˜log_likelihood_ratioâ€™: llr}
 elif llr &lt;= self.lower_bound:
 return {â€˜decisionâ€™: â€˜stop_control_winsâ€™, â€˜log_likelihood_ratioâ€™: llr}
 else:
 return {â€˜decisionâ€™: â€˜continueâ€™, â€˜log_likelihood_ratioâ€™: llr}</p>

<h1 id="usage-7">Usage</h1>
<p>sequential = SequentialTesting(alpha=0.05, beta=0.20, mde=0.05)</p>

<h1 id="check-daily">Check daily</h1>
<p>for day in range(1, 15):
 control_clicks = day * 450
 control_impressions = day * 10000
 treatment_clicks = day * 500
 treatment_impressions = day * 10000</p>

<p>result = sequential.should_stop(
 control_clicks, control_impressions,
 treatment_clicks, treatment_impressions
 )</p>

<p>print(fâ€ {day}: {result[â€˜decisionâ€™]}â€)</p>

<p>if result[â€˜decisionâ€™] != â€˜continueâ€™:
 print(fâ€ğŸ‰ Experiment can stop on day {day}!â€)
 break
``</p>

<h3 id="multi-armed-bandits">Multi-Armed Bandits</h3>

<p>Allocate traffic dynamically to better-performing variants.</p>

<p>``python
import numpy as np</p>

<p>class ThompsonSampling:
 â€œâ€â€
 Thompson Sampling for multi-armed bandit</p>

<p>Dynamically allocate traffic to maximize reward
 while exploring alternatives
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, num_variants):
 self.num_variants = num_variants</p>

<p># Beta distribution parameters for each variant
 # Beta(alpha, beta) represents posterior belief
 self.alpha = np.ones(num_variants) # Success count + 1
 self.beta = np.ones(num_variants) # Failure count + 1</p>

<p>def select_variant(self) -&gt; int:
 â€œâ€â€
 Select variant to show to next user</p>

<p>Sample from each variantâ€™s posterior and pick the best
 â€œâ€â€
 # Sample from each variantâ€™s posterior distribution
 sampled_values = [
 np.random.beta(self.alpha[i], self.beta[i])
 for i in range(self.num_variants)
 ]</p>

<p># Select variant with highest sample
 return np.argmax(sampled_values)</p>

<p>def update(self, variant: int, reward: float):
 â€œâ€â€
 Update beliefs after observing reward</p>

<p>Args:
 variant: Which variant was shown
 reward: 0 or 1 (failure or success)
 â€œâ€â€
 if reward &gt; 0:
 self.alpha[variant] += 1
 else:
 self.beta[variant] += 1</p>

<p>def get_statistics(self):
 â€œ"â€Get current statistics for each variantâ€â€â€
 stats = []</p>

<p>for i in range(self.num_variants):
 # Mean of Beta(alpha, beta) = alpha / (alpha + beta)
 mean = self.alpha[i] / (self.alpha[i] + self.beta[i])</p>

<p># 95% credible interval
 samples = np.random.beta(self.alpha[i], self.beta[i], size=10000)
 ci_lower, ci_upper = np.percentile(samples, [2.5, 97.5])</p>

<p>stats.append({
 â€˜variantâ€™: i,
 â€˜estimated_meanâ€™: mean,
 â€˜total_samplesâ€™: self.alpha[i] + self.beta[i] - 2,
 â€˜successesâ€™: self.alpha[i] - 1,
 â€˜credible_intervalâ€™: (ci_lower, ci_upper)
 })</p>

<p>return stats</p>

<h1 id="usage-8">Usage</h1>
<p>bandit = ThompsonSampling(num_variants=3)</p>

<h1 id="simulate-10000-users">Simulate 10,000 users</h1>
<p>for user in range(10000):
 # Select variant to show
 variant = bandit.select_variant()</p>

<p># Simulate user interaction (variant 2 is best: 6% CTR)
 true_ctrs = [0.04, 0.05, 0.06]
 clicked = np.random.random() &lt; true_ctrs[variant]</p>

<p># Update beliefs
 bandit.update(variant, 1.0 if clicked else 0.0)</p>

<h1 id="check-statistics">Check statistics</h1>
<p>stats = bandit.get_statistics()
for s in stats:
 print(fâ€Variant {s[â€˜variantâ€™]}: â€œ
 fâ€Estimated CTR = {s[â€˜estimated_meanâ€™]:.3f}, â€œ
 fâ€Samples = {s[â€˜total_samplesâ€™]}, â€œ
 fâ€95% CI = [{s[â€˜credible_intervalâ€™][0]:.3f}, {s[â€˜credible_intervalâ€™][1]:.3f}]â€)
``</p>

<h3 id="variance-reduction-cuped">Variance Reduction: CUPED</h3>

<p>Reduce variance by using pre-experiment covariates.</p>

<p>``python
class CUPED:
 â€œâ€â€
 Controlled-experiment Using Pre-Experiment Data</p>

<p>Reduces variance by adjusting for pre-experiment metrics
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 pass</p>

<p>def adjust_metric(
 self,
 y: np.ndarray, # Post-experiment metric
 x: np.ndarray, # Pre-experiment metric (covariate)
 ) -&gt; np.ndarray:
 â€œâ€â€
 Adjust post-experiment metric using pre-experiment data</p>

<p>Adjusted metric: y_adj = y - theta * (x - E[x])</p>

<p>Where theta is chosen to minimize variance of y_adj
 â€œâ€â€
 # Compute optimal theta
 # theta = Cov(y, x) / Var(x)
 mean_x = np.mean(x)
 mean_y = np.mean(y)</p>

<p>cov_yx = np.mean((y - mean_y) * (x - mean_x))
 var_x = np.var(x, ddof=1)</p>

<p>if var_x == 0:
 return y # No adjustment possible</p>

<p>theta = cov_yx / var_x</p>

<p># Adjust y
 y_adjusted = y - theta * (x - mean_x)</p>

<p>return y_adjusted</p>

<p>def compare_variants_with_cuped(
 self,
 control_post: np.ndarray,
 control_pre: np.ndarray,
 treatment_post: np.ndarray,
 treatment_pre: np.ndarray
 ) -&gt; dict:
 â€œâ€â€
 Compare variants using CUPED</p>

<p>Returns improvement in statistical power
 â€œâ€â€
 # Original comparison (without CUPED)
 from scipy import stats</p>

<p>original_t, original_p = stats.ttest_ind(treatment_post, control_post)
 original_var = np.var(treatment_post) + np.var(control_post)</p>

<p># Adjust metrics
 all_pre = np.concatenate([control_pre, treatment_pre])
 all_post = np.concatenate([control_post, treatment_post])</p>

<p>adjusted_post = self.adjust_metric(all_post, all_pre)</p>

<p># Split back
 n_control = len(control_post)
 control_post_adj = adjusted_post[:n_control]
 treatment_post_adj = adjusted_post[n_control:]</p>

<p># Adjusted comparison
 adjusted_t, adjusted_p = stats.ttest_ind(treatment_post_adj, control_post_adj)
 adjusted_var = np.var(treatment_post_adj) + np.var(control_post_adj)</p>

<p># Variance reduction
 variance_reduction = (original_var - adjusted_var) / original_var * 100</p>

<p>return {
 â€˜original_p_valueâ€™: original_p,
 â€˜adjusted_p_valueâ€™: adjusted_p,
 â€˜variance_reduction_pctâ€™: variance_reduction,
 â€˜power_improvementâ€™: (original_var / adjusted_var) ** 0.5
 }</p>

<h1 id="example-using-pre-experiment-purchase-history-to-reduce-variance">Example: Using pre-experiment purchase history to reduce variance</h1>
<p>control_pre = np.random.normal(100, 30, size=500) # Past purchases
control_post = control_pre + np.random.normal(5, 20, size=500) # Correlated</p>

<p>treatment_pre = np.random.normal(100, 30, size=500)
treatment_post = treatment_pre + np.random.normal(8, 20, size=500) # Slightly better</p>

<p>cuped = CUPED()
result = cuped.compare_variants_with_cuped(
 control_post, control_pre,
 treatment_post, treatment_pre
)</p>

<p>print(fâ€Original p-value: {result[â€˜original_p_valueâ€™]:.4f}â€)
print(fâ€Adjusted p-value: {result[â€˜adjusted_p_valueâ€™]:.4f}â€)
print(fâ€Variance reduction: {result[â€˜variance_reduction_pctâ€™]:.1f}%â€)
print(fâ€Power improvement: {result[â€˜power_improvementâ€™]:.2f}xâ€)
``</p>

<h3 id="stratified-sampling">Stratified Sampling</h3>

<p>Ensure balance across important user segments.</p>

<p>``python
class StratifiedAssignment:
 â€œâ€â€
 Assign users to experiments with stratification</p>

<p>Ensures balanced assignment within strata (e.g., country, platform)
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, num_variants=2):
 self.num_variants = num_variants
 self.strata_counters = {} # stratum â†’ variant counts</p>

<p>def assign_variant(self, user_id: str, stratum: str) -&gt; int:
 â€œâ€â€
 Assign user to variant, ensuring balance within stratum</p>

<p>Args:
 user_id: User identifier
 stratum: Stratum key (e.g., â€œUS_iOSâ€, â€œUK_Androidâ€)</p>

<p>Returns:
 Variant index
 â€œâ€â€
 # Initialize stratum if new
 if stratum not in self.strata_counters:
 self.strata_counters[stratum] = [0] * self.num_variants</p>

<p># Hash-based assignment (deterministic)
 import hashlib
 hash_input = fâ€{user_id}:{stratum}â€.encode(â€˜utf-8â€™)
 hash_value = int(hashlib.md5(hash_input).hexdigest(), 16)
 variant = hash_value % self.num_variants</p>

<p># Update counter
 self.strata_counters[stratum][variant] += 1</p>

<p>return variant</p>

<p>def get_balance_report(self) -&gt; dict:
 â€œ"â€Check balance within each stratumâ€â€â€
 report = {}</p>

<p>for stratum, counts in self.strata_counters.items():
 total = sum(counts)
 proportions = [c / total for c in counts]</p>

<p># Check if balanced (each variant should have ~1/num_variants)
 expected = 1 / self.num_variants
 max_deviation = max(abs(p - expected) for p in proportions)</p>

<p>report[stratum] = {
 â€˜countsâ€™: counts,
 â€˜proportionsâ€™: proportions,
 â€˜max_deviationâ€™: max_deviation,
 â€˜balancedâ€™: max_deviation &lt; 0.05 # Within 5% of expected
 }</p>

<p>return report</p>

<h1 id="usage-9">Usage</h1>
<p>stratified = StratifiedAssignment(num_variants=2)</p>

<h1 id="simulate-user-assignments">Simulate user assignments</h1>
<p>for i in range(10000):
 user_id = fâ€user_{i}â€</p>

<p># Assign stratum based on user
 if i % 3 == 0:
 stratum = â€œUS_iOSâ€
 elif i % 3 == 1:
 stratum = â€œUS_Androidâ€
 else:
 stratum = â€œUK_iOSâ€</p>

<p>variant = stratified.assign_variant(user_id, stratum)</p>

<h1 id="check-balance">Check balance</h1>
<p>balance = stratified.get_balance_report()
for stratum, stats in balance.items():
 print(fâ€{stratum}: {stats[â€˜countsâ€™]}, balanced={stats[â€˜balancedâ€™]}â€)
``</p>

<hr />

<h2 id="multiple-testing-correction">Multiple Testing Correction</h2>

<p>When running many experiments, control family-wise error rate.</p>

<h3 id="bonferroni-correction">Bonferroni Correction</h3>

<p>``python
def bonferroni_correction(p_values: List[float], alpha: float = 0.05) -&gt; List[bool]:
 â€œâ€â€
 Bonferroni correction for multiple comparisons</p>

<p>Adjusted alpha = alpha / num_tests</p>

<p>Args:
 p_values: List of p-values from multiple tests
 alpha: Family-wise error rate</p>

<p>Returns:
 List of booleans (True = significant after correction)
 â€œâ€â€
 num_tests = len(p_values)
 adjusted_alpha = alpha / num_tests</p>

<p>return [p &lt; adjusted_alpha for p in p_values]</p>

<h1 id="example-testing-10-variants">Example: Testing 10 variants</h1>
<p>p_values = [0.04, 0.06, 0.03, 0.08, 0.02, 0.09, 0.07, 0.05, 0.01, 0.10]</p>

<p>significant_uncorrected = [p &lt; 0.05 for p in p_values]
significant_corrected = bonferroni_correction(p_values, alpha=0.05)</p>

<p>print(fâ€Significant (uncorrected): {sum(significant_uncorrected)} / {len(p_values)}â€)
print(fâ€Significant (Bonferroni): {sum(significant_corrected)} / {len(p_values)}â€)
``</p>

<h3 id="false-discovery-rate-fdr---benjamini-hochberg">False Discovery Rate (FDR) - Benjamini-Hochberg</h3>

<p>``python
def benjamini_hochberg(p_values: List[float], alpha: float = 0.05) -&gt; List[bool]:
 â€œâ€â€
 Benjamini-Hochberg procedure for FDR control</p>

<p>Less conservative than Bonferroni</p>

<p>Args:
 p_values: List of p-values
 alpha: Desired FDR level</p>

<p>Returns:
 List of booleans (True = significant)
 â€œâ€â€
 num_tests = len(p_values)</p>

<p># Sort p-values with original indices
 indexed_p_values = [(p, i) for i, p in enumerate(p_values)]
 indexed_p_values.sort()</p>

<p># Find largest k such that p[k] &lt;= (k+1)/m * alpha
 significant_indices = set()</p>

<p>for k in range(num_tests - 1, -1, -1):
 p_value, original_idx = indexed_p_values[k]
 threshold = (k + 1) / num_tests * alpha</p>

<p>if p_value &lt;= threshold:
 # Mark this and all smaller p-values as significant
 for j in range(k + 1):
 significant_indices.add(indexed_p_values[j][1])
 break</p>

<p># Create result list
 return [i in significant_indices for i in range(num_tests)]</p>

<h1 id="compare-to-bonferroni">Compare to Bonferroni</h1>
<p>fdr_significant = benjamini_hochberg(p_values, alpha=0.05)</p>

<p>print(fâ€Significant (FDR): {sum(fdr_significant)} / {len(p_values)}â€)
``</p>

<hr />

<h2 id="layered-experiments">Layered Experiments</h2>

<p>Run multiple experiments simultaneously on orthogonal layers.</p>

<p>``python
class ExperimentLayer:
 â€œâ€â€
 Single experiment layer
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, layer_id: str, experiments: List[str]):
 self.layer_id = layer_id
 self.experiments = experiments
 self.num_experiments = len(experiments)</p>

<p>def assign_experiment(self, user_id: str) -&gt; str:
 â€œ"â€Assign user to one experiment in this layerâ€â€â€
 import hashlib</p>

<p>hash_input = fâ€{user_id}:{self.layer_id}â€.encode(â€˜utf-8â€™)
 hash_value = int(hashlib.md5(hash_input).hexdigest(), 16)</p>

<p>experiment_idx = hash_value % self.num_experiments
 return self.experiments[experiment_idx]</p>

<p>class LayeredExperimentPlatform:
 â€œâ€â€
 Platform supporting layered experiments</p>

<p>Layers should be independent (orthogonal)
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.layers = {}</p>

<p>def add_layer(self, layer_id: str, experiments: List[str]):
 â€œ"â€Add experiment layerâ€â€â€
 self.layers[layer_id] = ExperimentLayer(layer_id, experiments)</p>

<p>def assign_user(self, user_id: str) -&gt; dict:
 â€œâ€â€
 Assign user to experiments across all layers</p>

<p>Returns:
 Dict mapping layer_id â†’ experiment_id
 â€œâ€â€
 assignments = {}</p>

<p>for layer_id, layer in self.layers.items():
 experiment = layer.assign_experiment(user_id)
 assignments[layer_id] = experiment</p>

<p>return assignments</p>

<h1 id="usage-10">Usage</h1>
<p>platform = LayeredExperimentPlatform()</p>

<h1 id="layer-1-ranking-algorithm-tests">Layer 1: Ranking algorithm tests</h1>
<p>platform.add_layer(
 â€˜rankingâ€™,
 [â€˜ranking_baselineâ€™, â€˜ranking_ml_v1â€™, â€˜ranking_ml_v2â€™]
)</p>

<h1 id="layer-2-ui-tests-independent-of-ranking">Layer 2: UI tests (independent of ranking)</h1>
<p>platform.add_layer(
 â€˜uiâ€™,
 [â€˜ui_oldâ€™, â€˜ui_new_blueâ€™, â€˜ui_new_greenâ€™]
)</p>

<h1 id="layer-3-recommendation-tests">Layer 3: Recommendation tests</h1>
<p>platform.add_layer(
 â€˜recommendationsâ€™,
 [â€˜recs_baselineâ€™, â€˜recs_personalizedâ€™]
)</p>

<h1 id="assign-user-to-experiments">Assign user to experiments</h1>
<p>user_experiments = platform.assign_user(â€˜user_12345â€™)
print(fâ€User assigned to:â€)
for layer, experiment in user_experiments.items():
 print(fâ€ {layer}: {experiment}â€)</p>

<h1 id="user-gets-combination-like">User gets combination like:</h1>
<h1 id="ranking-ranking_ml_v2">ranking: ranking_ml_v2</h1>
<h1 id="ui-ui_new_blue">ui: ui_new_blue</h1>
<h1 id="recommendations-recs_personalized">recommendations: recs_personalized</h1>
<p>``</p>

<hr />

<h2 id="airbnbs-experiment-framework">Airbnbâ€™s Experiment Framework</h2>

<p>Real-world example of production experimentation.</p>

<p><strong>Key Components:</strong></p>

<ol>
  <li><strong>ERF (Experiment Reporting Framework)</strong>
    <ul>
      <li>Centralized metric definitions</li>
      <li>Automated metric computation</li>
      <li>Standardized reporting</li>
    </ul>
  </li>
  <li><strong>CUPED for Variance Reduction</strong>
    <ul>
      <li>Uses pre-experiment booking history</li>
      <li>50%+ variance reduction on key metrics</li>
      <li>Dramatically reduces required sample size</li>
    </ul>
  </li>
  <li><strong>Quasi-experiments</strong>
    <ul>
      <li>When randomization not possible (e.g., pricing tests)</li>
      <li>Difference-in-differences analysis</li>
      <li>Synthetic control methods</li>
    </ul>
  </li>
  <li><strong>Interference Handling</strong>
    <ul>
      <li>Network effects (one userâ€™s treatment affects others)</li>
      <li>Cluster randomization (randomize at city/market level)</li>
      <li>Ego-cluster randomization</li>
    </ul>
  </li>
</ol>

<p><strong>Metrics Hierarchy:</strong></p>

<p>``
Primary Metrics (move-the-needle)
â”œâ”€â”€ Bookings
â”œâ”€â”€ Revenue
â””â”€â”€ Guest Satisfaction</p>

<p>Secondary Metrics (understand mechanism)
â”œâ”€â”€ Search engagement
â”œâ”€â”€ Listing views
â””â”€â”€ Message rate</p>

<p>Guardrail Metrics (protect)
â”œâ”€â”€ Host satisfaction
â”œâ”€â”€ Cancellation rate
â””â”€â”€ Customer support tickets
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>âœ… <strong>Randomization</strong> via consistent hashing ensures unbiased assignment 
âœ… <strong>Statistical rigor</strong> prevents false positives, require p &lt; 0.05 and sufficient power 
âœ… <strong>Sample size</strong> calculation upfront prevents underpowered experiments 
âœ… <strong>Guardrail metrics</strong> protect against shipping harmful changes 
âœ… <strong>Real-time monitoring</strong> enables early stopping for clear wins/losses 
âœ… <strong>Sequential testing</strong> allows stopping early while controlling error rates 
âœ… <strong>Multi-armed bandits</strong> dynamically optimize traffic allocation 
âœ… <strong>CUPED</strong> reduces variance using pre-experiment data â†’ smaller samples needed 
âœ… <strong>Stratified sampling</strong> ensures balance across key user segments 
âœ… <strong>Multiple testing corrections</strong> control error rates when running many experiments 
âœ… <strong>Layered experiments</strong> increase experimentation velocity without conflicts 
âœ… <strong>Long-term holdouts</strong> measure sustained impact vs novelty effects</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0004-ab-testing-systems/">arunbaby.com/ml-system-design/0004-ab-testing-systems</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#ab-testing" class="page__taxonomy-item p-category" rel="tag">ab-testing</a><span class="sep">, </span>
    
      <a href="/tags/#experimentation" class="page__taxonomy-item p-category" rel="tag">experimentation</a><span class="sep">, </span>
    
      <a href="/tags/#metrics" class="page__taxonomy-item p-category" rel="tag">metrics</a><span class="sep">, </span>
    
      <a href="/tags/#statistical-testing" class="page__taxonomy-item p-category" rel="tag">statistical-testing</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0004-best-time-buy-sell-stock/" rel="permalink">Best Time to Buy and Sell Stock
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The single-pass pattern that powers streaming analytics, online algorithms, and real-time decision making in production systems.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0004-voice-activity-detection/" rel="permalink">Voice Activity Detection (VAD)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How voice assistants and video conferencing apps detect when youâ€™re speaking vs silence, the critical first step in every speech pipeline.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0004-tool-calling-fundamentals/" rel="permalink">Tool Calling Fundamentals
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">â€œGiving the Brain Hands to Act: The Interface Between Intelligence and Infrastructure.â€
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=A%2FB+Testing+Systems+for+ML%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0004-ab-testing-systems%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0004-ab-testing-systems%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0004-ab-testing-systems/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0003-data-preprocessing/" class="pagination--pager" title="Data Preprocessing Pipeline Design">Previous</a>
    
    
      <a href="/ml-system-design/0005-batch-realtime-inference/" class="pagination--pager" title="Batch vs Real-Time Inference">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
