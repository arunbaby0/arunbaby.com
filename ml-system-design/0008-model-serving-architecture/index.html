<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Model Serving Architecture - Arun Baby</title>
<meta name="description" content="Design production-grade model serving systems that deliver predictions at scale with low latency and high reliability.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Model Serving Architecture">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0008-model-serving-architecture/">


  <meta property="og:description" content="Design production-grade model serving systems that deliver predictions at scale with low latency and high reliability.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Model Serving Architecture">
  <meta name="twitter:description" content="Design production-grade model serving systems that deliver predictions at scale with low latency and high reliability.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0008-model-serving-architecture/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0008-model-serving-architecture/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Model Serving Architecture">
    <meta itemprop="description" content="Design production-grade model serving systems that deliver predictions at scale with low latency and high reliability.">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0008-model-serving-architecture/" itemprop="url">Model Serving Architecture
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#model-serving-architecture-overview">Model Serving Architecture Overview</a></li><li><a href="#serving-patterns">Serving Patterns</a><ul><li><a href="#pattern-1-rest-api-serving">Pattern 1: REST API Serving</a></li><li><a href="#pattern-2-grpc-serving">Pattern 2: gRPC Serving</a></li><li><a href="#pattern-3-batch-serving">Pattern 3: Batch Serving</a></li></ul></li><li><a href="#model-loading-strategies">Model Loading Strategies</a><ul><li><a href="#strategy-1-eager-loading">Strategy 1: Eager Loading</a></li><li><a href="#strategy-2-lazy-loading">Strategy 2: Lazy Loading</a></li><li><a href="#strategy-3-model-caching-with-expiration">Strategy 3: Model Caching with Expiration</a></li></ul></li><li><a href="#model-versioning--ab-testing">Model Versioning &amp; A/B Testing</a><ul><li><a href="#multi-model-serving">Multi-Model Serving</a></li></ul></li><li><a href="#optimization-techniques">Optimization Techniques</a><ul><li><a href="#1-model-quantization">1. Model Quantization</a></li><li><a href="#2-batch-inference">2. Batch Inference</a></li></ul></li><li><a href="#monitoring--observability">Monitoring &amp; Observability</a><ul><li><a href="#prediction-logging">Prediction Logging</a></li></ul></li><li><a href="#connection-to-bst-validation-dsa">Connection to BST Validation (DSA)</a></li><li><a href="#advanced-serving-patterns">Advanced Serving Patterns</a><ul><li><a href="#1-shadow-mode-deployment">1. Shadow Mode Deployment</a></li><li><a href="#2-canary-deployment">2. Canary Deployment</a></li><li><a href="#3-multi-armed-bandit-serving">3. Multi-Armed Bandit Serving</a></li></ul></li><li><a href="#infrastructure--deployment">Infrastructure &amp; Deployment</a><ul><li><a href="#containerized-serving-with-docker">Containerized Serving with Docker</a></li><li><a href="#kubernetes-deployment">Kubernetes Deployment</a></li></ul></li><li><a href="#feature-store-integration">Feature Store Integration</a></li><li><a href="#cost-optimization">Cost Optimization</a><ul><li><a href="#1-request-batching-for-cost-reduction">1. Request Batching for Cost Reduction</a></li><li><a href="#2-model-compression-for-cheaper-hosting">2. Model Compression for Cheaper Hosting</a></li></ul></li><li><a href="#troubleshooting--debugging">Troubleshooting &amp; Debugging</a><ul><li><a href="#prediction-debugging">Prediction Debugging</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Design production-grade model serving systems that deliver predictions at scale with low latency and high reliability.</strong></p>

<h2 id="introduction">Introduction</h2>

<p><strong>Model serving</strong> is the process of deploying ML models to production and making predictions available to end users or downstream systems.</p>

<p><strong>Why itâ€™s critical:</strong></p>
<ul>
  <li><strong>Bridge training and production:</strong> Trained models are useless without serving</li>
  <li><strong>Performance matters:</strong> Latency directly impacts user experience</li>
  <li><strong>Scale requirements:</strong> Handle millions of requests per second</li>
  <li><strong>Reliability:</strong> Downtime = lost revenue</li>
</ul>

<p><strong>Key challenges:</strong></p>
<ul>
  <li>Low latency (&lt; 100ms for many applications)</li>
  <li>High throughput (handle traffic spikes)</li>
  <li>Model versioning and rollback</li>
  <li>A/B testing and gradual rollouts</li>
  <li>Monitoring and debugging</li>
</ul>

<hr />

<h2 id="model-serving-architecture-overview">Model Serving Architecture Overview</h2>

<p><code class="language-plaintext highlighter-rouge">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client Applications â”‚
â”‚ (Web, Mobile, Backend Services) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚ HTTP/gRPC requests
 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Balancer â”‚
â”‚ (nginx, ALB, GCP Load Balancer) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â–¼ â–¼ â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Serving â”‚ â”‚ Serving â”‚ â”‚ Serving â”‚
 â”‚ Instanceâ”‚ â”‚ Instanceâ”‚ â”‚ Instanceâ”‚
 â”‚ 1 â”‚ â”‚ 2 â”‚ â”‚ N â”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
 â”‚ â”‚ â”‚
 â–¼ â–¼ â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Model Repository â”‚
 â”‚ (S3, GCS, Model Registry) â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></p>

<hr />

<h2 id="serving-patterns">Serving Patterns</h2>

<h3 id="pattern-1-rest-api-serving">Pattern 1: REST API Serving</h3>

<p><strong>Best for:</strong> Web applications, microservices</p>

<p>``python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import numpy as np
import joblib
from typing import List
import time</p>

<p>app = FastAPI()</p>

<h1 id="load-model-on-startup">Load model on startup</h1>
<p>model = None</p>

<p>@app.on_event(â€œstartupâ€)
async def load_model():
 â€œ"â€Load model when server startsâ€â€â€
 global model
 model = joblib.load(â€˜model.pklâ€™)
 print(â€œModel loaded successfullyâ€)</p>

<p>class PredictionRequest(BaseModel):
 â€œ"â€Request schemaâ€â€â€
 features: List[float]</p>

<p>class PredictionResponse(BaseModel):
 â€œ"â€Response schemaâ€â€â€
 prediction: float
 confidence: float
 model_version: str</p>

<p>@app.post(â€œ/predictâ€, response_model=PredictionResponse)
async def predict(request: PredictionRequest):
 â€œâ€â€
 Make prediction</p>

<p>Returns: Prediction with confidence
 â€œâ€â€
 try:
 # Convert to numpy array
 features = np.array([request.features])</p>

<p># Make prediction
 prediction = model.predict(features)[0]</p>

<p># Get confidence (if available)
 if hasattr(model, â€˜predict_probaâ€™):
 proba = model.predict_proba(features)[0]
 confidence = float(np.max(proba))
 else:
 confidence = 1.0</p>

<p>return PredictionResponse(
 prediction=float(prediction),
 confidence=confidence,
 model_version=â€v1.0â€
 )</p>

<p>except Exception as e:
 raise HTTPException(status_code=500, detail=str(e))</p>

<p>@app.get(â€œ/healthâ€)
async def health_check():
 â€œ"â€Health check endpointâ€â€â€
 if model is None:
 raise HTTPException(status_code=503, detail=â€Model not loadedâ€)
 return {â€œstatusâ€: â€œhealthyâ€, â€œmodel_loadedâ€: True}</p>

<p>@app.get(â€œ/readyâ€)
async def readiness_check():
 â€œ"â€Readiness probe endpointâ€â€â€
 # Optionally include lightweight self-test
 return {â€œreadyâ€: model is not None}</p>

<h1 id="run-with-uvicorn-appapp-host-0000-port-8000">Run with: uvicorn app:app â€“host 0.0.0.0 â€“port 8000</h1>
<p>``</p>

<p><strong>Usage:</strong>
<code class="language-plaintext highlighter-rouge">bash
curl -X POST "http://localhost:8000/predict" \
 -H "Content-Type: application/json" \
 -d '{"features": [1.0, 2.0, 3.0, 4.0]}'
</code></p>

<h3 id="pattern-2-grpc-serving">Pattern 2: gRPC Serving</h3>

<p><strong>Best for:</strong> High-performance, low-latency applications</p>

<p>``python</p>
<h1 id="predictionproto">prediction.proto</h1>
<p>â€â€â€
syntax = â€œproto3â€;</p>

<p>service PredictionService {
 rpc Predict (PredictRequest) returns (PredictResponse);
}</p>

<p>message PredictRequest {
 repeated float features = 1;
}</p>

<p>message PredictResponse {
 float prediction = 1;
 float confidence = 2;
}
â€œâ€â€</p>

<h1 id="serverpy">server.py</h1>
<p>import grpc
from concurrent import futures
import prediction_pb2
import prediction_pb2_grpc
import numpy as np
import joblib</p>

<p>class PredictionServicer(prediction_pb2_grpc.PredictionServiceServicer):
 â€œ"â€gRPC Prediction Serviceâ€â€â€</p>

<p>def <strong>init</strong>(self):
 self.model = joblib.load(â€˜model.pklâ€™)</p>

<p>def Predict(self, request, context):
 â€œ"â€Handle prediction requestâ€â€â€
 try:
 # Convert features
 features = np.array([list(request.features)])</p>

<p># Predict
 prediction = self.model.predict(features)[0]</p>

<p># Get confidence
 if hasattr(self.model, â€˜predict_probaâ€™):
 proba = self.model.predict_proba(features)[0]
 confidence = float(np.max(proba))
 else:
 confidence = 1.0</p>

<p>return prediction_pb2.PredictResponse(
 prediction=float(prediction),
 confidence=confidence
 )</p>

<p>except Exception as e:
 context.set_code(grpc.StatusCode.INTERNAL)
 context.set_details(str(e))
 return prediction_pb2.PredictResponse()</p>

<p>def serve():
 â€œ"â€Start gRPC serverâ€â€â€
 server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
 prediction_pb2_grpc.add_PredictionServiceServicer_to_server(
 PredictionServicer(), server
 )
 server.add_insecure_port(â€˜[::]:50051â€™)
 server.start()
 print(â€œgRPC server started on port 50051â€)
 server.wait_for_termination()</p>

<p>if <strong>name</strong> == â€˜<strong>main</strong>â€™:
 serve()
``</p>

<p><strong>Performance comparison:</strong>
<code class="language-plaintext highlighter-rouge">
Metric REST API gRPC
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Latency (p50) 15ms 5ms
Latency (p99) 50ms 20ms
Throughput 5K rps 15K rps
Payload size JSON Protocol Buffers (smaller)
</code></p>

<h3 id="pattern-3-batch-serving">Pattern 3: Batch Serving</h3>

<p><strong>Best for:</strong> Offline predictions, large-scale inference</p>

<p>``python
import pandas as pd
import numpy as np
from multiprocessing import Pool
import joblib</p>

<p>class BatchPredictor:
 â€œâ€â€
 Batch prediction system</p>

<p>Efficient for processing large datasets
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model_path, batch_size=1000, n_workers=4):
 self.model = joblib.load(model_path)
 self.batch_size = batch_size
 self.n_workers = n_workers</p>

<p>def predict_batch(self, features_df: pd.DataFrame) -&gt; np.ndarray:
 â€œâ€â€
 Predict on large dataset</p>

<p>Args:
 features_df: DataFrame with features</p>

<p>Returns:
 Array of predictions
 â€œâ€â€
 n_samples = len(features_df)
 n_batches = (n_samples + self.batch_size - 1) // self.batch_size</p>

<p>predictions = []</p>

<p>for i in range(n_batches):
 start_idx = i * self.batch_size
 end_idx = min((i + 1) * self.batch_size, n_samples)</p>

<p>batch = features_df.iloc[start_idx:end_idx].values
 batch_pred = self.model.predict(batch)
 predictions.extend(batch_pred)</p>

<p>if (i + 1) % 10 == 0:
 print(fâ€Processed {end_idx}/{n_samples} samplesâ€)</p>

<p>return np.array(predictions)</p>

<p>def predict_parallel(self, features_df: pd.DataFrame) -&gt; np.ndarray:
 â€œâ€â€
 Parallel batch prediction</p>

<p>Splits data across multiple processes
 â€œâ€â€
 # Split data into chunks
 chunk_size = len(features_df) // self.n_workers
 chunks = [
 features_df.iloc[i:i+chunk_size]
 for i in range(0, len(features_df), chunk_size)
 ]</p>

<p># Process in parallel
 with Pool(self.n_workers) as pool:
 results = pool.map(self._predict_chunk, chunks)</p>

<p># Combine results
 return np.concatenate(results)</p>

<p>def _predict_chunk(self, chunk_df):
 â€œ"â€Predict on single chunkâ€â€â€
 return self.model.predict(chunk_df.values)</p>

<h1 id="usage">Usage</h1>
<p>predictor = BatchPredictor(â€˜model.pklâ€™, batch_size=10000, n_workers=8)</p>

<h1 id="load-large-dataset">Load large dataset</h1>
<p>data = pd.read_parquet(â€˜features.parquetâ€™)</p>

<h1 id="predict">Predict</h1>
<p>predictions = predictor.predict_parallel(data)</p>

<h1 id="save-results">Save results</h1>
<p>results_df = data.copy()
results_df[â€˜predictionâ€™] = predictions
results_df.to_parquet(â€˜predictions.parquetâ€™)
``</p>

<hr />

<h2 id="model-loading-strategies">Model Loading Strategies</h2>

<h3 id="strategy-1-eager-loading">Strategy 1: Eager Loading</h3>

<p>``python
class EagerModelServer:
 â€œâ€â€
 Load model on server startup</p>

<p>Pros: Fast predictions, simple
 Cons: High startup time, high memory
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model_path):
 print(â€œLoading modelâ€¦â€)
 self.model = joblib.load(model_path)
 print(â€œModel loaded!â€)</p>

<p>def predict(self, features):
 â€œ"â€Make prediction (fast)â€â€â€
 return self.model.predict(features)
``</p>

<h3 id="strategy-2-lazy-loading">Strategy 2: Lazy Loading</h3>

<p>``python
class LazyModelServer:
 â€œâ€â€
 Load model on first request</p>

<p>Pros: Fast startup
 Cons: First request is slow
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model_path):
 self.model_path = model_path
 self.model = None</p>

<p>def predict(self, features):
 â€œ"â€Load model if needed, then predictâ€â€â€
 if self.model is None:
 print(â€œLoading model on first requestâ€¦â€)
 self.model = joblib.load(self.model_path)</p>

<p>return self.model.predict(features)
``</p>

<h3 id="strategy-3-model-caching-with-expiration">Strategy 3: Model Caching with Expiration</h3>

<p>``python
from datetime import datetime, timedelta
import threading</p>

<p>class CachedModelServer:
 â€œâ€â€
 Load model with cache expiration</p>

<p>Automatically reloads model periodically
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model_path, cache_ttl_minutes=60):
 self.model_path = model_path
 self.cache_ttl = timedelta(minutes=cache_ttl_minutes)
 self.model = None
 self.last_loaded = None
 self.lock = threading.Lock()</p>

<p>def _load_model(self):
 â€œ"â€Load model with lockâ€â€â€
 with self.lock:
 print(fâ€Loading model from {self.model_path}â€)
 self.model = joblib.load(self.model_path)
 self.last_loaded = datetime.now()</p>

<p>def predict(self, features):
 â€œ"â€Predict with cache checkâ€â€â€
 # Check if model needs refresh
 if (self.model is None or 
 datetime.now() - self.last_loaded &gt; self.cache_ttl):
 self._load_model()</p>

<p>return self.model.predict(features)
``</p>

<hr />

<h2 id="model-versioning--ab-testing">Model Versioning &amp; A/B Testing</h2>

<h3 id="multi-model-serving">Multi-Model Serving</h3>

<p>``python
from enum import Enum
from typing import Dict
import random</p>

<p>class ModelVersion(Enum):
 V1 = â€œv1â€
 V2 = â€œv2â€
 V3 = â€œv3â€</p>

<p>class MultiModelServer:
 â€œâ€â€
 Serve multiple model versions</p>

<p>Supports A/B testing and gradual rollouts
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.models: Dict[str, any] = {}
 self.traffic_split = {} # version â†’ weight</p>

<p>def load_model(self, version: ModelVersion, model_path: str):
 â€œ"â€Load a specific model versionâ€â€â€
 print(fâ€Loading {version.value} from {model_path}â€)
 self.models[version.value] = joblib.load(model_path)</p>

<p>def set_traffic_split(self, split: Dict[str, float]):
 â€œâ€â€
 Set traffic distribution</p>

<p>Args:
 split: Dict mapping version to weight
 e.g., {â€œv1â€: 0.9, â€œv2â€: 0.1}
 â€œâ€â€
 # Validate weights sum to 1
 total = sum(split.values())
 assert abs(total - 1.0) &lt; 1e-6, fâ€Weights must sum to 1, got {total}â€</p>

<p>self.traffic_split = split</p>

<p>def select_model(self, user_id: str = None) -&gt; str:
 â€œâ€â€
 Select model version based on traffic split</p>

<p>Args:
 user_id: Optional user ID for deterministic routing</p>

<p>Returns:
 Selected model version
 â€œâ€â€
 if user_id:
 # Deterministic selection (consistent for same user)
 import hashlib
 hash_val = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
 rand_val = (hash_val % 10000) / 10000.0
 else:
 # Random selection
 rand_val = random.random()</p>

<p># Select based on cumulative weights
 cumulative = 0
 for version, weight in self.traffic_split.items():
 cumulative += weight
 if rand_val &lt; cumulative:
 return version</p>

<p># Fallback to first version
 return list(self.traffic_split.keys())[0]</p>

<p>def predict(self, features, user_id: str = None):
 â€œâ€â€
 Make prediction with version selection</p>

<p>Returns: (prediction, version_used)
 â€œâ€â€
 version = self.select_model(user_id)
 model = self.models[version]
 prediction = model.predict(features)</p>

<p>return prediction, version</p>

<h1 id="usage-1">Usage</h1>
<p>server = MultiModelServer()</p>

<h1 id="load-models">Load models</h1>
<p>server.load_model(ModelVersion.V1, â€˜model_v1.pklâ€™)
server.load_model(ModelVersion.V2, â€˜model_v2.pklâ€™)</p>

<h1 id="start-with-90-v1-10-v2">Start with 90% v1, 10% v2</h1>
<p>server.set_traffic_split({â€œv1â€: 0.9, â€œv2â€: 0.1})</p>

<h1 id="make-predictions">Make predictions</h1>
<p>features = [[1, 2, 3, 4]]
prediction, version = server.predict(features, user_id=â€user_123â€)
print(fâ€Prediction: {prediction}, Version: {version}â€)</p>

<h1 id="gradually-increase-v2-traffic">Gradually increase v2 traffic</h1>
<p>server.set_traffic_split({â€œv1â€: 0.5, â€œv2â€: 0.5})
``</p>

<hr />

<h2 id="optimization-techniques">Optimization Techniques</h2>

<h3 id="1-model-quantization">1. Model Quantization</h3>

<p>``python
import torch
import torch.quantization</p>

<p>def quantize_model(model, example_input):
 â€œâ€â€
 Quantize PyTorch model to INT8</p>

<p>Reduces model size by ~4x, speeds up inference</p>

<p>Args:
 model: PyTorch model
 example_input: Sample input for calibration</p>

<p>Returns:
 Quantized model
 â€œâ€â€
 # Set model to eval mode
 model.eval()</p>

<p># Specify quantization configuration
 model.qconfig = torch.quantization.get_default_qconfig(â€˜fbgemmâ€™)</p>

<p># Prepare for quantization
 model_prepared = torch.quantization.prepare(model)</p>

<p># Calibrate with example data
 with torch.no_grad():
 model_prepared(example_input)</p>

<p># Convert to quantized model
 model_quantized = torch.quantization.convert(model_prepared)</p>

<p>return model_quantized</p>

<h1 id="example">Example</h1>
<p>model = torch.nn.Sequential(
 torch.nn.Linear(10, 50),
 torch.nn.ReLU(),
 torch.nn.Linear(50, 2)
)</p>

<p>example_input = torch.randn(1, 10)
quantized_model = quantize_model(model, example_input)</p>

<h1 id="quantized-model-is-4x-smaller-and-faster">Quantized model is ~4x smaller and faster</h1>
<p>print(fâ€Original size: {get_model_size(model):.2f} MBâ€)
print(fâ€Quantized size: {get_model_size(quantized_model):.2f} MBâ€)
``</p>

<h3 id="2-batch-inference">2. Batch Inference</h3>

<p>``python
import asyncio
from collections import deque
import time</p>

<p>class BatchingPredictor:
 â€œâ€â€
 Batch multiple requests for efficient inference</p>

<p>Collects requests and processes them in batches
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model, max_batch_size=32, max_wait_ms=10):
 self.model = model
 self.max_batch_size = max_batch_size
 self.max_wait_ms = max_wait_ms
 self.queue = deque()
 self.processing = False</p>

<p>async def predict(self, features):
 â€œâ€â€
 Add request to batch queue</p>

<p>Returns: Future that resolves with prediction
 â€œâ€â€
 future = asyncio.Future()
 self.queue.append((features, future))</p>

<p># Start batch processing if not already running
 if not self.processing:
 asyncio.create_task(self._process_batch())</p>

<p>return await future</p>

<p>async def _process_batch(self):
 â€œ"â€Process accumulated requests as batchâ€â€â€
 self.processing = True</p>

<p># Wait for batch to fill or timeout
 await asyncio.sleep(self.max_wait_ms / 1000.0)</p>

<p>if not self.queue:
 self.processing = False
 return</p>

<p># Collect batch
 batch = []
 futures = []</p>

<p>while self.queue and len(batch) &lt; self.max_batch_size:
 features, future = self.queue.popleft()
 batch.append(features)
 futures.append(future)</p>

<p># Run batch inference
 batch_array = np.array(batch)
 predictions = self.model.predict(batch_array)</p>

<p># Resolve futures
 for future, pred in zip(futures, predictions):
 future.set_result(pred)</p>

<p>self.processing = False</p>

<p># Process remaining queue
 if self.queue:
 asyncio.create_task(self._process_batch())</p>

<h1 id="usage-2">Usage</h1>
<p>predictor = BatchingPredictor(model, max_batch_size=32, max_wait_ms=10)</p>

<p>async def handle_request(features):
 prediction = await predictor.predict(features)
 return prediction
``</p>

<hr />

<h2 id="monitoring--observability">Monitoring &amp; Observability</h2>

<h3 id="prediction-logging">Prediction Logging</h3>

<p>``python
import logging
from dataclasses import dataclass, asdict
from datetime import datetime
import json</p>

<p>@dataclass
class PredictionLog:
 â€œ"â€Log entry for each predictionâ€â€â€
 timestamp: str
 model_version: str
 features: list
 prediction: float
 confidence: float
 latency_ms: float
 user_id: str = None</p>

<p>class MonitoredModelServer:
 â€œâ€â€
 Model server with comprehensive monitoring
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model, model_version):
 self.model = model
 self.model_version = model_version</p>

<p># Setup logging
 self.logger = logging.getLogger(â€˜model_serverâ€™)
 self.logger.setLevel(logging.INFO)</p>

<p># Metrics
 self.prediction_count = 0
 self.latencies = []
 self.error_count = 0</p>

<p>def predict(self, features, user_id=None):
 â€œâ€â€
 Make prediction with logging</p>

<p>Returns: (prediction, confidence, metadata)
 â€œâ€â€
 start_time = time.time()</p>

<p>try:
 # Make prediction
 prediction = self.model.predict([features])[0]</p>

<p># Get confidence
 if hasattr(self.model, â€˜predict_probaâ€™):
 proba = self.model.predict_proba([features])[0]
 confidence = float(np.max(proba))
 else:
 confidence = 1.0</p>

<p># Calculate latency
 latency_ms = (time.time() - start_time) * 1000</p>

<p># Log prediction
 log_entry = PredictionLog(
 timestamp=datetime.now().isoformat(),
 model_version=self.model_version,
 features=features,
 prediction=float(prediction),
 confidence=confidence,
 latency_ms=latency_ms,
 user_id=user_id
 )</p>

<p>self.logger.info(json.dumps(asdict(log_entry)))</p>

<p># Update metrics
 self.prediction_count += 1
 self.latencies.append(latency_ms)</p>

<p>return prediction, confidence, {â€˜latency_msâ€™: latency_ms}</p>

<p>except Exception as e:
 self.error_count += 1
 self.logger.error(fâ€Prediction failed: {str(e)}â€)
 raise</p>

<p>def get_metrics(self):
 â€œ"â€Get serving metricsâ€â€â€
 if not self.latencies:
 return {}</p>

<p>return {
 â€˜prediction_countâ€™: self.prediction_count,
 â€˜error_countâ€™: self.error_count,
 â€˜error_rateâ€™: self.error_count / max(self.prediction_count, 1),
 â€˜latency_p50â€™: np.percentile(self.latencies, 50),
 â€˜latency_p95â€™: np.percentile(self.latencies, 95),
 â€˜latency_p99â€™: np.percentile(self.latencies, 99),
 }
``</p>

<hr />

<h2 id="connection-to-bst-validation-dsa">Connection to BST Validation (DSA)</h2>

<p>Model serving systems validate predictions similar to BST range checking:</p>

<p>``python
class PredictionBoundsValidator:
 â€œâ€â€
 Validate predictions fall within expected ranges</p>

<p>Similar to BST validation with min/max bounds
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.bounds = {} # feature â†’ (min, max)</p>

<p>def set_bounds(self, feature_name, min_val, max_val):
 â€œ"â€Set validation boundsâ€â€â€
 self.bounds[feature_name] = (min_val, max_val)</p>

<p>def validate_input(self, features):
 â€œâ€â€
 Validate input features</p>

<p>Like BST range checking: each value must be in [min, max]
 â€œâ€â€
 violations = []</p>

<p>for feature_name, value in features.items():
 if feature_name in self.bounds:
 min_val, max_val = self.bounds[feature_name]</p>

<p># Range check (like BST validation)
 if value &lt; min_val or value &gt; max_val:
 violations.append({
 â€˜featureâ€™: feature_name,
 â€˜valueâ€™: value,
 â€˜boundsâ€™: (min_val, max_val)
 })</p>

<p>return len(violations) == 0, violations
``</p>

<hr />

<h2 id="advanced-serving-patterns">Advanced Serving Patterns</h2>

<h3 id="1-shadow-mode-deployment">1. Shadow Mode Deployment</h3>

<p>``python
class ShadowModeServer:
 â€œâ€â€
 Run new model in shadow mode</p>

<p>New model receives traffic but doesnâ€™t affect users
 Predictions are logged for comparison
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, production_model, shadow_model):
 self.production_model = production_model
 self.shadow_model = shadow_model
 self.comparison_logs = []</p>

<p>def predict(self, features):
 â€œâ€â€
 Make predictions with both models</p>

<p>Returns: Production prediction (shadow runs async)
 â€œâ€â€
 import asyncio</p>

<p># Production prediction (synchronous)
 prod_prediction = self.production_model.predict(features)</p>

<p># Shadow prediction (async, doesnâ€™t block)
 asyncio.create_task(self._shadow_predict(features, prod_prediction))</p>

<p>return prod_prediction</p>

<p>async def _shadow_predict(self, features, prod_prediction):
 â€œ"â€Run shadow model and log comparisonâ€â€â€
 try:
 shadow_prediction = self.shadow_model.predict(features)</p>

<p># Log comparison
 self.comparison_logs.append({
 â€˜featuresâ€™: features,
 â€˜productionâ€™: prod_prediction,
 â€˜shadowâ€™: shadow_prediction,
 â€˜differenceâ€™: abs(prod_prediction - shadow_prediction)
 })
 except Exception as e:
 print(fâ€Shadow prediction failed: {e}â€)</p>

<p>def get_shadow_metrics(self):
 â€œ"â€Analyze shadow model performanceâ€â€â€
 if not self.comparison_logs:
 return {}</p>

<p>differences = [log[â€˜differenceâ€™] for log in self.comparison_logs]</p>

<p>return {
 â€˜num_predictionsâ€™: len(self.comparison_logs),
 â€˜mean_differenceâ€™: np.mean(differences),
 â€˜max_differenceâ€™: np.max(differences),
 â€˜agreement_rateâ€™: sum(1 for d in differences if d &lt; 0.01) / len(differences)
 }</p>

<h1 id="usage-3">Usage</h1>
<p>shadow_server = ShadowModeServer(
 production_model=model_v1,
 shadow_model=model_v2
)</p>

<h1 id="normal-serving">Normal serving</h1>
<p>prediction = shadow_server.predict(features)</p>

<h1 id="analyze-shadow-performance">Analyze shadow performance</h1>
<p>metrics = shadow_server.get_shadow_metrics()
print(fâ€Shadow agreement rate: {metrics[â€˜agreement_rateâ€™]:.2%}â€)
``</p>

<h3 id="2-canary-deployment">2. Canary Deployment</h3>

<p>``python
class CanaryDeployment:
 â€œâ€â€
 Gradual rollout with automated rollback</p>

<p>Monitors metrics and automatically rolls back if issues detected
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, stable_model, canary_model):
 self.stable_model = stable_model
 self.canary_model = canary_model
 self.canary_percentage = 0.0
 self.metrics = {
 â€˜stableâ€™: {â€˜errorsâ€™: 0, â€˜predictionsâ€™: 0, â€˜latenciesâ€™: []},
 â€˜canaryâ€™: {â€˜errorsâ€™: 0, â€˜predictionsâ€™: 0, â€˜latenciesâ€™: []}
 }</p>

<p>def set_canary_percentage(self, percentage):
 â€œ"â€Set canary traffic percentageâ€â€â€
 assert 0 &lt;= percentage &lt;= 100
 self.canary_percentage = percentage
 print(fâ€Canary traffic: {percentage}%â€)</p>

<p>def predict(self, features, user_id=None):
 â€œâ€â€
 Predict with canary logic</p>

<p>Routes percentage of traffic to canary
 â€œâ€â€
 import random
 import time</p>

<p># Determine which model to use
 use_canary = random.random() &lt; (self.canary_percentage / 100)
 model_name = â€˜canaryâ€™ if use_canary else â€˜stableâ€™
 model = self.canary_model if use_canary else self.stable_model</p>

<p># Make prediction with metrics
 start_time = time.time()
 try:
 prediction = model.predict(features)
 latency = time.time() - start_time</p>

<p># Record metrics
 self.metrics[model_name][â€˜predictionsâ€™] += 1
 self.metrics[model_name][â€˜latenciesâ€™].append(latency)</p>

<p>return prediction, model_name</p>

<p>except Exception as e:
 # Record error
 self.metrics[model_name][â€˜errorsâ€™] += 1
 raise</p>

<p>def check_health(self):
 â€œâ€â€
 Check canary health</p>

<p>Returns: (is_healthy, should_rollback, reason)
 â€œâ€â€
 canary_metrics = self.metrics[â€˜canaryâ€™]
 stable_metrics = self.metrics[â€˜stableâ€™]</p>

<p>if canary_metrics[â€˜predictionsâ€™] &lt; 100:
 # Not enough data yet
 return True, False, â€œInsufficient dataâ€</p>

<p># Calculate error rates
 canary_error_rate = canary_metrics[â€˜errorsâ€™] / canary_metrics[â€˜predictionsâ€™]
 stable_error_rate = stable_metrics[â€˜errorsâ€™] / max(stable_metrics[â€˜predictionsâ€™], 1)</p>

<p># Check if error rate is significantly higher
 if canary_error_rate &gt; stable_error_rate * 2:
 return False, True, fâ€Error rate too high: {canary_error_rate:.2%}â€</p>

<p># Check latency
 canary_p95 = np.percentile(canary_metrics[â€˜latenciesâ€™], 95)
 stable_p95 = np.percentile(stable_metrics[â€˜latenciesâ€™], 95)</p>

<p>if canary_p95 &gt; stable_p95 * 1.5:
 return False, True, fâ€Latency too high: {canary_p95:.1f}msâ€</p>

<p>return True, False, â€œHealthyâ€</p>

<p>def auto_rollout(self, target_percentage=100, step=10, check_interval=60):
 â€œâ€â€
 Automatically increase canary traffic</p>

<p>Rolls back if health checks fail
 â€œâ€â€
 current = 0</p>

<p>while current &lt; target_percentage:
 # Increase canary traffic
 current = min(current + step, target_percentage)
 self.set_canary_percentage(current)</p>

<p># Wait and check health
 time.sleep(check_interval)</p>

<p>is_healthy, should_rollback, reason = self.check_health()</p>

<p>if should_rollback:
 print(fâ€âŒ Rollback triggered: {reason}â€)
 self.set_canary_percentage(0) # Rollback to stable
 return False</p>

<p>print(fâ€âœ“ Health check passed at {current}%â€)</p>

<p>print(fâ€ğŸ‰ Canary rollout complete!â€)
 return True</p>

<h1 id="usage-4">Usage</h1>
<p>canary = CanaryDeployment(stable_model=model_v1, canary_model=model_v2)</p>

<h1 id="start-with-5-traffic">Start with 5% traffic</h1>
<p>canary.set_canary_percentage(5)</p>

<h1 id="automatic-gradual-rollout">Automatic gradual rollout</h1>
<p>success = canary.auto_rollout(target_percentage=100, step=10, check_interval=300)
``</p>

<h3 id="3-multi-armed-bandit-serving">3. Multi-Armed Bandit Serving</h3>

<p>``python
class BanditModelServer:
 â€œâ€â€
 Multi-armed bandit for model selection</p>

<p>Dynamically allocates traffic based on performance
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, models: dict):
 â€œâ€â€
 Args:
 models: Dict of {model_name: model}
 â€œâ€â€
 self.models = models
 self.rewards = {name: [] for name in models.keys()}
 self.counts = {name: 0 for name in models.keys()}
 self.epsilon = 0.1 # Exploration rate</p>

<p>def select_model(self):
 â€œâ€â€
 Select model using epsilon-greedy strategy</p>

<p>Returns: model_name
 â€œâ€â€
 import random</p>

<p># Explore: random selection
 if random.random() &lt; self.epsilon:
 return random.choice(list(self.models.keys()))</p>

<p># Exploit: select best performing model
 avg_rewards = {
 name: np.mean(rewards) if rewards else 0
 for name, rewards in self.rewards.items()
 }</p>

<p>return max(avg_rewards, key=avg_rewards.get)</p>

<p>def predict(self, features, true_label=None):
 â€œâ€â€
 Make prediction and optionally update rewards</p>

<p>Args:
 features: Input features
 true_label: Optional ground truth for reward</p>

<p>Returns: (prediction, model_used)
 â€œâ€â€
 # Select model
 model_name = self.select_model()
 model = self.models[model_name]</p>

<p># Make prediction
 prediction = model.predict(features)
 self.counts[model_name] += 1</p>

<p># Update reward if ground truth available
 if true_label is not None:
 reward = 1.0 if prediction == true_label else 0.0
 self.rewards[model_name].append(reward)</p>

<p>return prediction, model_name</p>

<p>def get_model_stats(self):
 â€œ"â€Get statistics for each modelâ€â€â€
 stats = {}</p>

<p>for name in self.models.keys():
 if self.rewards[name]:
 stats[name] = {
 â€˜countâ€™: self.counts[name],
 â€˜avg_rewardâ€™: np.mean(self.rewards[name]),
 â€˜selection_rateâ€™: self.counts[name] / sum(self.counts.values())
 }
 else:
 stats[name] = {
 â€˜countâ€™: self.counts[name],
 â€˜avg_rewardâ€™: 0,
 â€˜selection_rateâ€™: 0
 }</p>

<p>return stats</p>

<h1 id="usage-5">Usage</h1>
<p>bandit = BanditModelServer({
 â€˜model_aâ€™: model_a,
 â€˜model_bâ€™: model_b,
 â€˜model_câ€™: model_c
})</p>

<h1 id="serve-with-automatic-optimization">Serve with automatic optimization</h1>
<p>for features, label in data_stream:
 prediction, model_used = bandit.predict(features, true_label=label)</p>

<h1 id="check-which-model-performs-best">Check which model performs best</h1>
<p>stats = bandit.get_model_stats()
for name, stat in stats.items():
 print(fâ€{name}: {stat[â€˜avg_rewardâ€™]:.2%} accuracy, {stat[â€˜selection_rateâ€™]:.1%} trafficâ€)
``</p>

<hr />

<h2 id="infrastructure--deployment">Infrastructure &amp; Deployment</h2>

<h3 id="containerized-serving-with-docker">Containerized Serving with Docker</h3>

<p>``dockerfile</p>
<h1 id="dockerfile-for-model-serving">Dockerfile for model serving</h1>
<p>FROM python:3.9-slim</p>

<p>WORKDIR /app</p>

<h1 id="install-dependencies">Install dependencies</h1>
<p>COPY requirements.txt .
RUN pip install â€“no-cache-dir -r requirements.txt</p>

<h1 id="copy-model-and-code">Copy model and code</h1>
<p>COPY model.pkl .
COPY serve.py .</p>

<h1 id="expose-port">Expose port</h1>
<p>EXPOSE 8000</p>

<h1 id="health-check">Health check</h1>
<p>HEALTHCHECK â€“interval=30s â€“timeout=3s <br />
 CMD curl -f http://localhost:8000/health || exit 1</p>

<h1 id="run-server">Run server</h1>
<p>CMD [â€œuvicornâ€, â€œserve:appâ€, â€œâ€“hostâ€, â€œ0.0.0.0â€, â€œâ€“portâ€, â€œ8000â€]
``</p>

<p>``yaml</p>
<h1 id="docker-composeyml">docker-compose.yml</h1>
<p>version: â€˜3.8â€™</p>

<p>services:
 model-server:
 build: .
 ports:</p>
<ul>
  <li>â€œ8000:8000â€
 environment:</li>
  <li>MODEL_PATH=/app/model.pkl</li>
  <li>LOG_LEVEL=INFO
 volumes:</li>
  <li>./models:/app/models
 deploy:
 replicas: 3
 resources:
 limits:
 cpus: â€˜2â€™
 memory: 4G
 healthcheck:
 test: [â€œCMDâ€, â€œcurlâ€, â€œ-fâ€, â€œhttp://localhost:8000/healthâ€]
 interval: 30s
 timeout: 10s
 retries: 3</li>
</ul>

<p>load-balancer:
 image: nginx:alpine
 ports:</p>
<ul>
  <li>â€œ80:80â€
 volumes:</li>
  <li>./nginx.conf:/etc/nginx/nginx.conf
 depends_on:</li>
  <li>model-server
``</li>
</ul>

<h3 id="kubernetes-deployment">Kubernetes Deployment</h3>

<p>``yaml</p>
<h1 id="deploymentyaml">deployment.yaml</h1>
<p>apiVersion: apps/v1
kind: Deployment
metadata:
 name: model-serving
spec:
 replicas: 5
 selector:
 matchLabels:
 app: model-serving
 template:
 metadata:
 labels:
 app: model-serving
 version: v1
 spec:
 containers:</p>
<ul>
  <li>name: model-server
 image: your-registry/model-serving:v1
 ports:</li>
  <li>containerPort: 8000
 env:</li>
  <li>name: MODEL_VERSION
 value: â€œv1.0â€
 resources:
 requests:
 memory: â€œ2Giâ€
 cpu: â€œ1000mâ€
 limits:
 memory: â€œ4Giâ€
 cpu: â€œ2000mâ€
 livenessProbe:
 httpGet:
 path: /health
 port: 8000
 initialDelaySeconds: 30
 periodSeconds: 10
 readinessProbe:
 httpGet:
 path: /ready
 port: 8000
 initialDelaySeconds: 5
 periodSeconds: 5
â€”
apiVersion: v1
kind: Service
metadata:
 name: model-serving-service
spec:
 selector:
 app: model-serving
 ports:</li>
  <li>protocol: TCP
 port: 80
 targetPort: 8000
 type: LoadBalancer
â€”
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
 name: model-serving-hpa
spec:
 scaleTargetRef:
 apiVersion: apps/v1
 kind: Deployment
 name: model-serving
 minReplicas: 3
 maxReplicas: 20
 metrics:</li>
  <li>type: Resource
 resource:
 name: cpu
 target:
 type: Utilization
 averageUtilization: 70</li>
  <li>type: Resource
 resource:
 name: memory
 target:
 type: Utilization
 averageUtilization: 80
``</li>
</ul>

<hr />

<h2 id="feature-store-integration">Feature Store Integration</h2>

<p>``python
class ModelServerWithFeatureStore:
 â€œâ€â€
 Model server integrated with feature store</p>

<p>Fetches features on-demand for prediction
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model, feature_store):
 self.model = model
 self.feature_store = feature_store</p>

<p>def predict_from_entity_id(self, entity_id: str):
 â€œâ€â€
 Make prediction given entity ID</p>

<p>Fetches features from feature store</p>

<p>Args:
 entity_id: ID to fetch features for</p>

<p>Returns: Prediction
 â€œâ€â€
 # Fetch features from feature store
 features = self.feature_store.get_online_features(
 entity_id=entity_id,
 feature_names=[
 â€˜user_ageâ€™,
 â€˜user_incomeâ€™,
 â€˜user_num_purchases_30dâ€™,
 â€˜user_avg_purchase_amountâ€™
 ]
 )</p>

<p># Convert to array
 feature_array = [
 features[â€˜user_ageâ€™],
 features[â€˜user_incomeâ€™],
 features[â€˜user_num_purchases_30dâ€™],
 features[â€˜user_avg_purchase_amountâ€™]
 ]</p>

<p># Make prediction
 prediction = self.model.predict([feature_array])[0]</p>

<p>return {
 â€˜entity_idâ€™: entity_id,
 â€˜predictionâ€™: float(prediction),
 â€˜features_usedâ€™: features
 }</p>

<h1 id="usage-with-caching">Usage with caching</h1>
<p>from functools import lru_cache</p>

<p>class CachedFeatureStore:
 â€œ"â€Feature store with cachingâ€â€â€</p>

<p>def <strong>init</strong>(self, backend):
 self.backend = backend</p>

<p>@lru_cache(maxsize=10000)
 def get_online_features(self, entity_id, feature_names):
 â€œ"â€Cached feature retrievalâ€â€â€
 return self.backend.get_features(entity_id, feature_names)
``</p>

<hr />

<h2 id="cost-optimization">Cost Optimization</h2>

<h3 id="1-request-batching-for-cost-reduction">1. Request Batching for Cost Reduction</h3>

<p>``python
class CostOptimizedServer:
 â€œâ€â€
 Optimize costs by batching and caching</p>

<p>Reduces number of model invocations
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model, batch_wait_ms=50, batch_size=32):
 self.model = model
 self.batch_wait_ms = batch_wait_ms
 self.batch_size = batch_size
 self.pending_requests = []
 self.cache = {}
 self.stats = {
 â€˜cache_hitsâ€™: 0,
 â€˜cache_missesâ€™: 0,
 â€˜batches_processedâ€™: 0,
 â€˜cost_savedâ€™: 0
 }</p>

<p>async def predict_with_caching(self, features, cache_key=None):
 â€œâ€â€
 Predict with caching</p>

<p>Args:
 features: Input features
 cache_key: Optional cache key</p>

<p>Returns: Prediction
 â€œâ€â€
 # Check cache
 if cache_key and cache_key in self.cache:
 self.stats[â€˜cache_hitsâ€™] += 1
 return self.cache[cache_key]</p>

<p>self.stats[â€˜cache_missesâ€™] += 1</p>

<p># Add to batch
 future = asyncio.Future()
 self.pending_requests.append((features, future, cache_key))</p>

<p># Trigger batch processing if needed
 if len(self.pending_requests) &gt;= self.batch_size:
 await self._process_batch()</p>

<p>return await future</p>

<p>async def _process_batch(self):
 â€œ"â€Process accumulated requests as batchâ€â€â€
 if not self.pending_requests:
 return</p>

<p># Extract batch
 batch_features = [req[0] for req in self.pending_requests]
 futures = [req[1] for req in self.pending_requests]
 cache_keys = [req[2] for req in self.pending_requests]</p>

<p># Run batch inference
 predictions = self.model.predict(batch_features)</p>

<p>self.stats[â€˜batches_processedâ€™] += 1</p>

<p># Distribute results
 for pred, future, cache_key in zip(predictions, futures, cache_keys):
 # Cache result
 if cache_key:
 self.cache[cache_key] = pred</p>

<p># Resolve future
 future.set_result(pred)</p>

<p># Clear requests
 self.pending_requests = []</p>

<p># Calculate cost savings (batching is cheaper)
 cost_per_single_request = 0.001 # $0.001 per request
 cost_per_batch = 0.010 # $0.01 per batch
 savings = (len(predictions) * cost_per_single_request) - cost_per_batch
 self.stats[â€˜cost_savedâ€™] += savings</p>

<p>def get_cost_stats(self):
 â€œ"â€Get cost optimization statisticsâ€â€â€
 total_requests = self.stats[â€˜cache_hitsâ€™] + self.stats[â€˜cache_missesâ€™]</p>

<p>return {
 â€˜total_requestsâ€™: total_requests,
 â€˜cache_hit_rateâ€™: self.stats[â€˜cache_hitsâ€™] / max(total_requests, 1),
 â€˜batches_processedâ€™: self.stats[â€˜batches_processedâ€™],
 â€˜avg_batch_sizeâ€™: total_requests / max(self.stats[â€˜batches_processedâ€™], 1),
 â€˜estimated_cost_savedâ€™: self.stats[â€˜cost_savedâ€™]
 }
``</p>

<h3 id="2-model-compression-for-cheaper-hosting">2. Model Compression for Cheaper Hosting</h3>

<p>``python
import torch</p>

<p>def compress_model_for_deployment(model, sample_input):
 â€œâ€â€
 Compress model for cheaper hosting</p>

<p>Techniques:</p>
<ul>
  <li>Quantization (INT8)</li>
  <li>Pruning</li>
  <li>Knowledge distillation</li>
</ul>

<p>Returns: Compressed model
 â€œâ€â€
 # 1. Quantization
 model.eval()
 model_quantized = torch.quantization.quantize_dynamic(
 model,
 {torch.nn.Linear},
 dtype=torch.qint8
 )</p>

<p># 2. Pruning (remove small weights)
 import torch.nn.utils.prune as prune</p>

<p>for name, module in model_quantized.named_modules():
 if isinstance(module, torch.nn.Linear):
 prune.l1_unstructured(module, name=â€™weightâ€™, amount=0.3)</p>

<p># 3. Verify accuracy
 with torch.no_grad():
 original_output = model(sample_input)
 compressed_output = model_quantized(sample_input)</p>

<p>diff = torch.abs(original_output - compressed_output).mean()
 print(fâ€Compression error: {diff:.4f}â€)</p>

<p>return model_quantized</p>

<h1 id="compare-costs">Compare costs</h1>
<p>original_size_mb = get_model_size(model)
compressed_size_mb = get_model_size(compressed_model)</p>

<p>print(fâ€Size reduction: {original_size_mb:.1f}MB â†’ {compressed_size_mb:.1f}MBâ€)
print(fâ€Cost savings: ~${(original_size_mb - compressed_size_mb) * 0.10:.2f}/monthâ€)
``</p>

<hr />

<h2 id="troubleshooting--debugging">Troubleshooting &amp; Debugging</h2>

<h3 id="prediction-debugging">Prediction Debugging</h3>

<p>``python
class DebuggableModelServer:
 â€œâ€â€
 Model server with debugging capabilities</p>

<p>Helps diagnose prediction issues
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model):
 self.model = model</p>

<p>def predict_with_debug(self, features, debug=False):
 â€œâ€â€
 Make prediction with optional debug info</p>

<p>Returns: (prediction, debug_info)
 â€œâ€â€
 debug_info = {}</p>

<p>if debug:
 # Record input stats
 debug_info[â€˜input_statsâ€™] = {
 â€˜meanâ€™: np.mean(features),
 â€˜stdâ€™: np.std(features),
 â€˜minâ€™: np.min(features),
 â€˜maxâ€™: np.max(features),
 â€˜nan_countâ€™: np.isnan(features).sum()
 }</p>

<p># Check for anomalies
 debug_info[â€˜anomaliesâ€™] = self._detect_anomalies(features)</p>

<p># Make prediction
 prediction = self.model.predict([features])[0]</p>

<p>if debug:
 # Record prediction confidence
 if hasattr(self.model, â€˜predict_probaâ€™):
 proba = self.model.predict_proba([features])[0]
 debug_info[â€˜confidenceâ€™] = float(np.max(proba))
 debug_info[â€˜class_probabilitiesâ€™] = proba.tolist()</p>

<p>return prediction, debug_info</p>

<p>def _detect_anomalies(self, features):
 â€œ"â€Detect input anomaliesâ€â€â€
 anomalies = []</p>

<p># Check for NaN
 if np.any(np.isnan(features)):
 anomalies.append(â€œContains NaN valuesâ€)</p>

<p># Check for extreme values
 z_scores = np.abs((features - np.mean(features)) / (np.std(features) + 1e-8))
 if np.any(z_scores &gt; 3):
 anomalies.append(â€œContains outliers (z-score &gt; 3)â€)</p>

<p>return anomalies</p>

<p>def explain_prediction(self, features):
 â€œâ€â€
 Explain prediction using SHAP or similar</p>

<p>Returns: Feature importance
 â€œâ€â€
 # Simplified explanation (in practice, use SHAP)
 if hasattr(self.model, â€˜feature_importances_â€™):
 importances = self.model.feature_importances_</p>

<p>return {
 fâ€™feature_{i}â€™: {â€˜valueâ€™: features[i], â€˜importanceâ€™: imp}
 for i, imp in enumerate(importances)
 }</p>

<p>return {}
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>âœ… <strong>Multiple serving patterns</strong> - REST, gRPC, batch for different needs 
âœ… <strong>Model versioning essential</strong> - Support A/B testing and rollbacks 
âœ… <strong>Optimize for latency</strong> - Quantization, batching, caching 
âœ… <strong>Monitor everything</strong> - Latency, errors, prediction distribution 
âœ… <strong>Validate inputs/outputs</strong> - Catch issues early 
âœ… <strong>Scale horizontally</strong> - Add more serving instances 
âœ… <strong>Connection to validation</strong> - Like BST range checking</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0008-model-serving-architecture/">arunbaby.com/ml-system-design/0008-model-serving-architecture</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#deployment" class="page__taxonomy-item p-category" rel="tag">deployment</a><span class="sep">, </span>
    
      <a href="/tags/#inference" class="page__taxonomy-item p-category" rel="tag">inference</a><span class="sep">, </span>
    
      <a href="/tags/#latency" class="page__taxonomy-item p-category" rel="tag">latency</a><span class="sep">, </span>
    
      <a href="/tags/#model-serving" class="page__taxonomy-item p-category" rel="tag">model-serving</a><span class="sep">, </span>
    
      <a href="/tags/#scalability" class="page__taxonomy-item p-category" rel="tag">scalability</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0008-validate-binary-search-tree/" rel="permalink">Validate Binary Search Tree
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master BST validation to understand data integrity in tree structures, critical for indexing and search systems.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0008-streaming-speech-pipeline/" rel="permalink">Streaming Speech Processing Pipeline
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Build real-time speech processing pipelines that handle audio streams with minimal latency for live transcription and voice interfaces.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0008-agent-workflow-patterns/" rel="permalink">Agent Workflow Patterns
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">â€œBetter workflows beat better models.â€ â€” Dr. Andrew Ng
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Model+Serving+Architecture%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0008-model-serving-architecture%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0008-model-serving-architecture%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0008-model-serving-architecture/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0007-feature-engineering/" class="pagination--pager" title="Feature Engineering at Scale">Previous</a>
    
    
      <a href="/ml-system-design/0009-online-learning-systems/" class="pagination--pager" title="Online Learning Systems">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
