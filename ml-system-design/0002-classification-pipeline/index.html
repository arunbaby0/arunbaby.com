<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Classification Pipeline Design - Arun Baby</title>
<meta name="description" content="From raw data to production predictions: building a classification pipeline that handles millions of requests with 99.9% uptime.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Classification Pipeline Design">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0002-classification-pipeline/">


  <meta property="og:description" content="From raw data to production predictions: building a classification pipeline that handles millions of requests with 99.9% uptime.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Classification Pipeline Design">
  <meta name="twitter:description" content="From raw data to production predictions: building a classification pipeline that handles millions of requests with 99.9% uptime.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0002-classification-pipeline/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0002-classification-pipeline/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Classification Pipeline Design">
    <meta itemprop="description" content="From raw data to production predictions: building a classification pipeline that handles millions of requests with 99.9% uptime.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0002-classification-pipeline/" itemprop="url">Classification Pipeline Design
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#problem-definition">Problem Definition</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li><li><a href="#example-use-case-spam-detection">Example Use Case: Spam Detection</a></li></ul></li><li><a href="#high-level-architecture">High-Level Architecture</a></li><li><a href="#component-1-input-validation">Component 1: Input Validation</a><ul><li><a href="#schema-validation-with-pydantic">Schema Validation with Pydantic</a></li><li><a href="#input-sanitization">Input Sanitization</a></li></ul></li><li><a href="#component-2-feature-engineering">Component 2: Feature Engineering</a><ul><li><a href="#feature-store-pattern">Feature Store Pattern</a></li><li><a href="#feature-transformation-pipeline">Feature Transformation Pipeline</a></li></ul></li><li><a href="#component-3-model-serving">Component 3: Model Serving</a><ul><li><a href="#multi-model-serving-with-ab-testing">Multi-Model Serving with A/B Testing</a></li><li><a href="#model-caching">Model Caching</a></li></ul></li><li><a href="#component-4-post-processing">Component 4: Post-Processing</a><ul><li><a href="#threshold-optimization">Threshold Optimization</a></li><li><a href="#calibration">Calibration</a></li></ul></li><li><a href="#component-5-explainability">Component 5: Explainability</a><ul><li><a href="#shap-values">SHAP Values</a></li><li><a href="#rule-based-explanations">Rule-Based Explanations</a></li></ul></li><li><a href="#monitoring--drift-detection">Monitoring &amp; Drift Detection</a><ul><li><a href="#metrics-collection">Metrics Collection</a></li><li><a href="#data-drift-detection">Data Drift Detection</a></li></ul></li><li><a href="#deployment-strategies">Deployment Strategies</a><ul><li><a href="#blue-green-deployment">Blue-Green Deployment</a></li></ul></li><li><a href="#complete-example-spam-classifier-service">Complete Example: Spam Classifier Service</a></li><li><a href="#key-takeaways">Key Takeaways</a></li><li><a href="#further-reading">Further Reading</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>From raw data to production predictions: building a classification pipeline that handles millions of requests with 99.9% uptime.</strong></p>

<h2 id="introduction">Introduction</h2>

<p>Classification is one of the most common machine learning tasks in production: spam detection, content moderation, fraud detection, sentiment analysis, image categorization, and countless others. While training a classifier might take hours in a Jupyter notebook, deploying it to production requires a sophisticated pipeline that handles:</p>

<ul>
  <li><strong>Real-time inference</strong> (&lt; 100ms latency)</li>
  <li><strong>Feature engineering</strong> at scale</li>
  <li><strong>Model versioning</strong> and A/B testing</li>
  <li><strong>Data drift</strong> detection and handling</li>
  <li><strong>Explainability</strong> for debugging and compliance</li>
  <li><strong>Monitoring</strong> for performance degradation</li>
  <li><strong>Graceful degradation</strong> when components fail</li>
</ul>

<p>This post focuses on building an end-to-end classification system that processes millions of predictions daily while maintaining high availability and performance.</p>

<p><strong>What you’ll learn:</strong></p>
<ul>
  <li>End-to-end pipeline architecture for production classification</li>
  <li>Feature engineering and feature store patterns</li>
  <li>Model serving strategies and optimization</li>
  <li>A/B testing and model deployment</li>
  <li>Monitoring, alerting, and data drift detection</li>
  <li>Real-world examples from Uber, Airbnb, and Meta</li>
</ul>

<hr />

<h2 id="problem-definition">Problem Definition</h2>

<p>Design a production classification system (example: spam detection for user messages) that:</p>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Real-time Inference</strong>
    <ul>
      <li>Classify incoming data in real-time</li>
      <li>Return predictions within latency budget</li>
      <li>Handle variable request rates</li>
    </ul>
  </li>
  <li><strong>Multi-class Support</strong>
    <ul>
      <li>Binary classification (spam/not spam)</li>
      <li>Multi-class (topic categorization)</li>
      <li>Multi-label (multiple tags per item)</li>
    </ul>
  </li>
  <li><strong>Feature Processing</strong>
    <ul>
      <li>Transform raw data into model-ready features</li>
      <li>Handle missing values and outliers</li>
      <li>Cache expensive feature computations</li>
    </ul>
  </li>
  <li><strong>Model Updates</strong>
    <ul>
      <li>Deploy new models without downtime</li>
      <li>A/B test model versions</li>
      <li>Rollback bad deployments quickly</li>
    </ul>
  </li>
  <li><strong>Explainability</strong>
    <ul>
      <li>Provide reasoning for predictions</li>
      <li>Support debugging and compliance</li>
      <li>Build user trust</li>
    </ul>
  </li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Latency</strong>
    <ul>
      <li>p50 &lt; 20ms (median)</li>
      <li>p99 &lt; 100ms (99th percentile)</li>
      <li>Tail latency critical for user experience</li>
    </ul>
  </li>
  <li><strong>Throughput</strong>
    <ul>
      <li>1M predictions per day</li>
      <li>~12 QPS average, ~100 QPS peak</li>
      <li>Horizontal scaling for growth</li>
    </ul>
  </li>
  <li><strong>Availability</strong>
    <ul>
      <li>99.9% uptime (&lt; 9 hours downtime/year)</li>
      <li>Graceful degradation on failures</li>
      <li>No single points of failure</li>
    </ul>
  </li>
  <li><strong>Accuracy</strong>
    <ul>
      <li>Maintain &gt; 90% precision</li>
      <li>Maintain &gt; 85% recall</li>
      <li>Monitor for drift</li>
    </ul>
  </li>
</ol>

<h3 id="example-use-case-spam-detection">Example Use Case: Spam Detection</h3>

<ul>
  <li><strong>Input:</strong> User message (text, metadata)</li>
  <li><strong>Output:</strong> {spam, not_spam, confidence}</li>
  <li><strong>Scale:</strong> 1M messages/day</li>
  <li><strong>Latency:</strong> &lt; 50ms p99</li>
  <li><strong>False positive cost:</strong> High (blocks legitimate messages)</li>
  <li><strong>False negative cost:</strong> Medium (spam gets through)</li>
</ul>

<hr />

<h2 id="high-level-architecture">High-Level Architecture</h2>

<p><code class="language-plaintext highlighter-rouge">
┌─────────────────────────────────────────────────────┐
│ Client Application │
└────────────────────┬────────────────────────────────┘
 │ HTTP/gRPC request
 ▼
┌─────────────────────────────────────────────────────┐
│ API Gateway │
│ • Rate limiting │
│ • Authentication │
│ • Request validation │
└────────────────────┬────────────────────────────────┘
 ▼
┌─────────────────────────────────────────────────────┐
│ Classification Service │
│ ┌──────────────────────────────────────────────┐ │
│ │ 1. Input Validation &amp; Preprocessing │ │
│ └──────────────┬───────────────────────────────┘ │
│ ▼ │
│ ┌──────────────────────────────────────────────┐ │
│ │ 2. Feature Engineering │ │
│ │ • Feature Store lookup (cached) │ │
│ │ • Real-time feature computation │ │
│ │ • Feature transformation │ │
│ └──────────────┬───────────────────────────────┘ │
│ ▼ │
│ ┌──────────────────────────────────────────────┐ │
│ │ 3. Model Inference │ │
│ │ • Model serving (TF/PyTorch) │ │
│ │ • A/B testing routing │ │
│ │ • Prediction caching │ │
│ └──────────────┬───────────────────────────────┘ │
│ ▼ │
│ ┌──────────────────────────────────────────────┐ │
│ │ 4. Post-processing │ │
│ │ • Threshold optimization │ │
│ │ • Calibration │ │
│ │ • Explainability generation │ │
│ └──────────────┬───────────────────────────────┘ │
│ ▼ │
│ ┌──────────────────────────────────────────────┐ │
│ │ 5. Logging &amp; Monitoring │ │
│ │ • Prediction logs → Kafka │ │
│ │ • Metrics → Prometheus │ │
│ │ • Traces → Jaeger │ │
│ └──────────────────────────────────────────────┘ │
└────────────────────┬────────────────────────────────┘
 ▼
 Response to client
</code></p>

<p><strong>Latency Budget (100ms total):</strong>
<code class="language-plaintext highlighter-rouge">
Input validation: 5ms
Feature extraction: 25ms ← Often bottleneck
Model inference: 40ms
Post-processing: 10ms
Logging (async): 0ms
Network overhead: 20ms
Total: 100ms ✓
</code></p>

<hr />

<h2 id="component-1-input-validation">Component 1: Input Validation</h2>

<h3 id="schema-validation-with-pydantic">Schema Validation with Pydantic</h3>

<p>``python
from pydantic import BaseModel, validator, Field
from typing import Optional
import re</p>

<p>class ClassificationRequest(BaseModel):
 “””
 Validate incoming classification requests
 “””
 text: str = Field(…, min_length=1, max_length=10000)
 user_id: int = Field(…, gt=0)
 language: Optional[str] = Field(default=”en”, regex=”^[a-z]{2}$”)
 metadata: Optional[dict] = Field(default_factory=dict)</p>

<p>@validator(‘text’)
 def text_not_empty(cls, v):
 if not v or v.isspace():
 raise ValueError(‘Text cannot be empty or whitespace only’)
 return v.strip()</p>

<p>@validator(‘text’)
 def text_length_check(cls, v):
 if len(v) &gt; 10000:
 # Truncate instead of rejecting
 return v[:10000]
 return v</p>

<p>@validator(‘metadata’)
 def metadata_size_check(cls, v):
 if v and len(str(v)) &gt; 1000:
 raise ValueError(‘Metadata too large’)
 return v</p>

<p>class Config:
 # Example for API docs
 schema_extra = {
 “example”: {
 “text”: “Check out this amazing offer!”,
 “user_id”: 12345,
 “language”: “en”,
 “metadata”: {“platform”: “web”}
 }
 }</p>

<h1 id="usage-in-api-endpoint">Usage in API endpoint</h1>
<p>from fastapi import FastAPI, HTTPException</p>

<p>app = FastAPI()</p>

<p>@app.post(“/classify”)
async def classify(request: ClassificationRequest):
 try:
 # Pydantic automatically validates
 result = await classifier.predict(request)
 return result
 except ValueError as e:
 raise HTTPException(status_code=400, detail=str(e))
``</p>

<h3 id="input-sanitization">Input Sanitization</h3>

<p>``python
import html
import unicodedata</p>

<p>def sanitize_text(text: str) -&gt; str:
 “””
 Clean and normalize input text
 “””
 # HTML unescape
 text = html.unescape(text)</p>

<p># Unicode normalization (NFKC = compatibility composition)
 text = unicodedata.normalize(‘NFKC’, text)</p>

<p># Remove control characters
 text = ‘‘.join(ch for ch in text if unicodedata.category(ch)[0] != ‘C’ or ch in ‘\n\r\t’)</p>

<p># Normalize whitespace
 text = ‘ ‘.join(text.split())</p>

<p>return text</p>

<h1 id="example">Example</h1>
<p>text = “Hello\u00A0world” # Non-breaking space
clean = sanitize_text(text) # “Hello world”
``</p>

<hr />

<h2 id="component-2-feature-engineering">Component 2: Feature Engineering</h2>

<h3 id="feature-store-pattern">Feature Store Pattern</h3>

<p>``python
from typing import Dict, Any
import redis
import json
from datetime import timedelta</p>

<p>class FeatureStore:
 “””
 Centralized feature storage with caching
 “””
 def <strong>init</strong>(self, redis_client: redis.Redis):
 self.redis = redis_client
 self.default_ttl = 3600 # 1 hour</p>

<p>def get_user_features(self, user_id: int) -&gt; Dict[str, Any]:
 “””
 Get cached user features or compute
 “””
 cache_key = f”features:user:{user_id}”</p>

<p># Try cache
 cached = self.redis.get(cache_key)
 if cached:
 return json.loads(cached)</p>

<p># Compute expensive features
 features = self._compute_user_features(user_id)</p>

<p># Cache for future requests
 self.redis.setex(
 cache_key,
 self.default_ttl,
 json.dumps(features)
 )</p>

<p>return features</p>

<p>def _compute_user_features(self, user_id: int) -&gt; Dict[str, Any]:
 “””
 Compute user-level features (expensive)
 “””
 # Query database
 user = db.get_user(user_id)</p>

<p>return {
 # Profile features
 ‘account_age_days’: (datetime.now() - user.created_at).days,
 ‘verified’: user.is_verified,
 ‘follower_count’: user.followers,</p>

<p># Behavioral features (aggregated)
 ‘messages_sent_7d’: self._count_messages(user_id, days=7),
 ‘spam_reports_received’: user.spam_reports,
 ‘avg_message_length’: user.avg_message_length,</p>

<p># Engagement features
 ‘reply_rate’: user.replies_received / max(user.messages_sent, 1),
 ‘block_rate’: user.blocks_received / max(user.messages_sent, 1)
 }</p>

<p>def extract_text_features(self, text: str) -&gt; Dict[str, Any]:
 “””
 Extract real-time text features (fast, no caching needed)
 “””
 return {
 # Length features
 ‘char_count’: len(text),
 ‘word_count’: len(text.split()),
 ‘avg_word_length’: sum(len(w) for w in text.split()) / len(text.split()),</p>

<p># Pattern features
 ‘url_count’: text.count(‘http’),
 ‘email_count’: text.count(‘@’),
 ‘exclamation_count’: text.count(‘!’),
 ‘question_count’: text.count(‘?’),
 ‘capital_ratio’: sum(c.isupper() for c in text) / len(text),</p>

<p># Linguistic features
 ‘unique_word_ratio’: len(set(text.lower().split())) / len(text.split()),
 ‘repeated_char_ratio’: self._count_repeated_chars(text) / len(text)
 }</p>

<p>def _count_repeated_chars(self, text: str) -&gt; int:
 “"”Count characters repeated 3+ times (e.g., ‘hellooo’)”””
 import re
 matches = re.findall(r’(.)\1{2,}’, text)
 return len(matches)
``</p>

<h3 id="feature-transformation-pipeline">Feature Transformation Pipeline</h3>

<p>``python
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np</p>

<p>class FeatureTransformer:
 “””
 Transform raw features into model-ready format
 “””
 def <strong>init</strong>(self):
 # Fit on training data
 self.scaler = StandardScaler()
 self.tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))</p>

<p># Feature names for debugging
 self.numerical_features = [
 ‘account_age_days’, ‘follower_count’, ‘messages_sent_7d’,
 ‘char_count’, ‘word_count’, ‘url_count’, ‘exclamation_count’,
 ‘capital_ratio’, ‘unique_word_ratio’
 ]</p>

<p>def transform(self, user_features: Dict, text_features: Dict, text: str) -&gt; np.ndarray:
 “””
 Combine and transform all features
 “””
 # Numerical features
 numerical = np.array([
 user_features.get(f, 0.0) for f in self.numerical_features
 ])
 numerical_scaled = self.scaler.transform(numerical.reshape(1, -1))</p>

<p># Text features (TF-IDF)
 text_vec = self.tfidf.transform([text]).toarray()</p>

<p># Concatenate all features
 features = np.concatenate([
 numerical_scaled,
 text_vec
 ], axis=1)</p>

<p>return features[0] # Return 1D array</p>

<p>def get_feature_names(self) -&gt; list:
 “"”Get all feature names for explainability”””
 return self.numerical_features + list(self.tfidf.get_feature_names_out())
``</p>

<hr />

<h2 id="component-3-model-serving">Component 3: Model Serving</h2>

<h3 id="multi-model-serving-with-ab-testing">Multi-Model Serving with A/B Testing</h3>

<p>``python
from typing import Tuple
import hashlib
import torch</p>

<p>class ModelServer:
 “””
 Serve multiple model versions with A/B testing
 “””
 def <strong>init</strong>(self):
 # Load models
 self.models = {
 ‘v1’: torch.jit.load(‘spam_classifier_v1.pt’),
 ‘v2’: torch.jit.load(‘spam_classifier_v2.pt’)
 }</p>

<p># Traffic split (%)
 self.traffic_split = {
 ‘v1’: 90,
 ‘v2’: 10
 }</p>

<p># Model metadata
 self.model_info = {
 ‘v1’: {‘deployed_at’: ‘2025-01-01’, ‘training_accuracy’: 0.92},
 ‘v2’: {‘deployed_at’: ‘2025-01-15’, ‘training_accuracy’: 0.94}
 }</p>

<p>def select_model(self, user_id: int) -&gt; str:
 “””
 Consistent hashing for A/B test assignment</p>

<p>Same user always gets same model (important for consistency)
 “””
 # Hash user_id to [0, 99]
 hash_val = int(hashlib.md5(str(user_id).encode()).hexdigest(), 16)
 bucket = hash_val % 100</p>

<p># Assign to model based on traffic split
 if bucket &lt; self.traffic_split[‘v1’]:
 return ‘v1’
 else:
 return ‘v2’</p>

<p>def predict(self, features: np.ndarray, user_id: int) -&gt; Tuple[int, np.ndarray, str]:
 “””
 Run inference with selected model</p>

<p>Returns:
 prediction: Class label (0 or 1)
 probabilities: Class probabilities
 model_version: Which model was used
 “””
 # Select model
 model_version = self.select_model(user_id)
 model = self.models[model_version]</p>

<p># Convert to tensor
 features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)</p>

<p># Inference
 with torch.no_grad():
 logits = model(features_tensor)
 probabilities = torch.softmax(logits, dim=1).numpy()[0]
 prediction = int(np.argmax(probabilities))</p>

<p>return prediction, probabilities, model_version
``</p>

<h3 id="model-caching">Model Caching</h3>

<p>``python
from functools import lru_cache
import hashlib</p>

<p>class CachedModelServer:
 “””
 Cache predictions for identical inputs
 “””
 def <strong>init</strong>(self, model_server: ModelServer, cache_size=10000):
 self.model_server = model_server
 self.cache_size = cache_size</p>

<p>def _feature_hash(self, features: np.ndarray) -&gt; str:
 “"”Create hash of feature vector”””
 return hashlib.sha256(features.tobytes()).hexdigest()</p>

<p>@lru_cache(maxsize=10000)
 def predict_cached(self, feature_hash: str, user_id: int) -&gt; Tuple:
 “"”Cached prediction (won’t actually work with mutable args, just illustrative)”””
 # In practice, use Redis or Memcached for distributed caching
 pass</p>

<p>def predict(self, features: np.ndarray, user_id: int) -&gt; Tuple:
 “””
 Try cache first, fallback to model
 “””
 feature_hash = self._feature_hash(features)
 cache_key = f”pred:{feature_hash}:{user_id}”</p>

<p># Try Redis cache
 cached = redis_client.get(cache_key)
 if cached:
 return json.loads(cached)</p>

<p># Cache miss - run model
 prediction, probabilities, model_version = self.model_server.predict(
 features, user_id
 )</p>

<p># Cache result (5 minute TTL)
 result = (prediction, probabilities.tolist(), model_version)
 redis_client.setex(cache_key, 300, json.dumps(result))</p>

<p>return result
``</p>

<hr />

<h2 id="component-4-post-processing">Component 4: Post-Processing</h2>

<h3 id="threshold-optimization">Threshold Optimization</h3>

<p>``python
from sklearn.metrics import precision_recall_curve, f1_score
import numpy as np</p>

<p>class ThresholdOptimizer:
 “””
 Find optimal classification threshold
 “””
 def <strong>init</strong>(self, target_precision=0.95):
 self.target_precision = target_precision
 self.threshold = 0.5 # Default</p>

<p>def optimize(self, y_true: np.ndarray, y_proba: np.ndarray) -&gt; float:
 “””
 Find threshold that maximizes recall while maintaining precision</p>

<p>Common in spam detection: high precision required (few false positives)
 “””
 precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)</p>

<p># Find highest recall where precision &gt;= target
 valid_indices = np.where(precisions &gt;= self.target_precision)[0]</p>

<p>if len(valid_indices) == 0:
 print(f”Warning: Cannot achieve {self.target_precision} precision”)
 # Fall back to threshold that maximizes F1
 f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
 best_idx = np.argmax(f1_scores)
 self.threshold = thresholds[best_idx]
 else:
 # Choose threshold with maximum recall among valid options
 best_idx = valid_indices[np.argmax(recalls[valid_indices])]
 self.threshold = thresholds[best_idx]</p>

<p>print(f”Optimal threshold: {self.threshold:.3f}”)
 print(f”Precision: {precisions[best_idx]:.3f}, Recall: {recalls[best_idx]:.3f}”)</p>

<p>return self.threshold</p>

<p>def predict(self, probabilities: np.ndarray) -&gt; np.ndarray:
 “"”Apply optimized threshold”””
 return (probabilities &gt;= self.threshold).astype(int)
``</p>

<h3 id="calibration">Calibration</h3>

<p>``python
from sklearn.calibration import CalibratedClassifierCV</p>

<p>class CalibratedClassifier:
 “””
 Ensure predicted probabilities match actual frequencies</p>

<p>Example: If model predicts 70% spam, ~70% should actually be spam
 “””
 def <strong>init</strong>(self, base_model):
 # Wrap model with calibration
 self.calibrated_model = CalibratedClassifierCV(
 base_model,
 method=’sigmoid’, # or ‘isotonic’
 cv=5
 )</p>

<p>def fit(self, X, y):
 “"”Train with calibration”””
 self.calibrated_model.fit(X, y)</p>

<p>def predict_proba(self, X):
 “"”Return calibrated probabilities”””
 return self.calibrated_model.predict_proba(X)</p>

<h1 id="before-calibration">Before calibration:</h1>
<h1 id="predicted-80-spam--actually-65-spam-overconfident">Predicted 80% spam → Actually 65% spam (overconfident)</h1>

<h1 id="after-calibration">After calibration:</h1>
<h1 id="predicted-80-spam--actually-78-spam-calibrated">Predicted 80% spam → Actually 78% spam (calibrated)</h1>
<p>``</p>

<hr />

<h2 id="component-5-explainability">Component 5: Explainability</h2>

<h3 id="shap-values">SHAP Values</h3>

<p>``python
import shap</p>

<p>class ExplainableClassifier:
 “””
 Generate explanations for predictions
 “””
 def <strong>init</strong>(self, model, feature_names):
 self.model = model
 self.feature_names = feature_names</p>

<p># Initialize SHAP explainer
 self.explainer = shap.TreeExplainer(model)</p>

<p>def explain(self, features: np.ndarray, top_k=3) -&gt; str:
 “””
 Generate human-readable explanation
 “””
 # Compute SHAP values
 shap_values = self.explainer.shap_values(features.reshape(1, -1))</p>

<p># Get top contributing features
 feature_contributions = list(zip(
 self.feature_names,
 shap_values[0]
))
 feature_contributions.sort(key=lambda x: abs(x[1]), reverse=True)</p>

<p># Format explanation
 top_features = feature_contributions[:top_k]
 explanation = “Key factors: “
 explanation += “, “.join([
 f”{name} ({value:+.3f})”
 for name, value in top_features
 ])</p>

<p>return explanation</p>

<h1 id="example-output">Example output:</h1>
<h1 id="key-factors-url_count-0234-capital_ratio-0156-exclamation_count-0089">“Key factors: url_count (+0.234), capital_ratio (+0.156), exclamation_count (+0.089)”</h1>
<p>``</p>

<h3 id="rule-based-explanations">Rule-Based Explanations</h3>

<p>``python
def generate_explanation(features: Dict, prediction: int) -&gt; str:
 “””
 Simple rule-based explanation (faster than SHAP)
 “””
 if prediction == 1: # Spam
 reasons = []</p>

<p>if features[‘url_count’] &gt; 2:
 reasons.append(“contains multiple URLs”)</p>

<p>if features[‘exclamation_count’] &gt; 3:
 reasons.append(“excessive exclamation marks”)</p>

<p>if features[‘capital_ratio’] &gt; 0.5:
 reasons.append(“too many capital letters”)</p>

<p>if features[‘repeated_char_ratio’] &gt; 0.1:
 reasons.append(“repeated characters”)</p>

<p>if not reasons:
 reasons.append(“multiple spam indicators detected”)</p>

<p>return f”Classified as spam because: {‘, ‘.join(reasons)}”</p>

<p>else: # Not spam
 return “No spam indicators detected”
``</p>

<hr />

<h2 id="monitoring--drift-detection">Monitoring &amp; Drift Detection</h2>

<h3 id="metrics-collection">Metrics Collection</h3>

<p>``python
from prometheus_client import Counter, Histogram, Gauge
import time</p>

<h1 id="define-metrics">Define metrics</h1>
<p>prediction_counter = Counter(
 ‘classification_predictions_total’,
 ‘Total predictions’,
 [‘model_version’, ‘prediction_class’]
)</p>

<p>latency_histogram = Histogram(
 ‘classification_latency_seconds’,
 ‘Prediction latency’,
 buckets=[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]
)</p>

<p>model_confidence = Histogram(
 ‘classification_confidence’,
 ‘Prediction confidence’,
 [‘model_version’]
)</p>

<p>class MonitoredClassifier:
 “””
 Classifier with built-in monitoring
 “””
 def <strong>init</strong>(self, classifier):
 self.classifier = classifier</p>

<p>def predict(self, features, user_id):
 start_time = time.time()</p>

<p># Run prediction
 prediction, probabilities, model_version = self.classifier.predict(
 features, user_id
 )</p>

<p># Record metrics
 latency = time.time() - start_time
 latency_histogram.observe(latency)</p>

<p>prediction_counter.labels(
 model_version=model_version,
 prediction_class=prediction
 ).inc()</p>

<p>confidence = max(probabilities)
 model_confidence.labels(model_version=model_version).observe(confidence)</p>

<p>return prediction, probabilities, model_version
``</p>

<h3 id="data-drift-detection">Data Drift Detection</h3>

<p>``python
from scipy import stats
import numpy as np</p>

<p>class DriftDetector:
 “””
 Detect distribution shift in features
 “””
 def <strong>init</strong>(self, reference_data: np.ndarray, feature_names: list):
 “””
 reference_data: Training data statistics
 “””
 self.reference_stats = {
 feature: {
 ‘mean’: reference_data[:, i].mean(),
 ‘std’: reference_data[:, i].std(),
 ‘min’: reference_data[:, i].min(),
 ‘max’: reference_data[:, i].max()
 }
 for i, feature in enumerate(feature_names)
 }</p>

<p>def detect_drift(self, current_data: np.ndarray, feature_names: list) -&gt; dict:
 “””
 Compare current data to reference distribution</p>

<p>Returns:
 Dictionary of features with significant drift
 “””
 drift_alerts = {}</p>

<p>for i, feature in enumerate(feature_names):
 ref_stats = self.reference_stats[feature]
 current_values = current_data[:, i]</p>

<p># Statistical tests
 # 1. KS test (distribution shift)
 ks_statistic, ks_pvalue = stats.ks_2samp(
 current_values,
 np.random.normal(ref_stats[‘mean’], ref_stats[‘std’], len(current_values))
 )</p>

<p># 2. Mean shift (Z-score)
 current_mean = current_values.mean()
 z_score = abs(current_mean - ref_stats[‘mean’]) / (ref_stats[‘std’] + 1e-10)</p>

<p># Alert if significant drift
 if ks_pvalue &lt; 0.01 or z_score &gt; 3:
 drift_alerts[feature] = {
 ‘z_score’: z_score,
 ‘ks_pvalue’: ks_pvalue,
 ‘current_mean’: current_mean,
 ‘reference_mean’: ref_stats[‘mean’]
 }</p>

<p>return drift_alerts</p>

<h1 id="usage">Usage</h1>
<p>detector = DriftDetector(training_data, feature_names)</p>

<h1 id="check-daily">Check daily</h1>
<p>current_batch = get_last_24h_features()
drift = detector.detect_drift(current_batch, feature_names)</p>

<p>if drift:
 send_alert(f”Drift detected in features: {list(drift.keys())}”)
 trigger_model_retraining()
``</p>

<hr />

<h2 id="deployment-strategies">Deployment Strategies</h2>

<h3 id="blue-green-deployment">Blue-Green Deployment</h3>

<p>``python
class BlueGreenDeployment:
 “””
 Zero-downtime deployment with instant rollback
 “””
 def <strong>init</strong>(self):
 self.models = {
 ‘blue’: None, # Current production
 ‘green’: None # New version
 }
 self.active = ‘blue’</p>

<p>def deploy_new_version(self, new_model):
 “””
 Deploy to green environment
 “””
 inactive = ‘green’ if self.active == ‘blue’ else ‘blue’</p>

<p># Load new model to inactive environment
 print(f”Loading new model to {inactive}…”)
 self.models[inactive] = new_model</p>

<p># Run smoke tests
 if not self.smoke_test(inactive):
 print(“Smoke tests failed! Keeping current version.”)
 return False</p>

<p># Switch traffic
 print(f”Switching traffic from {self.active} to {inactive}”)
 self.active = inactive</p>

<p>return True</p>

<p>def smoke_test(self, environment: str) -&gt; bool:
 “””
 Basic health checks before switching traffic
 “””
 model = self.models[environment]</p>

<p># Test with sample inputs
 test_cases = load_test_cases()</p>

<p>for input_data, expected_output in test_cases:
 try:
 output = model.predict(input_data)
 if output is None:
 return False
 except Exception as e:
 print(f”Smoke test failed: {e}”)
 return False</p>

<p>return True</p>

<p>def rollback(self):
 “””
 Instant rollback to previous version
 “””
 old = self.active
 self.active = ‘green’ if self.active == ‘blue’ else ‘blue’
 print(f”Rolled back from {old} to {self.active}”)
``</p>

<hr />

<h2 id="complete-example-spam-classifier-service">Complete Example: Spam Classifier Service</h2>

<p>``python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import asyncio</p>

<p>app = FastAPI(title=”Spam Classification Service”)</p>

<h1 id="initialize-components">Initialize components</h1>
<p>feature_store = FeatureStore(redis_client)
feature_transformer = FeatureTransformer()
model_server = ModelServer()
threshold_optimizer = ThresholdOptimizer(target_precision=0.95)
explainer = ExplainableClassifier(model_server.models[‘v1’], feature_names)</p>

<p>class SpamRequest(BaseModel):
 text: str
 user_id: int</p>

<p>class SpamResponse(BaseModel):
 is_spam: bool
 confidence: float
 explanation: str
 model_version: str
 latency_ms: float</p>

<p>@app.post(“/classify”, response_model=SpamResponse)
async def classify_message(request: SpamRequest):
 “””
 Main classification endpoint
 “””
 start_time = time.time()</p>

<p>try:
 # 1. Sanitize input
 clean_text = sanitize_text(request.text)</p>

<p># 2. Feature engineering (parallel)
 user_features_task = asyncio.create_task(
 asyncio.to_thread(feature_store.get_user_features, request.user_id)
 )
 text_features = feature_store.extract_text_features(clean_text)
 user_features = await user_features_task</p>

<p># 3. Transform features
 features = feature_transformer.transform(
 user_features,
 text_features,
 clean_text
 )</p>

<p># 4. Model inference
 prediction, probabilities, model_version = model_server.predict(
 features,
 request.user_id
 )</p>

<p># 5. Apply threshold
 is_spam = threshold_optimizer.predict(probabilities[1])
 confidence = float(probabilities[1])</p>

<p># 6. Generate explanation
 explanation = explainer.explain(features)</p>

<p># 7. Calculate latency
 latency_ms = (time.time() - start_time) * 1000</p>

<p># 8. Log prediction (async)
 asyncio.create_task(log_prediction(
 request, prediction, confidence, model_version
 ))</p>

<p>return SpamResponse(
 is_spam=bool(is_spam),
 confidence=confidence,
 explanation=explanation,
 model_version=model_version,
 latency_ms=latency_ms
 )</p>

<p>except Exception as e:
 # Log error
 logger.error(f”Classification error: {e}”, exc_info=True)
 raise HTTPException(status_code=500, detail=”Classification failed”)</p>

<p>async def log_prediction(request, prediction, confidence, model_version):
 “””
 Async logging to Kafka
 “””
 log_entry = {
 ‘timestamp’: datetime.now().isoformat(),
 ‘user_id’: request.user_id,
 ‘text_hash’: hashlib.sha256(request.text.encode()).hexdigest(),
 ‘prediction’: int(prediction),
 ‘confidence’: float(confidence),
 ‘model_version’: model_version
 }</p>

<p>kafka_producer.send(‘predictions’, json.dumps(log_entry))
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Feature stores</strong> centralize feature computation and caching 
✅ <strong>A/B testing</strong> enables safe model rollouts with consistent user assignment 
✅ <strong>Threshold optimization</strong> balances precision/recall for business needs 
✅ <strong>Monitoring</strong> catches drift and performance degradation early 
✅ <strong>Explainability</strong> builds trust and aids debugging
✅ <strong>Deployment strategies</strong> enable zero-downtime updates and instant rollback</p>

<hr />

<h2 id="further-reading">Further Reading</h2>

<p><strong>Papers:</strong></p>
<ul>
  <li><a href="https://developers.google.com/machine-learning/guides/rules-of-ml">Rules of Machine Learning (Google)</a></li>
  <li><a href="https://www.uber.com/blog/michelangelo-machine-learning-platform/">Michelangelo: Uber’s ML Platform</a></li>
  <li><a href="https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d">Airbnb’s ML Infrastructure</a></li>
</ul>

<p><strong>Tools:</strong></p>
<ul>
  <li><a href="https://mlflow.org/">MLflow</a> - ML lifecycle management</li>
  <li><a href="https://feast.dev/">Feast</a> - Feature store</li>
  <li><a href="https://www.bentoml.com/">BentoML</a> - Model serving</li>
  <li><a href="https://www.evidentlyai.com/">Evidently</a> - ML monitoring</li>
</ul>

<p><strong>Books:</strong></p>
<ul>
  <li><em>Machine Learning Design Patterns</em> (Lakshmanan et al.)</li>
  <li><em>Designing Machine Learning Systems</em> (Chip Huyen)</li>
</ul>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0002-classification-pipeline/">arunbaby.com/ml-system-design/0002-classification-pipeline</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#classification" class="page__taxonomy-item p-category" rel="tag">classification</a><span class="sep">, </span>
    
      <a href="/tags/#pipeline" class="page__taxonomy-item p-category" rel="tag">pipeline</a><span class="sep">, </span>
    
      <a href="/tags/#production-ml" class="page__taxonomy-item p-category" rel="tag">production-ml</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0002-valid-parentheses/" rel="permalink">Valid Parentheses
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Why a simple stack solves bracket matching, expression parsing, and even neural network depth management in one elegant pattern.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0002-speech-classification/" rel="permalink">Speech Command Classification
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          21 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How voice assistants recognize “turn on the lights” from raw audio in under 100ms without full ASR transcription.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0002-llm-capabilities-for-agents/" rel="permalink">LLM Capabilities for Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The Engine of Autonomy: Understanding the Agentic ‘Brain’.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Classification+Pipeline+Design%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0002-classification-pipeline%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0002-classification-pipeline%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0002-classification-pipeline/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0001-recommendation-system/" class="pagination--pager" title="Recommendation System: Candidate Retrieval">Previous</a>
    
    
      <a href="/ml-system-design/0003-data-preprocessing/" class="pagination--pager" title="Data Preprocessing Pipeline Design">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
