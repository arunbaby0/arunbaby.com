<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Caching Strategies for ML Systems - Arun Baby</title>
<meta name="description" content="Design efficient caching layers for ML systems to reduce latency, save compute costs, and improve user experience at scale.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Caching Strategies for ML Systems">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0010-caching-strategies/">


  <meta property="og:description" content="Design efficient caching layers for ML systems to reduce latency, save compute costs, and improve user experience at scale.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Caching Strategies for ML Systems">
  <meta name="twitter:description" content="Design efficient caching layers for ML systems to reduce latency, save compute costs, and improve user experience at scale.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0010-caching-strategies/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T16:05:30+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0010-caching-strategies/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Caching Strategies for ML Systems">
    <meta itemprop="description" content="Design efficient caching layers for ML systems to reduce latency, save compute costs, and improve user experience at scale.">
    <meta itemprop="datePublished" content="2025-12-29T16:05:30+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0010-caching-strategies/" itemprop="url">Caching Strategies for ML Systems
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          26 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#cache-hierarchy">Cache Hierarchy</a></li><li><a href="#cache-eviction-policies">Cache Eviction Policies</a><ul><li><a href="#lru-least-recently-used">LRU (Least Recently Used)</a></li><li><a href="#lfu-least-frequently-used">LFU (Least Frequently Used)</a></li><li><a href="#ttl-time-to-live-cache">TTL (Time-To-Live) Cache</a></li></ul></li><li><a href="#distributed-caching">Distributed Caching</a><ul><li><a href="#redis-based-cache">Redis-Based Cache</a></li><li><a href="#multi-level-cache">Multi-Level Cache</a></li></ul></li><li><a href="#cache-warming-strategies">Cache Warming Strategies</a><ul><li><a href="#proactive-cache-warming">Proactive Cache Warming</a></li></ul></li><li><a href="#cache-invalidation">Cache Invalidation</a><ul><li><a href="#push-based-invalidation">Push-Based Invalidation</a></li></ul></li><li><a href="#feature-store-caching">Feature Store Caching</a></li><li><a href="#connection-to-linked-lists-dsa">Connection to Linked Lists (DSA)</a></li><li><a href="#understanding-cache-performance">Understanding Cache Performance</a><ul><li><a href="#cache-hit-rate-analysis">Cache Hit Rate Analysis</a></li><li><a href="#cache-size-optimization">Cache Size Optimization</a></li></ul></li><li><a href="#advanced-caching-patterns">Advanced Caching Patterns</a><ul><li><a href="#write-through-vs-write-back-cache">Write-Through vs Write-Back Cache</a></li><li><a href="#cache-aside-pattern">Cache Aside Pattern</a></li></ul></li><li><a href="#cache-stampede-prevention">Cache Stampede Prevention</a><ul><li><a href="#problem-thundering-herd">Problem: Thundering Herd</a></li><li><a href="#probabilistic-early-expiration">Probabilistic Early Expiration</a></li></ul></li><li><a href="#distributed-cache-challenges">Distributed Cache Challenges</a><ul><li><a href="#cache-consistency">Cache Consistency</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Design efficient caching layers for ML systems to reduce latency, save compute costs, and improve user experience at scale.</strong></p>

<h2 id="introduction">Introduction</h2>

<p><strong>Caching</strong> temporarily stores computed results to serve future requests faster. In ML systems, caching is critical for:</p>

<p><strong>Why caching matters:</strong></p>
<ul>
  <li><strong>Latency reduction:</strong> ms instead of seconds for predictions</li>
  <li><strong>Cost savings:</strong> Avoid expensive model inference</li>
  <li><strong>Scalability:</strong> Handle more requests with same resources</li>
  <li><strong>Availability:</strong> Serve cached results if model service is down</li>
</ul>

<p><strong>Common caching scenarios in ML:</strong></p>
<ul>
  <li>Model predictions (feature → prediction)</li>
  <li>Feature computations (raw data → engineered features)</li>
  <li>Embeddings (entity → vector representation)</li>
  <li>Model artifacts (model weights, config)</li>
  <li>Training data (preprocessed datasets)</li>
</ul>

<hr />

<h2 id="cache-hierarchy">Cache Hierarchy</h2>

<p><code class="language-plaintext highlighter-rouge">
┌────────────────────────────────────────────────────┐
│ Client/Browser │
│ (Local Storage, Cookies) │
└──────────────────────┬─────────────────────────────┘
 │
 ▼
┌────────────────────────────────────────────────────┐
│ CDN Cache │
│ (CloudFlare, Akamai, CloudFront) │
└──────────────────────┬─────────────────────────────┘
 │
 ▼
┌────────────────────────────────────────────────────┐
│ Application Cache │
│ (Redis, Memcached, Local) │
└──────────────────────┬─────────────────────────────┘
 │
 ▼
┌────────────────────────────────────────────────────┐
│ ML Model Service │
│ (TensorFlow Serving, etc.) │
└──────────────────────┬─────────────────────────────┘
 │
 ▼
┌────────────────────────────────────────────────────┐
│ Database │
│ (PostgreSQL, MongoDB, etc.) │
└────────────────────────────────────────────────────┘
</code></p>

<hr />

<h2 id="cache-eviction-policies">Cache Eviction Policies</h2>

<h3 id="lru-least-recently-used">LRU (Least Recently Used)</h3>

<p><strong>Most common for ML systems</strong></p>

<p>``python
from collections import OrderedDict</p>

<p>class LRUCache:
 “””
 LRU Cache implementation</p>

<p>Evicts least recently used items when capacity is reached
 “””</p>

<p>def <strong>init</strong>(self, capacity: int):
 self.cache = OrderedDict()
 self.capacity = capacity</p>

<p>def get(self, key):
 “””
 Get value and mark as recently used</p>

<p>Time: O(1)
 “””
 if key not in self.cache:
 return None</p>

<p># Move to end (most recent)
 self.cache.move_to_end(key)
 return self.cache[key]</p>

<p>def put(self, key, value):
 “””
 Put key-value pair</p>

<p>Time: O(1)
 “””
 if key in self.cache:
 # Update and move to end
 self.cache.move_to_end(key)</p>

<p>self.cache[key] = value</p>

<p># Evict if over capacity
 if len(self.cache) &gt; self.capacity:
 # Remove first item (least recently used)
 self.cache.popitem(last=False)</p>

<p>def stats(self):
 “"”Get cache statistics”””
 return {
 ‘size’: len(self.cache),
 ‘capacity’: self.capacity,
 ‘utilization’: len(self.cache) / self.capacity
 }</p>

<h1 id="usage">Usage</h1>
<p>cache = LRUCache(capacity=1000)</p>

<h1 id="cache-predictions">Cache predictions</h1>
<p>def get_prediction_cached(features, model):
 cache_key = hash(tuple(features))</p>

<p># Check cache
 cached_result = cache.get(cache_key)
 if cached_result is not None:
 return cached_result</p>

<p># Compute prediction
 prediction = model.predict([features])[0]</p>

<p># Cache result
 cache.put(cache_key, prediction)</p>

<p>return prediction
``</p>

<h3 id="lfu-least-frequently-used">LFU (Least Frequently Used)</h3>

<p><strong>Good for skewed access patterns</strong></p>

<p>``python
from collections import defaultdict
import heapq</p>

<p>class LFUCache:
 “””
 LFU Cache - evicts least frequently used items</p>

<p>Better for “hot” items that are accessed repeatedly
 “””</p>

<p>def <strong>init</strong>(self, capacity: int):
 self.capacity = capacity
 self.cache = {} # key -&gt; (value, frequency)
 self.freq_map = defaultdict(set) # frequency -&gt; set of keys
 self.min_freq = 0
 self.access_count = 0</p>

<p>def get(self, key):
 “"”Get value and increment frequency”””
 if key not in self.cache:
 return None</p>

<p>value, freq = self.cache[key]</p>

<p># Update frequency
 self.freq_map[freq].remove(key)
 if not self.freq_map[freq] and freq == self.min_freq:
 self.min_freq += 1</p>

<p>new_freq = freq + 1
 self.freq_map[new_freq].add(key)
 self.cache[key] = (value, new_freq)</p>

<p>return value</p>

<p>def put(self, key, value):
 “"”Put key-value pair”””
 if self.capacity == 0:
 return</p>

<p>if key in self.cache:
 # Update existing key
 _, freq = self.cache[key]
 self.cache[key] = (value, freq)
 self.get(key) # Update frequency
 return</p>

<p># Evict if at capacity
 if len(self.cache) &gt;= self.capacity:
 # Remove item with minimum frequency
 evict_key = next(iter(self.freq_map[self.min_freq]))
 self.freq_map[self.min_freq].remove(evict_key)
 del self.cache[evict_key]</p>

<p># Add new key
 self.cache[key] = (value, 1)
 self.freq_map[1].add(key)
 self.min_freq = 1</p>

<p>def get_top_k(self, k: int):
 “"”Get top k most frequently accessed items”””
 items = [(freq, key) for key, (val, freq) in self.cache.items()]
 return heapq.nlargest(k, items)</p>

<h1 id="usage-for-embeddings-frequently-accessed">Usage for embeddings (frequently accessed)</h1>
<p>embedding_cache = LFUCache(capacity=10000)</p>

<p>def get_embedding_cached(entity_id, embedding_model):
 cached_emb = embedding_cache.get(entity_id)
 if cached_emb is not None:
 return cached_emb</p>

<p>embedding = embedding_model.encode(entity_id)
 embedding_cache.put(entity_id, embedding)</p>

<p>return embedding
``</p>

<h3 id="ttl-time-to-live-cache">TTL (Time-To-Live) Cache</h3>

<p><strong>Good for time-sensitive data</strong></p>

<p>``python
import time</p>

<p>class TTLCache:
 “””
 TTL Cache - items expire after specified time</p>

<p>Perfect for:</p>
<ul>
  <li>User sessions</li>
  <li>Real-time features (stock prices, weather)</li>
  <li>Model predictions that become stale
 “””</li>
</ul>

<p>def <strong>init</strong>(self, default_ttl_seconds=3600):
 self.cache = {} # key -&gt; (value, expiration_time)
 self.default_ttl = default_ttl_seconds</p>

<p>def get(self, key):
 “"”Get value if not expired”””
 if key not in self.cache:
 return None</p>

<p>value, expiration = self.cache[key]</p>

<p># Check expiration
 if time.time() &gt; expiration:
 del self.cache[key]
 return None</p>

<p>return value</p>

<p>def put(self, key, value, ttl=None):
 “"”Put key-value pair with TTL”””
 if ttl is None:
 ttl = self.default_ttl</p>

<p>expiration = time.time() + ttl
 self.cache[key] = (value, expiration)</p>

<p>def cleanup(self):
 “"”Remove expired entries”””
 current_time = time.time()
 expired_keys = [
 k for k, (v, exp) in self.cache.items()
 if current_time &gt; exp
 ]</p>

<p>for key in expired_keys:
 del self.cache[key]</p>

<p>return len(expired_keys)</p>

<h1 id="usage-for-time-sensitive-predictions">Usage for time-sensitive predictions</h1>
<p>prediction_cache = TTLCache(default_ttl_seconds=300) # 5 minutes</p>

<p>def predict_stock_price(symbol, model):
 “"”Predictions expire quickly for real-time data”””
 cached = prediction_cache.get(symbol)
 if cached is not None:
 return cached</p>

<p>prediction = model.predict(symbol)
 prediction_cache.put(symbol, prediction, ttl=60) # 1 minute TTL</p>

<p>return prediction
``</p>

<hr />

<h2 id="distributed-caching">Distributed Caching</h2>

<h3 id="redis-based-cache">Redis-Based Cache</h3>

<p>``python
import redis
import json
import pickle
import hashlib</p>

<p>class RedisMLCache:
 “””
 Redis-based cache for ML predictions</p>

<p>Features:</p>
<ul>
  <li>Distributed across multiple servers</li>
  <li>Persistence</li>
  <li>TTL support</li>
  <li>Pub/sub for cache invalidation
 “””</li>
</ul>

<p>def <strong>init</strong>(self, host=’localhost’, port=6379, db=0):
 self.redis_client = redis.Redis(
 host=host,
 port=port,
 db=db,
 decode_responses=False
 )</p>

<p>self.hits = 0
 self.misses = 0</p>

<p>def _serialize(self, obj):
 “"”Serialize Python object”””
 return pickle.dumps(obj)</p>

<p>def _deserialize(self, data):
 “"”Deserialize to Python object”””
 if data is None:
 return None
 return pickle.loads(data)</p>

<p>def _make_key(self, prefix, *args):
 “"”Generate cache key”””
 # Hash arguments for consistent key
 key_str = f”{prefix}:{‘:’.join(map(str, args))}”
 return key_str</p>

<p>def get_prediction(self, model_id, features):
 “””
 Get cached prediction</p>

<p>Args:
 model_id: Model identifier
 features: Feature vector (hashable)</p>

<p>Returns:
 Cached prediction or None
 “””
 # Create cache key
 feature_hash = hashlib.md5(
 str(features).encode()
 ).hexdigest()
 key = self._make_key(‘prediction’, model_id, feature_hash)</p>

<p># Get from Redis
 cached = self.redis_client.get(key)</p>

<p>if cached is not None:
 self.hits += 1
 return self._deserialize(cached)</p>

<p>self.misses += 1
 return None</p>

<p>def set_prediction(self, model_id, features, prediction, ttl=3600):
 “"”Cache prediction with TTL”””
 feature_hash = hashlib.md5(
 str(features).encode()
 ).hexdigest()
 key = self._make_key(‘prediction’, model_id, feature_hash)</p>

<p># Serialize and store
 value = self._serialize(prediction)
 self.redis_client.setex(key, ttl, value)</p>

<p>def get_embedding(self, entity_id):
 “"”Get cached embedding”””
 key = self._make_key(‘embedding’, entity_id)
 cached = self.redis_client.get(key)</p>

<p>if cached:
 self.hits += 1
 # Embeddings stored as JSON arrays
 return json.loads(cached)</p>

<p>self.misses += 1
 return None</p>

<p>def set_embedding(self, entity_id, embedding, ttl=None):
 “"”Cache embedding”””
 key = self._make_key(‘embedding’, entity_id)
 value = json.dumps(embedding.tolist() if hasattr(embedding, ‘tolist’) else embedding)</p>

<p>if ttl:
 self.redis_client.setex(key, ttl, value)
 else:
 self.redis_client.set(key, value)</p>

<p>def invalidate_model(self, model_id):
 “"”Invalidate all predictions for a model (SCAN + DEL)”””
 pattern = self._make_key(‘prediction’, model_id, ‘*’)
 cursor = 0
 total_deleted = 0</p>

<p>while True:
 cursor, keys = self.redis_client.scan(cursor=cursor, match=pattern, count=1000)
 if keys:
 total_deleted += self.redis_client.delete(*keys)
 if cursor == 0:
 break</p>

<p>return total_deleted</p>

<p>def get_stats(self):
 “"”Get cache statistics”””
 total_requests = self.hits + self.misses
 hit_rate = self.hits / total_requests if total_requests &gt; 0 else 0</p>

<p>return {
 ‘hits’: self.hits,
 ‘misses’: self.misses,
 ‘hit_rate’: hit_rate,
 ‘total_keys’: self.redis_client.dbsize()
 }</p>

<h1 id="usage-1">Usage</h1>
<p>cache = RedisMLCache(host=’localhost’, port=6379)</p>

<p>def predict_with_cache(features, model, model_id):
 “"”Predict with Redis caching”””
 # Check cache
 cached = cache.get_prediction(model_id, features)
 if cached is not None:
 return cached</p>

<p># Compute prediction
 prediction = model.predict([features])[0]</p>

<p># Cache result
 cache.set_prediction(model_id, features, prediction, ttl=3600)</p>

<p>return prediction</p>

<h1 id="check-cache-performance">Check cache performance</h1>
<p>stats = cache.get_stats()
print(f”Cache hit rate: {stats[‘hit_rate’]:.2%}”)
``</p>

<h3 id="multi-level-cache">Multi-Level Cache</h3>

<p>``python
class MultiLevelCache:
 “””
 Multi-level caching with L1 (local) and L2 (Redis)</p>

<p>Pattern:</p>
<ol>
  <li>Check L1 (in-memory, fastest)</li>
  <li>If miss, check L2 (Redis, shared)</li>
  <li>If miss, compute and populate both levels
 “””</li>
</ol>

<p>def <strong>init</strong>(self, l1_capacity=1000, redis_host=’localhost’):
 # L1: Local LRU cache
 self.l1 = LRUCache(capacity=l1_capacity)</p>

<p># L2: Redis cache
 self.l2 = RedisMLCache(host=redis_host)</p>

<p>self.l1_hits = 0
 self.l2_hits = 0
 self.misses = 0</p>

<p>def get(self, key):
 “"”Get value from multi-level cache”””
 # Try L1
 value = self.l1.get(key)
 if value is not None:
 self.l1_hits += 1
 return value</p>

<p># Try L2
 value = self.l2.redis_client.get(key)
 if value is not None:
 self.l2_hits += 1</p>

<p># Populate L1
 value = self.l2._deserialize(value)
 self.l1.put(key, value)</p>

<p>return value</p>

<p># Miss
 self.misses += 1
 return None</p>

<p>def put(self, key, value, ttl=3600):
 “"”Put value in both cache levels”””
 # Store in L1
 self.l1.put(key, value)</p>

<p># Store in L2
 self.l2.redis_client.setex(
 key,
 ttl,
 self.l2._serialize(value)
 )</p>

<p>def get_stats(self):
 “"”Get multi-level cache statistics”””
 total = self.l1_hits + self.l2_hits + self.misses</p>

<p>return {
 ‘l1_hits’: self.l1_hits,
 ‘l2_hits’: self.l2_hits,
 ‘misses’: self.misses,
 ‘total_requests’: total,
 ‘l1_hit_rate’: self.l1_hits / total if total &gt; 0 else 0,
 ‘l2_hit_rate’: self.l2_hits / total if total &gt; 0 else 0,
 ‘overall_hit_rate’: (self.l1_hits + self.l2_hits) / total if total &gt; 0 else 0
 }</p>

<h1 id="usage-2">Usage</h1>
<p>ml_cache = MultiLevelCache(l1_capacity=1000, redis_host=’localhost’)</p>

<p>def get_user_embedding(user_id, embedding_model):
 “"”Get user embedding with multi-level caching”””
 key = f”user_emb:{user_id}”</p>

<p># Try cache
 embedding = ml_cache.get(key)
 if embedding is not None:
 return embedding</p>

<p># Compute
 embedding = embedding_model.encode(user_id)</p>

<p># Cache
 ml_cache.put(key, embedding, ttl=3600)</p>

<p>return embedding
``</p>

<hr />

<h2 id="cache-warming-strategies">Cache Warming Strategies</h2>

<h3 id="proactive-cache-warming">Proactive Cache Warming</h3>

<p>``python
import threading
import time
from queue import Queue</p>

<p>class CacheWarmer:
 “””
 Proactively warm cache before requests arrive</p>

<p>Strategies:</p>
<ol>
  <li>Popular items (based on historical data)</li>
  <li>Scheduled warmup (daily, hourly)</li>
  <li>Predictive warmup (ML-based)
 “””</li>
</ol>

<p>def <strong>init</strong>(self, cache, compute_fn):
 self.cache = cache
 self.compute_fn = compute_fn</p>

<p>self.warmup_queue = Queue()
 self.is_running = False</p>

<p>def warm_popular_items(self, items, priority=’high’):
 “"”Warm cache with popular items”””
 print(f”Warming {len(items)} popular items…”)</p>

<p>for item in items:
 key, args = item</p>

<p># Check if already cached
 if self.cache.get(key) is not None:
 continue</p>

<p># Compute and cache
 try:
 result = self.compute_fn(*args)
 self.cache.put(key, result)
 except Exception as e:
 print(f”Error warming {key}: {e}”)</p>

<p>def warm_on_schedule(self, items, interval_seconds=3600):
 “"”Periodically warm cache”””
 def warmup_worker():
 while self.is_running:
 self.warm_popular_items(items)
 time.sleep(interval_seconds)</p>

<p>self.is_running = True
 worker = threading.Thread(target=warmup_worker, daemon=True)
 worker.start()</p>

<p>def stop(self):
 “"”Stop scheduled warmup”””
 self.is_running = False</p>

<h1 id="usage-3">Usage</h1>
<p>def compute_recommendation(user_id, model):
 “"”Expensive recommendation computation”””
 return model.recommend(user_id, n=10)</p>

<p>cache = LRUCache(capacity=10000)
warmer = CacheWarmer(cache, compute_recommendation)</p>

<h1 id="warm-cache-with-top-1000-users">Warm cache with top 1000 users</h1>
<p>popular_users = get_top_1000_active_users()
items = [
 (f”rec:{user_id}”, (user_id, recommendation_model))
 for user_id in popular_users
]</p>

<p>warmer.warm_popular_items(items)</p>

<h1 id="or-schedule-periodic-warmup">Or schedule periodic warmup</h1>
<p>warmer.warm_on_schedule(items, interval_seconds=3600)
``</p>

<hr />

<h2 id="cache-invalidation">Cache Invalidation</h2>

<h3 id="push-based-invalidation">Push-Based Invalidation</h3>

<p>``python
import redis</p>

<p>class CacheInvalidator:
 “””
 Cache invalidation using Redis Pub/Sub</p>

<p>Pattern:</p>
<ul>
  <li>When model updates, publish invalidation message</li>
  <li>All cache instances subscribe and clear relevant entries
 “””</li>
</ul>

<p>def <strong>init</strong>(self, redis_host=’localhost’):
 self.redis_pub = redis.Redis(host=redis_host)
 self.redis_sub = redis.Redis(host=redis_host)</p>

<p>self.cache = {}
 self.invalidation_count = 0</p>

<p>def subscribe_to_invalidations(self, channel=’cache:invalidate’):
 “"”Subscribe to invalidation messages”””
 pubsub = self.redis_sub.pubsub()
 pubsub.subscribe(channel)</p>

<p>def listen():
 for message in pubsub.listen():
 if message[‘type’] == ‘message’:
 self._handle_invalidation(message[‘data’])</p>

<p># Start listener thread
 listener = threading.Thread(target=listen, daemon=True)
 listener.start()</p>

<p>def _handle_invalidation(self, message):
 “"”Handle invalidation message”””
 # Message format: “model_id:v2”
 invalidation_key = message.decode(‘utf-8’)</p>

<p># Remove matching cache entries
 keys_to_remove = [
 k for k in self.cache.keys()
 if k.startswith(invalidation_key)
 ]</p>

<p>for key in keys_to_remove:
 del self.cache[key]</p>

<p>self.invalidation_count += len(keys_to_remove)
 print(f”Invalidated {len(keys_to_remove)} cache entries”)</p>

<p>def invalidate_model(self, model_id):
 “"”Publish invalidation message”””
 message = f”{model_id}:v”
 self.redis_pub.publish(‘cache:invalidate’, message)</p>

<h1 id="usage-4">Usage</h1>
<p>invalidator = CacheInvalidator()
invalidator.subscribe_to_invalidations()</p>

<h1 id="when-model-is-updated">When model is updated</h1>
<p>def update_model(model_id, new_model):
 “"”Update model and invalidate cache”””
 # Deploy new model
 deploy_model(new_model)</p>

<p># Invalidate all predictions for this model
 invalidator.invalidate_model(model_id)
``</p>

<hr />

<h2 id="feature-store-caching">Feature Store Caching</h2>

<p>``python
class FeatureStoreCache:
 “””
 Caching layer for feature store</p>

<p>Features:</p>
<ul>
  <li>Cache precomputed features</li>
  <li>Batch feature retrieval</li>
  <li>Freshness guarantees
 “””</li>
</ul>

<p>def <strong>init</strong>(self, redis_client, ttl=3600):
 self.redis = redis_client
 self.ttl = ttl</p>

<p>def get_features(self, entity_ids, feature_names):
 “””
 Get features for multiple entities (batch)</p>

<p>Args:
 entity_ids: List of entity IDs
 feature_names: List of feature names</p>

<p>Returns:
 Dict of entity_id -&gt; feature_dict
 “””
 results = {}
 cache_misses = []</p>

<p># Try cache first
 for entity_id in entity_ids:
 cache_key = f”features:{entity_id}”
 cached = self.redis.get(cache_key)</p>

<p>if cached:
 # Parse cached features
 features = json.loads(cached)</p>

<p># Filter to requested features
 filtered = {
 fname: features[fname]
 for fname in feature_names
 if fname in features
 }</p>

<p>if len(filtered) == len(feature_names):
 results[entity_id] = filtered
 else:
 cache_misses.append(entity_id)
 else:
 cache_misses.append(entity_id)</p>

<p># Compute missing features
 if cache_misses:
 computed = self._compute_features(cache_misses, feature_names)</p>

<p># Cache computed features
 for entity_id, features in computed.items():
 self._cache_features(entity_id, features)
 results[entity_id] = features</p>

<p>return results</p>

<p>def _compute_features(self, entity_ids, feature_names):
 “"”Compute features from feature store”””
 # Call actual feature store
 return compute_features_batch(entity_ids, feature_names)</p>

<p>def _cache_features(self, entity_id, features):
 “"”Cache features for entity”””
 cache_key = f”features:{entity_id}”
 self.redis.setex(
 cache_key,
 self.ttl,
 json.dumps(features)
 )</p>

<p>def invalidate_entity(self, entity_id):
 “"”Invalidate features for entity”””
 cache_key = f”features:{entity_id}”
 self.redis.delete(cache_key)</p>

<h1 id="usage-5">Usage</h1>
<p>feature_cache = FeatureStoreCache(redis_client, ttl=300)</p>

<h1 id="get-features-for-batch-of-users">Get features for batch of users</h1>
<p>user_ids = [123, 456, 789]
feature_names = [‘age’, ‘location’, ‘purchase_count’]</p>

<p>features = feature_cache.get_features(user_ids, feature_names)
``</p>

<hr />

<h2 id="connection-to-linked-lists-dsa">Connection to Linked Lists (DSA)</h2>

<p>Cache implementations heavily use linked list concepts:</p>

<p>``python
class DoublyLinkedNode:
 “"”Node for doubly-linked list (used in LRU)”””
 def <strong>init</strong>(self, key, value):
 self.key = key
 self.value = value
 self.prev = None
 self.next = None</p>

<p>class ProductionLRUCache:
 “””
 Production LRU cache using doubly-linked list</p>

<p>Connection to DSA:</p>
<ul>
  <li>Uses linked list for maintaining order</li>
  <li>Pointer manipulation similar to reversal</li>
  <li>O(1) operations through careful pointer management
 “””</li>
</ul>

<p>def <strong>init</strong>(self, capacity: int):
 self.capacity = capacity
 self.cache = {}</p>

<p># Dummy head and tail
 self.head = DoublyLinkedNode(0, 0)
 self.tail = DoublyLinkedNode(0, 0)
 self.head.next = self.tail
 self.tail.prev = self.head</p>

<p>def _add_node(self, node):
 “"”Add node right after head”””
 node.prev = self.head
 node.next = self.head.next</p>

<p>self.head.next.prev = node
 self.head.next = node</p>

<p>def _remove_node(self, node):
 “"”Remove node from list”””
 prev_node = node.prev
 next_node = node.next</p>

<p>prev_node.next = next_node
 next_node.prev = prev_node</p>

<p>def _move_to_head(self, node):
 “"”Move node to head (most recently used)”””
 self._remove_node(node)
 self._add_node(node)</p>

<p>def _pop_tail(self):
 “"”Remove least recently used (tail.prev)”””
 res = self.tail.prev
 self._remove_node(res)
 return res</p>

<p>def get(self, key):
 “"”Get value”””
 node = self.cache.get(key)
 if not node:
 return -1</p>

<p>self._move_to_head(node)
 return node.value</p>

<p>def put(self, key, value):
 “"”Put key-value”””
 node = self.cache.get(key)</p>

<p>if node:
 node.value = value
 self._move_to_head(node)
 else:
 new_node = DoublyLinkedNode(key, value)
 self.cache[key] = new_node
 self._add_node(new_node)</p>

<p>if len(self.cache) &gt; self.capacity:
 tail = self._pop_tail()
 del self.cache[tail.key]
``</p>

<hr />

<h2 id="understanding-cache-performance">Understanding Cache Performance</h2>

<h3 id="cache-hit-rate-analysis">Cache Hit Rate Analysis</h3>

<p>``python
class CachePerformanceAnalyzer:
 “””
 Analyze and optimize cache performance</p>

<p>Key metrics:</p>
<ul>
  <li>Hit rate: % of requests served from cache</li>
  <li>Miss rate: % of requests requiring computation</li>
  <li>Latency reduction: Time saved by caching</li>
  <li>Memory efficiency: Cache size vs hit rate
 “””</li>
</ul>

<p>def <strong>init</strong>(self):
 self.total_requests = 0
 self.cache_hits = 0
 self.cache_misses = 0</p>

<p>self.hit_latencies = []
 self.miss_latencies = []</p>

<p>def record_hit(self, latency_ms):
 “"”Record cache hit”””
 self.cache_hits += 1
 self.total_requests += 1
 self.hit_latencies.append(latency_ms)</p>

<p>def record_miss(self, latency_ms):
 “"”Record cache miss”””
 self.cache_misses += 1
 self.total_requests += 1
 self.miss_latencies.append(latency_ms)</p>

<p>def get_metrics(self):
 “"”Calculate performance metrics”””
 if self.total_requests == 0:
 return {}</p>

<p>hit_rate = self.cache_hits / self.total_requests
 miss_rate = self.cache_misses / self.total_requests</p>

<p>avg_hit_latency = (
 sum(self.hit_latencies) / len(self.hit_latencies)
 if self.hit_latencies else 0
 )</p>

<p>avg_miss_latency = (
 sum(self.miss_latencies) / len(self.miss_latencies)
 if self.miss_latencies else 0
 )</p>

<p># Calculate latency reduction
 avg_latency_with_cache = (
 hit_rate * avg_hit_latency + miss_rate * avg_miss_latency
 )</p>

<p>latency_reduction = (
 (avg_miss_latency - avg_latency_with_cache) / avg_miss_latency
 if avg_miss_latency &gt; 0 else 0
 )</p>

<p>return {
 ‘total_requests’: self.total_requests,
 ‘cache_hits’: self.cache_hits,
 ‘cache_misses’: self.cache_misses,
 ‘hit_rate’: hit_rate,
 ‘miss_rate’: miss_rate,
 ‘avg_hit_latency_ms’: avg_hit_latency,
 ‘avg_miss_latency_ms’: avg_miss_latency,
 ‘avg_overall_latency_ms’: avg_latency_with_cache,
 ‘latency_reduction_pct’: latency_reduction * 100
 }</p>

<p>def print_report(self):
 “"”Print performance report”””
 metrics = self.get_metrics()</p>

<p>print(“\n” + “=”<em>60)
 print(“CACHE PERFORMANCE REPORT”)
 print(“=”</em>60)
 print(f”Total Requests: {metrics[‘total_requests’]:,}”)
 print(f”Cache Hits: {metrics[‘cache_hits’]:,}”)
 print(f”Cache Misses: {metrics[‘cache_misses’]:,}”)
 print(f”Hit Rate: {metrics[‘hit_rate’]:.2%}”)
 print(f”Miss Rate: {metrics[‘miss_rate’]:.2%}”)
 print(f”\nLatency Analysis:”)
 print(f” Cache Hit: {metrics[‘avg_hit_latency_ms’]:.2f} ms”)
 print(f” Cache Miss: {metrics[‘avg_miss_latency_ms’]:.2f} ms”)
 print(f” Overall Average: {metrics[‘avg_overall_latency_ms’]:.2f} ms”)
 print(f” Latency Reduction: {metrics[‘latency_reduction_pct’]:.1f}%”)
 print(“=”*60)</p>

<h1 id="usage-example">Usage example</h1>
<p>analyzer = CachePerformanceAnalyzer()</p>

<h1 id="simulate-requests">Simulate requests</h1>
<p>import random
import time</p>

<p>cache = LRUCache(capacity=100)</p>

<p>for i in range(1000):
 key = f”key_{random.randint(1, 150)}”</p>

<p># Check cache
 start = time.perf_counter()
 value = cache.get(key)</p>

<p>if value is not None:
 # Cache hit (fast)
 latency = (time.perf_counter() - start) * 1000
 analyzer.record_hit(latency)
 else:
 # Cache miss (slow - simulate computation)
 time.sleep(0.001) # 1ms computation
 latency = (time.perf_counter() - start) * 1000
 analyzer.record_miss(latency)</p>

<p># Store in cache
 cache.put(key, f”value_{key}”)</p>

<p>analyzer.print_report()
``</p>

<h3 id="cache-size-optimization">Cache Size Optimization</h3>

<p>``python
class CacheSizeOptimizer:
 “””
 Find optimal cache size for given workload</p>

<p>Trade-off: Larger cache = higher hit rate but more memory
 “””</p>

<p>def <strong>init</strong>(self, workload):
 “””
 Args:
 workload: List of access patterns (keys)
 “””
 self.workload = workload</p>

<p>def find_optimal_size(self, max_size=10000, step=100):
 “””
 Test different cache sizes</p>

<p>Returns optimal size based on diminishing returns
 “””
 results = []</p>

<p>print(“Testing cache sizes…”)
 print(f”{‘Size’:&lt;10} {‘Hit Rate’:&lt;12} {‘Marginal Gain’:&lt;15}”)
 print(“-“ * 40)</p>

<p>prev_hit_rate = 0</p>

<p>for size in range(step, max_size + 1, step):
 hit_rate = self._simulate_cache(size)
 marginal_gain = hit_rate - prev_hit_rate</p>

<p>results.append({
 ‘size’: size,
 ‘hit_rate’: hit_rate,
 ‘marginal_gain’: marginal_gain
 })</p>

<p>print(f”{size:&lt;10} {hit_rate:&lt;12.2%} {marginal_gain:&lt;15.4%}”)</p>

<p>prev_hit_rate = hit_rate</p>

<p># Stop if marginal gain is too small
 if marginal_gain &lt; 0.001: # 0.1% gain
 print(f”\nDiminishing returns detected at size {size}”)
 break</p>

<p>return results</p>

<p>def _simulate_cache(self, size):
 “"”Simulate cache with given size”””
 cache = LRUCache(capacity=size)
 hits = 0</p>

<p>for key in self.workload:
 if cache.get(key) is not None:
 hits += 1
 else:
 cache.put(key, True)</p>

<p>return hits / len(self.workload)</p>

<h1 id="generate-workload-zipf-distribution---realistic-for-many-applications">Generate workload (Zipf distribution - realistic for many applications)</h1>
<p>import numpy as np</p>

<p>def generate_zipf_workload(n_items=1000, n_requests=10000, alpha=1.5):
 “””
 Generate Zipf-distributed workload</p>

<p>Zipf law: Some items are accessed much more frequently
 (80/20 rule, power law distribution)
 “””
 # Zipf distribution
 probabilities = np.array([1.0 / (i ** alpha) for i in range(1, n_items + 1)])
 probabilities /= probabilities.sum()</p>

<p># Generate requests
 workload = np.random.choice(
 [f”key_{i}” for i in range(n_items)],
 size=n_requests,
 p=probabilities
 )</p>

<p>return workload.tolist()</p>

<h1 id="find-optimal-cache-size">Find optimal cache size</h1>
<p>workload = generate_zipf_workload(n_items=1000, n_requests=10000)
optimizer = CacheSizeOptimizer(workload)
results = optimizer.find_optimal_size(max_size=500, step=50)</p>

<h1 id="plot-results">Plot results</h1>
<p>import matplotlib.pyplot as plt</p>

<p>sizes = [r[‘size’] for r in results]
hit_rates = [r[‘hit_rate’] for r in results]</p>

<p>plt.figure(figsize=(10, 6))
plt.plot(sizes, hit_rates, marker=’o’)
plt.xlabel(‘Cache Size’)
plt.ylabel(‘Hit Rate’)
plt.title(‘Cache Size vs Hit Rate’)
plt.grid(True)
plt.savefig(‘cache_size_optimization.png’)
``</p>

<hr />

<h2 id="advanced-caching-patterns">Advanced Caching Patterns</h2>

<h3 id="write-through-vs-write-back-cache">Write-Through vs Write-Back Cache</h3>

<p>``python
class WriteThroughCache:
 “””
 Write-through cache: Write to cache and database simultaneously</p>

<p>Pros:</p>
<ul>
  <li>Data consistency</li>
  <li>Simple to implement</li>
</ul>

<p>Cons:</p>
<ul>
  <li>Slower writes</li>
  <li>Every write hits database
 “””</li>
</ul>

<p>def <strong>init</strong>(self, cache, database):
 self.cache = cache
 self.database = database</p>

<p>def get(self, key):
 “"”Read with cache”””
 # Try cache first
 value = self.cache.get(key)
 if value is not None:
 return value</p>

<p># Cache miss: read from database
 value = self.database.get(key)
 if value is not None:
 self.cache.put(key, value)</p>

<p>return value</p>

<p>def put(self, key, value):
 “"”Write to both cache and database”””
 # Write to database first
 self.database.put(key, value)</p>

<p># Then update cache
 self.cache.put(key, value)</p>

<p>class WriteBackCache:
 “””
 Write-back cache: Write to cache only, flush to database later</p>

<p>Pros:</p>
<ul>
  <li>Fast writes</li>
  <li>Batching possible</li>
</ul>

<p>Cons:</p>
<ul>
  <li>Risk of data loss</li>
  <li>More complex
 “””</li>
</ul>

<p>def <strong>init</strong>(self, cache, database, flush_interval=5):
 self.cache = cache
 self.database = database
 self.flush_interval = flush_interval</p>

<p>self.dirty_keys = set()
 self.last_flush = time.time()</p>

<p>def get(self, key):
 “"”Read with cache”””
 value = self.cache.get(key)
 if value is not None:
 return value</p>

<p>value = self.database.get(key)
 if value is not None:
 self.cache.put(key, value)</p>

<p>return value</p>

<p>def put(self, key, value):
 “"”Write to cache only”””
 self.cache.put(key, value)
 self.dirty_keys.add(key)</p>

<p># Check if we need to flush
 if time.time() - self.last_flush &gt; self.flush_interval:
 self.flush()</p>

<p>def flush(self):
 “"”Flush dirty keys to database”””
 if not self.dirty_keys:
 return</p>

<p>print(f”Flushing {len(self.dirty_keys)} dirty keys…”)</p>

<p>for key in self.dirty_keys:
 value = self.cache.get(key)
 if value is not None:
 self.database.put(key, value)</p>

<p>self.dirty_keys.clear()
 self.last_flush = time.time()</p>

<h1 id="example-database-simulation">Example database simulation</h1>
<p>class SimpleDatabase:
 def <strong>init</strong>(self):
 self.data = {}
 self.read_count = 0
 self.write_count = 0</p>

<p>def get(self, key):
 self.read_count += 1
 time.sleep(0.001) # Simulate latency
 return self.data.get(key)</p>

<p>def put(self, key, value):
 self.write_count += 1
 time.sleep(0.001) # Simulate latency
 self.data[key] = value</p>

<h1 id="compare-write-through-vs-write-back">Compare write-through vs write-back</h1>
<p>db1 = SimpleDatabase()
cache1 = LRUCache(capacity=100)
write_through = WriteThroughCache(cache1, db1)</p>

<p>db2 = SimpleDatabase()
cache2 = LRUCache(capacity=100)
write_back = WriteBackCache(cache2, db2)</p>

<h1 id="benchmark-writes">Benchmark writes</h1>
<p>import time</p>

<h1 id="write-through">Write-through</h1>
<p>start = time.time()
for i in range(100):
 write_through.put(f”key_{i}”, f”value_{i}”)
wt_time = time.time() - start</p>

<h1 id="write-back">Write-back</h1>
<p>start = time.time()
for i in range(100):
 write_back.put(f”key_{i}”, f”value_{i}”)
write_back.flush() # Final flush
wb_time = time.time() - start</p>

<p>print(f”Write-through: {wt_time:.3f}s, DB writes: {db1.write_count}”)
print(f”Write-back: {wb_time:.3f}s, DB writes: {db2.write_count}”)
``</p>

<h3 id="cache-aside-pattern">Cache Aside Pattern</h3>

<p>``python
class CacheAsidePattern:
 “””
 Cache-aside (lazy loading): Application manages cache</p>

<p>Most common pattern for ML systems</p>

<p>Flow:</p>
<ol>
  <li>Check cache</li>
  <li>If miss, query database</li>
  <li>Store in cache</li>
  <li>Return result
 “””</li>
</ol>

<p>def <strong>init</strong>(self, cache, database):
 self.cache = cache
 self.database = database</p>

<p>self.stats = {
 ‘reads’: 0,
 ‘cache_hits’: 0,
 ‘cache_misses’: 0,
 ‘writes’: 0
 }</p>

<p>def get(self, key):
 “””
 Get with cache-aside pattern</p>

<p>Application is responsible for loading cache
 “””
 self.stats[‘reads’] += 1</p>

<p># Try cache first
 value = self.cache.get(key)
 if value is not None:
 self.stats[‘cache_hits’] += 1
 return value</p>

<p># Cache miss: load from database
 self.stats[‘cache_misses’] += 1
 value = self.database.get(key)</p>

<p>if value is not None:
 # Populate cache for next time
 self.cache.put(key, value)</p>

<p>return value</p>

<p>def put(self, key, value):
 “””
 Write to database, invalidate cache</p>

<p>Simple approach: Just write to DB and remove from cache
 Next read will repopulate
 “””
 self.stats[‘writes’] += 1</p>

<p># Write to database
 self.database.put(key, value)</p>

<p># Invalidate cache entry
 # (Could also update cache here - depends on use case)
 if key in self.cache.cache:
 del self.cache.cache[key]</p>

<p>def get_stats(self):
 “"”Get cache statistics”””
 hit_rate = (
 self.stats[‘cache_hits’] / self.stats[‘reads’]
 if self.stats[‘reads’] &gt; 0 else 0
 )</p>

<p>return {
 **self.stats,
 ‘hit_rate’: hit_rate
 }</p>

<h1 id="usage-for-ml-predictions">Usage for ML predictions</h1>
<p>class MLPredictionService:
 “””
 ML prediction service with cache-aside pattern
 “””</p>

<p>def <strong>init</strong>(self, model, cache_capacity=1000):
 self.model = model
 self.cache = LRUCache(capacity=cache_capacity)</p>

<p># Fake database for persisted predictions
 self.prediction_db = {}</p>

<p>self.pattern = CacheAsidePattern(
 self.cache,
 self.prediction_db
 )</p>

<p>def predict(self, features):
 “””
 Predict with caching</p>

<p>Args:
 features: Feature vector (tuple for hashability)</p>

<p>Returns:
 Prediction
 “””
 # Create cache key from features
 cache_key = hash(features)</p>

<p># Try cache-aside pattern
 cached_prediction = self.pattern.get(cache_key)
 if cached_prediction is not None:
 return cached_prediction</p>

<p># Compute prediction (expensive)
 prediction = self.model.predict([features])[0]</p>

<p># Store in database and cache
 self.pattern.put(cache_key, prediction)</p>

<p>return prediction</p>

<p>def get_cache_stats(self):
 “"”Get caching statistics”””
 return self.pattern.get_stats()</p>

<h1 id="example-usage">Example usage</h1>
<p>from sklearn.ensemble import RandomForestClassifier
import numpy as np</p>

<h1 id="train-simple-model">Train simple model</h1>
<p>X_train = np.random.randn(100, 5)
y_train = (X_train.sum(axis=1) &gt; 0).astype(int)
model = RandomForestClassifier(n_estimators=10)
model.fit(X_train, y_train)</p>

<h1 id="create-prediction-service">Create prediction service</h1>
<p>service = MLPredictionService(model, cache_capacity=100)</p>

<h1 id="make-predictions-some-repeated">Make predictions (some repeated)</h1>
<p>for _ in range(1000):
 # Generate features (with some repetition)
 features = tuple(np.random.randint(0, 10, size=5))
 prediction = service.predict(features)</p>

<p>print(“Cache statistics:”)
print(service.get_cache_stats())
``</p>

<hr />

<h2 id="cache-stampede-prevention">Cache Stampede Prevention</h2>

<h3 id="problem-thundering-herd">Problem: Thundering Herd</h3>

<p>``python
class CacheStampedeProtection:
 “””
 Prevent cache stampede (thundering herd)</p>

<p>Problem:</p>
<ul>
  <li>Cache entry expires</li>
  <li>Many requests try to regenerate simultaneously</li>
  <li>Database/model gets overwhelmed</li>
</ul>

<p>Solution:</p>
<ul>
  <li>Use locking to ensure only one request regenerates</li>
  <li>Others wait for that request to complete
 “””</li>
</ul>

<p>def <strong>init</strong>(self, cache, compute_fn):
 self.cache = cache
 self.compute_fn = compute_fn</p>

<p># Lock for each key
 self.locks = {}
 self.master_lock = threading.Lock()</p>

<p>def get(self, key):
 “””
 Get with stampede protection</p>

<p>Uses double-check locking pattern
 “””
 # First check: Try cache (no lock)
 value = self.cache.get(key)
 if value is not None:
 return value</p>

<p># Get or create lock for this key
 with self.master_lock:
 if key not in self.locks:
 self.locks[key] = threading.Lock()
 key_lock = self.locks[key]</p>

<p># Acquire key-specific lock
 with key_lock:
 # Second check: Try cache again (another thread might have filled it)
 value = self.cache.get(key)
 if value is not None:
 return value</p>

<p># Compute value (only one thread does this)
 print(f”Computing value for {key} (thread: {threading.current_thread().name})”)
 value = self.compute_fn(key)</p>

<p># Store in cache
 self.cache.put(key, value)</p>

<p>return value</p>

<h1 id="demo-simulate-stampede">Demo: Simulate stampede</h1>
<p>import threading
import time</p>

<p>def expensive_computation(key):
 “"”Simulate expensive computation”””
 time.sleep(0.1) # 100ms
 return f”computed_value_for_{key}”</p>

<p>cache = LRUCache(capacity=100)
protector = CacheStampedeProtection(cache, expensive_computation)</p>

<h1 id="simulate-stampede-10-threads-requesting-same-key">Simulate stampede: 10 threads requesting same key</h1>
<p>def make_request(key, results, index):
 start = time.time()
 result = protector.get(key)
 duration = time.time() - start
 results[index] = duration</p>

<p>results = [0] * 10
threads = []</p>

<h1 id="clear-cache-to-force-computation">Clear cache to force computation</h1>
<p>cache = LRUCache(capacity=100)
protector.cache = cache</p>

<p>print(“Simulating cache stampede for key ‘popular_item’…”)
start_time = time.time()</p>

<p>for i in range(10):
 t = threading.Thread(
 target=make_request,
 args=(‘popular_item’, results, i),
 name=f”Thread-{i}”
 )
 threads.append(t)
 t.start()</p>

<p>for t in threads:
 t.join()</p>

<p>total_time = time.time() - start_time</p>

<p>print(f”\nTotal time: {total_time:.3f}s”)
print(f”Average request time: {sum(results)/len(results):.3f}s”)
print(f”Max request time: {max(results):.3f}s”)
print(f”Min request time: {min(results):.3f}s”)
print(“\nWith protection, only one thread computed (others waited)”)
``</p>

<h3 id="probabilistic-early-expiration">Probabilistic Early Expiration</h3>

<p>``python
class ProbabilisticCache:
 “””
 Probabilistic early expiration to prevent stampede</p>

<p>Idea: Refresh cache before expiration with increasing probability
 This spreads out refresh operations
 “””</p>

<p>def <strong>init</strong>(self, cache, compute_fn, ttl=60, beta=1.0):
 “””
 Args:
 ttl: Time to live in seconds
 beta: Controls early expiration probability
 “””
 self.cache = cache
 self.compute_fn = compute_fn
 self.ttl = ttl
 self.beta = beta</p>

<p># Track insertion times
 self.insertion_times = {}</p>

<p>def get(self, key):
 “””
 Get with probabilistic early expiration</p>

<p>Formula: Should refresh if:
 current_time - stored_time * beta * log(random) &gt;= ttl
 “””
 # Check cache
 value = self.cache.get(key)</p>

<p>if value is not None and key in self.insertion_times:
 # Calculate age
 age = time.time() - self.insertion_times[key]</p>

<p># Probabilistic early expiration
 import random
 import math</p>

<p># XFetch algorithm
 delta = self.ttl - age
 if delta * self.beta * math.log(random.random()) &lt; 0:
 # Refresh early
 print(f”Early refresh for {key} (age: {age:.1f}s)”)
 value = self._refresh(key)</p>

<p>return value</p>

<p># Cache miss or expired
 return self._refresh(key)</p>

<p>def _refresh(self, key):
 “"”Refresh cache entry”””
 value = self.compute_fn(key)
 self.cache.put(key, value)
 self.insertion_times[key] = time.time()
 return value</p>

<h1 id="demo">Demo</h1>
<p>def compute_value(key):
 time.sleep(0.01)
 return f”value_{key}_{time.time()}”</p>

<p>pcache = ProbabilisticCache(
 LRUCache(capacity=100),
 compute_value,
 ttl=5, # 5 second TTL
 beta=1.0
)</p>

<h1 id="access-same-key-multiple-times">Access same key multiple times</h1>
<p>for i in range(20):
 value = pcache.get(‘test_key’)
 time.sleep(0.3) # 300ms between requests
``</p>

<hr />

<h2 id="distributed-cache-challenges">Distributed Cache Challenges</h2>

<h3 id="cache-consistency">Cache Consistency</h3>

<p>``python
class DistributedCacheCoordinator:
 “””
 Coordinate cache across multiple instances</p>

<p>Challenges:</p>
<ol>
  <li>Keeping caches in sync</li>
  <li>Handling partial failures</li>
  <li>Eventual consistency
 “””</li>
</ol>

<p>def <strong>init</strong>(self, redis_client, instance_id):
 self.redis = redis_client
 self.instance_id = instance_id</p>

<p># Local L1 cache
 self.local_cache = LRUCache(capacity=1000)</p>

<p># Subscribe to invalidation messages
 self.pubsub = self.redis.pubsub()
 self.pubsub.subscribe(‘cache:invalidate’)</p>

<p># Start listener thread
 self.listener_thread = threading.Thread(
 target=self._listen_for_invalidations,
 daemon=True
 )
 self.listener_thread.start()</p>

<p>def get(self, key):
 “””
 Get from multi-level cache</p>

<p>L1 (local) -&gt; L2 (Redis) -&gt; Compute
 “””
 # Try local cache
 value = self.local_cache.get(key)
 if value is not None:
 return value</p>

<p># Try Redis
 value = self.redis.get(key)
 if value is not None:
 value = pickle.loads(value)
 # Populate local cache
 self.local_cache.put(key, value)
 return value</p>

<p>return None</p>

<p>def put(self, key, value, ttl=3600):
 “””
 Put in both levels and notify others
 “””
 # Store in local cache
 self.local_cache.put(key, value)</p>

<p># Store in Redis
 self.redis.setex(key, ttl, pickle.dumps(value))</p>

<p># Notify other instances to invalidate their L1
 self.redis.publish(
 ‘cache:invalidate’,
 json.dumps({
 ‘key’: key,
 ‘source_instance’: self.instance_id
 })
 )</p>

<p>def _listen_for_invalidations(self):
 “"”Listen for invalidation messages”””
 for message in self.pubsub.listen():
 if message[‘type’] == ‘message’:
 data = json.loads(message[‘data’])</p>

<p># Don’t invalidate if we sent the message
 if data[‘source_instance’] != self.instance_id:
 key = data[‘key’]</p>

<p># Invalidate local cache
 if key in self.local_cache.cache:
 del self.local_cache.cache[key]
 print(f”Invalidated {key} from local cache”)</p>

<h1 id="usage-across-multiple-instances">Usage across multiple instances</h1>
<h1 id="instance-1">Instance 1</h1>
<p>coordinator1 = DistributedCacheCoordinator(redis_client, instance_id=’instance1’)</p>

<h1 id="instance-2">Instance 2</h1>
<p>coordinator2 = DistributedCacheCoordinator(redis_client, instance_id=’instance2’)</p>

<h1 id="instance-1-writes">Instance 1 writes</h1>
<p>coordinator1.put(‘shared_key’, ‘value_from_instance1’)</p>

<h1 id="instance-2-reads-will-get-from-redis">Instance 2 reads (will get from Redis)</h1>
<p>value = coordinator2.get(‘shared_key’)
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Multiple eviction policies</strong> - LRU, LFU, TTL for different use cases 
✅ <strong>Distributed caching</strong> - Redis for shared cache across services 
✅ <strong>Multi-level caching</strong> - L1 (local) + L2 (distributed) for optimal performance 
✅ <strong>Cache warming</strong> - Proactive population of hot items 
✅ <strong>Invalidation strategies</strong> - Push-based and pull-based 
✅ <strong>Linked list connection</strong> - Understanding pointer manipulation helps with cache implementation 
✅ <strong>Monitor cache metrics</strong> - Hit rate, latency, memory usage</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0010-caching-strategies/">arunbaby.com/ml-system-design/0010-caching-strategies</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#caching" class="page__taxonomy-item p-category" rel="tag">caching</a><span class="sep">, </span>
    
      <a href="/tags/#distributed-systems" class="page__taxonomy-item p-category" rel="tag">distributed-systems</a><span class="sep">, </span>
    
      <a href="/tags/#memcached" class="page__taxonomy-item p-category" rel="tag">memcached</a><span class="sep">, </span>
    
      <a href="/tags/#performance" class="page__taxonomy-item p-category" rel="tag">performance</a><span class="sep">, </span>
    
      <a href="/tags/#redis" class="page__taxonomy-item p-category" rel="tag">redis</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0010-reverse-linked-list/" rel="permalink">Reverse Linked List
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          27 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master linked list manipulation through reversal - a fundamental pattern for understanding pointer logic and in-place algorithms.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0010-voice-enhancement/" rel="permalink">Voice Enhancement &amp; Noise Reduction
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Build systems that enhance voice quality by removing noise, improving intelligibility, and optimizing audio for speech applications.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0010-document-processing/" rel="permalink">Document Processing for Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Garbage In, Garbage Out. The Art of Reading Messy Data.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Caching+Strategies+for+ML+Systems%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0010-caching-strategies%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0010-caching-strategies%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0010-caching-strategies/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0009-online-learning-systems/" class="pagination--pager" title="Online Learning Systems">Previous</a>
    
    
      <a href="/ml-system-design/0011-content-delivery-network/" class="pagination--pager" title="Content Delivery Networks (CDN)">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
