<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>RAG Systems - Arun Baby</title>
<meta name="description" content="“Grounding LLMs in facts, not hallucinations.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="RAG Systems">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0045-rag-systems/">


  <meta property="og:description" content="“Grounding LLMs in facts, not hallucinations.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="RAG Systems">
  <meta name="twitter:description" content="“Grounding LLMs in facts, not hallucinations.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0045-rag-systems/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-14T21:26:06+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0045-rag-systems/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="RAG Systems">
    <meta itemprop="description" content="“Grounding LLMs in facts, not hallucinations.”">
    <meta itemprop="datePublished" content="2025-12-14T21:26:06+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0045-rag-systems/" itemprop="url">RAG Systems
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction-the-hallucination-problem">1. Introduction: The Hallucination Problem</a></li><li><a href="#2-what-is-rag">2. What is RAG?</a></li><li><a href="#3-rag-architecture">3. RAG Architecture</a></li><li><a href="#4-building-a-rag-system">4. Building a RAG System</a><ul><li><a href="#step-1-document-ingestion">Step 1: Document Ingestion</a></li><li><a href="#step-2-embedding-and-indexing">Step 2: Embedding and Indexing</a></li><li><a href="#step-3-retrieval">Step 3: Retrieval</a></li><li><a href="#step-4-generation">Step 4: Generation</a></li></ul></li><li><a href="#5-chunking-strategies">5. Chunking Strategies</a><ul><li><a href="#51-fixed-size-chunking">5.1. Fixed-Size Chunking</a></li><li><a href="#52-recursive-chunking">5.2. Recursive Chunking</a></li><li><a href="#53-semantic-chunking">5.3. Semantic Chunking</a></li><li><a href="#54-agentic-chunking">5.4. Agentic Chunking</a></li></ul></li><li><a href="#6-retrieval-strategies">6. Retrieval Strategies</a><ul><li><a href="#61-dense-retrieval">6.1. Dense Retrieval</a></li><li><a href="#62-sparse-retrieval-bm25">6.2. Sparse Retrieval (BM25)</a></li><li><a href="#63-hybrid-retrieval">6.3. Hybrid Retrieval</a></li><li><a href="#64-reranking">6.4. Reranking</a></li></ul></li><li><a href="#7-query-processing">7. Query Processing</a><ul><li><a href="#71-query-expansion">7.1. Query Expansion</a></li><li><a href="#72-query-decomposition">7.2. Query Decomposition</a></li><li><a href="#73-hyde-hypothetical-document-embeddings">7.3. HyDE (Hypothetical Document Embeddings)</a></li></ul></li><li><a href="#8-advanced-rag-patterns">8. Advanced RAG Patterns</a><ul><li><a href="#81-multi-hop-rag">8.1. Multi-Hop RAG</a></li><li><a href="#82-self-rag">8.2. Self-RAG</a></li><li><a href="#83-corrective-rag">8.3. Corrective RAG</a></li></ul></li><li><a href="#9-evaluation-metrics">9. Evaluation Metrics</a><ul><li><a href="#91-retrieval-metrics">9.1. Retrieval Metrics</a></li><li><a href="#92-generation-metrics">9.2. Generation Metrics</a></li><li><a href="#93-end-to-end-metrics">9.3. End-to-End Metrics</a></li></ul></li><li><a href="#10-production-considerations">10. Production Considerations</a><ul><li><a href="#101-caching">10.1. Caching</a></li><li><a href="#102-streaming">10.2. Streaming</a></li><li><a href="#103-citation">10.3. Citation</a></li><li><a href="#104-guardrails">10.4. Guardrails</a></li></ul></li><li><a href="#11-system-design-enterprise-rag">11. System Design: Enterprise RAG</a></li><li><a href="#12-common-pitfalls">12. Common Pitfalls</a></li><li><a href="#13-interview-questions">13. Interview Questions</a></li><li><a href="#14-future-trends">14. Future Trends</a></li><li><a href="#15-conclusion">15. Conclusion</a></li><li><a href="#16-advanced-graph-rag">16. Advanced: Graph RAG</a></li><li><a href="#17-testing-rag-systems">17. Testing RAG Systems</a><ul><li><a href="#171-unit-tests">17.1. Unit Tests</a></li><li><a href="#172-integration-tests">17.2. Integration Tests</a></li><li><a href="#173-evaluation-datasets">17.3. Evaluation Datasets</a></li></ul></li><li><a href="#18-cost-analysis">18. Cost Analysis</a></li><li><a href="#19-langchain-implementation">19. LangChain Implementation</a></li><li><a href="#20-llamaindex-implementation">20. LlamaIndex Implementation</a></li><li><a href="#21-handling-updates">21. Handling Updates</a></li><li><a href="#22-mastery-checklist">22. Mastery Checklist</a></li><li><a href="#23-conclusion">23. Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Grounding LLMs in facts, not hallucinations.”</strong></p>

<h2 id="1-introduction-the-hallucination-problem">1. Introduction: The Hallucination Problem</h2>

<p><strong>LLMs have limitations:</strong></p>
<ul>
  <li><strong>Knowledge Cutoff:</strong> Training data has a date limit.</li>
  <li><strong>Hallucinations:</strong> Confidently generate false information.</li>
  <li><strong>No Private Data:</strong> Can’t access your organization’s knowledge.</li>
</ul>

<p><strong>Solution:</strong> <strong>Retrieval-Augmented Generation (RAG)</strong>—retrieve relevant documents and use them to ground the LLM’s response.</p>

<h2 id="2-what-is-rag">2. What is RAG?</h2>

<p><strong>RAG = Retrieval + Generation</strong></p>

<ol>
  <li><strong>Retrieval:</strong> Find relevant documents from a knowledge base.</li>
  <li><strong>Augmentation:</strong> Add retrieved documents to the LLM prompt.</li>
  <li><strong>Generation:</strong> LLM generates a response based on the context.</li>
</ol>

<p><strong>Formula:</strong>
\(P(\text{answer} | \text{query}) = P(\text{answer} | \text{query}, \text{retrieved\_docs})\)</p>

<h2 id="3-rag-architecture">3. RAG Architecture</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                    ┌─────────────────────┐
                    │   User Query        │
                    └─────────┬───────────┘
                              │
                    ┌─────────▼───────────┐
                    │   Query Embedding   │
                    │   (e.g., OpenAI)    │
                    └─────────┬───────────┘
                              │
                    ┌─────────▼───────────┐
                    │   Vector Database   │
                    │   (Pinecone, etc.)  │
                    └─────────┬───────────┘
                              │ Top-K docs
                    ┌─────────▼───────────┐
                    │   Prompt Builder    │
                    │   Query + Context   │
                    └─────────┬───────────┘
                              │
                    ┌─────────▼───────────┐
                    │   LLM Generation    │
                    │   (GPT-4, Claude)   │
                    └─────────┬───────────┘
                              │
                    ┌─────────▼───────────┐
                    │   Response          │
                    └─────────────────────┘
</code></pre></div></div>

<h2 id="4-building-a-rag-system">4. Building a RAG System</h2>

<h3 id="step-1-document-ingestion">Step 1: Document Ingestion</h3>

<p><strong>Collect and preprocess documents:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">PDFLoader</span><span class="p">,</span> <span class="n">WebLoader</span>
<span class="kn">from</span> <span class="n">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="c1"># Load documents
</span><span class="n">loader</span> <span class="o">=</span> <span class="nc">PDFLoader</span><span class="p">(</span><span class="sh">"</span><span class="s">knowledge_base.pdf</span><span class="sh">"</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>

<span class="c1"># Split into chunks
</span><span class="n">splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">50</span>
<span class="p">)</span>
<span class="n">chunks</span> <span class="o">=</span> <span class="n">splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="step-2-embedding-and-indexing">Step 2: Embedding and Indexing</h3>

<p><strong>Convert chunks to vectors and store:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.embeddings</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span> <span class="n">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Pinecone</span>

<span class="c1"># Create embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="nc">OpenAIEmbeddings</span><span class="p">()</span>

<span class="c1"># Store in vector database
</span><span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Pinecone</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span>
    <span class="n">chunks</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">index_name</span><span class="o">=</span><span class="sh">"</span><span class="s">knowledge-base</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div>

<h3 id="step-3-retrieval">Step 3: Retrieval</h3>

<p><strong>Find relevant chunks for a query:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># Embed query
</span>    <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">.</span><span class="nf">embed_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    
    <span class="c1"># Search vector database
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="nf">similarity_search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div>

<h3 id="step-4-generation">Step 4: Generation</h3>

<p><strong>Build prompt and generate response:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">generate_response</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">retrieved_docs</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">retrieved_docs</span><span class="p">])</span>
    
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">Answer the question based on the context below.
    
Context:
</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s">

Question: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">

Answer:</span><span class="sh">"""</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span>
</code></pre></div></div>

<h2 id="5-chunking-strategies">5. Chunking Strategies</h2>

<p><strong>The quality of chunks significantly impacts RAG performance.</strong></p>

<h3 id="51-fixed-size-chunking">5.1. Fixed-Size Chunking</h3>

<p><strong>Split by character count:</strong></p>
<ul>
  <li>Simple, predictable.</li>
  <li>May split mid-sentence.</li>
</ul>

<h3 id="52-recursive-chunking">5.2. Recursive Chunking</h3>

<p><strong>Split by hierarchy: paragraphs → sentences → words:</strong></p>
<ul>
  <li>Preserves semantic units.</li>
  <li>Variable chunk sizes.</li>
</ul>

<h3 id="53-semantic-chunking">5.3. Semantic Chunking</h3>

<p><strong>Split based on topic changes:</strong></p>
<ul>
  <li>Use sentence embeddings.</li>
  <li>Group semantically similar sentences.</li>
</ul>

<h3 id="54-agentic-chunking">5.4. Agentic Chunking</h3>

<p><strong>Use LLM to determine optimal splits:</strong></p>
<ul>
  <li>“Where would you split this document?”</li>
  <li>High quality but expensive.</li>
</ul>

<p><strong>Best Practice:</strong></p>
<ul>
  <li>Chunk size: 200-500 tokens.</li>
  <li>Overlap: 10-20% for context preservation.</li>
</ul>

<h2 id="6-retrieval-strategies">6. Retrieval Strategies</h2>

<h3 id="61-dense-retrieval">6.1. Dense Retrieval</h3>

<p><strong>Use embedding similarity:</strong></p>
<ul>
  <li>Embed query and documents.</li>
  <li>K-NN search.</li>
  <li>Good for semantic matching.</li>
</ul>

<h3 id="62-sparse-retrieval-bm25">6.2. Sparse Retrieval (BM25)</h3>

<p><strong>Traditional keyword matching:</strong></p>
<ul>
  <li>TF-IDF scoring.</li>
  <li>Good for exact matches.</li>
</ul>

<h3 id="63-hybrid-retrieval">6.3. Hybrid Retrieval</h3>

<p><strong>Combine dense and sparse:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">hybrid_search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="c1"># Dense search
</span>    <span class="n">dense_results</span> <span class="o">=</span> <span class="n">dense_retriever</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Sparse search
</span>    <span class="n">sparse_results</span> <span class="o">=</span> <span class="n">bm25_retriever</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Combine with reciprocal rank fusion
</span>    <span class="n">combined</span> <span class="o">=</span> <span class="nf">reciprocal_rank_fusion</span><span class="p">(</span><span class="n">dense_results</span><span class="p">,</span> <span class="n">sparse_results</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">combined</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="64-reranking">6.4. Reranking</h3>

<p><strong>Use a more powerful model to rerank top candidates:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">CrossEncoder</span>

<span class="n">reranker</span> <span class="o">=</span> <span class="nc">CrossEncoder</span><span class="p">(</span><span class="sh">'</span><span class="s">cross-encoder/ms-marco-MiniLM-L-6-v2</span><span class="sh">'</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">rerank</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">documents</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">query</span><span class="p">,</span> <span class="n">doc</span><span class="p">.</span><span class="n">content</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">]</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">reranker</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span>
    
    <span class="n">ranked</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">scores</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">doc</span> <span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">ranked</span><span class="p">[:</span><span class="n">top_k</span><span class="p">]]</span>
</code></pre></div></div>

<h2 id="7-query-processing">7. Query Processing</h2>

<h3 id="71-query-expansion">7.1. Query Expansion</h3>

<p><strong>Expand query with related terms:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">expand_query</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Generate 3 related search queries for: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="sh">"</span>
    <span class="n">expanded</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">query</span><span class="p">]</span> <span class="o">+</span> <span class="n">expanded</span>
</code></pre></div></div>

<h3 id="72-query-decomposition">7.2. Query Decomposition</h3>

<p><strong>Break complex queries into subqueries:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">decompose_query</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">Break this question into simpler sub-questions:
    Question: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">
    Sub-questions:</span><span class="sh">"""</span>
    
    <span class="n">subqueries</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">parse_subqueries</span><span class="p">(</span><span class="n">subqueries</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="73-hyde-hypothetical-document-embeddings">7.3. HyDE (Hypothetical Document Embeddings)</h3>

<p><strong>Generate a hypothetical answer, then search for similar documents:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">hyde_search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># Generate hypothetical answer
</span>    <span class="n">hypothetical</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Answer this question: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Search using hypothetical answer
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="nf">similarity_search</span><span class="p">(</span><span class="n">hypothetical</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div>

<h2 id="8-advanced-rag-patterns">8. Advanced RAG Patterns</h2>

<h3 id="81-multi-hop-rag">8.1. Multi-Hop RAG</h3>

<p><strong>Answer questions requiring multiple retrieval steps:</strong></p>
<ol>
  <li>Answer subquestion 1 using retrieval.</li>
  <li>Use answer to formulate subquestion 2.</li>
  <li>Retrieve and answer subquestion 2.</li>
  <li>Combine for final answer.</li>
</ol>

<h3 id="82-self-rag">8.2. Self-RAG</h3>

<p><strong>LLM decides when to retrieve:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">self_rag</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="c1"># Ask if retrieval is needed
</span>    <span class="n">needs_retrieval</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
        <span class="sa">f</span><span class="sh">"</span><span class="s">Does this question need external knowledge? </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="sh">"</span>
    <span class="p">)</span>
    
    <span class="k">if</span> <span class="sh">"</span><span class="s">yes</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">needs_retrieval</span><span class="p">.</span><span class="nf">lower</span><span class="p">():</span>
        <span class="n">docs</span> <span class="o">=</span> <span class="nf">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
        <span class="k">return</span> <span class="nf">generate_with_context</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">docs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="83-corrective-rag">8.3. Corrective RAG</h3>

<p><strong>Verify and correct retrieved information:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">corrective_rag</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="nf">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    
    <span class="c1"># Verify relevance
</span>    <span class="n">relevant_docs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
        <span class="n">relevance</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
            <span class="sa">f</span><span class="sh">"</span><span class="s">Is this relevant to </span><span class="sh">'</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="sh">'</span><span class="s">? Document: </span><span class="si">{</span><span class="n">doc</span><span class="p">.</span><span class="n">content</span><span class="si">}</span><span class="sh">"</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="sh">"</span><span class="s">yes</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">relevance</span><span class="p">.</span><span class="nf">lower</span><span class="p">():</span>
            <span class="n">relevant_docs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">relevant_docs</span><span class="p">:</span>
        <span class="c1"># Fall back to web search
</span>        <span class="n">relevant_docs</span> <span class="o">=</span> <span class="nf">web_search</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="nf">generate_with_context</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">relevant_docs</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="9-evaluation-metrics">9. Evaluation Metrics</h2>

<h3 id="91-retrieval-metrics">9.1. Retrieval Metrics</h3>

<ul>
  <li><strong>Recall@K:</strong> % of relevant docs in top K.</li>
  <li><strong>MRR (Mean Reciprocal Rank):</strong> Position of first relevant doc.</li>
  <li><strong>NDCG:</strong> Normalized discounted cumulative gain.</li>
</ul>

<h3 id="92-generation-metrics">9.2. Generation Metrics</h3>

<ul>
  <li><strong>Faithfulness:</strong> Does the answer match the context?</li>
  <li><strong>Relevance:</strong> Does the answer address the query?</li>
  <li><strong>Completeness:</strong> Does the answer cover all aspects?</li>
</ul>

<h3 id="93-end-to-end-metrics">9.3. End-to-End Metrics</h3>

<ul>
  <li><strong>Answer Accuracy:</strong> Correctness of the final answer.</li>
  <li><strong>Latency:</strong> Time from query to response.</li>
  <li><strong>User Satisfaction:</strong> Ratings, thumbs up/down.</li>
</ul>

<h2 id="10-production-considerations">10. Production Considerations</h2>

<h3 id="101-caching">10.1. Caching</h3>

<p><strong>Cache frequent queries:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="n">lru_cache</span>

<span class="nd">@lru_cache</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">cached_retrieve</span><span class="p">(</span><span class="n">query_hash</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Semantic caching:</strong> Match similar queries, not just exact.</p>

<h3 id="102-streaming">10.2. Streaming</h3>

<p><strong>Stream tokens as they’re generated:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">stream_rag_response</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="nf">build_prompt</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">docs</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">llm</span><span class="p">.</span><span class="nf">stream</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">token</span>
</code></pre></div></div>

<h3 id="103-citation">10.3. Citation</h3>

<p><strong>Include sources in the response:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">Answer based on the sources. Cite using [1], [2], etc.

Sources:
[1] </span><span class="si">{</span><span class="n">doc1</span><span class="p">.</span><span class="n">content</span><span class="si">}</span><span class="s">
[2] </span><span class="si">{</span><span class="n">doc2</span><span class="p">.</span><span class="n">content</span><span class="si">}</span><span class="s">

Question: </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">
Answer with citations:</span><span class="sh">"""</span>
</code></pre></div></div>

<h3 id="104-guardrails">10.4. Guardrails</h3>

<p><strong>Prevent harmful outputs:</strong></p>
<ul>
  <li>Content filtering.</li>
  <li>Fact-checking against sources.</li>
  <li>Refuse to answer if context is insufficient.</li>
</ul>

<h2 id="11-system-design-enterprise-rag">11. System Design: Enterprise RAG</h2>

<p><strong>Scenario:</strong> Build a RAG system for internal company documentation.</p>

<p><strong>Requirements:</strong></p>
<ul>
  <li>Index 100K+ documents.</li>
  <li>Support multiple file types (PDF, DOCX, HTML).</li>
  <li>Access control (users see only permitted docs).</li>
  <li>Real-time updates.</li>
</ul>

<p><strong>Architecture:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌───────────────────────────────────────────────────────┐
│                    API Gateway                        │
│                   (Auth, Rate Limit)                  │
└───────────────────────────┬───────────────────────────┘
                            │
┌───────────────────────────▼───────────────────────────┐
│                   RAG Service                         │
│   ┌─────────────┐  ┌─────────────┐  ┌──────────────┐ │
│   │   Query     │  │  Retrieval  │  │  Generation  │ │
│   │  Processor  │→ │   Engine    │→ │    Engine    │ │
│   └─────────────┘  └─────────────┘  └──────────────┘ │
└───────────────────────────┬───────────────────────────┘
                            │
         ┌──────────────────┴──────────────────┐
         │                                      │
┌────────▼────────┐                   ┌────────▼────────┐
│  Vector DB      │                   │  LLM Service    │
│  (Pinecone)     │                   │  (OpenAI/Azure) │
└─────────────────┘                   └─────────────────┘
         │
┌────────▼────────┐
│  Document Store │
│  (S3/GCS)       │
└─────────────────┘
</code></pre></div></div>

<h2 id="12-common-pitfalls">12. Common Pitfalls</h2>

<ul>
  <li><strong>Poor Chunking:</strong> Splitting mid-sentence loses context.</li>
  <li><strong>Wrong Embedding Model:</strong> Use domain-specific if available.</li>
  <li><strong>Ignoring Metadata:</strong> Filter by date, author, category.</li>
  <li><strong>Too Few/Many Retrieved Docs:</strong> 3-5 is usually optimal.</li>
  <li><strong>Prompt Stuffing:</strong> Context too long overwhelms LLM.</li>
  <li><strong>No Fallback:</strong> Handle “I don’t know” gracefully.</li>
</ul>

<h2 id="13-interview-questions">13. Interview Questions</h2>

<ol>
  <li><strong>What is RAG?</strong> Explain the architecture.</li>
  <li><strong>Dense vs Sparse Retrieval:</strong> When to use each?</li>
  <li><strong>Chunking Strategies:</strong> How do you choose chunk size?</li>
  <li><strong>Evaluation:</strong> How do you measure RAG quality?</li>
  <li><strong>Design:</strong> Build a Q&amp;A system for a legal document database.</li>
</ol>

<h2 id="14-future-trends">14. Future Trends</h2>

<p><strong>1. Agentic RAG:</strong></p>
<ul>
  <li>LLM decides when/what to retrieve.</li>
  <li>Multi-step reasoning with tool use.</li>
</ul>

<p><strong>2. Multimodal RAG:</strong></p>
<ul>
  <li>Retrieve images, tables, diagrams.</li>
  <li>Vision-language models for understanding.</li>
</ul>

<p><strong>3. Real-Time RAG:</strong></p>
<ul>
  <li>Index streaming data (news, social media).</li>
  <li>Sub-second updates to knowledge base.</li>
</ul>

<p><strong>4. Personalized RAG:</strong></p>
<ul>
  <li>User-specific retrieval preferences.</li>
  <li>Learn from interaction history.</li>
</ul>

<h2 id="15-conclusion">15. Conclusion</h2>

<p>RAG is the bridge between LLMs and real-world knowledge. It solves hallucinations, enables access to private data, and keeps information current.</p>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Chunking:</strong> Quality of splits impacts everything.</li>
  <li><strong>Retrieval:</strong> Hybrid (dense + sparse) often best.</li>
  <li><strong>Reranking:</strong> Improves precision significantly.</li>
  <li><strong>Evaluation:</strong> Measure retrieval and generation separately.</li>
  <li><strong>Production:</strong> Cache, stream, cite sources.</li>
</ul>

<p>As LLMs become central to knowledge work, RAG will be the standard pattern for grounding them in facts. Master it to build trustworthy AI systems.</p>

<h2 id="16-advanced-graph-rag">16. Advanced: Graph RAG</h2>

<p><strong>Limitation of Vector RAG:</strong> Flat retrieval misses relationships.</p>

<p><strong>Graph RAG Approach:</strong></p>
<ol>
  <li>Build a knowledge graph from documents.</li>
  <li>Use graph traversal to find related entities.</li>
  <li>Combine with vector search.</li>
</ol>

<p><strong>Implementation:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Build graph during indexing
</span><span class="k">def</span> <span class="nf">build_knowledge_graph</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">nx</span><span class="p">.</span><span class="nc">Graph</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">:</span>
        <span class="c1"># Extract entities
</span>        <span class="n">entities</span> <span class="o">=</span> <span class="nf">extract_entities</span><span class="p">(</span><span class="n">chunk</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>
        
        <span class="c1"># Add nodes
</span>        <span class="k">for</span> <span class="n">entity</span> <span class="ow">in</span> <span class="n">entities</span><span class="p">:</span>
            <span class="n">graph</span><span class="p">.</span><span class="nf">add_node</span><span class="p">(</span><span class="n">entity</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">entity</span><span class="p">.</span><span class="nb">type</span><span class="p">)</span>
        
        <span class="c1"># Add edges between co-occurring entities
</span>        <span class="k">for</span> <span class="n">e1</span><span class="p">,</span> <span class="n">e2</span> <span class="ow">in</span> <span class="nf">combinations</span><span class="p">(</span><span class="n">entities</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">graph</span><span class="p">.</span><span class="nf">add_edge</span><span class="p">(</span><span class="n">e1</span><span class="p">,</span> <span class="n">e2</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">chunk</span><span class="p">.</span><span class="nb">id</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">graph</span>

<span class="c1"># Query with graph context
</span><span class="k">def</span> <span class="nf">graph_rag_query</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">vectorstore</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># Vector search
</span>    <span class="n">vector_results</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="nf">similarity_search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># Extract entities from query
</span>    <span class="n">query_entities</span> <span class="o">=</span> <span class="nf">extract_entities</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    
    <span class="c1"># Find related entities via graph
</span>    <span class="n">related_entities</span> <span class="o">=</span> <span class="nf">set</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">entity</span> <span class="ow">in</span> <span class="n">query_entities</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">entity</span> <span class="ow">in</span> <span class="n">graph</span><span class="p">:</span>
            <span class="n">neighbors</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">graph</span><span class="p">.</span><span class="nf">neighbors</span><span class="p">(</span><span class="n">entity</span><span class="p">))[:</span><span class="mi">3</span><span class="p">]</span>
            <span class="n">related_entities</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">neighbors</span><span class="p">)</span>
    
    <span class="c1"># Retrieve chunks mentioning related entities
</span>    <span class="n">graph_results</span> <span class="o">=</span> <span class="nf">get_chunks_for_entities</span><span class="p">(</span><span class="n">related_entities</span><span class="p">)</span>
    
    <span class="c1"># Combine results
</span>    <span class="n">all_results</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">vector_results</span> <span class="o">+</span> <span class="n">graph_results</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">all_results</span>
</code></pre></div></div>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Better handling of relational queries.</li>
  <li>Explainable retrieval paths.</li>
</ul>

<h2 id="17-testing-rag-systems">17. Testing RAG Systems</h2>

<h3 id="171-unit-tests">17.1. Unit Tests</h3>

<p><strong>Retrieval Tests:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_retrieval_returns_relevant_docs</span><span class="p">():</span>
    <span class="n">query</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What is machine learning?</span><span class="sh">"</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="nf">retrieve</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span>
    <span class="k">assert</span> <span class="nf">any</span><span class="p">(</span><span class="sh">"</span><span class="s">machine learning</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">.</span><span class="n">content</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_chunking_preserves_sentences</span><span class="p">():</span>
    <span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">First sentence. Second sentence.</span><span class="sh">"</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="nf">chunk_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">:</span>
        <span class="c1"># Each chunk should have complete sentences
</span>        <span class="k">assert</span> <span class="ow">not</span> <span class="n">chunk</span><span class="p">.</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">chunk</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="172-integration-tests">17.2. Integration Tests</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_end_to_end_rag</span><span class="p">():</span>
    <span class="n">query</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What are the benefits of RAG?</span><span class="sh">"</span>
    
    <span class="c1"># Should return a coherent answer with citations
</span>    <span class="n">response</span> <span class="o">=</span> <span class="n">rag_system</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">answer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">100</span>
    <span class="k">assert</span> <span class="nf">len</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">sources</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="sh">"</span><span class="s">retrieval</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">answer</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="173-evaluation-datasets">17.3. Evaluation Datasets</h3>

<p><strong>Create a golden dataset:</strong></p>
<ul>
  <li>Questions with known answers.</li>
  <li>Expected source documents.</li>
  <li>Use for regression testing.</li>
</ul>

<h2 id="18-cost-analysis">18. Cost Analysis</h2>

<p><strong>Cost Components:</strong></p>
<ol>
  <li><strong>Embedding:</strong> $0.0001 per 1K tokens (OpenAI).</li>
  <li><strong>Vector Storage:</strong> $0.025 per GB/month (Pinecone).</li>
  <li><strong>LLM Generation:</strong> $0.01-0.10 per 1K tokens.</li>
</ol>

<p><strong>Example (100K documents, 1M queries/month):</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Embedding (one-time): 100K × 500 tokens × $0.0001/1K = $5
Storage (monthly): 100K × 1KB = 100MB = $0.003
Generation: 1M × 1K tokens × $0.03/1K = $30,000

Total monthly: ~$30,000 (dominated by generation)
</code></pre></div></div>

<p><strong>Optimization:</strong></p>
<ul>
  <li>Cache frequent queries.</li>
  <li>Use smaller models for simple questions.</li>
  <li>Batch embedding generation.</li>
</ul>

<h2 id="19-langchain-implementation">19. LangChain Implementation</h2>

<p><strong>Complete RAG Pipeline:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.chains</span> <span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="kn">from</span> <span class="n">langchain.embeddings</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span> <span class="n">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Pinecone</span>
<span class="kn">from</span> <span class="n">langchain.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">from</span> <span class="n">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">DirectoryLoader</span>
<span class="kn">from</span> <span class="n">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="c1"># Load documents
</span><span class="n">loader</span> <span class="o">=</span> <span class="nc">DirectoryLoader</span><span class="p">(</span><span class="sh">'</span><span class="s">data/</span><span class="sh">'</span><span class="p">,</span> <span class="n">glob</span><span class="o">=</span><span class="sh">'</span><span class="s">**/*.pdf</span><span class="sh">'</span><span class="p">)</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>

<span class="c1"># Split
</span><span class="n">splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">chunks</span> <span class="o">=</span> <span class="n">splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>

<span class="c1"># Embed and store
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="nc">OpenAIEmbeddings</span><span class="p">()</span>
<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Pinecone</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span><span class="n">chunks</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">index_name</span><span class="o">=</span><span class="sh">"</span><span class="s">rag-demo</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Create QA chain
</span><span class="n">qa_chain</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="p">.</span><span class="nf">from_chain_type</span><span class="p">(</span>
    <span class="n">llm</span><span class="o">=</span><span class="nc">OpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="n">chain_type</span><span class="o">=</span><span class="sh">"</span><span class="s">stuff</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">retriever</span><span class="o">=</span><span class="n">vectorstore</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span><span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">k</span><span class="sh">"</span><span class="p">:</span> <span class="mi">5</span><span class="p">}),</span>
    <span class="n">return_source_documents</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="c1"># Query
</span><span class="n">result</span> <span class="o">=</span> <span class="nf">qa_chain</span><span class="p">({</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is RAG?</span><span class="sh">"</span><span class="p">})</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Answer: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">result</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Sources: </span><span class="si">{</span><span class="p">[</span><span class="n">doc</span><span class="p">.</span><span class="n">metadata</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">source_documents</span><span class="sh">'</span><span class="p">]]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="20-llamaindex-implementation">20. LlamaIndex Implementation</h2>

<p><strong>Alternative Framework:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">llama_index</span> <span class="kn">import</span> <span class="n">VectorStoreIndex</span><span class="p">,</span> <span class="n">SimpleDirectoryReader</span>
<span class="kn">from</span> <span class="n">llama_index.embeddings</span> <span class="kn">import</span> <span class="n">OpenAIEmbedding</span>
<span class="kn">from</span> <span class="n">llama_index.llms</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="c1"># Load documents
</span><span class="n">documents</span> <span class="o">=</span> <span class="nc">SimpleDirectoryReader</span><span class="p">(</span><span class="sh">'</span><span class="s">data/</span><span class="sh">'</span><span class="p">).</span><span class="nf">load_data</span><span class="p">()</span>

<span class="c1"># Create index
</span><span class="n">index</span> <span class="o">=</span> <span class="n">VectorStoreIndex</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span>
    <span class="n">documents</span><span class="p">,</span>
    <span class="n">embed_model</span><span class="o">=</span><span class="nc">OpenAIEmbedding</span><span class="p">(),</span>
<span class="p">)</span>

<span class="c1"># Query
</span><span class="n">query_engine</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">as_query_engine</span><span class="p">(</span>
    <span class="n">llm</span><span class="o">=</span><span class="nc">OpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-3.5-turbo</span><span class="sh">"</span><span class="p">),</span>
    <span class="n">similarity_top_k</span><span class="o">=</span><span class="mi">5</span>
<span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">query_engine</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="sh">"</span><span class="s">What is RAG?</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="21-handling-updates">21. Handling Updates</h2>

<p><strong>Challenge:</strong> Documents change over time.</p>

<p><strong>Strategies:</strong></p>
<ol>
  <li><strong>Full Reindex:</strong> Rebuild entire index (simple but slow).</li>
  <li><strong>Incremental Updates:</strong> Add/update/delete individual documents.</li>
  <li><strong>Versioning:</strong> Keep multiple versions, timestamp queries.</li>
</ol>

<p><strong>Implementation:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RAGIndex</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vectorstore</span> <span class="o">=</span> <span class="nc">Pinecone</span><span class="p">(</span><span class="n">index_name</span><span class="o">=</span><span class="sh">"</span><span class="s">documents</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">add_document</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">doc</span><span class="p">):</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vectorstore</span><span class="p">.</span><span class="nf">upsert</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">doc</span><span class="p">.</span><span class="nb">id</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">update_document</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">doc</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">delete_document</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="nb">id</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">add_document</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">delete_document</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">doc_id</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vectorstore</span><span class="p">.</span><span class="nf">delete</span><span class="p">(</span><span class="nb">filter</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">doc_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">doc_id</span><span class="p">})</span>
</code></pre></div></div>

<h2 id="22-mastery-checklist">22. Mastery Checklist</h2>

<p><strong>Mastery Checklist:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Explain RAG architecture</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement chunking strategies</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Build a vector index (Pinecone, Chroma)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement hybrid retrieval (dense + BM25)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Add reranking with cross-encoder</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement query expansion/decomposition</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Add citation to responses</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Handle document updates</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Measure retrieval and generation quality</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Deploy with caching and streaming</li>
</ul>

<h2 id="23-conclusion">23. Conclusion</h2>

<p>RAG is the most important pattern for production LLM applications. It transforms LLMs from unreliable knowledge sources into grounded, trustworthy systems.</p>

<p><strong>The RAG Stack:</strong></p>
<ol>
  <li><strong>Documents:</strong> Your knowledge base.</li>
  <li><strong>Chunking:</strong> Transform into searchable units.</li>
  <li><strong>Embedding:</strong> Vector representation.</li>
  <li><strong>Vector Store:</strong> Efficient similarity search.</li>
  <li><strong>Retrieval:</strong> Find relevant context.</li>
  <li><strong>Generation:</strong> LLM produces grounded answer.</li>
</ol>

<p>Every step matters. Poor chunking cascades to poor retrieval. Poor retrieval cascades to poor answers. Master each component to build world-class RAG systems.</p>

<p><strong>The future is RAG + Agents:</strong> Systems that not only retrieve but reason, plan, and take action based on retrieved knowledge. Start with RAG fundamentals, then explore the agentic frontier.</p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#embeddings" class="page__taxonomy-item p-category" rel="tag">embeddings</a><span class="sep">, </span>
    
      <a href="/tags/#knowledge" class="page__taxonomy-item p-category" rel="tag">knowledge</a><span class="sep">, </span>
    
      <a href="/tags/#llm" class="page__taxonomy-item p-category" rel="tag">llm</a><span class="sep">, </span>
    
      <a href="/tags/#retrieval" class="page__taxonomy-item p-category" rel="tag">retrieval</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=RAG+Systems%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0045-rag-systems%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0045-rag-systems%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0045-rag-systems/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0044-llm-serving/" class="pagination--pager" title="LLM Serving Infrastructure">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
