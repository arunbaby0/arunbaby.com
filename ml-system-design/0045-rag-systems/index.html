<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>RAG Systems - Arun Baby</title>
<meta name="description" content="“Grounding LLMs in facts, not hallucinations.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="RAG Systems">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0045-rag-systems/">


  <meta property="og:description" content="“Grounding LLMs in facts, not hallucinations.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="RAG Systems">
  <meta name="twitter:description" content="“Grounding LLMs in facts, not hallucinations.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0045-rag-systems/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T16:05:30+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0045-rag-systems/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="RAG Systems">
    <meta itemprop="description" content="“Grounding LLMs in facts, not hallucinations.”">
    <meta itemprop="datePublished" content="2025-12-29T16:05:30+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0045-rag-systems/" itemprop="url">RAG Systems
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction-the-hallucination-problem">1. Introduction: The Hallucination Problem</a></li><li><a href="#2-what-is-rag">2. What is RAG?</a></li><li><a href="#3-rag-architecture">3. RAG Architecture</a></li><li><a href="#4-building-a-rag-system">4. Building a RAG System</a><ul><li><a href="#step-1-document-ingestion">Step 1: Document Ingestion</a></li><li><a href="#step-2-embedding-and-indexing">Step 2: Embedding and Indexing</a></li><li><a href="#step-3-retrieval">Step 3: Retrieval</a></li><li><a href="#step-4-generation">Step 4: Generation</a></li></ul></li><li><a href="#5-chunking-strategies">5. Chunking Strategies</a><ul><li><a href="#51-fixed-size-chunking">5.1. Fixed-Size Chunking</a></li><li><a href="#52-recursive-chunking">5.2. Recursive Chunking</a></li><li><a href="#53-semantic-chunking">5.3. Semantic Chunking</a></li><li><a href="#54-agentic-chunking">5.4. Agentic Chunking</a></li></ul></li><li><a href="#6-retrieval-strategies">6. Retrieval Strategies</a><ul><li><a href="#61-dense-retrieval">6.1. Dense Retrieval</a></li><li><a href="#62-sparse-retrieval-bm25">6.2. Sparse Retrieval (BM25)</a></li><li><a href="#63-hybrid-retrieval">6.3. Hybrid Retrieval</a></li><li><a href="#64-reranking">6.4. Reranking</a></li></ul></li><li><a href="#7-query-processing">7. Query Processing</a><ul><li><a href="#71-query-expansion">7.1. Query Expansion</a></li><li><a href="#72-query-decomposition">7.2. Query Decomposition</a></li><li><a href="#73-hyde-hypothetical-document-embeddings">7.3. HyDE (Hypothetical Document Embeddings)</a></li></ul></li><li><a href="#8-advanced-rag-patterns">8. Advanced RAG Patterns</a><ul><li><a href="#81-multi-hop-rag">8.1. Multi-Hop RAG</a></li><li><a href="#82-self-rag">8.2. Self-RAG</a></li><li><a href="#83-corrective-rag">8.3. Corrective RAG</a></li></ul></li><li><a href="#9-evaluation-metrics">9. Evaluation Metrics</a><ul><li><a href="#91-retrieval-metrics">9.1. Retrieval Metrics</a></li><li><a href="#92-generation-metrics">9.2. Generation Metrics</a></li><li><a href="#93-end-to-end-metrics">9.3. End-to-End Metrics</a></li></ul></li><li><a href="#10-production-considerations">10. Production Considerations</a><ul><li><a href="#101-caching">10.1. Caching</a></li><li><a href="#102-streaming">10.2. Streaming</a></li><li><a href="#103-citation">10.3. Citation</a></li><li><a href="#104-guardrails">10.4. Guardrails</a></li></ul></li><li><a href="#11-system-design-enterprise-rag">11. System Design: Enterprise RAG</a></li><li><a href="#12-common-pitfalls">12. Common Pitfalls</a></li><li><a href="#13-interview-questions">13. Interview Questions</a></li><li><a href="#14-future-trends">14. Future Trends</a></li><li><a href="#15-conclusion">15. Conclusion</a></li><li><a href="#16-advanced-graph-rag">16. Advanced: Graph RAG</a></li><li><a href="#17-testing-rag-systems">17. Testing RAG Systems</a><ul><li><a href="#171-unit-tests">17.1. Unit Tests</a></li><li><a href="#172-integration-tests">17.2. Integration Tests</a></li><li><a href="#173-evaluation-datasets">17.3. Evaluation Datasets</a></li></ul></li><li><a href="#18-cost-analysis">18. Cost Analysis</a></li><li><a href="#19-langchain-implementation">19. LangChain Implementation</a></li><li><a href="#20-llamaindex-implementation">20. LlamaIndex Implementation</a></li><li><a href="#21-handling-updates">21. Handling Updates</a></li><li><a href="#22-mastery-checklist">22. Mastery Checklist</a></li><li><a href="#23-conclusion">23. Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Grounding LLMs in facts, not hallucinations.”</strong></p>

<h2 id="1-introduction-the-hallucination-problem">1. Introduction: The Hallucination Problem</h2>

<p><strong>LLMs have limitations:</strong></p>
<ul>
  <li><strong>Knowledge Cutoff:</strong> Training data has a date limit.</li>
  <li><strong>Hallucinations:</strong> Confidently generate false information.</li>
  <li><strong>No Private Data:</strong> Can’t access your organization’s knowledge.</li>
</ul>

<p><strong>Solution:</strong> <strong>Retrieval-Augmented Generation (RAG)</strong>—retrieve relevant documents and use them to ground the LLM’s response.</p>

<h2 id="2-what-is-rag">2. What is RAG?</h2>

<p><strong>RAG = Retrieval + Generation</strong></p>

<ol>
  <li><strong>Retrieval:</strong> Find relevant documents from a knowledge base.</li>
  <li><strong>Augmentation:</strong> Add retrieved documents to the LLM prompt.</li>
  <li><strong>Generation:</strong> LLM generates a response based on the context.</li>
</ol>

<p><strong>Formula:</strong>
<code class="language-plaintext highlighter-rouge">P(\text{answer} | \text{query}) = P(\text{answer} | \text{query}, \text{retrieved\_docs})</code></p>

<h2 id="3-rag-architecture">3. RAG Architecture</h2>

<p><code class="language-plaintext highlighter-rouge">
 ┌─────────────────────┐
 │ User Query │
 └─────────┬───────────┘
 │
 ┌─────────▼───────────┐
 │ Query Embedding │
 │ (e.g., OpenAI) │
 └─────────┬───────────┘
 │
 ┌─────────▼───────────┐
 │ Vector Database │
 │ (Pinecone, etc.) │
 └─────────┬───────────┘
 │ Top-K docs
 ┌─────────▼───────────┐
 │ Prompt Builder │
 │ Query + Context │
 └─────────┬───────────┘
 │
 ┌─────────▼───────────┐
 │ LLM Generation │
 │ (GPT-4, Claude) │
 └─────────┬───────────┘
 │
 ┌─────────▼───────────┐
 │ Response │
 └─────────────────────┘
</code></p>

<h2 id="4-building-a-rag-system">4. Building a RAG System</h2>

<h3 id="step-1-document-ingestion">Step 1: Document Ingestion</h3>

<p><strong>Collect and preprocess documents:</strong>
``python
from langchain.document_loaders import PDFLoader, WebLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter</p>

<h1 id="load-documents">Load documents</h1>
<p>loader = PDFLoader(“knowledge_base.pdf”)
documents = loader.load()</p>

<h1 id="split-into-chunks">Split into chunks</h1>
<p>splitter = RecursiveCharacterTextSplitter(
 chunk_size=500,
 chunk_overlap=50
)
chunks = splitter.split_documents(documents)
``</p>

<h3 id="step-2-embedding-and-indexing">Step 2: Embedding and Indexing</h3>

<p><strong>Convert chunks to vectors and store:</strong>
``python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone</p>

<h1 id="create-embeddings">Create embeddings</h1>
<p>embeddings = OpenAIEmbeddings()</p>

<h1 id="store-in-vector-database">Store in vector database</h1>
<p>vectorstore = Pinecone.from_documents(
 chunks,
 embeddings,
 index_name=”knowledge-base”
)
``</p>

<h3 id="step-3-retrieval">Step 3: Retrieval</h3>

<p><strong>Find relevant chunks for a query:</strong>
``python
def retrieve(query, k=5):
 # Embed query
 query_embedding = embeddings.embed_query(query)</p>

<p># Search vector database
 results = vectorstore.similarity_search(query, k=k)</p>

<p>return results
``</p>

<h3 id="step-4-generation">Step 4: Generation</h3>

<p><strong>Build prompt and generate response:</strong>
``python
def generate_response(query, retrieved_docs):
 context = “\n\n”.join([doc.page_content for doc in retrieved_docs])</p>

<p>prompt = f”"”Answer the question based on the context below.</p>

<p>Context:
{context}</p>

<p>Question: {query}</p>

<p>Answer:”””</p>

<p>response = llm.generate(prompt)
 return response
``</p>

<h2 id="5-chunking-strategies">5. Chunking Strategies</h2>

<p><strong>The quality of chunks significantly impacts RAG performance.</strong></p>

<h3 id="51-fixed-size-chunking">5.1. Fixed-Size Chunking</h3>

<p><strong>Split by character count:</strong></p>
<ul>
  <li>Simple, predictable.</li>
  <li>May split mid-sentence.</li>
</ul>

<h3 id="52-recursive-chunking">5.2. Recursive Chunking</h3>

<p><strong>Split by hierarchy: paragraphs → sentences → words:</strong></p>
<ul>
  <li>Preserves semantic units.</li>
  <li>Variable chunk sizes.</li>
</ul>

<h3 id="53-semantic-chunking">5.3. Semantic Chunking</h3>

<p><strong>Split based on topic changes:</strong></p>
<ul>
  <li>Use sentence embeddings.</li>
  <li>Group semantically similar sentences.</li>
</ul>

<h3 id="54-agentic-chunking">5.4. Agentic Chunking</h3>

<p><strong>Use LLM to determine optimal splits:</strong></p>
<ul>
  <li>“Where would you split this document?”</li>
  <li>High quality but expensive.</li>
</ul>

<p><strong>Best Practice:</strong></p>
<ul>
  <li>Chunk size: 200-500 tokens.</li>
  <li>Overlap: 10-20% for context preservation.</li>
</ul>

<h2 id="6-retrieval-strategies">6. Retrieval Strategies</h2>

<h3 id="61-dense-retrieval">6.1. Dense Retrieval</h3>

<p><strong>Use embedding similarity:</strong></p>
<ul>
  <li>Embed query and documents.</li>
  <li>K-NN search.</li>
  <li>Good for semantic matching.</li>
</ul>

<h3 id="62-sparse-retrieval-bm25">6.2. Sparse Retrieval (BM25)</h3>

<p><strong>Traditional keyword matching:</strong></p>
<ul>
  <li>TF-IDF scoring.</li>
  <li>Good for exact matches.</li>
</ul>

<h3 id="63-hybrid-retrieval">6.3. Hybrid Retrieval</h3>

<p><strong>Combine dense and sparse:</strong>
``python
def hybrid_search(query, k=5, alpha=0.5):
 # Dense search
 dense_results = dense_retriever.search(query, k=k)</p>

<p># Sparse search
 sparse_results = bm25_retriever.search(query, k=k)</p>

<p># Combine with reciprocal rank fusion
 combined = reciprocal_rank_fusion(dense_results, sparse_results, alpha)</p>

<p>return combined[:k]
``</p>

<h3 id="64-reranking">6.4. Reranking</h3>

<p><strong>Use a more powerful model to rerank top candidates:</strong>
``python
from sentence_transformers import CrossEncoder</p>

<p>reranker = CrossEncoder(‘cross-encoder/ms-marco-MiniLM-L-6-v2’)</p>

<p>def rerank(query, documents, top_k=5):
 pairs = [(query, doc.content) for doc in documents]
 scores = reranker.predict(pairs)</p>

<p>ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
 return [doc for doc, score in ranked[:top_k]]
``</p>

<h2 id="7-query-processing">7. Query Processing</h2>

<h3 id="71-query-expansion">7.1. Query Expansion</h3>

<p><strong>Expand query with related terms:</strong>
<code class="language-plaintext highlighter-rouge">python
def expand_query(query):
 prompt = f"Generate 3 related search queries for: {query}"
 expanded = llm.generate(prompt)
 return [query] + expanded
</code></p>

<h3 id="72-query-decomposition">7.2. Query Decomposition</h3>

<p><strong>Break complex queries into subqueries:</strong>
``python
def decompose_query(query):
 prompt = f”"”Break this question into simpler sub-questions:
 Question: {query}
 Sub-questions:”””</p>

<p>subqueries = llm.generate(prompt)
 return parse_subqueries(subqueries)
``</p>

<h3 id="73-hyde-hypothetical-document-embeddings">7.3. HyDE (Hypothetical Document Embeddings)</h3>

<p><strong>Generate a hypothetical answer, then search for similar documents:</strong>
``python
def hyde_search(query, k=5):
 # Generate hypothetical answer
 hypothetical = llm.generate(f”Answer this question: {query}”)</p>

<p># Search using hypothetical answer
 results = vectorstore.similarity_search(hypothetical, k=k)
 return results
``</p>

<h2 id="8-advanced-rag-patterns">8. Advanced RAG Patterns</h2>

<h3 id="81-multi-hop-rag">8.1. Multi-Hop RAG</h3>

<p><strong>Answer questions requiring multiple retrieval steps:</strong></p>
<ol>
  <li>Answer subquestion 1 using retrieval.</li>
  <li>Use answer to formulate subquestion 2.</li>
  <li>Retrieve and answer subquestion 2.</li>
  <li>Combine for final answer.</li>
</ol>

<h3 id="82-self-rag">8.2. Self-RAG</h3>

<p><strong>LLM decides when to retrieve:</strong>
``python
def self_rag(query):
 # Ask if retrieval is needed
 needs_retrieval = llm.generate(
 f”Does this question need external knowledge? {query}”
 )</p>

<p>if “yes” in needs_retrieval.lower():
 docs = retrieve(query)
 return generate_with_context(query, docs)
 else:
 return llm.generate(query)
``</p>

<h3 id="83-corrective-rag">8.3. Corrective RAG</h3>

<p><strong>Verify and correct retrieved information:</strong>
``python
def corrective_rag(query):
 docs = retrieve(query)</p>

<p># Verify relevance
 relevant_docs = []
 for doc in docs:
 relevance = llm.generate(
 f”Is this relevant to ‘{query}’? Document: {doc.content}”
 )
 if “yes” in relevance.lower():
 relevant_docs.append(doc)</p>

<p>if not relevant_docs:
 # Fall back to web search
 relevant_docs = web_search(query)</p>

<p>return generate_with_context(query, relevant_docs)
``</p>

<h2 id="9-evaluation-metrics">9. Evaluation Metrics</h2>

<h3 id="91-retrieval-metrics">9.1. Retrieval Metrics</h3>

<ul>
  <li><strong>Recall@K:</strong> % of relevant docs in top K.</li>
  <li><strong>MRR (Mean Reciprocal Rank):</strong> Position of first relevant doc.</li>
  <li><strong>NDCG:</strong> Normalized discounted cumulative gain.</li>
</ul>

<h3 id="92-generation-metrics">9.2. Generation Metrics</h3>

<ul>
  <li><strong>Faithfulness:</strong> Does the answer match the context?</li>
  <li><strong>Relevance:</strong> Does the answer address the query?</li>
  <li><strong>Completeness:</strong> Does the answer cover all aspects?</li>
</ul>

<h3 id="93-end-to-end-metrics">9.3. End-to-End Metrics</h3>

<ul>
  <li><strong>Answer Accuracy:</strong> Correctness of the final answer.</li>
  <li><strong>Latency:</strong> Time from query to response.</li>
  <li><strong>User Satisfaction:</strong> Ratings, thumbs up/down.</li>
</ul>

<h2 id="10-production-considerations">10. Production Considerations</h2>

<h3 id="101-caching">10.1. Caching</h3>

<p><strong>Cache frequent queries:</strong>
``python
from functools import lru_cache</p>

<p>@lru_cache(maxsize=1000)
def cached_retrieve(query_hash):
 return retrieve(query)
``</p>

<p><strong>Semantic caching:</strong> Match similar queries, not just exact.</p>

<h3 id="102-streaming">10.2. Streaming</h3>

<p><strong>Stream tokens as they’re generated:</strong>
``python
async def stream_rag_response(query):
 docs = await retrieve(query)
 prompt = build_prompt(query, docs)</p>

<p>async for token in llm.stream(prompt):
 yield token
``</p>

<h3 id="103-citation">10.3. Citation</h3>

<p><strong>Include sources in the response:</strong>
``python
prompt = f”"”Answer based on the sources. Cite using [1], [2], etc.</p>

<p>Sources:
[1] {doc1.content}
[2] {doc2.content}</p>

<p>Question: {query}
Answer with citations:”””
``</p>

<h3 id="104-guardrails">10.4. Guardrails</h3>

<p><strong>Prevent harmful outputs:</strong></p>
<ul>
  <li>Content filtering.</li>
  <li>Fact-checking against sources.</li>
  <li>Refuse to answer if context is insufficient.</li>
</ul>

<h2 id="11-system-design-enterprise-rag">11. System Design: Enterprise RAG</h2>

<p><strong>Scenario:</strong> Build a RAG system for internal company documentation.</p>

<p><strong>Requirements:</strong></p>
<ul>
  <li>Index 100K+ documents.</li>
  <li>Support multiple file types (PDF, DOCX, HTML).</li>
  <li>Access control (users see only permitted docs).</li>
  <li>Real-time updates.</li>
</ul>

<p><strong>Architecture:</strong>
<code class="language-plaintext highlighter-rouge">
┌───────────────────────────────────────────────────────┐
│ API Gateway │
│ (Auth, Rate Limit) │
└───────────────────────────┬───────────────────────────┘
 │
┌───────────────────────────▼───────────────────────────┐
│ RAG Service │
│ ┌─────────────┐ ┌─────────────┐ ┌──────────────┐ │
│ │ Query │ │ Retrieval │ │ Generation │ │
│ │ Processor │→ │ Engine │→ │ Engine │ │
│ └─────────────┘ └─────────────┘ └──────────────┘ │
└───────────────────────────┬───────────────────────────┘
 │
 ┌──────────────────┴──────────────────┐
 │ │
┌────────▼────────┐ ┌────────▼────────┐
│ Vector DB │ │ LLM Service │
│ (Pinecone) │ │ (OpenAI/Azure) │
└─────────────────┘ └─────────────────┘
 │
┌────────▼────────┐
│ Document Store │
│ (S3/GCS) │
└─────────────────┘
</code></p>

<h2 id="12-common-pitfalls">12. Common Pitfalls</h2>

<ul>
  <li><strong>Poor Chunking:</strong> Splitting mid-sentence loses context.</li>
  <li><strong>Wrong Embedding Model:</strong> Use domain-specific if available.</li>
  <li><strong>Ignoring Metadata:</strong> Filter by date, author, category.</li>
  <li><strong>Too Few/Many Retrieved Docs:</strong> 3-5 is usually optimal.</li>
  <li><strong>Prompt Stuffing:</strong> Context too long overwhelms LLM.</li>
  <li><strong>No Fallback:</strong> Handle “I don’t know” gracefully.</li>
</ul>

<h2 id="13-interview-questions">13. Interview Questions</h2>

<ol>
  <li><strong>What is RAG?</strong> Explain the architecture.</li>
  <li><strong>Dense vs Sparse Retrieval:</strong> When to use each?</li>
  <li><strong>Chunking Strategies:</strong> How do you choose chunk size?</li>
  <li><strong>Evaluation:</strong> How do you measure RAG quality?</li>
  <li><strong>Design:</strong> Build a Q&amp;A system for a legal document database.</li>
</ol>

<h2 id="14-future-trends">14. Future Trends</h2>

<p><strong>1. Agentic RAG:</strong></p>
<ul>
  <li>LLM decides when/what to retrieve.</li>
  <li>Multi-step reasoning with tool use.</li>
</ul>

<p><strong>2. Multimodal RAG:</strong></p>
<ul>
  <li>Retrieve images, tables, diagrams.</li>
  <li>Vision-language models for understanding.</li>
</ul>

<p><strong>3. Real-Time RAG:</strong></p>
<ul>
  <li>Index streaming data (news, social media).</li>
  <li>Sub-second updates to knowledge base.</li>
</ul>

<p><strong>4. Personalized RAG:</strong></p>
<ul>
  <li>User-specific retrieval preferences.</li>
  <li>Learn from interaction history.</li>
</ul>

<h2 id="15-conclusion">15. Conclusion</h2>

<p>RAG is the bridge between LLMs and real-world knowledge. It solves hallucinations, enables access to private data, and keeps information current.</p>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Chunking:</strong> Quality of splits impacts everything.</li>
  <li><strong>Retrieval:</strong> Hybrid (dense + sparse) often best.</li>
  <li><strong>Reranking:</strong> Improves precision significantly.</li>
  <li><strong>Evaluation:</strong> Measure retrieval and generation separately.</li>
  <li><strong>Production:</strong> Cache, stream, cite sources.</li>
</ul>

<p>As LLMs become central to knowledge work, RAG will be the standard pattern for grounding them in facts. Master it to build trustworthy AI systems.</p>

<h2 id="16-advanced-graph-rag">16. Advanced: Graph RAG</h2>

<p><strong>Limitation of Vector RAG:</strong> Flat retrieval misses relationships.</p>

<p><strong>Graph RAG Approach:</strong></p>
<ol>
  <li>Build a knowledge graph from documents.</li>
  <li>Use graph traversal to find related entities.</li>
  <li>Combine with vector search.</li>
</ol>

<p><strong>Implementation:</strong>
``python</p>
<h1 id="build-graph-during-indexing">Build graph during indexing</h1>
<p>def build_knowledge_graph(chunks):
 graph = nx.Graph()</p>

<p>for chunk in chunks:
 # Extract entities
 entities = extract_entities(chunk.content)</p>

<p># Add nodes
 for entity in entities:
 graph.add_node(entity, type=entity.type)</p>

<p># Add edges between co-occurring entities
 for e1, e2 in combinations(entities, 2):
 graph.add_edge(e1, e2, source=chunk.id)</p>

<p>return graph</p>

<h1 id="query-with-graph-context">Query with graph context</h1>
<p>def graph_rag_query(query, graph, vectorstore, k=5):
 # Vector search
 vector_results = vectorstore.similarity_search(query, k=k)</p>

<p># Extract entities from query
 query_entities = extract_entities(query)</p>

<p># Find related entities via graph
 related_entities = set()
 for entity in query_entities:
 if entity in graph:
 neighbors = list(graph.neighbors(entity))[:3]
 related_entities.update(neighbors)</p>

<p># Retrieve chunks mentioning related entities
 graph_results = get_chunks_for_entities(related_entities)</p>

<p># Combine results
 all_results = list(set(vector_results + graph_results))
 return all_results
``</p>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Better handling of relational queries.</li>
  <li>Explainable retrieval paths.</li>
</ul>

<h2 id="17-testing-rag-systems">17. Testing RAG Systems</h2>

<h3 id="171-unit-tests">17.1. Unit Tests</h3>

<p><strong>Retrieval Tests:</strong>
``python
def test_retrieval_returns_relevant_docs():
 query = “What is machine learning?”
 docs = retrieve(query, k=5)</p>

<p>assert len(docs) == 5
 assert any(“machine learning” in doc.content.lower() for doc in docs)</p>

<p>def test_chunking_preserves_sentences():
 text = “First sentence. Second sentence.”
 chunks = chunk_text(text, chunk_size=50)</p>

<p>for chunk in chunks:
 # Each chunk should have complete sentences
 assert not chunk.startswith(“ “)
 assert chunk.endswith(“.”)
``</p>

<h3 id="172-integration-tests">17.2. Integration Tests</h3>

<p>``python
def test_end_to_end_rag():
 query = “What are the benefits of RAG?”</p>

<p># Should return a coherent answer with citations
 response = rag_system.query(query)</p>

<p>assert len(response.answer) &gt; 100
 assert len(response.sources) &gt; 0
 assert “retrieval” in response.answer.lower()
``</p>

<h3 id="173-evaluation-datasets">17.3. Evaluation Datasets</h3>

<p><strong>Create a golden dataset:</strong></p>
<ul>
  <li>Questions with known answers.</li>
  <li>Expected source documents.</li>
  <li>Use for regression testing.</li>
</ul>

<h2 id="18-cost-analysis">18. Cost Analysis</h2>

<p><strong>Cost Components:</strong></p>
<ol>
  <li><strong>Embedding:</strong> $0.0001 per 1K tokens (OpenAI).</li>
  <li><strong>Vector Storage:</strong> $0.025 per GB/month (Pinecone).</li>
  <li><strong>LLM Generation:</strong> $0.01-0.10 per 1K tokens.</li>
</ol>

<p><strong>Example (100K documents, 1M queries/month):</strong>
``
Embedding (one-time): 100K × 500 tokens × <code class="language-plaintext highlighter-rouge">0.0001/1K = </code>5
Storage (monthly): 100K × 1KB = 100MB = $0.003
Generation: 1M × 1K tokens × <code class="language-plaintext highlighter-rouge">0.03/1K = </code>30,000</p>

<p>Total monthly: ~$30,000 (dominated by generation)
``</p>

<p><strong>Optimization:</strong></p>
<ul>
  <li>Cache frequent queries.</li>
  <li>Use smaller models for simple questions.</li>
  <li>Batch embedding generation.</li>
</ul>

<h2 id="19-langchain-implementation">19. LangChain Implementation</h2>

<p><strong>Complete RAG Pipeline:</strong>
``python
from langchain.chains import RetrievalQA
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain.llms import OpenAI
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter</p>

<h1 id="load-documents-1">Load documents</h1>
<p>loader = DirectoryLoader(‘data/’, glob=’<em>*/</em>.pdf’)
documents = loader.load()</p>

<h1 id="split">Split</h1>
<p>splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(documents)</p>

<h1 id="embed-and-store">Embed and store</h1>
<p>embeddings = OpenAIEmbeddings()
vectorstore = Pinecone.from_documents(chunks, embeddings, index_name=”rag-demo”)</p>

<h1 id="create-qa-chain">Create QA chain</h1>
<p>qa_chain = RetrievalQA.from_chain_type(
 llm=OpenAI(temperature=0),
 chain_type=”stuff”,
 retriever=vectorstore.as_retriever(search_kwargs={“k”: 5}),
 return_source_documents=True
)</p>

<h1 id="query">Query</h1>
<p>result = qa_chain({“query”: “What is RAG?”})
print(f”Answer: {result[‘result’]}”)
print(f”Sources: {[doc.metadata for doc in result[‘source_documents’]]}”)
``</p>

<h2 id="20-llamaindex-implementation">20. LlamaIndex Implementation</h2>

<p><strong>Alternative Framework:</strong>
``python
from llama_index import VectorStoreIndex, SimpleDirectoryReader
from llama_index.embeddings import OpenAIEmbedding
from llama_index.llms import OpenAI</p>

<h1 id="load-documents-2">Load documents</h1>
<p>documents = SimpleDirectoryReader(‘data/’).load_data()</p>

<h1 id="create-index">Create index</h1>
<p>index = VectorStoreIndex.from_documents(
 documents,
 embed_model=OpenAIEmbedding(),
)</p>

<h1 id="query-1">Query</h1>
<p>query_engine = index.as_query_engine(
 llm=OpenAI(model=”gpt-3.5-turbo”),
 similarity_top_k=5
)</p>

<p>response = query_engine.query(“What is RAG?”)
print(response)
``</p>

<h2 id="21-handling-updates">21. Handling Updates</h2>

<p><strong>Challenge:</strong> Documents change over time.</p>

<p><strong>Strategies:</strong></p>
<ol>
  <li><strong>Full Reindex:</strong> Rebuild entire index (simple but slow).</li>
  <li><strong>Incremental Updates:</strong> Add/update/delete individual documents.</li>
  <li><strong>Versioning:</strong> Keep multiple versions, timestamp queries.</li>
</ol>

<p><strong>Implementation:</strong>
``python
class RAGIndex:
 def <strong>init</strong>(self):
 self.vectorstore = Pinecone(index_name=”documents”)</p>

<p>def add_document(self, doc):
 chunks = self.chunk(doc)
 embeddings = self.embed(chunks)
 self.vectorstore.upsert(embeddings, doc.id)</p>

<p>def update_document(self, doc):
 self.delete_document(doc.id)
 self.add_document(doc)</p>

<p>def delete_document(self, doc_id):
 self.vectorstore.delete(filter={“doc_id”: doc_id})
``</p>

<h2 id="22-mastery-checklist">22. Mastery Checklist</h2>

<p><strong>Mastery Checklist:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Explain RAG architecture</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement chunking strategies</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Build a vector index (Pinecone, Chroma)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement hybrid retrieval (dense + BM25)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Add reranking with cross-encoder</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement query expansion/decomposition</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Add citation to responses</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Handle document updates</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Measure retrieval and generation quality</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Deploy with caching and streaming</li>
</ul>

<h2 id="23-conclusion">23. Conclusion</h2>

<p>RAG is the most important pattern for production LLM applications. It transforms LLMs from unreliable knowledge sources into grounded, trustworthy systems.</p>

<p><strong>The RAG Stack:</strong></p>
<ol>
  <li><strong>Documents:</strong> Your knowledge base.</li>
  <li><strong>Chunking:</strong> Transform into searchable units.</li>
  <li><strong>Embedding:</strong> Vector representation.</li>
  <li><strong>Vector Store:</strong> Efficient similarity search.</li>
  <li><strong>Retrieval:</strong> Find relevant context.</li>
  <li><strong>Generation:</strong> LLM produces grounded answer.</li>
</ol>

<p>Every step matters. Poor chunking cascades to poor retrieval. Poor retrieval cascades to poor answers. Master each component to build world-class RAG systems.</p>

<p><strong>The future is RAG + Agents:</strong> Systems that not only retrieve but reason, plan, and take action based on retrieved knowledge. Start with RAG fundamentals, then explore the agentic frontier.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0045-rag-systems/">arunbaby.com/ml-system-design/0045-rag-systems</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#embeddings" class="page__taxonomy-item p-category" rel="tag">embeddings</a><span class="sep">, </span>
    
      <a href="/tags/#knowledge" class="page__taxonomy-item p-category" rel="tag">knowledge</a><span class="sep">, </span>
    
      <a href="/tags/#llm" class="page__taxonomy-item p-category" rel="tag">llm</a><span class="sep">, </span>
    
      <a href="/tags/#retrieval" class="page__taxonomy-item p-category" rel="tag">retrieval</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0045-sliding-window-maximum/" rel="permalink">Sliding Window Maximum
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Finding the king of every window.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0045-speech-emotion-recognition/" rel="permalink">Speech Emotion Recognition
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Teaching machines to hear feelings.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0045-data-leakage-prevention/" rel="permalink">Data Leakage Prevention
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Prevent leaks by design: minimize data access, redact outputs and logs, and enforce least privilege for tools and memory.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=RAG+Systems%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0045-rag-systems%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0045-rag-systems%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0045-rag-systems/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0044-llm-serving/" class="pagination--pager" title="LLM Serving Infrastructure">Previous</a>
    
    
      <a href="/ml-system-design/0046-transfer-learning/" class="pagination--pager" title="Transfer Learning Systems">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
