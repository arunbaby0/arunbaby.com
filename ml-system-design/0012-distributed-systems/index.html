<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Distributed ML Systems - Arun Baby</title>
<meta name="description" content="Design distributed ML systems that scale to billions of predictions: Master replication, sharding, consensus, and fault tolerance for production ML.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Distributed ML Systems">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0012-distributed-systems/">


  <meta property="og:description" content="Design distributed ML systems that scale to billions of predictions: Master replication, sharding, consensus, and fault tolerance for production ML.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Distributed ML Systems">
  <meta name="twitter:description" content="Design distributed ML systems that scale to billions of predictions: Master replication, sharding, consensus, and fault tolerance for production ML.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0012-distributed-systems/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0012-distributed-systems/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Distributed ML Systems">
    <meta itemprop="description" content="Design distributed ML systems that scale to billions of predictions: Master replication, sharding, consensus, and fault tolerance for production ML.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0012-distributed-systems/" itemprop="url">Distributed ML Systems
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          25 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem-statement">Problem Statement</a><ul><li><a href="#why-distributed-systems">Why Distributed Systems?</a></li><li><a href="#real-world-scale-examples">Real-World Scale Examples</a></li></ul></li><li><a href="#understanding-distributed-systems-fundamentals">Understanding Distributed Systems Fundamentals</a><ul><li><a href="#what-makes-systems-distributed">What Makes Systems â€œDistributedâ€?</a></li><li><a href="#the-cap-theorem">The CAP Theorem</a></li><li><a href="#key-concepts-for-junior-engineers">Key Concepts for Junior Engineers</a></li></ul></li><li><a href="#architecture-patterns">Architecture Patterns</a><ul><li><a href="#pattern-1-master-worker-for-training">Pattern 1: Master-Worker (for Training)</a></li><li><a href="#pattern-2-load-balancer--replicas-for-serving">Pattern 2: Load Balancer + Replicas (for Serving)</a></li><li><a href="#pattern-3-pub-sub-for-async-communication">Pattern 3: Pub-Sub for Async Communication</a></li></ul></li><li><a href="#handling-failures">Handling Failures</a><ul><li><a href="#types-of-failures">Types of Failures</a></li><li><a href="#fault-tolerance-strategies">Fault Tolerance Strategies</a></li></ul></li><li><a href="#consistency-models">Consistency Models</a><ul><li><a href="#strong-consistency">Strong Consistency</a></li><li><a href="#eventual-consistency">Eventual Consistency</a></li></ul></li><li><a href="#consensus-algorithms">Consensus Algorithms</a><ul><li><a href="#understanding-the-challenge">Understanding the Challenge</a></li><li><a href="#raft-algorithm-simplified">Raft Algorithm (Simplified)</a></li></ul></li><li><a href="#data-partitioning-strategies">Data Partitioning Strategies</a><ul><li><a href="#strategy-1-range-partitioning">Strategy 1: Range Partitioning</a></li><li><a href="#strategy-2-hash-partitioning">Strategy 2: Hash Partitioning</a></li><li><a href="#strategy-3-consistent-hashing">Strategy 3: Consistent Hashing</a></li></ul></li><li><a href="#real-world-case-study-netflix-recommendation-system">Real-World Case Study: Netflix Recommendation System</a><ul><li><a href="#architecture">Architecture</a></li><li><a href="#key-distributed-systems-principles-used">Key Distributed Systems Principles Used</a></li><li><a href="#numbers">Numbers</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Design distributed ML systems that scale to billions of predictions: Master replication, sharding, consensus, and fault tolerance for production ML.</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p>Design a <strong>distributed machine learning system</strong> that can:</p>
<ol>
  <li>Handle <strong>billions of predictions per day</strong> across multiple regions</li>
  <li>Train models on <strong>terabytes of data</strong> across multiple machines</li>
  <li>Serve models with <strong>low latency</strong> (&lt;100ms) and <strong>high availability</strong> (99.99%)</li>
  <li>Handle <strong>failures gracefully</strong> without data loss or service disruption</li>
  <li>Scale <strong>horizontally</strong> by adding more machines</li>
</ol>

<h3 id="why-distributed-systems">Why Distributed Systems?</h3>

<p><strong>The fundamental constraint</strong>: A single machine canâ€™t handle:</p>

<p><strong>Data:</strong></p>
<ul>
  <li>Training data: 10TB+ (wonâ€™t fit in RAM)</li>
  <li>Model size: 100GB+ (large language models, embeddings)</li>
  <li>Inference load: 100,000 requests/sec (CPU melts ğŸ”¥)</li>
</ul>

<p><strong>Computation:</strong></p>
<ul>
  <li>Training time: Days/weeks on single GPU</li>
  <li>Inference: Canâ€™t serve millions of users from one server</li>
</ul>

<p><strong>Geography:</strong></p>
<ul>
  <li>Users worldwide: Tokyo, London, New York, SÃ£o Paulo</li>
  <li>Latency: Canâ€™t serve Tokyo users from Virginia (150ms+ RTT)</li>
</ul>

<p><strong>Reliability:</strong></p>
<ul>
  <li>Single machine fails â†’ Entire service down âŒ</li>
  <li>Need redundancy and fault tolerance</li>
</ul>

<h3 id="real-world-scale-examples">Real-World Scale Examples</h3>

<table>
  <thead>
    <tr>
      <th>Company</th>
      <th>Scale</th>
      <th>Challenge</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Google Search</strong></td>
      <td>8.5B searches/day</td>
      <td>Distributed indexing + serving</td>
    </tr>
    <tr>
      <td><strong>Netflix</strong></td>
      <td>200M users, 1B hours/day</td>
      <td>Personalization at scale</td>
    </tr>
    <tr>
      <td><strong>Uber</strong></td>
      <td>19M trips/day</td>
      <td>Real-time matching + prediction</td>
    </tr>
    <tr>
      <td><strong>Meta</strong></td>
      <td>3B users</td>
      <td>Social graph + recommendation</td>
    </tr>
  </tbody>
</table>

<p><strong>Common pattern</strong>: All use distributed ML systems!</p>

<hr />

<h2 id="understanding-distributed-systems-fundamentals">Understanding Distributed Systems Fundamentals</h2>

<h3 id="what-makes-systems-distributed">What Makes Systems â€œDistributedâ€?</h3>

<p><strong>Definition</strong>: Multiple computers working together as one system.</p>

<p><strong>Simple analogy</strong>: Restaurant kitchen</p>
<ul>
  <li><strong>Single machine</strong>: One chef makes everything (slow, bottleneck)</li>
  <li><strong>Distributed</strong>: Multiple chefs, each specializing (fast, parallel)</li>
</ul>

<p>But coordination is hard:</p>
<ul>
  <li>How do chefs know what to cook?</li>
  <li>What if a chef is sick?</li>
  <li>How to avoid making duplicate orders?</li>
</ul>

<p>These are <strong>distributed systems problems</strong>!</p>

<h3 id="the-cap-theorem">The CAP Theorem</h3>

<p><strong>CAP Theorem states</strong>: You can only have 2 of 3:</p>

<ol>
  <li><strong>Consistency (C)</strong>: All nodes see same data at same time</li>
  <li><strong>Availability (A)</strong>: System always responds (even if some nodes down)</li>
  <li><strong>Partition Tolerance (P)</strong>: System works despite network failures</li>
</ol>

<p><strong>In practice</strong>: Network partitions happen, so you must have P.
<strong>Real choice</strong>: Consistency (CP) vs Availability (AP)</p>

<p><strong>Example scenarios:</strong></p>

<p>``
Scenario: Network split between US and EU data centers</p>

<p>CP System (Choose Consistency):</p>
<ul>
  <li>Reject writes until partition healed</li>
  <li>Data stays consistent</li>
  <li>But users in EU canâ€™t use system! âŒ</li>
</ul>

<p>AP System (Choose Availability):</p>
<ul>
  <li>Accept writes in both regions</li>
  <li>Users happy! âœ“</li>
  <li>But data may conflict later (eventual consistency)
``</li>
</ul>

<p><strong>For ML systems:</strong></p>
<ul>
  <li><strong>Training</strong>: CP (want consistent data)</li>
  <li><strong>Serving</strong>: AP (availability critical for user experience)</li>
</ul>

<h3 id="key-concepts-for-junior-engineers">Key Concepts for Junior Engineers</h3>

<p><strong>1. Horizontal vs Vertical Scaling</strong></p>

<p>``
Vertical Scaling (Scale UP):
 1 machine â†’ Bigger machine
 4 CPU â†’ 64 CPU
 16GB RAM â†’ 512GB RAM</p>

<p>Pros: Simple, no code changes
 Cons: Expensive, limited (canâ€™t buy infinite RAM), single point of failure</p>

<p>Horizontal Scaling (Scale OUT):
 1 machine â†’ 10 machines</p>

<p>Pros: Cheaper, unlimited, fault-tolerant
 Cons: Complex (distributed systems problems!)
``</p>

<p><strong>ML systems need horizontal scaling</strong> because:</p>
<ul>
  <li>Data too big for one machine</li>
  <li>Training too slow on one machine</li>
  <li>Serving load too high for one machine</li>
</ul>

<p><strong>2. Replication vs Sharding</strong></p>

<p><strong>Replication</strong>: Same data on multiple machines
``
Machine 1: [A, B, C, D]
Machine 2: [A, B, C, D] â† Same data!
Machine 3: [A, B, C, D]</p>

<p>Use case: High availability, load distribution
Example: Model weights replicated to 100 servers
``</p>

<p><strong>Sharding</strong>: Different data on each machine
``
Machine 1: [A, B]
Machine 2: [C, D] â† Different data!
Machine 3: [E, F]</p>

<p>Use case: Data too big for one machine
Example: Training data split across 10 machines
``</p>

<p><strong>3. Synchronous vs Asynchronous</strong></p>

<p><strong>Synchronous</strong>: Wait for response before continuing
<code class="language-plaintext highlighter-rouge">python
result = call_other_service() # Block here
process(result) # Wait until call returns
</code></p>
<ul>
  <li><strong>Pros</strong>: Simple, consistent</li>
  <li><strong>Cons</strong>: Slow (latency adds up)</li>
</ul>

<p><strong>Asynchronous</strong>: Donâ€™t wait, continue immediately
<code class="language-plaintext highlighter-rouge">python
future = call_other_service_async() # Don't block
do_other_work() # Continue immediately
result = future.get() # Get result when needed
</code></p>
<ul>
  <li><strong>Pros</strong>: Fast, better resource usage</li>
  <li><strong>Cons</strong>: Complex, harder to debug</li>
</ul>

<hr />

<h2 id="architecture-patterns">Architecture Patterns</h2>

<h3 id="pattern-1-master-worker-for-training">Pattern 1: Master-Worker (for Training)</h3>

<p><strong>Use case</strong>: Distributed model training</p>

<p><code class="language-plaintext highlighter-rouge">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MASTER NODE â”‚
â”‚ â€¢ Coordinates workers â”‚
â”‚ â€¢ Aggregates gradients â”‚
â”‚ â€¢ Updates global model â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚ â”‚ â”‚
 â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”
 â”‚Worker 1 â”‚ â”‚Worker 2â”‚ â”‚Worker 3â”‚
 â”‚ GPU 1 â”‚ â”‚ GPU 2 â”‚ â”‚ GPU 3 â”‚
 â”‚Batch 1 â”‚ â”‚Batch 2 â”‚ â”‚Batch 3 â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></p>

<p><strong>How it works:</strong></p>

<ol>
  <li>Master distributes data batches to workers</li>
  <li>Each worker computes gradients on its batch</li>
  <li>Workers send gradients back to master</li>
  <li>Master averages gradients, updates model</li>
  <li>Master broadcasts updated model to workers</li>
  <li>Repeat</li>
</ol>

<p><strong>Python implementation:</strong></p>

<p>``python
class MasterNode:
 â€œâ€â€
 Master node for distributed training</p>

<p>Coordinates multiple worker nodes
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model, workers):
 self.model = model
 self.workers = workers
 self.global_step = 0</p>

<p>def train_step(self, data_batches):
 â€œâ€â€
 One distributed training step</p>

<ol>
  <li>Send model to workers</li>
  <li>Workers compute gradients</li>
  <li>Aggregate gradients</li>
  <li>Update model
 â€œâ€â€
 # Distribute work to workers
 futures = []
 for worker, batch in zip(self.workers, data_batches):
 # Send model and data to worker
 future = worker.compute_gradients_async(
 self.model.state_dict(),
 batch
 )
 futures.append(future)</li>
</ol>

<p># Wait for all workers (synchronous)
 gradients = [future.get() for future in futures]</p>

<p># Aggregate gradients (averaging)
 avg_gradients = self._average_gradients(gradients)</p>

<p># Update model
 self.model.update(avg_gradients)
 self.global_step += 1</p>

<p>return self.model</p>

<p>def _average_gradients(self, gradients_list):
 â€œ"â€Average gradients from all workersâ€â€â€
 avg_grads = {}</p>

<p>for param_name in gradients_list[0].keys():
 # Average this parameterâ€™s gradients
 param_grads = [g[param_name] for g in gradients_list]
 avg_grads[param_name] = sum(param_grads) / len(param_grads)</p>

<p>return avg_grads</p>

<p>class WorkerNode:
 â€œâ€â€
 Worker node that computes gradients
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, worker_id, device=â€™cudaâ€™):
 self.worker_id = worker_id
 self.device = device</p>

<p>def compute_gradients_async(self, model_state, batch):
 â€œâ€â€
 Compute gradients on local batch</p>

<p>Returns: Future that will contain gradients
 â€œâ€â€
 import concurrent.futures</p>

<p>executor = concurrent.futures.ThreadPoolExecutor()
 future = executor.submit(
 self._compute_gradients,
 model_state,
 batch
 )</p>

<p>return future</p>

<p>def _compute_gradients(self, model_state, batch):
 â€œ"â€Actually compute gradientsâ€â€â€
 import torch</p>

<p># Load model
 model = load_model()
 model.load_state_dict(model_state)
 model.to(self.device)</p>

<p># Forward + backward
 loss = model(batch)
 loss.backward()</p>

<p># Extract gradients
 gradients = {
 name: param.grad.cpu()
 for name, param in model.named_parameters()
 }</p>

<p>return gradients
``</p>

<p><strong>Challenges:</strong></p>

<ol>
  <li><strong>Straggler problem</strong>: Slowest worker delays everyone
    <ul>
      <li><strong>Solution</strong>: Asynchronous updates, backup tasks</li>
    </ul>
  </li>
  <li><strong>Communication overhead</strong>: Sending gradients is expensive
    <ul>
      <li><strong>Solution</strong>: Gradient compression, local updates</li>
    </ul>
  </li>
  <li><strong>Fault tolerance</strong>: What if worker crashes?
    <ul>
      <li><strong>Solution</strong>: Checkpoint frequently, redistribute work</li>
    </ul>
  </li>
</ol>

<h3 id="pattern-2-load-balancer--replicas-for-serving">Pattern 2: Load Balancer + Replicas (for Serving)</h3>

<p><strong>Use case</strong>: Serving ML predictions at scale</p>

<p><code class="language-plaintext highlighter-rouge">
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 Requests â”€â”€â†’ â”‚Load Balancer â”‚
 â”‚ (Round Robin)â”‚
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â–¼ â–¼ â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Replica 1â”‚ â”‚Replica 2â”‚ â”‚Replica 3â”‚
 â”‚ Model â”‚ â”‚ Model â”‚ â”‚ Model â”‚
 â”‚+ Cache â”‚ â”‚+ Cache â”‚ â”‚+ Cache â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></p>

<p><strong>Benefits:</strong></p>

<ul>
  <li><strong>High availability</strong>: If one replica dies, others handle load</li>
  <li><strong>Load distribution</strong>: 10K req/sec across 10 replicas = 1K each</li>
  <li><strong>Zero-downtime deploys</strong>: Update replicas one at a time</li>
</ul>

<p><strong>Implementation:</strong></p>

<p>``python
class LoadBalancer:
 â€œâ€â€
 Simple round-robin load balancer</p>

<p>Distributes requests across healthy replicas
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, replicas):
 self.replicas = replicas
 self.current_index = 0
 self.health_checker = HealthChecker(replicas)
 self.health_checker.start()</p>

<p>def route_request(self, request):
 â€œâ€â€
 Route request to healthy replica</p>

<p>Uses round-robin for simplicity
 â€œâ€â€
 # Get healthy replicas
 healthy = self.health_checker.get_healthy_replicas()</p>

<p>if not healthy:
 raise Exception(â€œNo healthy replicas available!â€)</p>

<p># Round-robin selection
 replica = healthy[self.current_index % len(healthy)]
 self.current_index += 1</p>

<p># Forward request
 try:
 response = replica.predict(request)
 return response
 except Exception as e:
 # Retry with different replica
 return self._retry_request(request, exclude=[replica])</p>

<p>def _retry_request(self, request, exclude=None):
 â€œ"â€Retry failed request on different replicaâ€â€â€
 exclude = exclude or []
 healthy = [
 r for r in self.health_checker.get_healthy_replicas()
 if r not in exclude
 ]</p>

<p>if not healthy:
 raise Exception(â€œAll replicas failedâ€)</p>

<p>return healthy[0].predict(request)</p>

<p>class HealthChecker:
 â€œâ€â€
 Continuously monitor replica health</p>

<p>Marks unhealthy replicas so LB doesnâ€™t route to them
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, replicas, check_interval=10):
 self.replicas = replicas
 self.check_interval = check_interval
 self.health_status = {r: True for r in replicas}
 self.running = False</p>

<p>def start(self):
 â€œ"â€Start health checking in backgroundâ€â€â€
 import threading</p>

<p>self.running = True
 self.thread = threading.Thread(
 target=self._health_check_loop,
 daemon=True
 )
 self.thread.start()</p>

<p>def _health_check_loop(self):
 â€œ"â€Continuously check replica healthâ€â€â€
 import time</p>

<p>while self.running:
 for replica in self.replicas:
 is_healthy = replica.health_check()
 self.health_status[replica] = is_healthy</p>

<p>if not is_healthy:
 print(fâ€âš ï¸ Replica {replica.id} unhealthy!â€)</p>

<p>time.sleep(self.check_interval)</p>

<p>def get_healthy_replicas(self):
 â€œ"â€Get list of currently healthy replicasâ€â€â€
 return [
 replica for replica in self.replicas
 if self.health_status[replica]
 ]
``</p>

<h3 id="pattern-3-pub-sub-for-async-communication">Pattern 3: Pub-Sub for Async Communication</h3>

<p><strong>Use case</strong>: Model updates, feature updates, async tasks</p>

<p><code class="language-plaintext highlighter-rouge">
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Message Bus â”‚
 â”‚ (Kafka) â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â–¼ â–¼ â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Subscriber 1 â”‚ â”‚ Subscriber 2 â”‚ â”‚ Subscriber 3 â”‚
 â”‚ Update model â”‚ â”‚ Update cache â”‚ â”‚ Log metrics â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></p>

<p><strong>When to use:</strong></p>

<ul>
  <li>Model deployment: Notify all servers to reload model</li>
  <li>Feature updates: Broadcast new feature values</li>
  <li>Logging: Send metrics/logs asynchronously</li>
  <li>Training triggers: Data arrives â†’ trigger training job</li>
</ul>

<p><strong>Implementation:</strong></p>

<p>``python
class PubSubSystem:
 â€œâ€â€
 Publish-Subscribe system for async communication</p>

<p>Publishers send messages, subscribers receive them
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.subscribers = {} # topic -&gt; [subscribers]</p>

<p>def subscribe(self, topic, callback):
 â€œâ€â€
 Subscribe to a topic</p>

<p>Args:
 topic: Topic name (e.g., â€˜model.updatedâ€™)
 callback: Function to call when message received
 â€œâ€â€
 if topic not in self.subscribers:
 self.subscribers[topic] = []</p>

<p>self.subscribers[topic].append(callback)
 print(fâ€âœ“ Subscribed to {topic}â€)</p>

<p>def publish(self, topic, message):
 â€œâ€â€
 Publish message to topic</p>

<p>All subscribers will receive it asynchronously
 â€œâ€â€
 if topic not in self.subscribers:
 return</p>

<p>for callback in self.subscribers[topic]:
 # Call asynchronously (non-blocking)
 import threading
 thread = threading.Thread(
 target=callback,
 args=(message,)
 )
 thread.start()</p>

<p>print(fâ€ğŸ“¢ Published to {topic}: {message}â€)</p>

<h1 id="example-usage">Example usage</h1>
<p>pubsub = PubSubSystem()</p>

<h1 id="subscriber-1-model-server-that-reloads-on-updates">Subscriber 1: Model server that reloads on updates</h1>
<p>def reload_model(message):
 print(fâ€ğŸ”„ Reloading model: {message[â€˜model_versionâ€™]}â€)
 # Load new modelâ€¦</p>

<p>pubsub.subscribe(â€˜model.updatedâ€™, reload_model)</p>

<h1 id="subscriber-2-cache-that-invalidates-on-updates">Subscriber 2: Cache that invalidates on updates</h1>
<p>def invalidate_cache(message):
 print(fâ€ğŸ—‘ï¸ Invalidating cache for: {message[â€˜model_versionâ€™]}â€)
 # Clear cacheâ€¦</p>

<p>pubsub.subscribe(â€˜model.updatedâ€™, invalidate_cache)</p>

<h1 id="publisher-training-job-publishes-when-done">Publisher: Training job publishes when done</h1>
<p>def training_complete(model_path, version):
 pubsub.publish(â€˜model.updatedâ€™, {
 â€˜model_pathâ€™: model_path,
 â€˜model_versionâ€™: version,
 â€˜timestampâ€™: time.time()
 })</p>

<h1 id="trigger">Trigger</h1>
<p>training_complete(â€˜s3://models/v123â€™, â€˜v123â€™)</p>
<h1 id="both-subscribers-receive-message-asynchronously">Both subscribers receive message asynchronously!</h1>
<p>``</p>

<hr />

<h2 id="handling-failures">Handling Failures</h2>

<p><strong>Key principle</strong>: In distributed systems, failures are <strong>normal</strong>, not exceptional!</p>

<h3 id="types-of-failures">Types of Failures</h3>

<ol>
  <li><strong>Machine failure</strong>: Server crashes</li>
  <li><strong>Network partition</strong>: Network splits, canâ€™t communicate</li>
  <li><strong>Slow nodes</strong>: â€œStragglersâ€ delay entire system</li>
  <li><strong>Corrupted data</strong>: Silent data corruption</li>
  <li><strong>Cascading failures</strong>: One failure triggers others</li>
</ol>

<h3 id="fault-tolerance-strategies">Fault Tolerance Strategies</h3>

<p><strong>1. Replication (Multiple Copies)</strong></p>

<p>``python
class ReplicatedStorage:
 â€œâ€â€
 Store data on multiple nodes</p>

<p>If one fails, others have copy
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, nodes, replication_factor=3):
 self.nodes = nodes
 self.replication_factor = replication_factor</p>

<p>def write(self, key, value):
 â€œâ€â€
 Write to multiple nodes</p>

<p>Succeeds if majority succeed (quorum)
 â€œâ€â€
 # Pick nodes to write to
 target_nodes = self._pick_nodes(key, self.replication_factor)</p>

<p># Write to all (parallel)
 import concurrent.futures
 with concurrent.futures.ThreadPoolExecutor() as executor:
 futures = [
 executor.submit(node.write, key, value)
 for node in target_nodes
 ]</p>

<p># Wait for majority
 successes = sum(1 for f in futures if f.result())</p>

<p># Require majority for success (quorum)
 quorum = (self.replication_factor // 2) + 1</p>

<p>if successes &gt;= quorum:
 return True
 else:
 raise Exception(fâ€Write failed: only {successes}/{self.replication_factor} succeededâ€)</p>

<p>def read(self, key):
 â€œâ€â€
 Read from multiple nodes, return most recent</p>

<p>Handles node failures gracefully
 â€œâ€â€
 target_nodes = self._pick_nodes(key, self.replication_factor)</p>

<p># Read from all
 values = []
 for node in target_nodes:
 try:
 value = node.read(key)
 values.append(value)
 except Exception:
 # Node failed, skip it
 continue</p>

<p>if not values:
 raise Exception(â€œAll replicas failed!â€)</p>

<p># Return most recent (highest version)
 return max(values, key=lambda v: v[â€˜versionâ€™])
``</p>

<p><strong>2. Checkpointing (Save Progress)</strong></p>

<p>``python
class CheckpointedTraining:
 â€œâ€â€
 Save training progress periodically</p>

<p>If crash, resume from last checkpoint
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model, checkpoint_dir, checkpoint_every=1000):
 self.model = model
 self.checkpoint_dir = checkpoint_dir
 self.checkpoint_every = checkpoint_every
 self.global_step = 0</p>

<p>def train(self, data_loader):
 â€œ"â€Train with checkpointingâ€â€â€
 # Try to resume from checkpoint
 self.global_step = self._load_checkpoint()</p>

<p>for batch in data_loader:
 # Skip batches weâ€™ve already processed
 if self.global_step &lt; batch.id:
 continue</p>

<p># Training step
 loss = self.model.train_step(batch)
 self.global_step += 1</p>

<p># Checkpoint periodically
 if self.global_step % self.checkpoint_every == 0:
 self._save_checkpoint()
 print(fâ€âœ“ Checkpoint saved at step {self.global_step}â€)</p>

<p>def _save_checkpoint(self):
 â€œ"â€Save model + training stateâ€â€â€
 import torch</p>

<p>checkpoint = {
 â€˜model_stateâ€™: self.model.state_dict(),
 â€˜global_stepâ€™: self.global_step,
 â€˜timestampâ€™: time.time()
 }</p>

<p>path = fâ€{self.checkpoint_dir}/ckpt-{self.global_step}.ptâ€
 torch.save(checkpoint, path)</p>

<p>def _load_checkpoint(self):
 â€œ"â€Load latest checkpoint if existsâ€â€â€
 import glob
 import torch</p>

<p>checkpoints = glob.glob(fâ€{self.checkpoint_dir}/ckpt-*.ptâ€)</p>

<p>if not checkpoints:
 return 0</p>

<p># Load latest
 latest = max(checkpoints, key=lambda p: int(p.split(â€˜-â€˜)[1].split(â€˜.â€™)[0]))
 checkpoint = torch.load(latest)</p>

<p>self.model.load_state_dict(checkpoint[â€˜model_stateâ€™])
 print(fâ€âœ“ Resumed from step {checkpoint[â€˜global_stepâ€™]}â€)</p>

<p>return checkpoint[â€˜global_stepâ€™]
``</p>

<p><strong>3. Circuit Breaker (Prevent Cascading Failures)</strong></p>

<p>``python
class CircuitBreaker:
 â€œâ€â€
 Prevent cascading failures</p>

<p>If service keeps failing, stop calling it (open circuit)
 Give it time to recover, then try again
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, failure_threshold=5, timeout=60):
 self.failure_threshold = failure_threshold
 self.timeout = timeout
 self.failures = 0
 self.state = â€˜closedâ€™ # closed, open, half_open
 self.last_failure_time = 0</p>

<p>def call(self, func, *args, **kwargs):
 â€œâ€â€
 Call function with circuit breaker protection
 â€œâ€â€
 import time
 # Check if circuit is open
 if self.state == â€˜openâ€™:
 # Check if timeout passed
 if time.time() - self.last_failure_time &gt; self.timeout:
 self.state = â€˜half_openâ€™
 print(â€œğŸ”„ Circuit half-open, trying againâ€¦â€)
 else:
 raise Exception(â€œCircuit breaker OPEN - service unavailableâ€)</p>

<p># Try the call
 try:
 result = func(*args, **kwargs)</p>

<p># Success! Reset if we were half-open
 if self.state == â€˜half_openâ€™:
 self.state = â€˜closedâ€™
 self.failures = 0
 print(â€œâœ“ Circuit closed - service recoveredâ€)</p>

<p>return result</p>

<p>except Exception as e:
 # Failure
 self.failures += 1
 self.last_failure_time = time.time()</p>

<p># Open circuit if too many failures
 if self.failures &gt;= self.failure_threshold:
 self.state = â€˜openâ€™
 print(fâ€âš ï¸ Circuit breaker OPEN after {self.failures} failuresâ€)</p>

<p>raise e</p>

<h1 id="example-usage-1">Example usage</h1>
<p>circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=30)</p>

<p>def call_unreliable_service(data):
 â€œ"â€This service sometimes failsâ€â€â€
 import random
 if random.random() &lt; 0.5:
 raise Exception(â€œService failed!â€)
 return â€œSuccessâ€</p>

<h1 id="try-calling-with-circuit-breaker">Try calling with circuit breaker</h1>
<p>for i in range(10):
 try:
 result = circuit_breaker.call(call_unreliable_service, â€œdataâ€)
 print(fâ€Request {i}: {result}â€)
 except Exception as e:
 print(fâ€Request {i}: {e}â€)</p>

<p>time.sleep(1)
``</p>

<hr />

<h2 id="consistency-models">Consistency Models</h2>

<h3 id="strong-consistency">Strong Consistency</h3>

<p><strong>Guarantee</strong>: All reads see the most recent write</p>

<p>``python
class StronglyConsistentStore:
 â€œâ€â€
 Every read returns the latest write</p>

<p>Achieved by: Single master, synchronous replication
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.master = {} # Single source of truth
 self.replicas = [{}, {}] # Read replicas
 self.version = 0</p>

<p>def write(self, key, value):
 â€œâ€â€
 Write to master, then synchronously replicate</p>

<p>Slow but consistent!
 â€œâ€â€
 # Update version
 self.version += 1</p>

<p># Write to master
 self.master[key] = {â€˜valueâ€™: value, â€˜versionâ€™: self.version}</p>

<p># Synchronously replicate to all replicas
 for replica in self.replicas:
 replica[key] = {â€˜valueâ€™: value, â€˜versionâ€™: self.version}</p>

<p># Only return after all replicas updated
 print(fâ€âœ“ Write {key}={value} replicated to allâ€)</p>

<p>def read(self, key):
 â€œâ€â€
 Read from master (always latest)
 â€œâ€â€
 return self.master.get(key, {}).get(â€˜valueâ€™)
``</p>

<p><strong>Pros</strong>: Simple to reason about
<strong>Cons</strong>: Slow (sync replication), single point of failure</p>

<h3 id="eventual-consistency">Eventual Consistency</h3>

<p><strong>Guarantee</strong>: Reads <strong>eventually</strong> see the latest write (but not immediately)</p>

<p>``python
class EventuallyConsistentStore:
 â€œâ€â€
 Reads may see stale data temporarily</p>

<p>Achieved by: Asynchronous replication
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.replicas = [{}, {}, {}]
 self.version = 0</p>

<p>def write(self, key, value):
 â€œâ€â€
 Write to one replica, asynchronously propagate</p>

<p>Fast but eventually consistent
 â€œâ€â€
 self.version += 1</p>

<p># Write to first replica immediately
 self.replicas[0][key] = {â€˜valueâ€™: value, â€˜versionâ€™: self.version}</p>

<p># Asynchronously replicate to others
 import threading
 for replica in self.replicas[1:]:
 thread = threading.Thread(
 target=self._async_replicate,
 args=(replica, key, value, self.version)
 )
 thread.start()</p>

<p># Return immediately (donâ€™t wait for replication)
 return â€œOKâ€</p>

<p>def _async_replicate(self, replica, key, value, version):
 â€œ"â€Replicate asynchronouslyâ€â€â€
 import time
 time.sleep(0.1) # Simulate network delay
 replica[key] = {â€˜valueâ€™: value, â€˜versionâ€™: version}</p>

<p>def read(self, key):
 â€œâ€â€
 Read from random replica</p>

<p>May return stale data if replication not complete!
 â€œâ€â€
 import random
 replica = random.choice(self.replicas)
 return replica.get(key, {}).get(â€˜valueâ€™)
``</p>

<p><strong>Pros</strong>: Fast, highly available
<strong>Cons</strong>: Can read stale data temporarily</p>

<p><strong>For ML systems:</strong></p>
<ul>
  <li><strong>Model weights</strong>: Eventual consistency OK (small staleness acceptable)</li>
  <li><strong>Feature store</strong>: Strong consistency for critical features</li>
  <li><strong>Predictions</strong>: No consistency needed (stateless)</li>
</ul>

<hr />

<h2 id="consensus-algorithms">Consensus Algorithms</h2>

<p><strong>Problem</strong>: How do multiple nodes agree on a value when some might fail?</p>

<p><strong>Example</strong>: Leader election - which node should be the master?</p>

<h3 id="understanding-the-challenge">Understanding the Challenge</h3>

<p>``
Scenario: 3 nodes need to elect a leader</p>

<p>Node A thinks: â€œI should be leader!â€
Node B thinks: â€œNo, I should be leader!â€
Node C crashes before voting</p>

<p>Challenge:</p>
<ul>
  <li>Network delays mean messages arrive out of order</li>
  <li>Nodes might fail mid-process</li>
  <li>Must guarantee exactly ONE leader elected
``</li>
</ul>

<p><strong>This is the consensus problem!</strong></p>

<h3 id="raft-algorithm-simplified">Raft Algorithm (Simplified)</h3>

<p><strong>Raft</strong> is easier to understand than Paxos, achieving the same goal.</p>

<p><strong>Key concepts:</strong></p>

<ol>
  <li><strong>States</strong>: Each node is in one of three states:
    <ul>
      <li><strong>Follower</strong>: Accepts commands from leader</li>
      <li><strong>Candidate</strong>: Trying to become leader</li>
      <li><strong>Leader</strong>: Sends commands to followers</li>
    </ul>
  </li>
  <li><strong>Terms</strong>: Time divided into terms (like presidencies)
    <ul>
      <li>Each term has at most one leader</li>
      <li>Term number increases after each election</li>
    </ul>
  </li>
  <li><strong>Election process:</strong></li>
</ol>

<p>``python
class RaftNode:
 â€œâ€â€
 Simplified Raft consensus node</p>

<p>Real implementation is more complex!
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, node_id, peers):
 self.node_id = node_id
 self.peers = peers
 self.state = â€˜followerâ€™
 self.current_term = 0
 self.voted_for = None
 import random, time
 self.election_timeout = random.uniform(150, 300) # ms
 self.last_heartbeat = time.time()</p>

<p>def start_election(self):
 â€œâ€â€
 Become candidate and request votes</p>

<p>Called when election timeout expires without hearing from leader
 â€œâ€â€
 # Increment term
 self.current_term += 1
 self.state = â€˜candidateâ€™
 self.voted_for = self.node_id # Vote for self</p>

<p>print(fâ€Node {self.node_id}: Starting election for term {self.current_term}â€)</p>

<p># Request votes from all peers
 votes_received = 1 # Self vote</p>

<p>for peer in self.peers:
 if self._request_vote(peer):
 votes_received += 1</p>

<p># Check if won election (majority)
 majority = (len(self.peers) + 1) // 2 + 1</p>

<p>if votes_received &gt;= majority:
 self._become_leader()
 else:
 # Lost election, revert to follower
 self.state = â€˜followerâ€™</p>

<p>def _request_vote(self, peer):
 â€œâ€â€
 Request vote from peer</p>

<p>Peer grants vote if:</p>
<ul>
  <li>Havenâ€™t voted in this term yet</li>
  <li>Candidateâ€™s log is at least as up-to-date
 â€œâ€â€
 request = {
 â€˜termâ€™: self.current_term,
 â€˜candidate_idâ€™: self.node_id
 }</li>
</ul>

<p>response = peer.handle_vote_request(request)</p>

<p>return response.get(â€˜vote_grantedâ€™, False)</p>

<p>def _become_leader(self):
 â€œâ€â€
 Become leader for this term</p>

<p>Start sending heartbeats to maintain leadership
 â€œâ€â€
 self.state = â€˜leaderâ€™
 print(fâ€Node {self.node_id}: Became leader for term {self.current_term}â€)</p>

<p># Send heartbeats to all followers
 self._send_heartbeats()</p>

<p>def _send_heartbeats(self):
 â€œâ€â€
 Send periodic heartbeats to prevent new elections</p>

<p>Leader must send heartbeats &lt; election_timeout
 â€œâ€â€
 import time
 while self.state == â€˜leaderâ€™:
 for peer in self.peers:
 peer.receive_heartbeat({
 â€˜termâ€™: self.current_term,
 â€˜leader_idâ€™: self.node_id
 })</p>

<p>time.sleep(0.05) # 50ms heartbeat interval</p>

<p>def receive_heartbeat(self, message):
 â€œâ€â€
 Receive heartbeat from leader</p>

<p>Reset election timeout
 â€œâ€â€
 # Check term
 if message[â€˜termâ€™] &gt;= self.current_term:
 self.current_term = message[â€˜termâ€™]
 self.state = â€˜followerâ€™
 self.last_heartbeat = time.time()
 # Reset election timeout</p>

<p>return {â€˜successâ€™: True}</p>

<p>def handle_vote_request(self, request):
 â€œâ€â€
 Handle vote request from candidate</p>

<p>Grant vote if havenâ€™t voted in this term yet
 â€œâ€â€
 # Check term
 if request[â€˜termâ€™] &lt; self.current_term:
 return {â€˜vote_grantedâ€™: False}</p>

<p># Check if already voted
 if self.voted_for is None or self.voted_for == request[â€˜candidate_idâ€™]:
 self.voted_for = request[â€˜candidate_idâ€™]
 self.current_term = request[â€˜termâ€™]
 return {â€˜vote_grantedâ€™: True}</p>

<p>return {â€˜vote_grantedâ€™: False}
``</p>

<p><strong>Why this works:</strong></p>

<ol>
  <li><strong>Split votes</strong>: If multiple candidates, may get no majority â†’ retry</li>
  <li><strong>Random timeouts</strong>: Reduces likelihood of split votes</li>
  <li><strong>Term numbers</strong>: Ensures old messages ignored</li>
  <li><strong>Majority requirement</strong>: Ensures at most one leader per term</li>
</ol>

<p><strong>Use in ML systems:</strong></p>

<ul>
  <li><strong>Distributed training</strong>: Elect master node</li>
  <li><strong>Model serving</strong>: Elect coordinator for A/B test assignments</li>
  <li><strong>Feature store</strong>: Elect primary for writes</li>
</ul>

<hr />

<h2 id="data-partitioning-strategies">Data Partitioning Strategies</h2>

<p><strong>Problem</strong>: Training data is 10TB. Canâ€™t fit on one machine!</p>

<p><strong>Solution</strong>: Partition (shard) across multiple machines.</p>

<h3 id="strategy-1-range-partitioning">Strategy 1: Range Partitioning</h3>

<p><strong>Idea</strong>: Split data by key ranges</p>

<p>``
User IDs: 0 - 1,000,000</p>

<p>Partition 1: Users 0 - 250,000
Partition 2: Users 250,001 - 500,000
Partition 3: Users 500,001 - 750,000
Partition 4: Users 750,001 - 1,000,000
``</p>

<p><strong>Pros</strong>: Simple, range queries efficient
<strong>Cons</strong>: Hotspots if data skewed</p>

<p><strong>Example:</strong></p>

<p>``python
class RangePartitioner:
 â€œâ€â€
 Partition data by key ranges
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, partitions):
 self.partitions = partitions # [(0, 250000, node1), (250001, 500000, node2), â€¦]</p>

<p>def get_partition(self, key):
 â€œâ€â€
 Find which partition handles this key
 â€œâ€â€
 for start, end, node in self.partitions:
 if start &lt;= key &lt;= end:
 return node</p>

<p>raise ValueError(fâ€Key {key} not in any partitionâ€)</p>

<p>def write(self, key, value):
 â€œ"â€Write to appropriate partitionâ€â€â€
 node = self.get_partition(key)
 node.write(key, value)</p>

<p>def read(self, key):
 â€œ"â€Read from appropriate partitionâ€â€â€
 node = self.get_partition(key)
 return node.read(key)</p>

<h1 id="usage">Usage</h1>
<p>partitioner = RangePartitioner([
 (0, 250000, node1),
 (250001, 500000, node2),
 (500001, 750000, node3),
 (750001, 1000000, node4)
])</p>

<h1 id="write-user-data">Write user data</h1>
<p>partitioner.write(user_id=123456, value={â€˜nameâ€™: â€˜Aliceâ€™, â€¦})</p>

<h1 id="read-user-data">Read user data</h1>
<p>user_data = partitioner.read(user_id=123456)
``</p>

<p><strong>Hotspot problem:</strong></p>

<p>``
If most users have IDs 0-100,000:
 Partition 1: Overloaded! ğŸ“ˆ
 Partition 2-4: Idle ğŸ’¤</p>

<p>Unbalanced load!
``</p>

<h3 id="strategy-2-hash-partitioning">Strategy 2: Hash Partitioning</h3>

<p><strong>Idea</strong>: Hash key, use hash to determine partition</p>

<p>``
key â†’ hash(key) â†’ partition</p>

<p>Example:
user_id = 123456
hash(123456) = 42
partition = 42 % 4 = 2
â†’ Send to Partition 2
``</p>

<p><strong>Pros</strong>: Even distribution (no hotspots)
<strong>Cons</strong>: Range queries impossible</p>

<p>``python
class HashPartitioner:
 â€œâ€â€
 Partition data by hash of key
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, nodes):
 self.nodes = nodes
 self.num_nodes = len(nodes)</p>

<p>def get_partition(self, key):
 â€œâ€â€
 Hash key to determine partition
 â€œâ€â€
 # Hash key
 hash_value = hash(key)</p>

<p># Modulo to get partition index
 partition_idx = hash_value % self.num_nodes</p>

<p>return self.nodes[partition_idx]</p>

<p>def write(self, key, value):
 node = self.get_partition(key)
 node.write(key, value)</p>

<p>def read(self, key):
 node = self.get_partition(key)
 return node.read(key)</p>

<h1 id="usage-1">Usage</h1>
<p>partitioner = HashPartitioner([node1, node2, node3, node4])</p>

<h1 id="even-distribution">Even distribution!</h1>
<p>partitioner.write(1, â€˜data1â€™) # node2
partitioner.write(2, â€˜data2â€™) # node4
partitioner.write(3, â€˜data3â€™) # node1
partitioner.write(123456, â€˜dataâ€™) # node2
``</p>

<p><strong>Problem with adding/removing nodes:</strong></p>

<p>``
With 4 nodes: hash(key) % 4 = 2 â†’ node2
Add node5 (now 5 nodes): hash(key) % 5 = 4 â†’ node5</p>

<p>All keys need remapping! ğŸ˜±
Expensive!
``</p>

<h3 id="strategy-3-consistent-hashing">Strategy 3: Consistent Hashing</h3>

<p><strong>Idea</strong>: Minimize remapping when adding/removing nodes</p>

<p><strong>How it works:</strong></p>

<ol>
  <li>Hash both keys and nodes to same space (e.g., 0-360Â°)</li>
  <li>Place nodes on circle</li>
  <li>Key goes to next node clockwise</li>
</ol>

<p>``
Circle (0-360Â°):
 0Â°
 |
 Node B (45Â°)
 |
 Node C (120Â°)
 |
 Node D (200Â°)
 |
 Node A (290Â°)
 |
 360Â° (= 0Â°)</p>

<p>Key x hashes to 100Â° â†’ Goes to Node C (next clockwise at 120Â°)
Key y hashes to 250Â° â†’ Goes to Node A (next clockwise at 290Â°)</p>

<p>Add Node E at 160Â°:</p>
<ul>
  <li>Only keys between 120Â° and 160Â° move from C to E</li>
  <li>All other keys unchanged!
``</li>
</ul>

<p>``python
import bisect</p>

<p>class ConsistentHashRing:
 â€œâ€â€
 Consistent hashing for minimal remapping
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, nodes, virtual_nodes=150):
 self.virtual_nodes = virtual_nodes
 self.ring = []
 self.node_map = {}</p>

<p>for node in nodes:
 self._add_node(node)</p>

<p>def _add_node(self, node):
 â€œâ€â€
 Add node to ring with multiple virtual nodes</p>

<p>Virtual nodes for better distribution
 â€œâ€â€
 for i in range(self.virtual_nodes):
 # Hash node + replica number
 virtual_key = fâ€{node.id}-{i}â€
 hash_value = hash(virtual_key) % (2**32)</p>

<p># Insert into sorted ring
 bisect.insort(self.ring, hash_value)
 self.node_map[hash_value] = node</p>

<p>def get_node(self, key):
 â€œâ€â€
 Find node for key</p>

<p>O(log N) lookup using binary search
 â€œâ€â€
 # Hash key
 hash_value = hash(key) % (2**32)</p>

<p># Find next node clockwise
 idx = bisect.bisect_right(self.ring, hash_value)</p>

<p>if idx == len(self.ring):
 idx = 0 # Wrap around</p>

<p>ring_position = self.ring[idx]
 return self.node_map[ring_position]</p>

<p>def add_node(self, node):
 â€œâ€â€
 Add new node</p>

<p>Only ~1/N keys need remapping!
 â€œâ€â€
 self._add_node(node)
 print(fâ€Added {node.id}, only ~{100/len(self.ring)*self.virtual_nodes:.1f}% keys remappedâ€)</p>

<p>def remove_node(self, node):
 â€œ"â€Remove node from ringâ€â€â€
 for i in range(self.virtual_nodes):
 virtual_key = fâ€{node.id}-{i}â€
 hash_value = hash(virtual_key) % (2**32)</p>

<p>idx = self.ring.index(hash_value)
 del self.ring[idx]
 del self.node_map[hash_value]</p>

<h1 id="usage-2">Usage</h1>
<p>ring = ConsistentHashRing([node1, node2, node3, node4])</p>

<h1 id="keys-distributed-evenly">Keys distributed evenly</h1>
<p>key1_node = ring.get_node(â€˜user_123â€™)
key2_node = ring.get_node(â€˜user_456â€™)</p>

<h1 id="add-node---minimal-disruption">Add node - minimal disruption!</h1>
<p>ring.add_node(node5)
``</p>

<p><strong>Use in ML:</strong></p>

<ul>
  <li><strong>Feature store</strong>: Partition features by entity ID</li>
  <li><strong>Training data</strong>: Distribute examples across workers</li>
  <li><strong>Model serving</strong>: Distribute prediction requests</li>
</ul>

<hr />

<h2 id="real-world-case-study-netflix-recommendation-system">Real-World Case Study: Netflix Recommendation System</h2>

<h3 id="architecture">Architecture</h3>

<p><code class="language-plaintext highlighter-rouge">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Global Load Balancer â”‚
â”‚ (GeoDNS) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â–¼ â–¼ â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ US â”‚ â”‚ EU â”‚ â”‚ APAC â”‚ â† Regional clusters
â”‚ Region â”‚ â”‚ Region â”‚ â”‚ Region â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
 â”‚ â”‚ â”‚
 â–¼ â–¼ â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cassandra (User Profiles) â”‚ â† Distributed database
â”‚ Replicated across regions â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Recommendation Service â”‚ â† 1000s of instances
â”‚ (Load balanced) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”
 â–¼ â–¼
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚Cacheâ”‚ â”‚Modelâ”‚ â† Redis cache + Model replicas
â”‚Redisâ”‚ â”‚Serveâ”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
</code></p>

<h3 id="key-distributed-systems-principles-used">Key Distributed Systems Principles Used</h3>

<ol>
  <li><strong>Geographic distribution</strong>: Users routed to nearest region (low latency)</li>
  <li><strong>Replication</strong>: User data replicated across 3 regions (high availability)</li>
  <li><strong>Caching</strong>: Hot recommendations cached (reduce compute)</li>
  <li><strong>Load balancing</strong>: Requests distributed across 1000s of servers</li>
  <li><strong>Eventual consistency</strong>: Viewing history can be slightly stale</li>
  <li><strong>Partitioning</strong>: Users partitioned by user_id (horizontal scaling)</li>
</ol>

<h3 id="numbers">Numbers</h3>

<ul>
  <li><strong>200M+ users</strong></li>
  <li><strong>1B+ recommendation requests/day</strong></li>
  <li><strong>3 regions</strong> (US, EU, APAC)</li>
  <li><strong>1000s of servers</strong> per region</li>
  <li><strong>&lt; 100ms</strong> p99 latency for recommendations</li>
</ul>

<p><strong>How they handle failure:</strong></p>

<ul>
  <li><strong>Region failure</strong>: Route traffic to other regions</li>
  <li><strong>Server failure</strong>: Load balancer removes from pool</li>
  <li><strong>Cache miss</strong>: Fall back to model inference</li>
  <li><strong>Database failure</strong>: Serve stale data from replica</li>
</ul>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>âœ… <strong>Horizontal scaling</strong> - Add machines, not bigger machines 
âœ… <strong>Replication</strong> - Multiple copies for availability 
âœ… <strong>Sharding</strong> - Split data for scalability 
âœ… <strong>Load balancing</strong> - Distribute requests evenly 
âœ… <strong>Fault tolerance</strong> - Design for failure, not perfection 
âœ… <strong>Async communication</strong> - Pub-sub for decoupling 
âœ… <strong>Consistency trade-offs</strong> - CP vs AP based on use case</p>

<p><strong>Core principles:</strong></p>
<ol>
  <li>Failures are normal - design for them</li>
  <li>Network is unreliable - use retries, timeouts</li>
  <li>Consistency costs performance - choose wisely</li>
  <li>Monitoring is essential - you canâ€™t fix what you canâ€™t see</li>
</ol>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0012-distributed-systems/">arunbaby.com/ml-system-design/0012-distributed-systems</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#consistency" class="page__taxonomy-item p-category" rel="tag">consistency</a><span class="sep">, </span>
    
      <a href="/tags/#distributed-systems" class="page__taxonomy-item p-category" rel="tag">distributed-systems</a><span class="sep">, </span>
    
      <a href="/tags/#fault-tolerance" class="page__taxonomy-item p-category" rel="tag">fault-tolerance</a><span class="sep">, </span>
    
      <a href="/tags/#load-balancing" class="page__taxonomy-item p-category" rel="tag">load-balancing</a><span class="sep">, </span>
    
      <a href="/tags/#microservices" class="page__taxonomy-item p-category" rel="tag">microservices</a><span class="sep">, </span>
    
      <a href="/tags/#scalability" class="page__taxonomy-item p-category" rel="tag">scalability</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0012-add-two-numbers/" rel="permalink">Add Two Numbers
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master digit-by-digit addition with linked lists: Handle carry propagation elegantly. Classic problem teaching pointer manipulation and edge cases.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0012-multi-speaker-asr/" rel="permalink">Multi-Speaker ASR
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0012-context-window-management/" rel="permalink">Context Window Management
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">â€œThe Finite Canvas of Intelligence: Managing the Agentâ€™s RAM.â€
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Distributed+ML+Systems%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0012-distributed-systems%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0012-distributed-systems%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0012-distributed-systems/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0011-content-delivery-network/" class="pagination--pager" title="Content Delivery Networks (CDN)">Previous</a>
    
    
      <a href="/ml-system-design/0013-resource-allocation-for-ml/" class="pagination--pager" title="Resource Allocation for ML">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
