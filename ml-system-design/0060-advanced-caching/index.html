<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Advanced Caching for ML Systems - Arun Baby</title>
<meta name="description" content="“In the world of high-scale machine learning, the fastest inference is the one you never had to compute. Caching is not just about saving time; it’s about making the impossible latency targets possible.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Advanced Caching for ML Systems">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0060-advanced-caching/">


  <meta property="og:description" content="“In the world of high-scale machine learning, the fastest inference is the one you never had to compute. Caching is not just about saving time; it’s about making the impossible latency targets possible.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Advanced Caching for ML Systems">
  <meta name="twitter:description" content="“In the world of high-scale machine learning, the fastest inference is the one you never had to compute. Caching is not just about saving time; it’s about making the impossible latency targets possible.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0060-advanced-caching/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0060-advanced-caching/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Advanced Caching for ML Systems">
    <meta itemprop="description" content="“In the world of high-scale machine learning, the fastest inference is the one you never had to compute. Caching is not just about saving time; it’s about making the impossible latency targets possible.”">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0060-advanced-caching/" itemprop="url">Advanced Caching for ML Systems
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction-the-inference-bottleneck">1. Introduction: The Inference Bottleneck</a></li><li><a href="#2-functional-requirements-of-an-ml-cache">2. Functional Requirements of an ML Cache</a></li><li><a href="#3-the-architecture-the-latency-hierarchy">3. The Architecture: The Latency Hierarchy</a><ul><li><a href="#31-tier-1-the-local-in-process-cache">3.1 Tier 1: The Local (In-Process) Cache</a></li><li><a href="#32-tier-2-the-distributed-remote-cache">3.2 Tier 2: The Distributed (Remote) Cache</a></li><li><a href="#33-tier-3-the-persistent-ssd-cache">3.3 Tier 3: The Persistent (SSD) Cache</a></li><li><a href="#34-summary-table">3.4 Summary Table</a></li></ul></li><li><a href="#4-deep-dive-consistent-hashing-for-distributed-caching">4. Deep Dive: Consistent Hashing for Distributed Caching</a><ul><li><a href="#41-the-solution-virtual-nodes-on-a-ring">4.1 The Solution: Virtual Nodes on a Ring</a></li><li><a href="#42-replication-for-read-scalability">4.2 Replication for Read Scalability</a></li></ul></li><li><a href="#5-embedding-caching-the-ml-specific-challenge">5. Embedding Caching: The ML-Specific Challenge</a><ul><li><a href="#51-storage-optimization">5.1 Storage Optimization</a></li><li><a href="#52-batch-fetching">5.2 Batch Fetching</a></li><li><a href="#103-the-dual-write-anti-pattern">10.3 The Dual-Write Anti-Pattern</a></li></ul></li><li><a href="#11-monitoring-and-observability">11. Monitoring and Observability</a><ul><li><a href="#111-key-metrics-to-track">11.1 Key Metrics to Track</a></li><li><a href="#112-instrumenting-your-cache-client">11.2 Instrumenting Your Cache Client</a></li><li><a href="#113-dashboards-you-need">11.3 Dashboards You Need</a></li></ul></li><li><a href="#12-kv-cache-for-transformers-llm-inference">12. KV Cache for Transformers (LLM Inference)</a><ul><li><a href="#121-the-problem">12.1 The Problem</a></li><li><a href="#122-the-kv-cache-solution">12.2 The KV Cache Solution</a></li><li><a href="#123-optimization-paged-attention-vllm">12.3 Optimization: Paged Attention (vLLM)</a></li></ul></li><li><a href="#13-production-case-study-pinterests-feature-store-caching">13. Production Case Study: Pinterest’s Feature Store Caching</a><ul><li><a href="#131-the-architecture">13.1 The Architecture</a></li><li><a href="#132-key-optimizations">13.2 Key Optimizations</a></li><li><a href="#133-results">13.3 Results</a></li></ul></li><li><a href="#14-advanced-topic-negative-caching">14. Advanced Topic: Negative Caching</a><ul><li><a href="#141-the-solution-cache-the-null">14.1 The Solution: Cache the Null</a></li><li><a href="#142-danger-botnet-attacks">14.2 Danger: Botnet Attacks</a></li></ul></li><li><a href="#15-cache-warming-strategies">15. Cache Warming Strategies</a><ul><li><a href="#151-pre-warming-on-deploy">15.1 Pre-Warming on Deploy</a></li><li><a href="#152-shadow-traffic">15.2 Shadow Traffic</a></li><li><a href="#153-scheduled-warming-for-time-bound-features">15.3 Scheduled Warming for Time-Bound Features</a></li></ul></li><li><a href="#16-production-best-practices">16. Production Best Practices</a><ul><li><a href="#161-separate-caches-by-sla">16.1 Separate Caches by SLA</a></li><li><a href="#162-use-client-side-connection-pooling">16.2 Use Client-Side Connection Pooling</a></li><li><a href="#163-monitor-memory-fragmentation">16.3 Monitor Memory Fragmentation</a></li><li><a href="#164-implement-circuit-breakers">16.4 Implement Circuit Breakers</a></li></ul></li><li><a href="#17-key-takeaways">17. Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“In the world of high-scale machine learning, the fastest inference is the one you never had to compute. Caching is not just about saving time; it’s about making the impossible latency targets possible.”</strong></p>

<h2 id="1-introduction-the-inference-bottleneck">1. Introduction: The Inference Bottleneck</h2>

<p>As machine learning models grow from simple Decision Trees to 70B parameter Transformers, the “Cost per Inference” has become a defining business metric.</p>
<ul>
  <li>A single LLM generate call can cost cents.</li>
  <li>A real-time ranking call for 10,000 items can take hundreds of milliseconds.</li>
  <li>Storing pre-computed features for 1 billion users often requires Petabytes of storage.</li>
</ul>

<p>If you are designing the Recommendation System for TikTok or the Search Ranking for Amazon, you cannot afford to call the heavy Re-ranking Model for every single user interaction. You must use <strong>Advanced Caching Strategies</strong>.</p>

<p>This post moves beyond the basics of “put it in Redis.” We explore the <strong>Multi-Tiered Cache Architecture</strong>, <strong>Semantic Caching for LLMs</strong>, <strong>Feature Store Hydration</strong>, and how to solve the dreaded <strong>Thundering Herd</strong> problem in distributed systems.</p>

<hr />

<h2 id="2-functional-requirements-of-an-ml-cache">2. Functional Requirements of an ML Cache</h2>

<p>ML Caching is distinct from standard Web Caching (e.g., caching HTML pages).</p>
<ol>
  <li><strong>High Throughput, Low Latency</strong>: We need to fetch 1,000 features for a candidate batch in &lt; 5ms.</li>
  <li><strong>Approximate Correctness</strong>: Unlike a Bank Ledger, it’s okay if a user’s “Interest Vector” is 5 minutes stale.</li>
  <li><strong>High Dimensionality</strong>: We aren’t just caching strings; we are caching Vectors (Embeddings) and Tensors.</li>
  <li><strong>Semantic Retrieval</strong>: Use Vector Similarity to find “near-matches” for user queries, allowing us to reuse LLM generations for similar prompts.</li>
</ol>

<hr />

<h2 id="3-the-architecture-the-latency-hierarchy">3. The Architecture: The Latency Hierarchy</h2>

<p>Modern ML systems (like those at Netflix or Facebook) use a strictly tiered memory hierarchy.</p>

<h3 id="31-tier-1-the-local-in-process-cache">3.1 Tier 1: The Local (In-Process) Cache</h3>
<ul>
  <li><strong>Technology</strong>: Python <code class="language-plaintext highlighter-rouge">dict</code>, C++ <code class="language-plaintext highlighter-rouge">std::unordered_map</code>.</li>
  <li><strong>Latency</strong>: Nanoseconds (Reference access) to Microseconds.</li>
  <li><strong>Capacity</strong>: Small (1GB - 10GB).</li>
  <li><strong>Strategy</strong>: <strong>Hot-Keys Only</strong>. We use algorithms like <strong>TinyLFU</strong> to greedily admit only the top 1% most popular items (e.g., The “Global Bias” features or “Justin Bieber” embedding).</li>
  <li><strong>Gotcha</strong>: Cache Coherency. If Node A updates the cache, Node B doesn’t know. We accept this inconsistency.</li>
</ul>

<h3 id="32-tier-2-the-distributed-remote-cache">3.2 Tier 2: The Distributed (Remote) Cache</h3>
<ul>
  <li><strong>Technology</strong>: Redis (Cluster Mode), Memcached, Amazon ElastiCache.</li>
  <li><strong>Latency</strong>: 1ms - 5ms (Network RTT).</li>
  <li><strong>Capacity</strong>: Medium to Large (Terabytes).</li>
  <li><strong>Strategy</strong>: <strong>Sharded &amp; Replica</strong>. We shard data across 100+ nodes using <strong>Consistent Hashing</strong>.</li>
</ul>

<h3 id="33-tier-3-the-persistent-ssd-cache">3.3 Tier 3: The Persistent (SSD) Cache</h3>
<ul>
  <li><strong>Technology</strong>: Aerospike, DynamoDB (DAX), NVMe Pools.</li>
  <li><strong>Latency</strong>: 5ms - 20ms.</li>
  <li><strong>Capacity</strong>: Petabytes.</li>
  <li><strong>Role</strong>: <strong>Feature Store History</strong>. When you need to backfill data or fetch user history from 30 days ago.</li>
</ul>

<h3 id="34-summary-table">3.4 Summary Table</h3>
<p>| Tier | Technology | Latency | Capacity | Use Case |
| :— | :— | :— | :— | :— |
| <strong>L1</strong> | Python dict, C++ map | &lt; 0.1ms | 1-10GB | Hot keys, Global features |
| <strong>L2</strong> | Redis, Memcached | 1-5ms | 1-10TB | User features, Session data |
| <strong>L3</strong> | Aerospike, DAX | 5-20ms | Petabytes | Historical features, Cold users |</p>

<h2 id="4-deep-dive-consistent-hashing-for-distributed-caching">4. Deep Dive: Consistent Hashing for Distributed Caching</h2>

<p>When you scale Redis to 100 nodes, efficient key distribution is critical.
Naive hashing <code class="language-plaintext highlighter-rouge">hash(key) % N</code> is broken. If one node dies, <code class="language-plaintext highlighter-rouge">N</code> changes to <code class="language-plaintext highlighter-rouge">N-1</code>, and <strong>100% of keys are remapped</strong>. This causes a “Cache Miss Storm” that can take down your database.</p>

<h3 id="41-the-solution-virtual-nodes-on-a-ring">4.1 The Solution: Virtual Nodes on a Ring</h3>
<p>We map both Nodes and Keys to a 32-bit integer ring.</p>
<ul>
  <li>A Key belongs to the first Node found moving clockwise on the ring.</li>
  <li><strong>Virtual Nodes</strong>: To balance load, one physical server is hashed to 1,000 points on the ring.</li>
  <li><strong>Result</strong>: If a node dies, only <code class="language-plaintext highlighter-rouge">1/N</code> of keys need to be moved to the neighbor. <code class="language-plaintext highlighter-rouge">(N-1)/N</code> keys stay put.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">hashlib</span>
<span class="kn">import</span> <span class="n">bisect</span>

<span class="k">class</span> <span class="nc">ConsistentHash</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">nodes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">replicas</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">replicas</span> <span class="o">=</span> <span class="n">replicas</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ring</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sorted_keys</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">nodes</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">add_node</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add_node</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">replicas</span><span class="p">):</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_hash</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">node</span><span class="si">}</span><span class="s">:</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">ring</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span>
            <span class="n">bisect</span><span class="p">.</span><span class="nf">insort</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sorted_keys</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">remove_node</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">replicas</span><span class="p">):</span>
            <span class="n">key</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_hash</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">node</span><span class="si">}</span><span class="s">:</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="k">del</span> <span class="n">self</span><span class="p">.</span><span class="n">ring</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="n">self</span><span class="p">.</span><span class="n">sorted_keys</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_node</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">ring</span><span class="p">:</span> <span class="k">return</span> <span class="bp">None</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_hash</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="c1"># Binary Search for the next node on the ring
</span>        <span class="n">idx</span> <span class="o">=</span> <span class="n">bisect</span><span class="p">.</span><span class="nf">bisect</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sorted_keys</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sorted_keys</span><span class="p">):</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">ring</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">sorted_keys</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span>

    <span class="k">def</span> <span class="nf">_hash</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">int</span><span class="p">(</span><span class="n">hashlib</span><span class="p">.</span><span class="nf">md5</span><span class="p">(</span><span class="n">key</span><span class="p">.</span><span class="nf">encode</span><span class="p">()).</span><span class="nf">hexdigest</span><span class="p">(),</span> <span class="mi">16</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="42-replication-for-read-scalability">4.2 Replication for Read Scalability</h3>
<p>Each shard can have read replicas.</p>
<ul>
  <li><strong>Primary</strong>: Handles writes.</li>
  <li><strong>Replicas (2-3)</strong>: Handle reads.</li>
  <li><strong>Trade-off</strong>: Replication Lag. A write to Primary might not be visible on Replica for 10ms. For ML features, this is usually acceptable.</li>
</ul>

<hr />

<h2 id="5-embedding-caching-the-ml-specific-challenge">5. Embedding Caching: The ML-Specific Challenge</h2>

<p>Standard caching assumes values are small (&lt; 1KB). ML embeddings are large (512-dim float32 = 2KB) and there are billions of them.</p>

<h3 id="51-storage-optimization">5.1 Storage Optimization</h3>
<ul>
  <li><strong>Quantization</strong>: Store embeddings as <code class="language-plaintext highlighter-rouge">int8</code> instead of <code class="language-plaintext highlighter-rouge">float32</code>. Reduces storage by 4x with &lt; 1% accuracy loss.</li>
  <li><strong>Dimension Reduction</strong>: Use PCA or learned projections to reduce 512-dim to 128-dim. 4x storage reduction.</li>
</ul>

<h3 id="52-batch-fetching">5.2 Batch Fetching</h3>
<p>When ranking 1000 items, we need 1000 embeddings.</p>
<ul>
  <li><strong>Naive</strong>: 1000 sequential <code class="language-plaintext highlighter-rouge">GET</code> calls = 1000ms.</li>
  <li><strong>Pipeline</strong>: <code class="language-plaintext highlighter-rouge">MGET</code> (multi-get) = 5ms.
```python
    <h1 id="bad-sequential">Bad: Sequential</h1>
    <p>embeddings = [redis.get(f”item:{id}”) for id in item_ids]</p>
  </li>
</ul>

<h1 id="good-batch-pipeline">Good: Batch Pipeline</h1>
<p>pipe = redis.pipeline()
for id in item_ids:
    pipe.get(f”item:{id}”)
embeddings = pipe.execute()</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
### 5.3 Precomputation vs. On-Demand
-   **User Embeddings**: Change slowly (daily). Precompute and cache with 24h TTL.
-   **Item Embeddings**: Change on edit. Cache on first access, invalidate on update.
-   **Query Embeddings**: Ephemeral. Compute on-the-fly, cache for 5 minutes for repeat queries.

---

## 5. The "Thundering Herd" Problem &amp; Solutions

Scenario: A celebrity tweets. Millions of users request their profile.
The cache key `user_profile:123` expires at 12:00:00.
At 12:00:01, 10,000 concurrent requests hit the cache -&gt; Miss -&gt; Hit the Database.
**The Database Dies.**

### 5.1 Solution 1: Request Coalescing (SingleFlight)
Only ONE request per key is allowed to compute. All other requests for that key **wait** (block) until the first one returns.
-   Implemented in Go (`singleflight`), Nginx (`proxy_cache_lock`).

### 5.2 Solution 2: Probabilistic Early Expiration (PER)
Instead of a hard TTL, we fetch based on a probability.
-   `gap = time.now() - lease_start`
-   `probability = gap / TTL`
-   If `random() &lt; probability`, we recompute *before* expiry.
-   This spreads the refresh load over time.

---

## 6. Semantic Caching for LLMs (GPT-4 / Llama 3)

LLM queries are expensive and slow.
-   Q1: "Who is the President of France?"
-   Q2: "Who is the current leader of France?"
Traditional cache says **MISS** (Strings don't match).
**Semantic Cache** says **HIT**.

### 6.1 The Logic
1.  **Embed Query**: `v1 = BERT("Who is the President of France?")`
2.  **Vector Search**: Search Milvus/Faiss for nearest neighbor `v2`.
3.  **Distance Threshold**: If `Cosine_Similarity(v1, v2) &gt; 0.95`, return the cached answer associated with `v2`.

### 6.2 Architecture
-   **Store**: Vectors in Faiss, Metadata in Redis.
-   **Tradeoff**: 5ms Latency (Embedding + Search) vs 2000ms Latency (LLM Generation).
-   **Cost Savings**: Massive. 30-40% of queries in production chat apps are semantically identical.

---

## 7. Feature Caching Strategies

In Recommender Systems, we need to fetch user features (`[age, gender, clicks_7d]`) and item features (`[category, price, ctr]`).

### 7.1 Row-Oriented vs Column-Oriented Caching
-   **Row Cache (Redis Strings)**: `key=user:123` -&gt; `val=protobuf_blob`.
    -   Good for fetching single entities.
    -   **Serialization Formats**: Protobuf (fast, compact), JSON (readable, slower), MessagePack (good balance).
-   **Column Cache (Redis Lists/Bitmaps)**: `key=clicked_item:55` -&gt; `val=[user1, user2, user3]`.
    -   Good for "Find all users who clicked X."
    -   **Use Case**: Co-occurrence matrices for collaborative filtering.

### 7.2 Feature Versioning
ML features evolve. You might change `ctr_7d` calculation from "clicks/views" to "clicks/(views + 100)".
-   **Problem**: Old cached values are now stale/incorrect.
-   **Solution**: Include version in key: `user:123:v2`.
-   **Deployment**: On deploy, warm cache with new version keys. Old keys expire naturally.

### 7.3 TTL Strategies by Feature Type
| Feature Type | TTL | Rationale |
| :--- | :--- | :--- |
| **Static** (age, gender) | 24h | Changes rarely |
| **Slowly Changing** (preferences) | 1h | Updates daily |
| **Real-Time** (clicks_5min) | 60s | Must be fresh |
| **Computed** (ML score) | 5min | Expensive to compute |

### 7.2 The Cache Pushing Pattern (Write-Around vs Write-Through)
-   **Write-Through**: The application writes to DB and Cache simultaneously. Safe but slow.
-   **Cache-Aside (Lazy Loading)**: Read DB on miss. Preferred for most features.
-   **Push-Based (Stream Hydration)**:
    -   User clicks item.
    -   Kafka Event triggers Flink Job.
    -   Flink computes `clicks_7d`.
    -   Flink **Pushes** update directly to Redis.
    -   The Inference service serves strictly from Redis (never computes).

---

## 8. Failure Modes

### 8.1 Hot Keys
One key (e.g., "Justin Bieber") gets more traffic than a single Redis shard can handle.
-   **Fix**: **Local Cache Replica**. Detect hot keys and replicate them to L1 (Application Memory).
-   **Fix**: **Key Splitting**. Store `bieber` as `bieber:1`, `bieber:2`. Randomly read one.

### 8.2 Network Bandwidth Saturation
Fetching 1000 items * 2KB embedding = 2MB payload per request.
At 1000 QPS, that is **2 GB/s**. This saturates a 10Gbps NIC.
-   **Fix**: Compression (Zstd, LZ4).
-   **Fix**: Quantization (Store float32 embeddings as int8).

---

## 9. Real-World Look: Facebook TAO

Facebook's "The Association Object" (TAO) cache handles billions of reads/sec.
-   **Graph-Aware**: It caches edges (`User -&gt; Likes -&gt; Page`) not just keys.
-   **Eventual Consistency**: It allows followers to be stale for seconds.
-   **Lease Mechanism**: Preventing thundering herds using "Leases" (tokens) given to clients.

---

## 10. Cache Invalidation: The Two Hardest Problems

Phil Karlton famously said: "There are only two hard things in Computer Science: cache invalidation and naming things."

### 10.1 Time-Based Invalidation (TTL)
The simplest strategy. Set `EXPIRE key 3600` (1 hour).
-   **Pros**: Simple. No coordination needed.
-   **Cons**: User might see stale data for up to 1 hour. Hot features like "Live Stock Price" cannot tolerate this.

### 10.2 Event-Based Invalidation (CDC)
Use Change Data Capture (Debezium, Maxwell) to listen to database changes.
-   User updates their profile in PostgreSQL.
-   Debezium captures the WAL event.
-   Kafka delivers `{user_id: 123, type: UPDATE}` to our Invalidation Consumer.
-   Consumer calls `DEL user:123` on Redis.
-   Next read fetches fresh data from DB and populates cache.

**Implementation Pattern**:
```python
from kafka import KafkaConsumer
import redis

consumer = KafkaConsumer('db.users', bootstrap_servers=['kafka:9092'])
r = redis.Redis()

for message in consumer:
    event = json.loads(message.value)
    user_id = event['after']['id']
    
    # Invalidate the cache for this user
    r.delete(f"user:{user_id}")
    r.delete(f"user_embedding:{user_id}")
    
    print(f"Invalidated cache for user {user_id}")
</code></pre></div></div>

<h3 id="103-the-dual-write-anti-pattern">10.3 The Dual-Write Anti-Pattern</h3>
<p>Never do this in production:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># WRONG: Race Condition!
</span><span class="n">db</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">user</span><span class="p">)</span>
<span class="n">cache</span><span class="p">.</span><span class="nf">delete</span><span class="p">(</span><span class="n">user_key</span><span class="p">)</span>
</code></pre></div></div>
<p>If <code class="language-plaintext highlighter-rouge">cache.delete</code> fails (network hiccup), your cache is forever stale.</p>

<p><strong>Safer Pattern</strong>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># RIGHT: Transactional Outbox
</span><span class="n">db</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">user</span><span class="p">)</span>
<span class="n">db</span><span class="p">.</span><span class="nf">insert_into_outbox</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>

<span class="c1"># Separate worker polls outbox
</span><span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">db</span><span class="p">.</span><span class="nf">poll_outbox</span><span class="p">():</span>
    <span class="n">cache</span><span class="p">.</span><span class="nf">delete</span><span class="p">(</span><span class="n">event</span><span class="p">.</span><span class="n">key</span><span class="p">)</span>
    <span class="n">db</span><span class="p">.</span><span class="nf">delete_from_outbox</span><span class="p">(</span><span class="n">event</span><span class="p">.</span><span class="nb">id</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="11-monitoring-and-observability">11. Monitoring and Observability</h2>

<p>You cannot manage what you cannot measure. A production cache requires rigorous observability.</p>

<h3 id="111-key-metrics-to-track">11.1 Key Metrics to Track</h3>
<p>| Metric | Target | Alarm Threshold |
| :— | :— | :— |
| <strong>Cache Hit Ratio (CHR)</strong> | &gt; 90% | &lt; 80% |
| <strong>p99 Latency (Redis)</strong> | &lt; 5ms | &gt; 15ms |
| <strong>Memory Utilization</strong> | &lt; 80% | &gt; 90% |
| <strong>Eviction Rate</strong> | Low | Sudden Spike |
| <strong>Connection Pool Exhaustion</strong> | 0 | &gt; 0 |</p>

<h3 id="112-instrumenting-your-cache-client">11.2 Instrumenting Your Cache Client</h3>
<p>Wrap your Redis calls with metrics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="n">prometheus_client</span> <span class="kn">import</span> <span class="n">Histogram</span><span class="p">,</span> <span class="n">Counter</span>

<span class="n">CACHE_LATENCY</span> <span class="o">=</span> <span class="nc">Histogram</span><span class="p">(</span><span class="sh">'</span><span class="s">cache_latency_seconds</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Cache operation latency</span><span class="sh">'</span><span class="p">,</span> <span class="p">[</span><span class="sh">'</span><span class="s">op</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">status</span><span class="sh">'</span><span class="p">])</span>
<span class="n">CACHE_HITS</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span><span class="sh">'</span><span class="s">cache_hits_total</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Number of cache hits</span><span class="sh">'</span><span class="p">)</span>
<span class="n">CACHE_MISSES</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span><span class="sh">'</span><span class="s">cache_misses_total</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Number of cache misses</span><span class="sh">'</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">InstrumentedCache</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">redis_client</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">redis</span> <span class="o">=</span> <span class="n">redis_client</span>
        
    <span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">redis</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
        <span class="n">duration</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">CACHE_MISSES</span><span class="p">.</span><span class="nf">inc</span><span class="p">()</span>
            <span class="n">CACHE_LATENCY</span><span class="p">.</span><span class="nf">labels</span><span class="p">(</span><span class="n">op</span><span class="o">=</span><span class="sh">'</span><span class="s">get</span><span class="sh">'</span><span class="p">,</span> <span class="n">status</span><span class="o">=</span><span class="sh">'</span><span class="s">miss</span><span class="sh">'</span><span class="p">).</span><span class="nf">observe</span><span class="p">(</span><span class="n">duration</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">CACHE_HITS</span><span class="p">.</span><span class="nf">inc</span><span class="p">()</span>
            <span class="n">CACHE_LATENCY</span><span class="p">.</span><span class="nf">labels</span><span class="p">(</span><span class="n">op</span><span class="o">=</span><span class="sh">'</span><span class="s">get</span><span class="sh">'</span><span class="p">,</span> <span class="n">status</span><span class="o">=</span><span class="sh">'</span><span class="s">hit</span><span class="sh">'</span><span class="p">).</span><span class="nf">observe</span><span class="p">(</span><span class="n">duration</span><span class="p">)</span>
            
        <span class="k">return</span> <span class="n">value</span>
</code></pre></div></div>

<h3 id="113-dashboards-you-need">11.3 Dashboards You Need</h3>
<ol>
  <li><strong>Hit Ratio over Time</strong>: Detects model drift or changed usage patterns.</li>
  <li><strong>Latency Heatmap</strong>: Identifies slow shards or network issues.</li>
  <li><strong>Memory Fragmentation</strong>: Redis can use 2x memory due to fragmentation. Track <code class="language-plaintext highlighter-rouge">mem_fragmentation_ratio</code>.</li>
</ol>

<hr />

<h2 id="12-kv-cache-for-transformers-llm-inference">12. KV Cache for Transformers (LLM Inference)</h2>

<p>This is a specialized cache that lives <em>inside</em> the model inference, not in the infrastructure.</p>

<h3 id="121-the-problem">12.1 The Problem</h3>
<p>In Transformers (GPT-4, Llama 3), generating 100 tokens requires computing Attention N times.
Each Attention computation re-uses the same Key and Value vectors for the <em>previous</em> tokens.
Without caching, you recompute these KV pairs at every step. This is O(N^2) for N tokens.</p>

<h3 id="122-the-kv-cache-solution">12.2 The KV Cache Solution</h3>
<p>We store the Key and Value matrices in GPU VRAM.</p>
<ul>
  <li>Token 1: Compute K1, V1. Store.</li>
  <li>Token 2: Load K1, V1. Compute K2, V2. Store K1, K2, V1, V2.</li>
  <li>Token 50: Load K1..K49, V1..V49. Compute K50, V50. Store.</li>
</ul>

<p><strong>VRAM Cost</strong>: For Llama 70B, the KV Cache for 4096 tokens can consume <strong>40GB</strong> of VRAM. This is often the bottleneck, not the model weights.</p>

<h3 id="123-optimization-paged-attention-vllm">12.3 Optimization: Paged Attention (vLLM)</h3>
<p>vLLM (from UC Berkeley) treats KV Cache like OS Virtual Memory.</p>
<ul>
  <li><strong>Pages</strong>: Divide the cache into fixed-size “Pages” (e.g., 256 tokens per page).</li>
  <li><strong>On-Demand Allocation</strong>: Only allocate pages when needed.</li>
  <li><strong>Sharing</strong>: If two prompts share a prefix, they can share the KV pages for that prefix.</li>
  <li><strong>Result</strong>: 24x higher throughput than naive HuggingFace <code class="language-plaintext highlighter-rouge">generate()</code>.</li>
</ul>

<hr />

<h2 id="13-production-case-study-pinterests-feature-store-caching">13. Production Case Study: Pinterest’s Feature Store Caching</h2>

<p>Pinterest serves 500M+ users. Their recommender system needs to fetch ~1000 features per request.</p>

<h3 id="131-the-architecture">13.1 The Architecture</h3>
<ul>
  <li><strong>L1 (Application)</strong>: 5GB LRU cache per pod. Holds global features (e.g., “is_weekend”).</li>
  <li><strong>L2 (Redis Cluster)</strong>: 500 nodes, 10TB capacity. Holds user features.</li>
  <li><strong>L3 (Rockstore/SSTable)</strong>: Hourly snapshots of the feature store for cold users.</li>
</ul>

<h3 id="132-key-optimizations">13.2 Key Optimizations</h3>
<ol>
  <li><strong>Feature Packing</strong>: Instead of 1000 <code class="language-plaintext highlighter-rouge">GET</code> calls, they pack all features for a user into a single Protobuf blob. 1 <code class="language-plaintext highlighter-rouge">GET</code> per user.</li>
  <li><strong>Speculative Warming</strong>: When a user logs in, a background job pre-fetches their features into L2 before the first recommendation request.</li>
  <li><strong>Tiered Eviction</strong>: Global features have TTL=infinity. User features have TTL=7 days. Ephemeral features (session data) have TTL=10 minutes.</li>
</ol>

<h3 id="133-results">13.3 Results</h3>
<ul>
  <li><strong>p50 Latency</strong>: 2ms (L1 hit).</li>
  <li><strong>p99 Latency</strong>: 8ms (L2 hit).</li>
  <li><strong>Cache Hit Ratio</strong>: 98.5%.</li>
  <li><strong>Cost Savings</strong>: 90% reduction in Feature Store SSD reads.</li>
</ul>

<hr />

<h2 id="14-advanced-topic-negative-caching">14. Advanced Topic: Negative Caching</h2>

<p>What happens when you query for a user that doesn’t exist?</p>
<ul>
  <li>Request: <code class="language-plaintext highlighter-rouge">GET user:99999999</code></li>
  <li>Redis: MISS</li>
  <li>Database: <code class="language-plaintext highlighter-rouge">SELECT * FROM users WHERE id=99999999</code> -&gt; Empty.</li>
  <li><strong>Problem</strong>: Next request repeats this expensive DB query.</li>
</ul>

<h3 id="141-the-solution-cache-the-null">14.1 The Solution: Cache the Null</h3>
<p>Store a special sentinel value:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">value</span> <span class="o">=</span> <span class="n">db</span><span class="p">.</span><span class="nf">get_user</span><span class="p">(</span><span class="n">user_id</span><span class="p">)</span>
<span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">cache</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">user:</span><span class="si">{</span><span class="n">user_id</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">NULL</span><span class="sh">"</span><span class="p">,</span> <span class="n">ttl</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>  <span class="c1"># Cache the non-existence
</span><span class="k">else</span><span class="p">:</span>
    <span class="n">cache</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">user:</span><span class="si">{</span><span class="n">user_id</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="nf">serialize</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="n">ttl</span><span class="o">=</span><span class="mi">3600</span><span class="p">)</span>
</code></pre></div></div>
<p>On read:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cached</span> <span class="o">=</span> <span class="n">cache</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">user:</span><span class="si">{</span><span class="n">user_id</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="k">if</span> <span class="n">cached</span> <span class="o">==</span> <span class="sh">"</span><span class="s">NULL</span><span class="sh">"</span><span class="p">:</span>
    <span class="k">return</span> <span class="bp">None</span>  <span class="c1"># Don't hit DB
</span></code></pre></div></div>

<h3 id="142-danger-botnet-attacks">14.2 Danger: Botnet Attacks</h3>
<p>If an attacker queries millions of random non-existent IDs, you fill your cache with “NULL” entries, evicting real data.</p>
<ul>
  <li><strong>Fix</strong>: Use a <strong>Bloom Filter</strong> in front of the cache. Only check DB if the Bloom Filter says “Maybe Exists.”</li>
</ul>

<hr />

<h2 id="15-cache-warming-strategies">15. Cache Warming Strategies</h2>

<p>A cold cache is a dangerous cache. The first 5 minutes after deployment can see massive latency spikes.</p>

<h3 id="151-pre-warming-on-deploy">15.1 Pre-Warming on Deploy</h3>
<p>Before routing traffic to a new pod:</p>
<ol>
  <li><strong>Query Log Replay</strong>: Replay the last hour of cache access logs against the new pod.</li>
  <li><strong>Feature Snapshot Load</strong>: Load a snapshot of the top 10,000 most popular features directly into L1.</li>
  <li><strong>Health Check with Warm Threshold</strong>: Don’t mark the pod “Ready” until Cache Hit Ratio &gt; 50%.</li>
</ol>

<h3 id="152-shadow-traffic">15.2 Shadow Traffic</h3>
<p>Before cutting over to a new cache cluster:</p>
<ul>
  <li><strong>Fork Traffic</strong>: Send a copy of all production requests to the new cluster (without returning results to users).</li>
  <li><strong>Goal</strong>: Pre-populate the cache with real access patterns.</li>
  <li><strong>Duration</strong>: Run for 10 minutes before cutting over.</li>
</ul>

<h3 id="153-scheduled-warming-for-time-bound-features">15.3 Scheduled Warming for Time-Bound Features</h3>
<p>Features like <code class="language-plaintext highlighter-rouge">day_of_week</code> or <code class="language-plaintext highlighter-rouge">is_holiday</code> are known in advance.</p>
<ul>
  <li><strong>CRON Job</strong>: At midnight, compute and push all time-based features for the next 24 hours.</li>
  <li><strong>Result</strong>: Zero cold-start latency for the first recommendation of the day.</li>
</ul>

<hr />

<h2 id="16-production-best-practices">16. Production Best Practices</h2>

<p>Based on experience operating ML caches at scale:</p>

<h3 id="161-separate-caches-by-sla">16.1 Separate Caches by SLA</h3>
<p>Don’t mix real-time serving cache with batch processing cache.</p>
<ul>
  <li><strong>Serving Cache</strong>: Low TTL, High Memory Priority, Strict SLA.</li>
  <li><strong>Training Cache</strong>: High TTL, Lower Priority, Best Effort.</li>
  <li><strong>Risk</strong>: A batch job could evict serving data.</li>
</ul>

<h3 id="162-use-client-side-connection-pooling">16.2 Use Client-Side Connection Pooling</h3>
<p>Every Redis call opens a TCP connection. Opening connections is slow (3ms handshake).</p>
<ul>
  <li>Use a <strong>Connection Pool</strong> (e.g., <code class="language-plaintext highlighter-rouge">redis-py</code> with <code class="language-plaintext highlighter-rouge">ConnectionPool</code>).</li>
  <li>Set <code class="language-plaintext highlighter-rouge">max_connections</code> based on your QPS and concurrency.</li>
</ul>

<h3 id="163-monitor-memory-fragmentation">16.3 Monitor Memory Fragmentation</h3>
<p>Redis uses jemalloc. Heavy SET/DEL cycles lead to fragmentation.</p>
<ul>
  <li><strong>Metric</strong>: <code class="language-plaintext highlighter-rouge">mem_fragmentation_ratio</code> &gt; 1.5 is a warning sign.</li>
  <li><strong>Fix</strong>: Schedule periodic <code class="language-plaintext highlighter-rouge">MEMORY DOCTOR</code> and consider <code class="language-plaintext highlighter-rouge">activedefrag yes</code>.</li>
</ul>

<h3 id="164-implement-circuit-breakers">16.4 Implement Circuit Breakers</h3>
<p>If Redis is down, don’t hammer it with retries.</p>
<ul>
  <li>Use a <strong>Circuit Breaker</strong> (Hystrix, resilience4j, pybreaker).</li>
  <li>When open, serve from L1 cache only or return a default/fallback prediction.</li>
</ul>

<hr />

<h2 id="17-key-takeaways">17. Key Takeaways</h2>

<ol>
  <li><strong>Caching is the First Line of Defense</strong>: Protect your expensive GPU inference layers with a multi-tier hierarchy.</li>
  <li><strong>Consistency vs. Latency</strong>: In ML, it is often better to serve a stale prediction (eventual consistency) than to wait.</li>
  <li><strong>Context Matters</strong>: Choose your eviction policy based on the “intent” of the data. LFU for popular items, LRU for temporal access.</li>
  <li><strong>Semantic Search is the Future</strong>: Move beyond exact-match keys to vector similarity for LLM caching.</li>
  <li><strong>Invalidation is Hard</strong>: Use CDC/Event-based invalidation, not dual-writes.</li>
  <li><strong>Monitor Everything</strong>: Cache Hit Ratio, Latency p99, Eviction Rate are your core metrics.</li>
</ol>

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0060-advanced-caching/">arunbaby.com/ml-system-design/0060-advanced-caching</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#caching" class="page__taxonomy-item p-category" rel="tag">caching</a><span class="sep">, </span>
    
      <a href="/tags/#consistency" class="page__taxonomy-item p-category" rel="tag">consistency</a><span class="sep">, </span>
    
      <a href="/tags/#distributed-systems" class="page__taxonomy-item p-category" rel="tag">distributed-systems</a><span class="sep">, </span>
    
      <a href="/tags/#feature-store" class="page__taxonomy-item p-category" rel="tag">feature-store</a><span class="sep">, </span>
    
      <a href="/tags/#infrastructure" class="page__taxonomy-item p-category" rel="tag">infrastructure</a><span class="sep">, </span>
    
      <a href="/tags/#latent-variables" class="page__taxonomy-item p-category" rel="tag">latent-variables</a><span class="sep">, </span>
    
      <a href="/tags/#redis" class="page__taxonomy-item p-category" rel="tag">redis</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0060-lfu-cache/" rel="permalink">LFU Cache (Least Frequently Used)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Designing an LFU Cache is the ultimate exercise in composite data structures—it forces you to synchronize multiple hash maps and linked lists to achieve O(1...</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0060-multi-tier-speech-caching/" rel="permalink">Multi-tier Speech Caching Architecture
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          16 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Speech models are computationally the most expensive per byte of input. Multi-tier caching is the only way to scale voice assistants to millions of users wi...</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0060-future-of-ai-agents/" rel="permalink">The Future of AI Agents: 2025 and Beyond
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The agents of today are assistants; the agents of tomorrow will be colleagues. We are moving from a world where we tell AI what to do, to a world where AI t...</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Advanced+Caching+for+ML+Systems%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0060-advanced-caching%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0060-advanced-caching%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0060-advanced-caching/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0059-automl-systems/" class="pagination--pager" title="AutoML Systems at Scale">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
