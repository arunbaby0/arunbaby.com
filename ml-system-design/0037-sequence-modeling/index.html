<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Sequence Modeling in ML - Arun Baby</title>
<meta name="description" content="“Predicting the next word, the next stock price, the next frame.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Sequence Modeling in ML">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">


  <meta property="og:description" content="“Predicting the next word, the next stock price, the next frame.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Sequence Modeling in ML">
  <meta name="twitter:description" content="“Predicting the next word, the next stock price, the next frame.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-02T21:46:58+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Sequence Modeling in ML">
    <meta itemprop="description" content="“Predicting the next word, the next stock price, the next frame.”">
    <meta itemprop="datePublished" content="2025-12-02T21:46:58+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/" itemprop="url">Sequence Modeling in ML
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-the-problem-sequential-data">1. The Problem: Sequential Data</a></li><li><a href="#2-evolution-of-sequence-models">2. Evolution of Sequence Models</a><ul><li><a href="#1-recurrent-neural-networks-rnn">1. Recurrent Neural Networks (RNN)</a></li><li><a href="#2-lstm-long-short-term-memory">2. LSTM (Long Short-Term Memory)</a></li><li><a href="#3-transformer-attention-is-all-you-need">3. Transformer (Attention is All You Need)</a></li></ul></li><li><a href="#3-transformer-architecture">3. Transformer Architecture</a></li><li><a href="#4-training-strategies">4. Training Strategies</a><ul><li><a href="#1-teacher-forcing">1. Teacher Forcing</a></li><li><a href="#2-scheduled-sampling">2. Scheduled Sampling</a></li><li><a href="#3-curriculum-learning">3. Curriculum Learning</a></li></ul></li><li><a href="#5-inference-strategies">5. Inference Strategies</a><ul><li><a href="#1-greedy-decoding">1. Greedy Decoding</a></li><li><a href="#2-beam-search">2. Beam Search</a></li><li><a href="#3-sampling">3. Sampling</a></li></ul></li><li><a href="#6-system-design-real-time-translation">6. System Design: Real-Time Translation</a></li><li><a href="#7-case-study-gpt-3-serving">7. Case Study: GPT-3 Serving</a></li><li><a href="#8-summary">8. Summary</a></li><li><a href="#9-deep-dive-attention-mechanism-math">9. Deep Dive: Attention Mechanism Math</a></li><li><a href="#10-deep-dive-kv-caching-for-autoregressive-decoding">10. Deep Dive: KV Caching for Autoregressive Decoding</a></li><li><a href="#11-deep-dive-flash-attention">11. Deep Dive: Flash Attention</a></li><li><a href="#12-deep-dive-positional-encoding">12. Deep Dive: Positional Encoding</a></li><li><a href="#13-system-design-chatbot-with-context">13. System Design: Chatbot with Context</a></li><li><a href="#14-deep-dive-beam-search-implementation">14. Deep Dive: Beam Search Implementation</a></li><li><a href="#15-production-optimizations">15. Production Optimizations</a><ul><li><a href="#1-model-quantization">1. Model Quantization</a></li><li><a href="#2-distillation">2. Distillation</a></li><li><a href="#3-pruning">3. Pruning</a></li></ul></li><li><a href="#16-summary">16. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Predicting the next word, the next stock price, the next frame.”</strong></p>

<h2 id="1-the-problem-sequential-data">1. The Problem: Sequential Data</h2>

<p>Many real-world problems involve sequences:</p>
<ul>
  <li><strong>Text:</strong> “The cat sat on the ___” → “mat”</li>
  <li><strong>Time Series:</strong> Stock prices, weather, sensor data.</li>
  <li><strong>Video:</strong> Predict next frame.</li>
  <li><strong>Audio:</strong> Speech recognition, music generation.</li>
</ul>

<p><strong>Challenge:</strong> The output depends on <strong>context</strong> (previous elements in the sequence).</p>

<h2 id="2-evolution-of-sequence-models">2. Evolution of Sequence Models</h2>

<h3 id="1-recurrent-neural-networks-rnn">1. Recurrent Neural Networks (RNN)</h3>
<ul>
  <li><strong>Idea:</strong> Hidden state $h_t$ carries information from previous steps.</li>
  <li><strong>Equation:</strong> $h_t = \tanh(W_h h_{t-1} + W_x x_t + b)$</li>
  <li><strong>Problem:</strong> Vanishing gradients. Can’t remember long-term dependencies.</li>
</ul>

<h3 id="2-lstm-long-short-term-memory">2. LSTM (Long Short-Term Memory)</h3>
<ul>
  <li><strong>Gates:</strong> Forget, Input, Output gates control information flow.</li>
  <li><strong>Advantage:</strong> Can remember 100+ steps.</li>
  <li><strong>Disadvantage:</strong> Sequential processing (can’t parallelize).</li>
</ul>

<h3 id="3-transformer-attention-is-all-you-need">3. Transformer (Attention is All You Need)</h3>
<ul>
  <li><strong>Self-Attention:</strong> Every token attends to every other token.</li>
  <li><strong>Parallelization:</strong> Process entire sequence at once.</li>
  <li><strong>Scalability:</strong> Powers GPT, BERT, LLaMA.</li>
</ul>

<h2 id="3-transformer-architecture">3. Transformer Architecture</h2>

<p><strong>Key Components:</strong></p>

<ol>
  <li><strong>Self-Attention:</strong>
\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)
    <ul>
      <li><strong>Query (Q):</strong> “What am I looking for?”</li>
      <li><strong>Key (K):</strong> “What do I have?”</li>
      <li><strong>Value (V):</strong> “What information do I provide?”</li>
    </ul>
  </li>
  <li><strong>Multi-Head Attention:</strong>
    <ul>
      <li>Run attention multiple times with different learned projections.</li>
      <li>Allows model to attend to different aspects (syntax, semantics, etc.).</li>
    </ul>
  </li>
  <li><strong>Positional Encoding:</strong>
    <ul>
      <li>Transformers have no notion of order.</li>
      <li>Add sinusoidal encodings: $PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$</li>
    </ul>
  </li>
  <li><strong>Feed-Forward Network:</strong>
    <ul>
      <li>Two linear layers with ReLU: $\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$</li>
    </ul>
  </li>
</ol>

<h2 id="4-training-strategies">4. Training Strategies</h2>

<h3 id="1-teacher-forcing">1. Teacher Forcing</h3>
<ul>
  <li>During training, feed ground truth as input (even if model predicted wrong).</li>
  <li><strong>Pro:</strong> Faster convergence.</li>
  <li><strong>Con:</strong> Exposure bias (model never sees its own mistakes during training).</li>
</ul>

<h3 id="2-scheduled-sampling">2. Scheduled Sampling</h3>
<ul>
  <li>Gradually mix ground truth with model predictions during training.</li>
  <li>Start with 100% teacher forcing, decay to 0%.</li>
</ul>

<h3 id="3-curriculum-learning">3. Curriculum Learning</h3>
<ul>
  <li>Start with short sequences, gradually increase length.</li>
</ul>

<h2 id="5-inference-strategies">5. Inference Strategies</h2>

<h3 id="1-greedy-decoding">1. Greedy Decoding</h3>
<ul>
  <li>At each step, pick the token with highest probability.</li>
  <li><strong>Fast</strong> but <strong>suboptimal</strong>.</li>
</ul>

<h3 id="2-beam-search">2. Beam Search</h3>
<ul>
  <li>Keep top-$k$ candidates at each step.</li>
  <li>Explore multiple paths, pick the best overall sequence.</li>
  <li><strong>Better quality</strong> but <strong>slower</strong>.</li>
</ul>

<h3 id="3-sampling">3. Sampling</h3>
<ul>
  <li>Sample from the probability distribution.</li>
  <li><strong>Temperature:</strong> Control randomness.
    <ul>
      <li>$T \to 0$: Greedy (deterministic).</li>
      <li>$T \to \infty$: Uniform (random).</li>
    </ul>
  </li>
</ul>

<h2 id="6-system-design-real-time-translation">6. System Design: Real-Time Translation</h2>

<p><strong>Scenario:</strong> Google Translate (Text-to-Text).</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Encoder:</strong> Processes source sentence (English).</li>
  <li><strong>Decoder:</strong> Generates target sentence (French).</li>
  <li><strong>Attention:</strong> Decoder attends to relevant encoder states.</li>
</ol>

<p><strong>Optimizations:</strong></p>
<ul>
  <li><strong>Caching:</strong> Cache encoder output (source doesn’t change).</li>
  <li><strong>Batching:</strong> Process multiple requests together.</li>
  <li><strong>Quantization:</strong> INT8 for 4x speedup.</li>
</ul>

<h2 id="7-case-study-gpt-3-serving">7. Case Study: GPT-3 Serving</h2>

<p><strong>Challenge:</strong> Serve a 175B parameter model with low latency.</p>

<p><strong>Solutions:</strong></p>
<ol>
  <li><strong>Model Parallelism:</strong> Split model across 8 GPUs.</li>
  <li><strong>KV Cache:</strong> Cache key/value tensors from previous tokens.</li>
  <li><strong>Speculative Decoding:</strong> Use a small model to draft, large model to verify.</li>
</ol>

<h2 id="8-summary">8. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: left">Pros</th>
      <th style="text-align: left">Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>RNN</strong></td>
      <td style="text-align: left">Simple</td>
      <td style="text-align: left">Vanishing gradients</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>LSTM</strong></td>
      <td style="text-align: left">Long memory</td>
      <td style="text-align: left">Sequential (slow)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Transformer</strong></td>
      <td style="text-align: left">Parallel, Scalable</td>
      <td style="text-align: left">$O(N^2)$ memory</td>
    </tr>
  </tbody>
</table>

<h2 id="9-deep-dive-attention-mechanism-math">9. Deep Dive: Attention Mechanism Math</h2>

<p>Let’s break down the attention formula step by step.</p>

<p><strong>Input:</strong> Sequence of vectors $X = [x_1, x_2, …, x_n]$, each $x_i \in \mathbb{R}^d$.</p>

<p><strong>Step 1: Linear Projections</strong>
\(Q = XW_Q, \quad K = XW_K, \quad V = XW_V\)</p>
<ul>
  <li>$W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}$ are learned weight matrices.</li>
</ul>

<p><strong>Step 2: Compute Attention Scores</strong>
\(S = \frac{QK^T}{\sqrt{d_k}}\)</p>
<ul>
  <li>$S \in \mathbb{R}^{n \times n}$. Entry $S_{ij}$ is the “compatibility” between query $i$ and key $j$.</li>
  <li>Divide by $\sqrt{d_k}$ to prevent dot products from becoming too large (which would make softmax saturate).</li>
</ul>

<p><strong>Step 3: Softmax</strong>
\(A = \text{softmax}(S)\)</p>
<ul>
  <li>Each row sums to 1. $A_{ij}$ is the “attention weight” from position $i$ to position $j$.</li>
</ul>

<p><strong>Step 4: Weighted Sum</strong>
\(\text{Output} = AV\)</p>
<ul>
  <li>Each output vector is a weighted combination of all value vectors.</li>
</ul>

<h2 id="10-deep-dive-kv-caching-for-autoregressive-decoding">10. Deep Dive: KV Caching for Autoregressive Decoding</h2>

<p><strong>Problem:</strong> When generating token $t$, we recompute attention for tokens $1, 2, …, t-1$ (wasteful!).</p>

<p><strong>Solution:</strong> Cache the Key and Value matrices.</p>

<p><strong>Without Caching:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
    <span class="c1"># Recompute K, V for all previous tokens
</span>    <span class="n">K</span> <span class="o">=</span> <span class="nf">compute_keys</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># O(t * d^2)
</span>    <span class="n">V</span> <span class="o">=</span> <span class="nf">compute_values</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nf">attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>With Caching:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">K_cache</span><span class="p">,</span> <span class="n">V_cache</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
    <span class="c1"># Only compute K, V for new token
</span>    <span class="n">k_t</span> <span class="o">=</span> <span class="nf">compute_key</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>  <span class="c1"># O(d^2)
</span>    <span class="n">v_t</span> <span class="o">=</span> <span class="nf">compute_value</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    
    <span class="n">K_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">k_t</span><span class="p">)</span>
    <span class="n">V_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">v_t</span><span class="p">)</span>
    
    <span class="n">K</span> <span class="o">=</span> <span class="nf">concat</span><span class="p">(</span><span class="n">K_cache</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">concat</span><span class="p">(</span><span class="n">V_cache</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nf">attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Speedup:</strong> $O(T^2)$ → $O(T)$ per token.</p>

<h2 id="11-deep-dive-flash-attention">11. Deep Dive: Flash Attention</h2>

<p><strong>Problem:</strong> Standard attention requires materializing the $N \times N$ attention matrix in memory.</p>
<ul>
  <li>For $N = 4096$, that’s 16M floats = 64MB per head.</li>
  <li>GPT-3 has 96 heads → 6GB just for attention!</li>
</ul>

<p><strong>Flash Attention (Dao et al., 2022):</strong></p>
<ul>
  <li><strong>Idea:</strong> Compute attention in blocks, never materialize the full matrix.</li>
  <li><strong>Algorithm:</strong>
    <ol>
      <li>Divide $Q, K, V$ into blocks.</li>
      <li>Load one block of $Q$ and one block of $K$ into SRAM.</li>
      <li>Compute partial attention scores.</li>
      <li>Accumulate results.</li>
    </ol>
  </li>
  <li><strong>Speedup:</strong> 2-4x faster, uses less memory.</li>
</ul>

<h2 id="12-deep-dive-positional-encoding">12. Deep Dive: Positional Encoding</h2>

<p>Transformers have no notion of order. We add positional information.</p>

<p><strong>Sinusoidal Encoding (Original Transformer):</strong>
\(PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)\)
\(PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)\)</p>

<p><strong>Why this works:</strong></p>
<ul>
  <li>Different frequencies for different dimensions.</li>
  <li>Model can learn to attend to relative positions.</li>
</ul>

<p><strong>Learned Positional Embeddings (BERT, GPT):</strong></p>
<ul>
  <li>Treat position as a lookup table.</li>
  <li>More flexible, but limited to max sequence length seen during training.</li>
</ul>

<p><strong>Rotary Position Embedding (RoPE):</strong></p>
<ul>
  <li>Used in LLaMA, PaLM.</li>
  <li>Rotates query and key vectors based on position.</li>
  <li>Allows extrapolation to longer sequences.</li>
</ul>

<h2 id="13-system-design-chatbot-with-context">13. System Design: Chatbot with Context</h2>

<p><strong>Scenario:</strong> Build a chatbot that remembers conversation history.</p>

<p><strong>Challenge:</strong> Context grows with each turn. How to handle long conversations?</p>

<p><strong>Approach 1: Sliding Window</strong></p>
<ul>
  <li>Keep only last $N$ tokens (e.g., 2048).</li>
  <li><strong>Pro:</strong> Fixed memory.</li>
  <li><strong>Con:</strong> Forgets old context.</li>
</ul>

<p><strong>Approach 2: Summarization</strong></p>
<ul>
  <li>Periodically summarize old context.</li>
  <li><strong>Pro:</strong> Retains important info.</li>
  <li><strong>Con:</strong> Lossy compression.</li>
</ul>

<p><strong>Approach 3: Retrieval-Augmented</strong></p>
<ul>
  <li>Store conversation in a vector DB.</li>
  <li>Retrieve relevant past messages based on current query.</li>
  <li><strong>Pro:</strong> Scalable to infinite history.</li>
  <li><strong>Con:</strong> Requires embedding model + DB.</li>
</ul>

<h2 id="14-deep-dive-beam-search-implementation">14. Deep Dive: Beam Search Implementation</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">beam_search</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">start_token</span><span class="p">,</span> <span class="n">beam_width</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="c1"># Initialize beam with start token
</span>    <span class="n">beams</span> <span class="o">=</span> <span class="p">[(</span><span class="n">start_token</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)]</span>  <span class="c1"># (sequence, log_prob)
</span>    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
        <span class="n">candidates</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">seq</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">beams</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">EOS_TOKEN</span><span class="p">:</span>
                <span class="n">candidates</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">seq</span><span class="p">,</span> <span class="n">score</span><span class="p">))</span>
                <span class="k">continue</span>
            
            <span class="c1"># Get next token probabilities
</span>            <span class="n">logits</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
            <span class="n">log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Top-k tokens
</span>            <span class="n">top_k_probs</span><span class="p">,</span> <span class="n">top_k_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">beam_width</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">top_k_probs</span><span class="p">,</span> <span class="n">top_k_tokens</span><span class="p">):</span>
                <span class="n">new_seq</span> <span class="o">=</span> <span class="n">seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="nf">item</span><span class="p">()]</span>
                <span class="n">new_score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">+</span> <span class="n">prob</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
                <span class="n">candidates</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">new_seq</span><span class="p">,</span> <span class="n">new_score</span><span class="p">))</span>
        
        <span class="c1"># Keep top beam_width candidates
</span>        <span class="n">beams</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">candidates</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="n">beam_width</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">beams</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Best sequence
</span></code></pre></div></div>

<h2 id="15-production-optimizations">15. Production Optimizations</h2>

<h3 id="1-model-quantization">1. Model Quantization</h3>
<ul>
  <li>Convert FP32 → INT8.</li>
  <li><strong>Speedup:</strong> 4x.</li>
  <li><strong>Accuracy Loss:</strong> &lt; 1% with careful calibration.</li>
</ul>

<h3 id="2-distillation">2. Distillation</h3>
<ul>
  <li>Train a small “student” model to mimic a large “teacher”.</li>
  <li><strong>DistilBERT:</strong> 40% smaller, 60% faster, 97% of BERT’s performance.</li>
</ul>

<h3 id="3-pruning">3. Pruning</h3>
<ul>
  <li>Remove unimportant weights (e.g., magnitude pruning).</li>
  <li><strong>Sparse Transformers:</strong> 90% sparsity with minimal accuracy loss.</li>
</ul>

<h2 id="16-summary">16. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: left">Pros</th>
      <th style="text-align: left">Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>RNN</strong></td>
      <td style="text-align: left">Simple</td>
      <td style="text-align: left">Vanishing gradients</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>LSTM</strong></td>
      <td style="text-align: left">Long memory</td>
      <td style="text-align: left">Sequential (slow)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Transformer</strong></td>
      <td style="text-align: left">Parallel, Scalable</td>
      <td style="text-align: left">$O(N^2)$ memory</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Flash Attention</strong></td>
      <td style="text-align: left">Memory efficient</td>
      <td style="text-align: left">Complex implementation</td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">arunbaby.com/ml-system-design/0037-sequence-modeling</a></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#attention" class="page__taxonomy-item p-category" rel="tag">attention</a><span class="sep">, </span>
    
      <a href="/tags/#lstm" class="page__taxonomy-item p-category" rel="tag">lstm</a><span class="sep">, </span>
    
      <a href="/tags/#rnn" class="page__taxonomy-item p-category" rel="tag">rnn</a><span class="sep">, </span>
    
      <a href="/tags/#time-series" class="page__taxonomy-item p-category" rel="tag">time-series</a><span class="sep">, </span>
    
      <a href="/tags/#transformer" class="page__taxonomy-item p-category" rel="tag">transformer</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml_system_design</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Sequence+Modeling+in+ML%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0037-sequence-modeling%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0037-sequence-modeling%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0036-resource-partitioning/" class="pagination--pager" title="Resource Partitioning in ML Clusters">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
