<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Sequence Modeling in ML - Arun Baby</title>
<meta name="description" content="“Predicting the next word, the next stock price, the next frame.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Sequence Modeling in ML">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">


  <meta property="og:description" content="“Predicting the next word, the next stock price, the next frame.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Sequence Modeling in ML">
  <meta name="twitter:description" content="“Predicting the next word, the next stock price, the next frame.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-20T18:15:29+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Sequence Modeling in ML">
    <meta itemprop="description" content="“Predicting the next word, the next stock price, the next frame.”">
    <meta itemprop="datePublished" content="2025-12-20T18:15:29+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/" itemprop="url">Sequence Modeling in ML
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-the-problem-sequential-data">1. The Problem: Sequential Data</a></li><li><a href="#2-evolution-of-sequence-models">2. Evolution of Sequence Models</a><ul><li><a href="#1-recurrent-neural-networks-rnn">1. Recurrent Neural Networks (RNN)</a></li><li><a href="#2-lstm-long-short-term-memory">2. LSTM (Long Short-Term Memory)</a></li><li><a href="#3-transformer-attention-is-all-you-need">3. Transformer (Attention is All You Need)</a></li></ul></li><li><a href="#3-transformer-architecture">3. Transformer Architecture</a></li><li><a href="#4-training-strategies">4. Training Strategies</a><ul><li><a href="#1-teacher-forcing">1. Teacher Forcing</a></li><li><a href="#2-scheduled-sampling">2. Scheduled Sampling</a></li><li><a href="#3-curriculum-learning">3. Curriculum Learning</a></li></ul></li><li><a href="#5-inference-strategies">5. Inference Strategies</a><ul><li><a href="#1-greedy-decoding">1. Greedy Decoding</a></li><li><a href="#2-beam-search">2. Beam Search</a></li><li><a href="#3-sampling">3. Sampling</a></li></ul></li><li><a href="#6-system-design-real-time-translation">6. System Design: Real-Time Translation</a></li><li><a href="#7-case-study-gpt-3-serving">7. Case Study: GPT-3 Serving</a></li><li><a href="#8-summary">8. Summary</a></li><li><a href="#9-deep-dive-attention-mechanism-math">9. Deep Dive: Attention Mechanism Math</a></li><li><a href="#10-deep-dive-kv-caching-for-autoregressive-decoding">10. Deep Dive: KV Caching for Autoregressive Decoding</a></li><li><a href="#11-deep-dive-flash-attention">11. Deep Dive: Flash Attention</a></li><li><a href="#12-deep-dive-positional-encoding">12. Deep Dive: Positional Encoding</a></li><li><a href="#13-system-design-chatbot-with-context">13. System Design: Chatbot with Context</a></li><li><a href="#14-deep-dive-beam-search-implementation">14. Deep Dive: Beam Search Implementation</a></li><li><a href="#15-production-optimizations">15. Production Optimizations</a><ul><li><a href="#1-model-quantization">1. Model Quantization</a></li><li><a href="#2-distillation">2. Distillation</a></li><li><a href="#3-pruning">3. Pruning</a></li></ul></li><li><a href="#16-deep-dive-sequence-to-sequence-models-seq2seq">16. Deep Dive: Sequence-to-Sequence Models (Seq2Seq)</a></li><li><a href="#17-advanced-sparse-attention-mechanisms">17. Advanced: Sparse Attention Mechanisms</a></li><li><a href="#18-case-study-alphafold-protein-folding">18. Case Study: AlphaFold (Protein Folding)</a></li><li><a href="#19-deep-dive-mixture-of-experts-moe">19. Deep Dive: Mixture of Experts (MoE)</a></li><li><a href="#20-production-serving-architecture">20. Production Serving Architecture</a></li><li><a href="#21-evaluation-metrics-for-sequence-models">21. Evaluation Metrics for Sequence Models</a></li><li><a href="#22-common-pitfalls-and-how-to-avoid-them">22. Common Pitfalls and How to Avoid Them</a></li><li><a href="#23-deep-dive-state-space-models-mamba--s4">23. Deep Dive: State Space Models (Mamba / S4)</a></li><li><a href="#24-deep-dive-reinforcement-learning-from-human-feedback-rlhf">24. Deep Dive: Reinforcement Learning from Human Feedback (RLHF)</a></li><li><a href="#25-advanced-long-context-transformers-ring-attention">25. Advanced: Long-Context Transformers (Ring Attention)</a></li><li><a href="#26-interview-questions-for-sequence-modeling">26. Interview Questions for Sequence Modeling</a></li><li><a href="#27-ethical-considerations">27. Ethical Considerations</a></li><li><a href="#28-common-mistakes-in-sequence-modeling">28. Common Mistakes in Sequence Modeling</a></li><li><a href="#29-glossary-of-terms">29. Glossary of Terms</a></li><li><a href="#30-further-reading">30. Further Reading</a></li><li><a href="#30-conclusion">30. Conclusion</a></li><li><a href="#31-summary">31. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Predicting the next word, the next stock price, the next frame.”</strong></p>

<h2 id="1-the-problem-sequential-data">1. The Problem: Sequential Data</h2>

<p>Many real-world problems involve sequences:</p>
<ul>
  <li><strong>Text:</strong> “The cat sat on the ___” → “mat”</li>
  <li><strong>Time Series:</strong> Stock prices, weather, sensor data.</li>
  <li><strong>Video:</strong> Predict next frame.</li>
  <li><strong>Audio:</strong> Speech recognition, music generation.</li>
</ul>

<p><strong>Challenge:</strong> The output depends on <strong>context</strong> (previous elements in the sequence).</p>

<h2 id="2-evolution-of-sequence-models">2. Evolution of Sequence Models</h2>

<h3 id="1-recurrent-neural-networks-rnn">1. Recurrent Neural Networks (RNN)</h3>
<ul>
  <li><strong>Idea:</strong> Hidden state $h_t$ carries information from previous steps.</li>
  <li><strong>Equation:</strong> $h_t = \tanh(W_h h_{t-1} + W_x x_t + b)$</li>
  <li><strong>Problem:</strong> Vanishing gradients. Can’t remember long-term dependencies.</li>
</ul>

<h3 id="2-lstm-long-short-term-memory">2. LSTM (Long Short-Term Memory)</h3>
<ul>
  <li><strong>Gates:</strong> Forget, Input, Output gates control information flow.</li>
  <li><strong>Advantage:</strong> Can remember 100+ steps.</li>
  <li><strong>Disadvantage:</strong> Sequential processing (can’t parallelize).</li>
</ul>

<h3 id="3-transformer-attention-is-all-you-need">3. Transformer (Attention is All You Need)</h3>
<ul>
  <li><strong>Self-Attention:</strong> Every token attends to every other token.</li>
  <li><strong>Parallelization:</strong> Process entire sequence at once.</li>
  <li><strong>Scalability:</strong> Powers GPT, BERT, LLaMA.</li>
</ul>

<h2 id="3-transformer-architecture">3. Transformer Architecture</h2>

<p><strong>Key Components:</strong></p>

<ol>
  <li><strong>Self-Attention:</strong>
\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)
    <ul>
      <li><strong>Query (Q):</strong> “What am I looking for?”</li>
      <li><strong>Key (K):</strong> “What do I have?”</li>
      <li><strong>Value (V):</strong> “What information do I provide?”</li>
    </ul>
  </li>
  <li><strong>Multi-Head Attention:</strong>
    <ul>
      <li>Run attention multiple times with different learned projections.</li>
      <li>Allows model to attend to different aspects (syntax, semantics, etc.).</li>
    </ul>
  </li>
  <li><strong>Positional Encoding:</strong>
    <ul>
      <li>Transformers have no notion of order.</li>
      <li>Add sinusoidal encodings: $PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$</li>
    </ul>
  </li>
  <li><strong>Feed-Forward Network:</strong>
    <ul>
      <li>Two linear layers with ReLU: $\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$</li>
    </ul>
  </li>
</ol>

<h2 id="4-training-strategies">4. Training Strategies</h2>

<h3 id="1-teacher-forcing">1. Teacher Forcing</h3>
<ul>
  <li>During training, feed ground truth as input (even if model predicted wrong).</li>
  <li><strong>Pro:</strong> Faster convergence.</li>
  <li><strong>Con:</strong> Exposure bias (model never sees its own mistakes during training).</li>
</ul>

<h3 id="2-scheduled-sampling">2. Scheduled Sampling</h3>
<ul>
  <li>Gradually mix ground truth with model predictions during training.</li>
  <li>Start with 100% teacher forcing, decay to 0%.</li>
</ul>

<h3 id="3-curriculum-learning">3. Curriculum Learning</h3>
<ul>
  <li>Start with short sequences, gradually increase length.</li>
</ul>

<h2 id="5-inference-strategies">5. Inference Strategies</h2>

<h3 id="1-greedy-decoding">1. Greedy Decoding</h3>
<ul>
  <li>At each step, pick the token with highest probability.</li>
  <li><strong>Fast</strong> but <strong>suboptimal</strong>.</li>
</ul>

<h3 id="2-beam-search">2. Beam Search</h3>
<ul>
  <li>Keep top-$k$ candidates at each step.</li>
  <li>Explore multiple paths, pick the best overall sequence.</li>
  <li><strong>Better quality</strong> but <strong>slower</strong>.</li>
</ul>

<h3 id="3-sampling">3. Sampling</h3>
<ul>
  <li>Sample from the probability distribution.</li>
  <li><strong>Temperature:</strong> Control randomness.
    <ul>
      <li>$T \to 0$: Greedy (deterministic).</li>
      <li>$T \to \infty$: Uniform (random).</li>
    </ul>
  </li>
</ul>

<h2 id="6-system-design-real-time-translation">6. System Design: Real-Time Translation</h2>

<p><strong>Scenario:</strong> Google Translate (Text-to-Text).</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Encoder:</strong> Processes source sentence (English).</li>
  <li><strong>Decoder:</strong> Generates target sentence (French).</li>
  <li><strong>Attention:</strong> Decoder attends to relevant encoder states.</li>
</ol>

<p><strong>Optimizations:</strong></p>
<ul>
  <li><strong>Caching:</strong> Cache encoder output (source doesn’t change).</li>
  <li><strong>Batching:</strong> Process multiple requests together.</li>
  <li><strong>Quantization:</strong> INT8 for 4x speedup.</li>
</ul>

<h2 id="7-case-study-gpt-3-serving">7. Case Study: GPT-3 Serving</h2>

<p><strong>Challenge:</strong> Serve a 175B parameter model with low latency.</p>

<p><strong>Solutions:</strong></p>
<ol>
  <li><strong>Model Parallelism:</strong> Split model across 8 GPUs.</li>
  <li><strong>KV Cache:</strong> Cache key/value tensors from previous tokens.</li>
  <li><strong>Speculative Decoding:</strong> Use a small model to draft, large model to verify.</li>
</ol>

<h2 id="8-summary">8. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: left">Pros</th>
      <th style="text-align: left">Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>RNN</strong></td>
      <td style="text-align: left">Simple</td>
      <td style="text-align: left">Vanishing gradients</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>LSTM</strong></td>
      <td style="text-align: left">Long memory</td>
      <td style="text-align: left">Sequential (slow)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Transformer</strong></td>
      <td style="text-align: left">Parallel, Scalable</td>
      <td style="text-align: left">$O(N^2)$ memory</td>
    </tr>
  </tbody>
</table>

<h2 id="9-deep-dive-attention-mechanism-math">9. Deep Dive: Attention Mechanism Math</h2>

<p>Let’s break down the attention formula step by step.</p>

<p><strong>Input:</strong> Sequence of vectors $X = [x_1, x_2, …, x_n]$, each $x_i \in \mathbb{R}^d$.</p>

<p><strong>Step 1: Linear Projections</strong>
\(Q = XW_Q, \quad K = XW_K, \quad V = XW_V\)</p>
<ul>
  <li>$W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}$ are learned weight matrices.</li>
</ul>

<p><strong>Step 2: Compute Attention Scores</strong>
\(S = \frac{QK^T}{\sqrt{d_k}}\)</p>
<ul>
  <li>$S \in \mathbb{R}^{n \times n}$. Entry $S_{ij}$ is the “compatibility” between query $i$ and key $j$.</li>
  <li>Divide by $\sqrt{d_k}$ to prevent dot products from becoming too large (which would make softmax saturate).</li>
</ul>

<p><strong>Step 3: Softmax</strong>
\(A = \text{softmax}(S)\)</p>
<ul>
  <li>Each row sums to 1. $A_{ij}$ is the “attention weight” from position $i$ to position $j$.</li>
</ul>

<p><strong>Step 4: Weighted Sum</strong>
\(\text{Output} = AV\)</p>
<ul>
  <li>Each output vector is a weighted combination of all value vectors.</li>
</ul>

<h2 id="10-deep-dive-kv-caching-for-autoregressive-decoding">10. Deep Dive: KV Caching for Autoregressive Decoding</h2>

<p><strong>Problem:</strong> When generating token $t$, we recompute attention for tokens $1, 2, …, t-1$ (wasteful!).</p>

<p><strong>Solution:</strong> Cache the Key and Value matrices.</p>

<p><strong>Without Caching:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
    <span class="c1"># Recompute K, V for all previous tokens
</span>    <span class="n">K</span> <span class="o">=</span> <span class="nf">compute_keys</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># O(t * d^2)
</span>    <span class="n">V</span> <span class="o">=</span> <span class="nf">compute_values</span><span class="p">(</span><span class="n">tokens</span><span class="p">[:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nf">attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>With Caching:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">K_cache</span><span class="p">,</span> <span class="n">V_cache</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
    <span class="c1"># Only compute K, V for new token
</span>    <span class="n">k_t</span> <span class="o">=</span> <span class="nf">compute_key</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>  <span class="c1"># O(d^2)
</span>    <span class="n">v_t</span> <span class="o">=</span> <span class="nf">compute_value</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    
    <span class="n">K_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">k_t</span><span class="p">)</span>
    <span class="n">V_cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">v_t</span><span class="p">)</span>
    
    <span class="n">K</span> <span class="o">=</span> <span class="nf">concat</span><span class="p">(</span><span class="n">K_cache</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nf">concat</span><span class="p">(</span><span class="n">V_cache</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nf">attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Speedup:</strong> $O(T^2)$ → $O(T)$ per token.</p>

<h2 id="11-deep-dive-flash-attention">11. Deep Dive: Flash Attention</h2>

<p><strong>Problem:</strong> Standard attention requires materializing the $N \times N$ attention matrix in memory.</p>
<ul>
  <li>For $N = 4096$, that’s 16M floats = 64MB per head.</li>
  <li>GPT-3 has 96 heads → 6GB just for attention!</li>
</ul>

<p><strong>Flash Attention (Dao et al., 2022):</strong></p>
<ul>
  <li><strong>Idea:</strong> Compute attention in blocks, never materialize the full matrix.</li>
  <li><strong>Algorithm:</strong>
    <ol>
      <li>Divide $Q, K, V$ into blocks.</li>
      <li>Load one block of $Q$ and one block of $K$ into SRAM.</li>
      <li>Compute partial attention scores.</li>
      <li>Accumulate results.</li>
    </ol>
  </li>
  <li><strong>Speedup:</strong> 2-4x faster, uses less memory.</li>
</ul>

<h2 id="12-deep-dive-positional-encoding">12. Deep Dive: Positional Encoding</h2>

<p>Transformers have no notion of order. We add positional information.</p>

<p><strong>Sinusoidal Encoding (Original Transformer):</strong>
\(PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)\)
\(PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)\)</p>

<p><strong>Why this works:</strong></p>
<ul>
  <li>Different frequencies for different dimensions.</li>
  <li>Model can learn to attend to relative positions.</li>
</ul>

<p><strong>Learned Positional Embeddings (BERT, GPT):</strong></p>
<ul>
  <li>Treat position as a lookup table.</li>
  <li>More flexible, but limited to max sequence length seen during training.</li>
</ul>

<p><strong>Rotary Position Embedding (RoPE):</strong></p>
<ul>
  <li>Used in LLaMA, PaLM.</li>
  <li>Rotates query and key vectors based on position.</li>
  <li>Allows extrapolation to longer sequences.</li>
</ul>

<h2 id="13-system-design-chatbot-with-context">13. System Design: Chatbot with Context</h2>

<p><strong>Scenario:</strong> Build a chatbot that remembers conversation history.</p>

<p><strong>Challenge:</strong> Context grows with each turn. How to handle long conversations?</p>

<p><strong>Approach 1: Sliding Window</strong></p>
<ul>
  <li>Keep only last $N$ tokens (e.g., 2048).</li>
  <li><strong>Pro:</strong> Fixed memory.</li>
  <li><strong>Con:</strong> Forgets old context.</li>
</ul>

<p><strong>Approach 2: Summarization</strong></p>
<ul>
  <li>Periodically summarize old context.</li>
  <li><strong>Pro:</strong> Retains important info.</li>
  <li><strong>Con:</strong> Lossy compression.</li>
</ul>

<p><strong>Approach 3: Retrieval-Augmented</strong></p>
<ul>
  <li>Store conversation in a vector DB.</li>
  <li>Retrieve relevant past messages based on current query.</li>
  <li><strong>Pro:</strong> Scalable to infinite history.</li>
  <li><strong>Con:</strong> Requires embedding model + DB.</li>
</ul>

<h2 id="14-deep-dive-beam-search-implementation">14. Deep Dive: Beam Search Implementation</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">beam_search</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">start_token</span><span class="p">,</span> <span class="n">beam_width</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
    <span class="c1"># Initialize beam with start token
</span>    <span class="n">beams</span> <span class="o">=</span> <span class="p">[(</span><span class="n">start_token</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)]</span>  <span class="c1"># (sequence, log_prob)
</span>    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
        <span class="n">candidates</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">seq</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">beams</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">EOS_TOKEN</span><span class="p">:</span>
                <span class="n">candidates</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">seq</span><span class="p">,</span> <span class="n">score</span><span class="p">))</span>
                <span class="k">continue</span>
            
            <span class="c1"># Get next token probabilities
</span>            <span class="n">logits</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
            <span class="n">log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Top-k tokens
</span>            <span class="n">top_k_probs</span><span class="p">,</span> <span class="n">top_k_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">beam_width</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">top_k_probs</span><span class="p">,</span> <span class="n">top_k_tokens</span><span class="p">):</span>
                <span class="n">new_seq</span> <span class="o">=</span> <span class="n">seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">token</span><span class="p">.</span><span class="nf">item</span><span class="p">()]</span>
                <span class="n">new_score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">+</span> <span class="n">prob</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
                <span class="n">candidates</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">new_seq</span><span class="p">,</span> <span class="n">new_score</span><span class="p">))</span>
        
        <span class="c1"># Keep top beam_width candidates
</span>        <span class="n">beams</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">candidates</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="n">beam_width</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">beams</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Best sequence
</span></code></pre></div></div>

<h2 id="15-production-optimizations">15. Production Optimizations</h2>

<h3 id="1-model-quantization">1. Model Quantization</h3>
<ul>
  <li>Convert FP32 → INT8.</li>
  <li><strong>Speedup:</strong> 4x.</li>
  <li><strong>Accuracy Loss:</strong> &lt; 1% with careful calibration.</li>
</ul>

<h3 id="2-distillation">2. Distillation</h3>
<ul>
  <li>Train a small “student” model to mimic a large “teacher”.</li>
  <li><strong>DistilBERT:</strong> 40% smaller, 60% faster, 97% of BERT’s performance.</li>
</ul>

<h3 id="3-pruning">3. Pruning</h3>
<ul>
  <li>Remove unimportant weights (e.g., magnitude pruning).</li>
  <li>
    <p><strong>Sparse Transformers:</strong> 90% sparsity with minimal accuracy loss.</p>
  </li>
  <li><strong>Sparse Transformers:</strong> 90% sparsity with minimal accuracy loss.</li>
</ul>

<h2 id="16-deep-dive-sequence-to-sequence-models-seq2seq">16. Deep Dive: Sequence-to-Sequence Models (Seq2Seq)</h2>

<p><strong>Architecture:</strong> Encoder-Decoder with Attention.</p>

<p><strong>Use Cases:</strong></p>
<ul>
  <li>Machine Translation (English → French)</li>
  <li>Summarization (Long article → Short summary)</li>
  <li>Question Answering (Context + Question → Answer)</li>
</ul>

<p><strong>Key Innovation: Attention Mechanism (Bahdanau et al., 2015)</strong></p>
<ul>
  <li><strong>Problem:</strong> Fixed-length context vector bottleneck.</li>
  <li><strong>Solution:</strong> Decoder attends to all encoder states.</li>
</ul>

<p><strong>Implementation:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Seq2SeqWithAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">):</span>
        <span class="c1"># Encode
</span>        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        
        <span class="c1"># Decode with attention
</span>        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">h</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">tgt</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
            <span class="c1"># Compute attention scores
</span>            <span class="n">scores</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span>
                <span class="n">decoder_hidden</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                <span class="n">encoder_outputs</span>
            <span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
            
            <span class="c1"># Attention weights
</span>            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            
            <span class="c1"># Context vector
</span>            <span class="n">context</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn_weights</span> <span class="o">*</span> <span class="n">encoder_outputs</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            
            <span class="c1"># Decoder step
</span>            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">tgt</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">context</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>
            
            <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">output</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="17-advanced-sparse-attention-mechanisms">17. Advanced: Sparse Attention Mechanisms</h2>

<p><strong>Problem:</strong> Full attention is $O(N^2)$. For $N = 100k$ (long documents), this is prohibitive.</p>

<p><strong>Solutions:</strong></p>

<p><strong>1. Longformer (Beltagy et al., 2020):</strong></p>
<ul>
  <li><strong>Local Attention:</strong> Each token attends to $w$ neighbors (sliding window).</li>
  <li><strong>Global Attention:</strong> A few tokens (e.g., <code class="language-plaintext highlighter-rouge">[CLS]</code>) attend to everything.</li>
  <li><strong>Complexity:</strong> $O(N \cdot w)$ instead of $O(N^2)$.</li>
</ul>

<p><strong>2. BigBird (Zaheer et al., 2020):</strong></p>
<ul>
  <li><strong>Random Attention:</strong> Each token attends to $r$ random tokens.</li>
  <li><strong>Block Attention:</strong> Divide sequence into blocks, attend within blocks.</li>
  <li><strong>Global Attention:</strong> Special tokens attend globally.</li>
</ul>

<p><strong>3. Linformer (Wang et al., 2020):</strong></p>
<ul>
  <li><strong>Low-Rank Projection:</strong> Project $K, V$ to lower dimension.</li>
  <li><strong>Complexity:</strong> $O(N \cdot k)$ where $k \ll N$.</li>
</ul>

<p><strong>Code (Longformer-style Local Attention):</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">local_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">256</span><span class="p">):</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">shape</span>
    
    <span class="c1"># Create attention mask (band matrix)
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">start</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">end</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="c1"># Compute attention
</span>    <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">attn</span> <span class="o">@</span> <span class="n">V</span>
</code></pre></div></div>

<h2 id="18-case-study-alphafold-protein-folding">18. Case Study: AlphaFold (Protein Folding)</h2>

<p><strong>Problem:</strong> Predict 3D protein structure from amino acid sequence.</p>

<p><strong>Why Sequence Modeling?</strong></p>
<ul>
  <li>Protein is a sequence of amino acids (A, C, D, E, …).</li>
  <li>Structure depends on long-range interactions (residue 10 affects residue 500).</li>
</ul>

<p><strong>AlphaFold 2 Architecture:</strong></p>
<ol>
  <li><strong>Evoformer:</strong> Transformer variant with:
    <ul>
      <li><strong>MSA (Multiple Sequence Alignment) Attention:</strong> Attend across evolutionary related sequences.</li>
      <li><strong>Pair Representation:</strong> Attend to pairwise residue relationships.</li>
    </ul>
  </li>
  <li><strong>Structure Module:</strong> Predicts 3D coordinates.</li>
</ol>

<p><strong>Key Innovation:</strong> Attention over both sequence and structure space.</p>

<p><strong>Result:</strong> Solved a 50-year-old problem. Accuracy comparable to experimental methods.</p>

<h2 id="19-deep-dive-mixture-of-experts-moe">19. Deep Dive: Mixture of Experts (MoE)</h2>

<p><strong>Idea:</strong> Instead of one giant model, use many small “expert” models. Route each input to the most relevant experts.</p>

<p><strong>Architecture:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → Router (Gating Network) → Top-K Experts → Combine Outputs
</code></pre></div></div>

<p><strong>Example: Switch Transformer (Google, 2021)</strong></p>
<ul>
  <li>1.6 <strong>Trillion</strong> parameters.</li>
  <li>But only 10B active per token (sparse activation).</li>
  <li><strong>Routing:</strong> Each token is sent to 1 expert (out of 2048).</li>
</ul>

<p><strong>Benefits:</strong></p>
<ul>
  <li><strong>Scalability:</strong> Add more experts without increasing per-token compute.</li>
  <li><strong>Specialization:</strong> Expert 1 learns math, Expert 2 learns code, etc.</li>
</ul>

<p><strong>Challenges:</strong></p>
<ul>
  <li><strong>Load Balancing:</strong> Ensure all experts are used equally.</li>
  <li><strong>Training Instability:</strong> Router can collapse (send everything to one expert).</li>
</ul>

<p><strong>Code:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MoELayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">router</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">experts</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_experts</span><span class="p">)</span>
        <span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Route
</span>        <span class="n">router_logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">router</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">router_probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">router_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Top-1 expert per token
</span>        <span class="n">expert_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">router_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># Apply experts
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">expert</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">experts</span><span class="p">):</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">expert_idx</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">mask</span><span class="p">.</span><span class="nf">any</span><span class="p">():</span>
                <span class="n">output</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="nf">expert</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
        
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<h2 id="20-production-serving-architecture">20. Production Serving Architecture</h2>

<p><strong>Scenario:</strong> Serve GPT-3-scale model (175B params) with &lt;1s latency.</p>

<p><strong>Architecture:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────┐
│   Client    │
└──────┬──────┘
       │
       ▼
┌─────────────────┐
│  Load Balancer  │
└──────┬──────────┘
       │
   ┌───┴───┐
   ▼       ▼
┌─────┐ ┌─────┐
│ GPU │ │ GPU │  (Model Parallelism: 8 GPUs per replica)
│  1  │ │  2  │
└─────┘ └─────┘
</code></pre></div></div>

<p><strong>Optimizations:</strong></p>

<p><strong>1. Continuous Batching (Orca, 2022):</strong></p>
<ul>
  <li>Don’t wait for all requests to finish.</li>
  <li>As soon as one finishes, add a new request to the batch.</li>
  <li><strong>Speedup:</strong> 2-3x higher throughput.</li>
</ul>

<p><strong>2. PagedAttention (vLLM, 2023):</strong></p>
<ul>
  <li>Store KV cache in paged memory (like OS virtual memory).</li>
  <li><strong>Benefit:</strong> Reduce memory waste from fragmentation.</li>
</ul>

<p><strong>3. Speculative Decoding:</strong></p>
<ul>
  <li>Use a small model (1B) to draft 5 tokens.</li>
  <li>Use large model (175B) to verify in parallel.</li>
  <li><strong>Speedup:</strong> 2-3x for same quality.</li>
</ul>

<h2 id="21-evaluation-metrics-for-sequence-models">21. Evaluation Metrics for Sequence Models</h2>

<p><strong>1. Perplexity (Language Modeling):</strong>
\(PPL = \exp\left(-\frac{1}{N}\sum_{i=1}^N \log P(w_i | w_{&lt;i})\right)\)</p>
<ul>
  <li>Lower is better.</li>
  <li><strong>Interpretation:</strong> “How surprised is the model by the test data?”</li>
</ul>

<p><strong>2. BLEU (Machine Translation):</strong></p>
<ul>
  <li>Measures n-gram overlap between prediction and reference.</li>
  <li><strong>Range:</strong> 0-100. BLEU &gt; 40 is considered good.</li>
</ul>

<p><strong>3. ROUGE (Summarization):</strong></p>
<ul>
  <li>Similar to BLEU, but focuses on recall.</li>
</ul>

<p><strong>4. Exact Match (QA):</strong></p>
<ul>
  <li>Percentage of predictions that exactly match ground truth.</li>
</ul>

<h2 id="22-common-pitfalls-and-how-to-avoid-them">22. Common Pitfalls and How to Avoid Them</h2>

<p><strong>Pitfall 1: Exposure Bias (Teacher Forcing)</strong></p>
<ul>
  <li>Model trained on ground truth, tested on its own predictions.</li>
  <li><strong>Fix:</strong> Scheduled sampling or reinforcement learning.</li>
</ul>

<p><strong>Pitfall 2: Length Bias in Beam Search</strong></p>
<ul>
  <li>Longer sequences have lower cumulative probability.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Fix:</strong> Length normalization: $\text{score} = \frac{\log P(y)}{</td>
          <td>y</td>
          <td>^\alpha}$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p><strong>Pitfall 3: Catastrophic Forgetting (Fine-Tuning)</strong></p>
<ul>
  <li>Fine-tuning on Task B makes model forget Task A.</li>
  <li><strong>Fix:</strong> Elastic Weight Consolidation (EWC) or multi-task learning.</li>
</ul>

<p><strong>Pitfall 4: OOM (Out of Memory) During Training</strong></p>
<ul>
  <li>Gradient accumulation: Simulate large batch with small batches.</li>
  <li><strong>Fix:</strong> <code class="language-plaintext highlighter-rouge">loss.backward(); if step % 4 == 0: optimizer.step()</code></li>
</ul>

<p><strong>Pitfall 5: Ignoring Positional Encoding Limits</strong></p>
<ul>
  <li>BERT trained on 512 tokens can’t handle 1024.</li>
  <li>
    <p><strong>Fix:</strong> Use RoPE or ALiBi (Attention with Linear Biases).</p>
  </li>
  <li><strong>Fix:</strong> Use RoPE or ALiBi (Attention with Linear Biases).</li>
</ul>

<h2 id="23-deep-dive-state-space-models-mamba--s4">23. Deep Dive: State Space Models (Mamba / S4)</h2>

<p><strong>Problem:</strong> Transformers are $O(N^2)$. RNNs are $O(N)$ but hard to train. Can we get the best of both?</p>

<p><strong>Solution:</strong> Structured State Space Models (SSMs).</p>

<p><strong>Key Idea:</strong></p>
<ul>
  <li>Model the sequence as a continuous-time system:
\(h'(t) = Ah(t) + Bx(t)\)
\(y(t) = Ch(t)\)</li>
  <li><strong>Discretize</strong> it for digital computers.</li>
  <li><strong>Training:</strong> Can be computed as a <strong>Convolution</strong> (Parallelizable like Transformers).</li>
  <li><strong>Inference:</strong> Can be computed as a <strong>Recurrence</strong> (Constant memory like RNNs).</li>
</ul>

<p><strong>Mamba (Gu &amp; Dao, 2023):</strong></p>
<ul>
  <li>Introduces <strong>Selection Mechanism</strong>: The matrices $B$ and $C$ depend on the input $x_t$.</li>
  <li>Allows the model to “selectively” remember or ignore information.</li>
  <li><strong>Performance:</strong> Matches Transformers on language modeling, but with linear scaling $O(N)$.</li>
</ul>

<p><strong>Code (Simplified Mamba Block):</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MambaBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">in_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv1d</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">d_model</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">x_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dt_rank</span> <span class="o">+</span> <span class="n">B_rank</span> <span class="o">+</span> <span class="n">C_rank</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 1. Project
</span>        <span class="n">x_and_res</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">in_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">res</span> <span class="o">=</span> <span class="n">x_and_res</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1"># 2. Conv
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv1d</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># 3. SSM (Selective Scan)
</span>        <span class="c1"># This is the core Mamba magic (usually implemented in CUDA)
</span>        <span class="n">y</span> <span class="o">=</span> <span class="nf">selective_scan</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">x_proj</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        
        <span class="c1"># 4. Output
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">out_proj</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">F</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="24-deep-dive-reinforcement-learning-from-human-feedback-rlhf">24. Deep Dive: Reinforcement Learning from Human Feedback (RLHF)</h2>

<p><strong>Problem:</strong> LLMs are trained to predict the next token, not to be helpful or safe.</p>

<p><strong>Pipeline (InstructGPT / ChatGPT):</strong></p>

<ol>
  <li><strong>Supervised Fine-Tuning (SFT):</strong>
    <ul>
      <li>Collect human demonstrations (Question + Answer).</li>
      <li>Fine-tune base model.</li>
    </ul>
  </li>
  <li><strong>Reward Modeling (RM):</strong>
    <ul>
      <li>Collect comparison data (Model generates A and B; Human says A &gt; B).</li>
      <li>Train a Reward Model to predict human preference.</li>
      <li>Loss: $\log(\sigma(r(A) - r(B)))$.</li>
    </ul>
  </li>
  <li><strong>PPO (Proximal Policy Optimization):</strong>
    <ul>
      <li>Optimize the SFT model to maximize the reward from RM.</li>
      <li>Constraint: Don’t drift too far from SFT model (KL Divergence penalty).</li>
    </ul>
  </li>
</ol>

<p><strong>Impact:</strong></p>
<ul>
  <li>Aligns model with human intent.</li>
  <li>Reduces toxicity and hallucinations (mostly).</li>
</ul>

<h2 id="25-advanced-long-context-transformers-ring-attention">25. Advanced: Long-Context Transformers (Ring Attention)</h2>

<p><strong>Problem:</strong> How to train on 1 Million tokens?</p>
<ul>
  <li>Memory is the bottleneck. Even with Flash Attention, activations don’t fit on one GPU.</li>
</ul>

<p><strong>Ring Attention (Liu et al., 2023):</strong></p>
<ul>
  <li><strong>Idea:</strong> Distribute the sequence across multiple GPUs in a ring.</li>
  <li>Each GPU holds a chunk of Query, Key, Value.</li>
  <li><strong>Step 1:</strong> Compute local attention.</li>
  <li><strong>Step 2:</strong> Pass Key/Value blocks to the next GPU in the ring.</li>
  <li><strong>Step 3:</strong> Compute attention with new KV blocks.</li>
  <li><strong>Repeat:</strong> Until all KV blocks have visited all GPUs.</li>
</ul>

<p><strong>Result:</strong></p>
<ul>
  <li>Train on sequences length = Number of GPUs $\times$ Memory per GPU.</li>
  <li>Enables “Needle in a Haystack” retrieval over entire books.</li>
</ul>

<h2 id="26-interview-questions-for-sequence-modeling">26. Interview Questions for Sequence Modeling</h2>

<p><strong>Q1: Why do we divide by $\sqrt{d_k}$ in Attention?</strong>
<em>Answer:</em> To scale the dot products. If $d_k$ is large, dot products become huge, pushing Softmax into regions with extremely small gradients (vanishing gradients).</p>

<p><strong>Q2: What is the difference between Post-Norm and Pre-Norm?</strong>
<em>Answer:</em></p>
<ul>
  <li><strong>Post-Norm:</strong> <code class="language-plaintext highlighter-rouge">LayerNorm(x + Sublayer(x))</code>. Original Transformer. Harder to train (warmup needed).</li>
  <li><strong>Pre-Norm:</strong> <code class="language-plaintext highlighter-rouge">x + Sublayer(LayerNorm(x))</code>. Used in GPT-3/LLaMA. More stable gradients, easier to train.</li>
</ul>

<p><strong>Q3: How does RoPE handle sequence length extrapolation?</strong>
<em>Answer:</em> By rotating the vectors, the attention score depends only on relative distance $(m-n)$. This relative property generalizes better to unseen lengths than absolute positional embeddings.</p>

<p><strong>Q4: Explain the “KV Cache” memory usage.</strong>
<em>Answer:</em> Memory = $2 \times \text{Batch} \times \text{SeqLen} \times \text{Layers} \times \text{HiddenSize}$. For long sequences, this dominates memory. PagedAttention helps reduce fragmentation.</p>

<p><strong>Q5: Why use MoE?</strong>
<em>Answer:</em> To decouple model size (parameters) from compute cost (FLOPs). We can have a 1T parameter model but only use 10B parameters per token, enabling massive capacity with reasonable inference latency.</p>

<h2 id="27-ethical-considerations">27. Ethical Considerations</h2>

<p><strong>1. Hallucinations:</strong></p>
<ul>
  <li>Sequence models are “stochastic parrots”. They generate plausible-sounding but potentially false text.</li>
  <li><strong>Risk:</strong> Misinformation in medical/legal contexts.</li>
  <li><strong>Mitigation:</strong> RAG (Retrieval Augmented Generation) to ground answers in facts.</li>
</ul>

<p><strong>2. Bias and Toxicity:</strong></p>
<ul>
  <li>Models learn biases present in training data (internet).</li>
  <li><strong>Risk:</strong> Hate speech, stereotypes.</li>
  <li><strong>Mitigation:</strong> RLHF, careful data curation, red-teaming.</li>
</ul>

<p><strong>3. Dual Use:</strong></p>
<ul>
  <li>Code generation models can write malware.</li>
  <li>
    <p><strong>Mitigation:</strong> Safety guardrails, refusal to answer harmful queries.</p>
  </li>
  <li><strong>Mitigation:</strong> Safety guardrails, refusal to answer harmful queries.</li>
</ul>

<h2 id="28-common-mistakes-in-sequence-modeling">28. Common Mistakes in Sequence Modeling</h2>

<p><strong>1. Training on Test Data (Data Leakage):</strong></p>
<ul>
  <li>Accidentally including the test set in the pre-training corpus.</li>
  <li><strong>Consequence:</strong> Overestimated performance.</li>
  <li><strong>Fix:</strong> De-duplication (MinHash) against evaluation benchmarks.</li>
</ul>

<p><strong>2. Ignoring Tokenizer Issues:</strong></p>
<ul>
  <li>Different tokenizers (BPE, WordPiece) handle whitespace and special characters differently.</li>
  <li><strong>Consequence:</strong> Poor performance on code or multilingual text.</li>
  <li><strong>Fix:</strong> Use a robust tokenizer like Tiktoken or SentencePiece.</li>
</ul>

<p><strong>3. Incorrect Masking in Causal Attention:</strong></p>
<ul>
  <li>Allowing tokens to attend to future tokens during training.</li>
  <li><strong>Consequence:</strong> Model learns to cheat, fails at inference.</li>
  <li><strong>Fix:</strong> Verify the causal mask (upper triangular matrix is $-\infty$).</li>
</ul>

<p><strong>4. Underestimating Inference Cost:</strong></p>
<ul>
  <li>Focusing only on training loss.</li>
  <li><strong>Consequence:</strong> Model is too slow/expensive to deploy.</li>
  <li><strong>Fix:</strong> Monitor FLOPs per token and KV cache size during design.</li>
</ul>

<h2 id="29-glossary-of-terms">29. Glossary of Terms</h2>

<ul>
  <li><strong>Token:</strong> The atomic unit of text processing (word, subword, or character).</li>
  <li><strong>Embedding:</strong> A dense vector representation of a token.</li>
  <li><strong>Attention:</strong> Mechanism to weigh the importance of different input tokens.</li>
  <li><strong>Self-Attention:</strong> Attention applied to the sequence itself.</li>
  <li><strong>Cross-Attention:</strong> Attention applied between two sequences (e.g., Encoder-Decoder).</li>
  <li><strong>Logits:</strong> The raw, unnormalized scores output by the last layer.</li>
  <li><strong>Softmax:</strong> Function to convert logits into probabilities.</li>
  <li><strong>Temperature:</strong> A hyperparameter that controls the randomness of sampling.</li>
  <li><strong>Beam Search:</strong> A heuristic search algorithm that explores a graph by expanding the most promising node in a limited set.</li>
  <li><strong>Perplexity:</strong> A measurement of how well a probability model predicts a sample.</li>
</ul>

<h2 id="30-further-reading">30. Further Reading</h2>

<ol>
  <li><strong>“Attention Is All You Need” (Vaswani et al., 2017):</strong> The Transformer paper.</li>
  <li><strong>“BERT: Pre-training of Deep Bidirectional Transformers” (Devlin et al., 2019):</strong> Masked language modeling.</li>
  <li><strong>“GPT-3: Language Models are Few-Shot Learners” (Brown et al., 2020):</strong> Scaling laws.</li>
  <li><strong>“Flash Attention” (Dao et al., 2022):</strong> Memory-efficient attention.</li>
  <li><strong>“Switch Transformers” (Fedus et al., 2021):</strong> Mixture of Experts at scale.</li>
</ol>

<h2 id="30-conclusion">30. Conclusion</h2>

<p>Sequence modeling has transformed from simple statistical methods (n-grams) to the behemoth Large Language Models that define the current AI era. The journey from RNNs to LSTMs and finally to Transformers represents a shift from sequential processing to parallel, attention-based architectures. This evolution has enabled us to model not just language, but code, biology, and even robot actions as sequences. As we look forward, the challenges of infinite context length, efficient inference, and alignment with human values remain the frontier of research. Whether you are building a chatbot, a translation system, or a protein folder, understanding the underlying mechanics of attention and state management is the key to unlocking the potential of sequence models.</p>

<h2 id="31-summary">31. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: left">Pros</th>
      <th style="text-align: left">Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>RNN</strong></td>
      <td style="text-align: left">Simple</td>
      <td style="text-align: left">Vanishing gradients</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>LSTM</strong></td>
      <td style="text-align: left">Long memory</td>
      <td style="text-align: left">Sequential (slow)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Transformer</strong></td>
      <td style="text-align: left">Parallel, Scalable</td>
      <td style="text-align: left">$O(N^2)$ memory</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Flash Attention</strong></td>
      <td style="text-align: left">Memory efficient</td>
      <td style="text-align: left">Complex implementation</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>MoE</strong></td>
      <td style="text-align: left">Trillion-scale models</td>
      <td style="text-align: left">Load balancing challenges</td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">arunbaby.com/ml-system-design/0037-sequence-modeling</a></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#attention" class="page__taxonomy-item p-category" rel="tag">attention</a><span class="sep">, </span>
    
      <a href="/tags/#lstm" class="page__taxonomy-item p-category" rel="tag">lstm</a><span class="sep">, </span>
    
      <a href="/tags/#rnn" class="page__taxonomy-item p-category" rel="tag">rnn</a><span class="sep">, </span>
    
      <a href="/tags/#time-series" class="page__taxonomy-item p-category" rel="tag">time-series</a><span class="sep">, </span>
    
      <a href="/tags/#transformer" class="page__taxonomy-item p-category" rel="tag">transformer</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml_system_design</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Sequence+Modeling+in+ML%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0037-sequence-modeling%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0037-sequence-modeling%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0036-resource-partitioning/" class="pagination--pager" title="Resource Partitioning in ML Clusters">Previous</a>
    
    
      <a href="/ml-system-design/0038-hyperparameter-optimization/" class="pagination--pager" title="Hyperparameter Optimization">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
