<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Sequence Modeling in ML - Arun Baby</title>
<meta name="description" content="“Predicting the next word, the next stock price, the next frame.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Sequence Modeling in ML">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">


  <meta property="og:description" content="“Predicting the next word, the next stock price, the next frame.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Sequence Modeling in ML">
  <meta name="twitter:description" content="“Predicting the next word, the next stock price, the next frame.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T16:05:30+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Sequence Modeling in ML">
    <meta itemprop="description" content="“Predicting the next word, the next stock price, the next frame.”">
    <meta itemprop="datePublished" content="2025-12-29T16:05:30+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/" itemprop="url">Sequence Modeling in ML
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-the-problem-sequential-data">1. The Problem: Sequential Data</a></li><li><a href="#2-evolution-of-sequence-models">2. Evolution of Sequence Models</a><ul><li><a href="#1-recurrent-neural-networks-rnn">1. Recurrent Neural Networks (RNN)</a></li><li><a href="#2-lstm-long-short-term-memory">2. LSTM (Long Short-Term Memory)</a></li><li><a href="#3-transformer-attention-is-all-you-need">3. Transformer (Attention is All You Need)</a></li></ul></li><li><a href="#3-transformer-architecture">3. Transformer Architecture</a></li><li><a href="#4-training-strategies">4. Training Strategies</a><ul><li><a href="#1-teacher-forcing">1. Teacher Forcing</a></li><li><a href="#2-scheduled-sampling">2. Scheduled Sampling</a></li><li><a href="#3-curriculum-learning">3. Curriculum Learning</a></li></ul></li><li><a href="#5-inference-strategies">5. Inference Strategies</a><ul><li><a href="#1-greedy-decoding">1. Greedy Decoding</a></li><li><a href="#2-beam-search">2. Beam Search</a></li><li><a href="#3-sampling">3. Sampling</a></li></ul></li><li><a href="#6-system-design-real-time-translation">6. System Design: Real-Time Translation</a></li><li><a href="#7-case-study-gpt-3-serving">7. Case Study: GPT-3 Serving</a></li><li><a href="#8-summary">8. Summary</a></li><li><a href="#9-deep-dive-attention-mechanism-math">9. Deep Dive: Attention Mechanism Math</a></li><li><a href="#10-deep-dive-kv-caching-for-autoregressive-decoding">10. Deep Dive: KV Caching for Autoregressive Decoding</a></li><li><a href="#11-deep-dive-flash-attention">11. Deep Dive: Flash Attention</a></li><li><a href="#12-deep-dive-positional-encoding">12. Deep Dive: Positional Encoding</a></li><li><a href="#13-system-design-chatbot-with-context">13. System Design: Chatbot with Context</a></li><li><a href="#14-deep-dive-beam-search-implementation">14. Deep Dive: Beam Search Implementation</a></li><li><a href="#15-production-optimizations">15. Production Optimizations</a><ul><li><a href="#1-model-quantization">1. Model Quantization</a></li><li><a href="#2-distillation">2. Distillation</a></li><li><a href="#3-pruning">3. Pruning</a></li></ul></li><li><a href="#16-deep-dive-sequence-to-sequence-models-seq2seq">16. Deep Dive: Sequence-to-Sequence Models (Seq2Seq)</a></li><li><a href="#17-advanced-sparse-attention-mechanisms">17. Advanced: Sparse Attention Mechanisms</a></li><li><a href="#18-case-study-alphafold-protein-folding">18. Case Study: AlphaFold (Protein Folding)</a></li><li><a href="#19-deep-dive-mixture-of-experts-moe">19. Deep Dive: Mixture of Experts (MoE)</a></li><li><a href="#20-production-serving-architecture">20. Production Serving Architecture</a></li><li><a href="#21-evaluation-metrics-for-sequence-models">21. Evaluation Metrics for Sequence Models</a></li><li><a href="#22-common-pitfalls-and-how-to-avoid-them">22. Common Pitfalls and How to Avoid Them</a></li><li><a href="#23-deep-dive-state-space-models-mamba--s4">23. Deep Dive: State Space Models (Mamba / S4)</a></li><li><a href="#24-deep-dive-reinforcement-learning-from-human-feedback-rlhf">24. Deep Dive: Reinforcement Learning from Human Feedback (RLHF)</a></li><li><a href="#25-advanced-long-context-transformers-ring-attention">25. Advanced: Long-Context Transformers (Ring Attention)</a></li><li><a href="#26-interview-questions-for-sequence-modeling">26. Interview Questions for Sequence Modeling</a></li><li><a href="#27-ethical-considerations">27. Ethical Considerations</a></li><li><a href="#28-common-mistakes-in-sequence-modeling">28. Common Mistakes in Sequence Modeling</a></li><li><a href="#29-glossary-of-terms">29. Glossary of Terms</a></li><li><a href="#30-further-reading">30. Further Reading</a></li><li><a href="#30-conclusion">30. Conclusion</a></li><li><a href="#31-summary">31. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Predicting the next word, the next stock price, the next frame.”</strong></p>

<h2 id="1-the-problem-sequential-data">1. The Problem: Sequential Data</h2>

<p>Many real-world problems involve sequences:</p>
<ul>
  <li><strong>Text:</strong> “The cat sat on the ___” → “mat”</li>
  <li><strong>Time Series:</strong> Stock prices, weather, sensor data.</li>
  <li><strong>Video:</strong> Predict next frame.</li>
  <li><strong>Audio:</strong> Speech recognition, music generation.</li>
</ul>

<p><strong>Challenge:</strong> The output depends on <strong>context</strong> (previous elements in the sequence).</p>

<h2 id="2-evolution-of-sequence-models">2. Evolution of Sequence Models</h2>

<h3 id="1-recurrent-neural-networks-rnn">1. Recurrent Neural Networks (RNN)</h3>
<ul>
  <li><strong>Idea:</strong> Hidden state <code class="language-plaintext highlighter-rouge">h_t</code> carries information from previous steps.</li>
  <li><strong>Equation:</strong> <code class="language-plaintext highlighter-rouge">h_t = \tanh(W_h h_{t-1} + W_x x_t + b)</code></li>
  <li><strong>Problem:</strong> Vanishing gradients. Can’t remember long-term dependencies.</li>
</ul>

<h3 id="2-lstm-long-short-term-memory">2. LSTM (Long Short-Term Memory)</h3>
<ul>
  <li><strong>Gates:</strong> Forget, Input, Output gates control information flow.</li>
  <li><strong>Advantage:</strong> Can remember 100+ steps.</li>
  <li><strong>Disadvantage:</strong> Sequential processing (can’t parallelize).</li>
</ul>

<h3 id="3-transformer-attention-is-all-you-need">3. Transformer (Attention is All You Need)</h3>
<ul>
  <li><strong>Self-Attention:</strong> Every token attends to every other token.</li>
  <li><strong>Parallelization:</strong> Process entire sequence at once.</li>
  <li><strong>Scalability:</strong> Powers GPT, BERT, LLaMA.</li>
</ul>

<h2 id="3-transformer-architecture">3. Transformer Architecture</h2>

<p><strong>Key Components:</strong></p>

<ol>
  <li><strong>Self-Attention:</strong>
 <code class="language-plaintext highlighter-rouge">\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V</code>
    <ul>
      <li><strong>Query (Q):</strong> “What am I looking for?”</li>
      <li><strong>Key (K):</strong> “What do I have?”</li>
      <li><strong>Value (V):</strong> “What information do I provide?”</li>
    </ul>
  </li>
  <li><strong>Multi-Head Attention:</strong>
    <ul>
      <li>Run attention multiple times with different learned projections.</li>
      <li>Allows model to attend to different aspects (syntax, semantics, etc.).</li>
    </ul>
  </li>
  <li><strong>Positional Encoding:</strong>
    <ul>
      <li>Transformers have no notion of order.</li>
      <li>Add sinusoidal encodings: <code class="language-plaintext highlighter-rouge">PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})</code></li>
    </ul>
  </li>
  <li><strong>Feed-Forward Network:</strong>
    <ul>
      <li>Two linear layers with ReLU: <code class="language-plaintext highlighter-rouge">\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2</code></li>
    </ul>
  </li>
</ol>

<h2 id="4-training-strategies">4. Training Strategies</h2>

<h3 id="1-teacher-forcing">1. Teacher Forcing</h3>
<ul>
  <li>During training, feed ground truth as input (even if model predicted wrong).</li>
  <li><strong>Pro:</strong> Faster convergence.</li>
  <li><strong>Con:</strong> Exposure bias (model never sees its own mistakes during training).</li>
</ul>

<h3 id="2-scheduled-sampling">2. Scheduled Sampling</h3>
<ul>
  <li>Gradually mix ground truth with model predictions during training.</li>
  <li>Start with 100% teacher forcing, decay to 0%.</li>
</ul>

<h3 id="3-curriculum-learning">3. Curriculum Learning</h3>
<ul>
  <li>Start with short sequences, gradually increase length.</li>
</ul>

<h2 id="5-inference-strategies">5. Inference Strategies</h2>

<h3 id="1-greedy-decoding">1. Greedy Decoding</h3>
<ul>
  <li>At each step, pick the token with highest probability.</li>
  <li><strong>Fast</strong> but <strong>suboptimal</strong>.</li>
</ul>

<h3 id="2-beam-search">2. Beam Search</h3>
<ul>
  <li>Keep top-<code class="language-plaintext highlighter-rouge">k</code> candidates at each step.</li>
  <li>Explore multiple paths, pick the best overall sequence.</li>
  <li><strong>Better quality</strong> but <strong>slower</strong>.</li>
</ul>

<h3 id="3-sampling">3. Sampling</h3>
<ul>
  <li>Sample from the probability distribution.</li>
  <li><strong>Temperature:</strong> Control randomness.</li>
  <li><code class="language-plaintext highlighter-rouge">T \to 0</code>: Greedy (deterministic).</li>
  <li><code class="language-plaintext highlighter-rouge">T \to \infty</code>: Uniform (random).</li>
</ul>

<h2 id="6-system-design-real-time-translation">6. System Design: Real-Time Translation</h2>

<p><strong>Scenario:</strong> Google Translate (Text-to-Text).</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Encoder:</strong> Processes source sentence (English).</li>
  <li><strong>Decoder:</strong> Generates target sentence (French).</li>
  <li><strong>Attention:</strong> Decoder attends to relevant encoder states.</li>
</ol>

<p><strong>Optimizations:</strong></p>
<ul>
  <li><strong>Caching:</strong> Cache encoder output (source doesn’t change).</li>
  <li><strong>Batching:</strong> Process multiple requests together.</li>
  <li><strong>Quantization:</strong> INT8 for 4x speedup.</li>
</ul>

<h2 id="7-case-study-gpt-3-serving">7. Case Study: GPT-3 Serving</h2>

<p><strong>Challenge:</strong> Serve a 175B parameter model with low latency.</p>

<p><strong>Solutions:</strong></p>
<ol>
  <li><strong>Model Parallelism:</strong> Split model across 8 GPUs.</li>
  <li><strong>KV Cache:</strong> Cache key/value tensors from previous tokens.</li>
  <li><strong>Speculative Decoding:</strong> Use a small model to draft, large model to verify.</li>
</ol>

<h2 id="8-summary">8. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: left">Pros</th>
      <th style="text-align: left">Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>RNN</strong></td>
      <td style="text-align: left">Simple</td>
      <td style="text-align: left">Vanishing gradients</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>LSTM</strong></td>
      <td style="text-align: left">Long memory</td>
      <td style="text-align: left">Sequential (slow)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Transformer</strong></td>
      <td style="text-align: left">Parallel, Scalable</td>
      <td style="text-align: left">O(N^2) memory</td>
    </tr>
  </tbody>
</table>

<h2 id="9-deep-dive-attention-mechanism-math">9. Deep Dive: Attention Mechanism Math</h2>

<p>Let’s break down the attention formula step by step.</p>

<p><strong>Input:</strong> Sequence of vectors <code class="language-plaintext highlighter-rouge">X = [x_1, x_2, ..., x_n]</code>, each <code class="language-plaintext highlighter-rouge">x_i \in \mathbb{R}^d</code>.</p>

<p><strong>Step 1: Linear Projections</strong>
<code class="language-plaintext highlighter-rouge">Q = XW_Q, \quad K = XW_K, \quad V = XW_V</code></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}</code> are learned weight matrices.</li>
</ul>

<p><strong>Step 2: Compute Attention Scores</strong>
<code class="language-plaintext highlighter-rouge">S = \frac{QK^T}{\sqrt{d_k}}</code></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">S \in \mathbb{R}^{n \times n}</code>. Entry <code class="language-plaintext highlighter-rouge">S_{ij}</code> is the “compatibility” between query <code class="language-plaintext highlighter-rouge">i</code> and key <code class="language-plaintext highlighter-rouge">j</code>.</li>
  <li>Divide by <code class="language-plaintext highlighter-rouge">\sqrt{d_k}</code> to prevent dot products from becoming too large (which would make softmax saturate).</li>
</ul>

<p><strong>Step 3: Softmax</strong>
<code class="language-plaintext highlighter-rouge">A = \text{softmax}(S)</code></p>
<ul>
  <li>Each row sums to 1. <code class="language-plaintext highlighter-rouge">A_{ij}</code> is the “attention weight” from position <code class="language-plaintext highlighter-rouge">i</code> to position <code class="language-plaintext highlighter-rouge">j</code>.</li>
</ul>

<p><strong>Step 4: Weighted Sum</strong>
<code class="language-plaintext highlighter-rouge">\text{Output} = AV</code></p>
<ul>
  <li>Each output vector is a weighted combination of all value vectors.</li>
</ul>

<h2 id="10-deep-dive-kv-caching-for-autoregressive-decoding">10. Deep Dive: KV Caching for Autoregressive Decoding</h2>

<p><strong>Problem:</strong> When generating token <code class="language-plaintext highlighter-rouge">t</code>, we recompute attention for tokens <code class="language-plaintext highlighter-rouge">1, 2, ..., t-1</code> (wasteful!).</p>

<p><strong>Solution:</strong> Cache the Key and Value matrices.</p>

<p><strong>Without Caching:</strong>
<code class="language-plaintext highlighter-rouge">python
for t in range(max_len):
 # Recompute K, V for all previous tokens
 K = compute_keys(tokens[:t+1]) # O(t * d^2)
 V = compute_values(tokens[:t+1])
 output = attention(Q, K, V)
</code></p>

<p><strong>With Caching:</strong>
``python
K_cache, V_cache = [], []</p>

<p>for t in range(max_len):
 # Only compute K, V for new token
 k_t = compute_key(tokens[t]) # O(d^2)
 v_t = compute_value(tokens[t])</p>

<p>K_cache.append(k_t)
 V_cache.append(v_t)</p>

<p>K = concat(K_cache)
 V = concat(V_cache)
 output = attention(Q, K, V)
``</p>

<p><strong>Speedup:</strong> O(T^2) → O(T) per token.</p>

<h2 id="11-deep-dive-flash-attention">11. Deep Dive: Flash Attention</h2>

<p><strong>Problem:</strong> Standard attention requires materializing the <code class="language-plaintext highlighter-rouge">N \times N</code> attention matrix in memory.</p>
<ul>
  <li>For <code class="language-plaintext highlighter-rouge">N = 4096</code>, that’s 16M floats = 64MB per head.</li>
  <li>GPT-3 has 96 heads → 6GB just for attention!</li>
</ul>

<p><strong>Flash Attention (Dao et al., 2022):</strong></p>
<ul>
  <li><strong>Idea:</strong> Compute attention in blocks, never materialize the full matrix.</li>
  <li><strong>Algorithm:</strong>
    <ol>
      <li>Divide <code class="language-plaintext highlighter-rouge">Q, K, V</code> into blocks.</li>
      <li>Load one block of <code class="language-plaintext highlighter-rouge">Q</code> and one block of <code class="language-plaintext highlighter-rouge">K</code> into SRAM.</li>
      <li>Compute partial attention scores.</li>
      <li>Accumulate results.</li>
    </ol>
  </li>
  <li><strong>Speedup:</strong> 2-4x faster, uses less memory.</li>
</ul>

<h2 id="12-deep-dive-positional-encoding">12. Deep Dive: Positional Encoding</h2>

<p>Transformers have no notion of order. We add positional information.</p>

<p><strong>Sinusoidal Encoding (Original Transformer):</strong>
<code class="language-plaintext highlighter-rouge">PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)</code>
<code class="language-plaintext highlighter-rouge">PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)</code></p>

<p><strong>Why this works:</strong></p>
<ul>
  <li>Different frequencies for different dimensions.</li>
  <li>Model can learn to attend to relative positions.</li>
</ul>

<p><strong>Learned Positional Embeddings (BERT, GPT):</strong></p>
<ul>
  <li>Treat position as a lookup table.</li>
  <li>More flexible, but limited to max sequence length seen during training.</li>
</ul>

<p><strong>Rotary Position Embedding (RoPE):</strong></p>
<ul>
  <li>Used in LLaMA, PaLM.</li>
  <li>Rotates query and key vectors based on position.</li>
  <li>Allows extrapolation to longer sequences.</li>
</ul>

<h2 id="13-system-design-chatbot-with-context">13. System Design: Chatbot with Context</h2>

<p><strong>Scenario:</strong> Build a chatbot that remembers conversation history.</p>

<p><strong>Challenge:</strong> Context grows with each turn. How to handle long conversations?</p>

<p><strong>Approach 1: Sliding Window</strong></p>
<ul>
  <li>Keep only last <code class="language-plaintext highlighter-rouge">N</code> tokens (e.g., 2048).</li>
  <li><strong>Pro:</strong> Fixed memory.</li>
  <li><strong>Con:</strong> Forgets old context.</li>
</ul>

<p><strong>Approach 2: Summarization</strong></p>
<ul>
  <li>Periodically summarize old context.</li>
  <li><strong>Pro:</strong> Retains important info.</li>
  <li><strong>Con:</strong> Lossy compression.</li>
</ul>

<p><strong>Approach 3: Retrieval-Augmented</strong></p>
<ul>
  <li>Store conversation in a vector DB.</li>
  <li>Retrieve relevant past messages based on current query.</li>
  <li><strong>Pro:</strong> Scalable to infinite history.</li>
  <li><strong>Con:</strong> Requires embedding model + DB.</li>
</ul>

<h2 id="14-deep-dive-beam-search-implementation">14. Deep Dive: Beam Search Implementation</h2>

<p>``python
def beam_search(model, start_token, beam_width=5, max_len=50):
 # Initialize beam with start token
 beams = [(start_token, 0.0)] # (sequence, log_prob)</p>

<p>for _ in range(max_len):
 candidates = []</p>

<p>for seq, score in beams:
 if seq[-1] == EOS_TOKEN:
 candidates.append((seq, score))
 continue</p>

<p># Get next token probabilities
 logits = model(seq)
 log_probs = F.log_softmax(logits, dim=-1)</p>

<p># Top-k tokens
 top_k_probs, top_k_tokens = torch.topk(log_probs, beam_width)</p>

<p>for prob, token in zip(top_k_probs, top_k_tokens):
 new_seq = seq + [token.item()]
 new_score = score + prob.item()
 candidates.append((new_seq, new_score))</p>

<p># Keep top beam_width candidates
 beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]</p>

<p>return beams[0][0] # Best sequence
``</p>

<h2 id="15-production-optimizations">15. Production Optimizations</h2>

<h3 id="1-model-quantization">1. Model Quantization</h3>
<ul>
  <li>Convert FP32 → INT8.</li>
  <li><strong>Speedup:</strong> 4x.</li>
  <li><strong>Accuracy Loss:</strong> &lt; 1% with careful calibration.</li>
</ul>

<h3 id="2-distillation">2. Distillation</h3>
<ul>
  <li>Train a small “student” model to mimic a large “teacher”.</li>
  <li><strong>DistilBERT:</strong> 40% smaller, 60% faster, 97% of BERT’s performance.</li>
</ul>

<h3 id="3-pruning">3. Pruning</h3>
<ul>
  <li>Remove unimportant weights (e.g., magnitude pruning).</li>
  <li>
    <p><strong>Sparse Transformers:</strong> 90% sparsity with minimal accuracy loss.</p>
  </li>
  <li><strong>Sparse Transformers:</strong> 90% sparsity with minimal accuracy loss.</li>
</ul>

<h2 id="16-deep-dive-sequence-to-sequence-models-seq2seq">16. Deep Dive: Sequence-to-Sequence Models (Seq2Seq)</h2>

<p><strong>Architecture:</strong> Encoder-Decoder with Attention.</p>

<p><strong>Use Cases:</strong></p>
<ul>
  <li>Machine Translation (English → French)</li>
  <li>Summarization (Long article → Short summary)</li>
  <li>Question Answering (Context + Question → Answer)</li>
</ul>

<p><strong>Key Innovation: Attention Mechanism (Bahdanau et al., 2015)</strong></p>
<ul>
  <li><strong>Problem:</strong> Fixed-length context vector bottleneck.</li>
  <li><strong>Solution:</strong> Decoder attends to all encoder states.</li>
</ul>

<p><strong>Implementation:</strong>
``python
class Seq2SeqWithAttention(nn.Module):
 def <strong>init</strong>(self, vocab_size, hidden_size):
 super().<strong>init</strong>()
 self.encoder = nn.LSTM(vocab_size, hidden_size, bidirectional=True)
 self.decoder = nn.LSTM(vocab_size, hidden_size)
 self.attention = nn.Linear(hidden_size * 3, 1)
 self.output = nn.Linear(hidden_size, vocab_size)</p>

<p>def forward(self, src, tgt):
 # Encode
 encoder_outputs, (h, c) = self.encoder(src)</p>

<p># Decode with attention
 decoder_hidden = h
 outputs = []</p>

<p>for t in range(tgt.size(0)):
 # Compute attention scores
 scores = self.attention(torch.cat([
 decoder_hidden.expand(encoder_outputs.size(0), -1, -1),
 encoder_outputs
 ], dim=2))</p>

<p># Attention weights
 attn_weights = F.softmax(scores, dim=0)</p>

<p># Context vector
 context = (attn_weights * encoder_outputs).sum(dim=0)</p>

<p># Decoder step
 decoder_input = torch.cat([tgt[t], context], dim=1)
 output, (decoder_hidden, c) = self.decoder(decoder_input.unsqueeze(0), (decoder_hidden, c))</p>

<p>outputs.append(self.output(output))</p>

<p>return torch.cat(outputs, dim=0)
``</p>

<h2 id="17-advanced-sparse-attention-mechanisms">17. Advanced: Sparse Attention Mechanisms</h2>

<p><strong>Problem:</strong> Full attention is O(N^2). For <code class="language-plaintext highlighter-rouge">N = 100k</code> (long documents), this is prohibitive.</p>

<p><strong>Solutions:</strong></p>

<p><strong>1. Longformer (Beltagy et al., 2020):</strong></p>
<ul>
  <li><strong>Local Attention:</strong> Each token attends to <code class="language-plaintext highlighter-rouge">w</code> neighbors (sliding window).</li>
  <li><strong>Global Attention:</strong> A few tokens (e.g., <code class="language-plaintext highlighter-rouge">[CLS]</code>) attend to everything.</li>
  <li><strong>Complexity:</strong> O(N \cdot w) instead of O(N^2).</li>
</ul>

<p><strong>2. BigBird (Zaheer et al., 2020):</strong></p>
<ul>
  <li><strong>Random Attention:</strong> Each token attends to <code class="language-plaintext highlighter-rouge">r</code> random tokens.</li>
  <li><strong>Block Attention:</strong> Divide sequence into blocks, attend within blocks.</li>
  <li><strong>Global Attention:</strong> Special tokens attend globally.</li>
</ul>

<p><strong>3. Linformer (Wang et al., 2020):</strong></p>
<ul>
  <li><strong>Low-Rank Projection:</strong> Project <code class="language-plaintext highlighter-rouge">K, V</code> to lower dimension.</li>
  <li><strong>Complexity:</strong> O(N \cdot k) where <code class="language-plaintext highlighter-rouge">k \ll N</code>.</li>
</ul>

<p><strong>Code (Longformer-style Local Attention):</strong>
``python
def local_attention(Q, K, V, window_size=256):
 N, d = Q.shape</p>

<p># Create attention mask (band matrix)
 mask = torch.zeros(N, N)
 for i in range(N):
 start = max(0, i - window_size // 2)
 end = min(N, i + window_size // 2)
 mask[i, start:end] = 1</p>

<p># Compute attention
 scores = (Q @ K.T) / math.sqrt(d)
 scores = scores.masked_fill(mask == 0, float(‘-inf’))
 attn = F.softmax(scores, dim=-1)</p>

<p>return attn @ V
``</p>

<h2 id="18-case-study-alphafold-protein-folding">18. Case Study: AlphaFold (Protein Folding)</h2>

<p><strong>Problem:</strong> Predict 3D protein structure from amino acid sequence.</p>

<p><strong>Why Sequence Modeling?</strong></p>
<ul>
  <li>Protein is a sequence of amino acids (A, C, D, E, …).</li>
  <li>Structure depends on long-range interactions (residue 10 affects residue 500).</li>
</ul>

<p><strong>AlphaFold 2 Architecture:</strong></p>
<ol>
  <li><strong>Evoformer:</strong> Transformer variant with:
    <ul>
      <li><strong>MSA (Multiple Sequence Alignment) Attention:</strong> Attend across evolutionary related sequences.</li>
      <li><strong>Pair Representation:</strong> Attend to pairwise residue relationships.</li>
    </ul>
  </li>
  <li><strong>Structure Module:</strong> Predicts 3D coordinates.</li>
</ol>

<p><strong>Key Innovation:</strong> Attention over both sequence and structure space.</p>

<p><strong>Result:</strong> Solved a 50-year-old problem. Accuracy comparable to experimental methods.</p>

<h2 id="19-deep-dive-mixture-of-experts-moe">19. Deep Dive: Mixture of Experts (MoE)</h2>

<p><strong>Idea:</strong> Instead of one giant model, use many small “expert” models. Route each input to the most relevant experts.</p>

<p><strong>Architecture:</strong>
<code class="language-plaintext highlighter-rouge">
Input → Router (Gating Network) → Top-K Experts → Combine Outputs
</code></p>

<p><strong>Example: Switch Transformer (Google, 2021)</strong></p>
<ul>
  <li>1.6 <strong>Trillion</strong> parameters.</li>
  <li>But only 10B active per token (sparse activation).</li>
  <li><strong>Routing:</strong> Each token is sent to 1 expert (out of 2048).</li>
</ul>

<p><strong>Benefits:</strong></p>
<ul>
  <li><strong>Scalability:</strong> Add more experts without increasing per-token compute.</li>
  <li><strong>Specialization:</strong> Expert 1 learns math, Expert 2 learns code, etc.</li>
</ul>

<p><strong>Challenges:</strong></p>
<ul>
  <li><strong>Load Balancing:</strong> Ensure all experts are used equally.</li>
  <li><strong>Training Instability:</strong> Router can collapse (send everything to one expert).</li>
</ul>

<p><strong>Code:</strong>
``python
class MoELayer(nn.Module):
 def <strong>init</strong>(self, num_experts, hidden_size):
 super().<strong>init</strong>()
 self.router = nn.Linear(hidden_size, num_experts)
 self.experts = nn.ModuleList([
 nn.Sequential(
 nn.Linear(hidden_size, hidden_size * 4),
 nn.ReLU(),
 nn.Linear(hidden_size * 4, hidden_size)
 ) for _ in range(num_experts)
 ])</p>

<p>def forward(self, x):
 # Route
 router_logits = self.router(x)
 router_probs = F.softmax(router_logits, dim=-1)</p>

<p># Top-1 expert per token
 expert_idx = torch.argmax(router_probs, dim=-1)</p>

<p># Apply experts
 output = torch.zeros_like(x)
 for i, expert in enumerate(self.experts):
 mask = (expert_idx == i)
 if mask.any():
 output[mask] = expert(x[mask])</p>

<p>return output
``</p>

<h2 id="20-production-serving-architecture">20. Production Serving Architecture</h2>

<p><strong>Scenario:</strong> Serve GPT-3-scale model (175B params) with &lt;1s latency.</p>

<p><strong>Architecture:</strong>
<code class="language-plaintext highlighter-rouge">
┌─────────────┐
│ Client │
└──────┬──────┘
 │
 ▼
┌─────────────────┐
│ Load Balancer │
└──────┬──────────┘
 │
 ┌───┴───┐
 ▼ ▼
┌─────┐ ┌─────┐
│ GPU │ │ GPU │ (Model Parallelism: 8 GPUs per replica)
│ 1 │ │ 2 │
└─────┘ └─────┘
</code></p>

<p><strong>Optimizations:</strong></p>

<p><strong>1. Continuous Batching (Orca, 2022):</strong></p>
<ul>
  <li>Don’t wait for all requests to finish.</li>
  <li>As soon as one finishes, add a new request to the batch.</li>
  <li><strong>Speedup:</strong> 2-3x higher throughput.</li>
</ul>

<p><strong>2. PagedAttention (vLLM, 2023):</strong></p>
<ul>
  <li>Store KV cache in paged memory (like OS virtual memory).</li>
  <li><strong>Benefit:</strong> Reduce memory waste from fragmentation.</li>
</ul>

<p><strong>3. Speculative Decoding:</strong></p>
<ul>
  <li>Use a small model (1B) to draft 5 tokens.</li>
  <li>Use large model (175B) to verify in parallel.</li>
  <li><strong>Speedup:</strong> 2-3x for same quality.</li>
</ul>

<h2 id="21-evaluation-metrics-for-sequence-models">21. Evaluation Metrics for Sequence Models</h2>

<p><strong>1. Perplexity (Language Modeling):</strong>
<code class="language-plaintext highlighter-rouge">PPL = \exp\left(-\frac{1}{N}\sum_{i=1}^N \log P(w_i | w_{&lt;i})\right)</code></p>
<ul>
  <li>Lower is better.</li>
  <li><strong>Interpretation:</strong> “How surprised is the model by the test data?”</li>
</ul>

<p><strong>2. BLEU (Machine Translation):</strong></p>
<ul>
  <li>Measures n-gram overlap between prediction and reference.</li>
  <li><strong>Range:</strong> 0-100. BLEU &gt; 40 is considered good.</li>
</ul>

<p><strong>3. ROUGE (Summarization):</strong></p>
<ul>
  <li>Similar to BLEU, but focuses on recall.</li>
</ul>

<p><strong>4. Exact Match (QA):</strong></p>
<ul>
  <li>Percentage of predictions that exactly match ground truth.</li>
</ul>

<h2 id="22-common-pitfalls-and-how-to-avoid-them">22. Common Pitfalls and How to Avoid Them</h2>

<p><strong>Pitfall 1: Exposure Bias (Teacher Forcing)</strong></p>
<ul>
  <li>Model trained on ground truth, tested on its own predictions.</li>
  <li><strong>Fix:</strong> Scheduled sampling or reinforcement learning.</li>
</ul>

<p><strong>Pitfall 2: Length Bias in Beam Search</strong></p>
<ul>
  <li>Longer sequences have lower cumulative probability.</li>
  <li><strong>Fix:</strong> Length normalization: <code class="language-plaintext highlighter-rouge">\text{score} = \frac{\log P(y)}{|y|^\alpha}</code></li>
</ul>

<p><strong>Pitfall 3: Catastrophic Forgetting (Fine-Tuning)</strong></p>
<ul>
  <li>Fine-tuning on Task B makes model forget Task A.</li>
  <li><strong>Fix:</strong> Elastic Weight Consolidation (EWC) or multi-task learning.</li>
</ul>

<p><strong>Pitfall 4: OOM (Out of Memory) During Training</strong></p>
<ul>
  <li>Gradient accumulation: Simulate large batch with small batches.</li>
  <li><strong>Fix:</strong> <code class="language-plaintext highlighter-rouge">loss.backward(); if step % 4 == 0: optimizer.step()</code></li>
</ul>

<p><strong>Pitfall 5: Ignoring Positional Encoding Limits</strong></p>
<ul>
  <li>BERT trained on 512 tokens can’t handle 1024.</li>
  <li>
    <p><strong>Fix:</strong> Use RoPE or ALiBi (Attention with Linear Biases).</p>
  </li>
  <li><strong>Fix:</strong> Use RoPE or ALiBi (Attention with Linear Biases).</li>
</ul>

<h2 id="23-deep-dive-state-space-models-mamba--s4">23. Deep Dive: State Space Models (Mamba / S4)</h2>

<p><strong>Problem:</strong> Transformers are O(N^2). RNNs are O(N) but hard to train. Can we get the best of both?</p>

<p><strong>Solution:</strong> Structured State Space Models (SSMs).</p>

<p><strong>Key Idea:</strong></p>
<ul>
  <li>Model the sequence as a continuous-time system:
 <code class="language-plaintext highlighter-rouge">h'(t) = Ah(t) + Bx(t)</code>
 <code class="language-plaintext highlighter-rouge">y(t) = Ch(t)</code></li>
  <li><strong>Discretize</strong> it for digital computers.</li>
  <li><strong>Training:</strong> Can be computed as a <strong>Convolution</strong> (Parallelizable like Transformers).</li>
  <li><strong>Inference:</strong> Can be computed as a <strong>Recurrence</strong> (Constant memory like RNNs).</li>
</ul>

<p><strong>Mamba (Gu &amp; Dao, 2023):</strong></p>
<ul>
  <li>Introduces <strong>Selection Mechanism</strong>: The matrices <code class="language-plaintext highlighter-rouge">B</code> and <code class="language-plaintext highlighter-rouge">C</code> depend on the input <code class="language-plaintext highlighter-rouge">x_t</code>.</li>
  <li>Allows the model to “selectively” remember or ignore information.</li>
  <li><strong>Performance:</strong> Matches Transformers on language modeling, but with linear scaling O(N).</li>
</ul>

<p><strong>Code (Simplified Mamba Block):</strong>
``python
class MambaBlock(nn.Module):
 def <strong>init</strong>(self, d_model):
 super().<strong>init</strong>()
 self.in_proj = nn.Linear(d_model, d_model * 2)
 self.conv1d = nn.Conv1d(d_model, d_model, kernel_size=4, groups=d_model)
 self.x_proj = nn.Linear(d_model, dt_rank + B_rank + C_rank)
 self.out_proj = nn.Linear(d_model, d_model)</p>

<p>def forward(self, x):
 # 1. Project
 x_and_res = self.in_proj(x)
 x, res = x_and_res.chunk(2, dim=-1)</p>

<p># 2. Conv
 x = self.conv1d(x.transpose(1, 2)).transpose(1, 2)
 x = F.silu(x)</p>

<p># 3. SSM (Selective Scan)
 # This is the core Mamba magic (usually implemented in CUDA)
 y = selective_scan(x, self.x_proj(x))</p>

<p># 4. Output
 return self.out_proj(y * F.silu(res))
``</p>

<h2 id="24-deep-dive-reinforcement-learning-from-human-feedback-rlhf">24. Deep Dive: Reinforcement Learning from Human Feedback (RLHF)</h2>

<p><strong>Problem:</strong> LLMs are trained to predict the next token, not to be helpful or safe.</p>

<p><strong>Pipeline (InstructGPT / ChatGPT):</strong></p>

<ol>
  <li><strong>Supervised Fine-Tuning (SFT):</strong>
    <ul>
      <li>Collect human demonstrations (Question + Answer).</li>
      <li>Fine-tune base model.</li>
    </ul>
  </li>
  <li><strong>Reward Modeling (RM):</strong>
    <ul>
      <li>Collect comparison data (Model generates A and B; Human says A &gt; B).</li>
      <li>Train a Reward Model to predict human preference.</li>
      <li>Loss: <code class="language-plaintext highlighter-rouge">\log(\sigma(r(A) - r(B)))</code>.</li>
    </ul>
  </li>
  <li><strong>PPO (Proximal Policy Optimization):</strong>
    <ul>
      <li>Optimize the SFT model to maximize the reward from RM.</li>
      <li>Constraint: Don’t drift too far from SFT model (KL Divergence penalty).</li>
    </ul>
  </li>
</ol>

<p><strong>Impact:</strong></p>
<ul>
  <li>Aligns model with human intent.</li>
  <li>Reduces toxicity and hallucinations (mostly).</li>
</ul>

<h2 id="25-advanced-long-context-transformers-ring-attention">25. Advanced: Long-Context Transformers (Ring Attention)</h2>

<p><strong>Problem:</strong> How to train on 1 Million tokens?</p>
<ul>
  <li>Memory is the bottleneck. Even with Flash Attention, activations don’t fit on one GPU.</li>
</ul>

<p><strong>Ring Attention (Liu et al., 2023):</strong></p>
<ul>
  <li><strong>Idea:</strong> Distribute the sequence across multiple GPUs in a ring.</li>
  <li>Each GPU holds a chunk of Query, Key, Value.</li>
  <li><strong>Step 1:</strong> Compute local attention.</li>
  <li><strong>Step 2:</strong> Pass Key/Value blocks to the next GPU in the ring.</li>
  <li><strong>Step 3:</strong> Compute attention with new KV blocks.</li>
  <li><strong>Repeat:</strong> Until all KV blocks have visited all GPUs.</li>
</ul>

<p><strong>Result:</strong></p>
<ul>
  <li>Train on sequences length = Number of GPUs <code class="language-plaintext highlighter-rouge">\times</code> Memory per GPU.</li>
  <li>Enables “Needle in a Haystack” retrieval over entire books.</li>
</ul>

<h2 id="26-interview-questions-for-sequence-modeling">26. Interview Questions for Sequence Modeling</h2>

<p><strong>Q1: Why do we divide by <code class="language-plaintext highlighter-rouge">\sqrt{d_k}</code> in Attention?</strong>
<em>Answer:</em> To scale the dot products. If <code class="language-plaintext highlighter-rouge">d_k</code> is large, dot products become huge, pushing Softmax into regions with extremely small gradients (vanishing gradients).</p>

<p><strong>Q2: What is the difference between Post-Norm and Pre-Norm?</strong>
<em>Answer:</em></p>
<ul>
  <li><strong>Post-Norm:</strong> <code class="language-plaintext highlighter-rouge">LayerNorm(x + Sublayer(x))</code>. Original Transformer. Harder to train (warmup needed).</li>
  <li><strong>Pre-Norm:</strong> <code class="language-plaintext highlighter-rouge">x + Sublayer(LayerNorm(x))</code>. Used in GPT-3/LLaMA. More stable gradients, easier to train.</li>
</ul>

<p><strong>Q3: How does RoPE handle sequence length extrapolation?</strong>
<em>Answer:</em> By rotating the vectors, the attention score depends only on relative distance <code class="language-plaintext highlighter-rouge">(m-n)</code>. This relative property generalizes better to unseen lengths than absolute positional embeddings.</p>

<p><strong>Q4: Explain the “KV Cache” memory usage.</strong>
<em>Answer:</em> Memory = <code class="language-plaintext highlighter-rouge">2 \times \text{Batch} \times \text{SeqLen} \times \text{Layers} \times \text{HiddenSize}</code>. For long sequences, this dominates memory. PagedAttention helps reduce fragmentation.</p>

<p><strong>Q5: Why use MoE?</strong>
<em>Answer:</em> To decouple model size (parameters) from compute cost (FLOPs). We can have a 1T parameter model but only use 10B parameters per token, enabling massive capacity with reasonable inference latency.</p>

<h2 id="27-ethical-considerations">27. Ethical Considerations</h2>

<p><strong>1. Hallucinations:</strong></p>
<ul>
  <li>Sequence models are “stochastic parrots”. They generate plausible-sounding but potentially false text.</li>
  <li><strong>Risk:</strong> Misinformation in medical/legal contexts.</li>
  <li><strong>Mitigation:</strong> RAG (Retrieval Augmented Generation) to ground answers in facts.</li>
</ul>

<p><strong>2. Bias and Toxicity:</strong></p>
<ul>
  <li>Models learn biases present in training data (internet).</li>
  <li><strong>Risk:</strong> Hate speech, stereotypes.</li>
  <li><strong>Mitigation:</strong> RLHF, careful data curation, red-teaming.</li>
</ul>

<p><strong>3. Dual Use:</strong></p>
<ul>
  <li>Code generation models can write malware.</li>
  <li>
    <p><strong>Mitigation:</strong> Safety guardrails, refusal to answer harmful queries.</p>
  </li>
  <li><strong>Mitigation:</strong> Safety guardrails, refusal to answer harmful queries.</li>
</ul>

<h2 id="28-common-mistakes-in-sequence-modeling">28. Common Mistakes in Sequence Modeling</h2>

<p><strong>1. Training on Test Data (Data Leakage):</strong></p>
<ul>
  <li>Accidentally including the test set in the pre-training corpus.</li>
  <li><strong>Consequence:</strong> Overestimated performance.</li>
  <li><strong>Fix:</strong> De-duplication (MinHash) against evaluation benchmarks.</li>
</ul>

<p><strong>2. Ignoring Tokenizer Issues:</strong></p>
<ul>
  <li>Different tokenizers (BPE, WordPiece) handle whitespace and special characters differently.</li>
  <li><strong>Consequence:</strong> Poor performance on code or multilingual text.</li>
  <li><strong>Fix:</strong> Use a robust tokenizer like Tiktoken or SentencePiece.</li>
</ul>

<p><strong>3. Incorrect Masking in Causal Attention:</strong></p>
<ul>
  <li>Allowing tokens to attend to future tokens during training.</li>
  <li><strong>Consequence:</strong> Model learns to cheat, fails at inference.</li>
  <li><strong>Fix:</strong> Verify the causal mask (upper triangular matrix is <code class="language-plaintext highlighter-rouge">-\infty</code>).</li>
</ul>

<p><strong>4. Underestimating Inference Cost:</strong></p>
<ul>
  <li>Focusing only on training loss.</li>
  <li><strong>Consequence:</strong> Model is too slow/expensive to deploy.</li>
  <li><strong>Fix:</strong> Monitor FLOPs per token and KV cache size during design.</li>
</ul>

<h2 id="29-glossary-of-terms">29. Glossary of Terms</h2>

<ul>
  <li><strong>Token:</strong> The atomic unit of text processing (word, subword, or character).</li>
  <li><strong>Embedding:</strong> A dense vector representation of a token.</li>
  <li><strong>Attention:</strong> Mechanism to weigh the importance of different input tokens.</li>
  <li><strong>Self-Attention:</strong> Attention applied to the sequence itself.</li>
  <li><strong>Cross-Attention:</strong> Attention applied between two sequences (e.g., Encoder-Decoder).</li>
  <li><strong>Logits:</strong> The raw, unnormalized scores output by the last layer.</li>
  <li><strong>Softmax:</strong> Function to convert logits into probabilities.</li>
  <li><strong>Temperature:</strong> A hyperparameter that controls the randomness of sampling.</li>
  <li><strong>Beam Search:</strong> A heuristic search algorithm that explores a graph by expanding the most promising node in a limited set.</li>
  <li><strong>Perplexity:</strong> A measurement of how well a probability model predicts a sample.</li>
</ul>

<h2 id="30-further-reading">30. Further Reading</h2>

<ol>
  <li><strong>“Attention Is All You Need” (Vaswani et al., 2017):</strong> The Transformer paper.</li>
  <li><strong>“BERT: Pre-training of Deep Bidirectional Transformers” (Devlin et al., 2019):</strong> Masked language modeling.</li>
  <li><strong>“GPT-3: Language Models are Few-Shot Learners” (Brown et al., 2020):</strong> Scaling laws.</li>
  <li><strong>“Flash Attention” (Dao et al., 2022):</strong> Memory-efficient attention.</li>
  <li><strong>“Switch Transformers” (Fedus et al., 2021):</strong> Mixture of Experts at scale.</li>
</ol>

<h2 id="30-conclusion">30. Conclusion</h2>

<p>Sequence modeling has transformed from simple statistical methods (n-grams) to the behemoth Large Language Models that define the current AI era. The journey from RNNs to LSTMs and finally to Transformers represents a shift from sequential processing to parallel, attention-based architectures. This evolution has enabled us to model not just language, but code, biology, and even robot actions as sequences. As we look forward, the challenges of infinite context length, efficient inference, and alignment with human values remain the frontier of research. Whether you are building a chatbot, a translation system, or a protein folder, understanding the underlying mechanics of attention and state management is the key to unlocking the potential of sequence models.</p>

<h2 id="31-summary">31. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Model</th>
      <th style="text-align: left">Pros</th>
      <th style="text-align: left">Cons</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>RNN</strong></td>
      <td style="text-align: left">Simple</td>
      <td style="text-align: left">Vanishing gradients</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>LSTM</strong></td>
      <td style="text-align: left">Long memory</td>
      <td style="text-align: left">Sequential (slow)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Transformer</strong></td>
      <td style="text-align: left">Parallel, Scalable</td>
      <td style="text-align: left">O(N^2) memory</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Flash Attention</strong></td>
      <td style="text-align: left">Memory efficient</td>
      <td style="text-align: left">Complex implementation</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>MoE</strong></td>
      <td style="text-align: left">Trillion-scale models</td>
      <td style="text-align: left">Load balancing challenges</td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/">arunbaby.com/ml-system-design/0037-sequence-modeling</a></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#attention" class="page__taxonomy-item p-category" rel="tag">attention</a><span class="sep">, </span>
    
      <a href="/tags/#lstm" class="page__taxonomy-item p-category" rel="tag">lstm</a><span class="sep">, </span>
    
      <a href="/tags/#rnn" class="page__taxonomy-item p-category" rel="tag">rnn</a><span class="sep">, </span>
    
      <a href="/tags/#time-series" class="page__taxonomy-item p-category" rel="tag">time-series</a><span class="sep">, </span>
    
      <a href="/tags/#transformer" class="page__taxonomy-item p-category" rel="tag">transformer</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml_system_design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0037-longest-increasing-subsequence/" rel="permalink">Longest Increasing Subsequence (LIS)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Finding the longest upward trend in chaos.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0037-sequence-to-sequence-speech/" rel="permalink">Sequence-to-Sequence Speech Models
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“From waveforms to words, and back again.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0037-code-execution-agents/" rel="permalink">Code Execution Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Let agents run code safely: sandbox execution, cap damage, and verify outputs like a production system.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Sequence+Modeling+in+ML%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0037-sequence-modeling%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0037-sequence-modeling%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0037-sequence-modeling/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0036-resource-partitioning/" class="pagination--pager" title="Resource Partitioning in ML Clusters">Previous</a>
    
    
      <a href="/ml-system-design/0038-hyperparameter-optimization/" class="pagination--pager" title="Hyperparameter Optimization">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
