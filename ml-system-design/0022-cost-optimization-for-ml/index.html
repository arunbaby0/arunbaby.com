<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Cost Optimization for ML - Arun Baby</title>
<meta name="description" content="A comprehensive guide to FinOps for Machine Learning: reducing TCO without compromising accuracy or latency.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Cost Optimization for ML">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0022-cost-optimization-for-ml/">


  <meta property="og:description" content="A comprehensive guide to FinOps for Machine Learning: reducing TCO without compromising accuracy or latency.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Cost Optimization for ML">
  <meta name="twitter:description" content="A comprehensive guide to FinOps for Machine Learning: reducing TCO without compromising accuracy or latency.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0022-cost-optimization-for-ml/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-05T20:36:26+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0022-cost-optimization-for-ml/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Cost Optimization for ML">
    <meta itemprop="description" content="A comprehensive guide to FinOps for Machine Learning: reducing TCO without compromising accuracy or latency.">
    <meta itemprop="datePublished" content="2025-12-05T20:36:26+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0022-cost-optimization-for-ml/" itemprop="url">Cost Optimization for ML
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          16 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#the-challenge-efficiency-vs-performance">The Challenge: Efficiency vs. Performance</a></li><li><a href="#glossary-of-finops-terms">Glossary of FinOps Terms</a></li><li><a href="#the-anatomy-of-ml-costs">The Anatomy of ML Costs</a><ul><li><a href="#1-compute-the-big-one">1. Compute (The Big One)</a></li><li><a href="#2-storage">2. Storage</a></li><li><a href="#3-data-transfer-networking">3. Data Transfer (Networking)</a></li></ul></li><li><a href="#high-level-architecture-the-cost-aware-pipeline">High-Level Architecture: The Cost-Aware Pipeline</a></li><li><a href="#strategy-1-the-spot-instance-revolution">Strategy 1: The Spot Instance Revolution</a><ul><li><a href="#how-to-tame-the-spot-beast">How to Tame the Spot Beast</a><ul><li><a href="#for-training">For Training</a></li><li><a href="#for-inference">For Inference</a></li></ul></li></ul></li><li><a href="#strategy-2-model-optimization-make-it-smaller">Strategy 2: Model Optimization (Make it Smaller)</a><ul><li><a href="#1-quantization">1. Quantization</a></li><li><a href="#2-pruning">2. Pruning</a></li><li><a href="#3-knowledge-distillation">3. Knowledge Distillation</a></li></ul></li><li><a href="#strategy-3-hardware-selection-deep-dive">Strategy 3: Hardware Selection Deep Dive</a><ul><li><a href="#nvidia-gpus-the-workhorses">NVIDIA GPUs: The Workhorses</a></li><li><a href="#google-tpus-tensor-processing-units">Google TPUs (Tensor Processing Units)</a></li><li><a href="#aws-inferentia--trainium">AWS Inferentia / Trainium</a></li></ul></li><li><a href="#strategy-4-kubernetes-cost-optimization">Strategy 4: Kubernetes Cost Optimization</a><ul><li><a href="#1-node-pools">1. Node Pools</a></li><li><a href="#2-taints-and-tolerations">2. Taints and Tolerations</a></li><li><a href="#3-resource-requests--limits">3. Resource Requests &amp; Limits</a></li></ul></li><li><a href="#detailed-case-study-the-expensive-classifier">Detailed Case Study: The “Expensive Classifier”</a></li><li><a href="#implementation-cost-aware-router">Implementation: Cost-Aware Router</a></li><li><a href="#monitoring--metrics-the-finops-dashboard">Monitoring &amp; Metrics: The FinOps Dashboard</a></li><li><a href="#vendor-comparison-aws-vs-gcp-vs-azure">Vendor Comparison: AWS vs GCP vs Azure</a></li><li><a href="#green-ai-the-hidden-cost">Green AI: The Hidden Cost</a></li><li><a href="#future-trends">Future Trends</a></li><li><a href="#checklist-for-junior-engineers">Checklist for Junior Engineers</a></li><li><a href="#appendix-a-system-design-interview-transcript">Appendix A: System Design Interview Transcript</a></li><li><a href="#appendix-b-faq">Appendix B: FAQ</a></li><li><a href="#conclusion">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>A comprehensive guide to FinOps for Machine Learning: reducing TCO without compromising accuracy or latency.</strong></p>

<h2 id="the-challenge-efficiency-vs-performance">The Challenge: Efficiency vs. Performance</h2>

<p>In the world of Machine Learning System Design, building a model that achieves 99.9% accuracy is only half the battle. The other half is ensuring that this model doesn’t bankrupt your company.</p>

<p><strong>Cost Optimization for ML</strong> is the art and science of reducing the financial footprint of your ML workloads without compromising on user experience (latency) or model quality (accuracy).</p>

<p>As a junior engineer, you might think, “Cost is a manager’s problem.” But in modern tech companies, <strong>FinOps</strong> (Financial Operations) is everyone’s responsibility. An engineer who can design a system that saves the company $50,000 a month is often more valuable than one who improves model accuracy by 0.1%.</p>

<p>In this deep dive, we will explore the entire stack—from hardware selection to model compression to architectural patterns—to uncover where the money goes and how to save it. We will compare cloud providers, dive into the physics of semiconductors, and write actual Kubernetes configuration files.</p>

<h2 id="glossary-of-finops-terms">Glossary of FinOps Terms</h2>

<p>Before we begin, let’s define the language of money in tech.</p>

<ul>
  <li><strong>CAPEX (Capital Expenditure):</strong> Upfront money spent on buying physical servers (e.g., buying 100 H100s for your own data center).</li>
  <li><strong>OPEX (Operational Expenditure):</strong> Ongoing money spent on cloud services (e.g., renting AWS EC2 instances). Most modern ML is OPEX.</li>
  <li><strong>TCO (Total Cost of Ownership):</strong> The sum of all costs (compute + storage + networking + engineering salaries + maintenance) over the life of the project.</li>
  <li><strong>ROI (Return on Investment):</strong> (Revenue - Cost) / Cost. If your model costs $100 to run and generates $110 in value, the ROI is 10%.</li>
  <li><strong>Unit Economics:</strong> The cost to serve <em>one</em> unit of value (e.g., “Cost per 1000 predictions”). This is the most important metric for scaling.</li>
  <li><strong>Commitment Savings Plan (CSP):</strong> A contract where you promise to spend $X/hour for 1-3 years in exchange for a 30-50% discount.</li>
</ul>

<h2 id="the-anatomy-of-ml-costs">The Anatomy of ML Costs</h2>

<p>To fix a leak, you first have to find it. Let’s break down the bill.</p>

<h3 id="1-compute-the-big-one">1. Compute (The Big One)</h3>
<p>This is usually 70-80% of the cost.</p>
<ul>
  <li><strong>Training:</strong> Massive bursts of high-end GPU usage (e.g., NVIDIA A100s, H100s). Training a large language model can cost millions.</li>
  <li><strong>Inference:</strong> Continuous usage of smaller GPUs (T4, L4) or CPUs. While per-hour cost is lower, this runs 24/7, so it adds up.</li>
  <li><strong>Development:</strong> Notebooks (Jupyter/Colab) left running overnight. This is “zombie spend.”</li>
</ul>

<h3 id="2-storage">2. Storage</h3>
<ul>
  <li><strong>Object Storage (S3/GCS):</strong> Storing petabytes of raw data, logs, and model checkpoints.</li>
  <li><strong>Block Storage (EBS/Persistent Disk):</strong> High-speed disks attached to GPU instances. These are expensive!</li>
  <li><strong>Feature Store:</strong> Low-latency databases (Redis/DynamoDB) for serving features.</li>
</ul>

<h3 id="3-data-transfer-networking">3. Data Transfer (Networking)</h3>
<ul>
  <li><strong>Egress:</strong> Moving data out of the cloud provider (e.g., serving images to users).</li>
  <li><strong>Cross-Zone/Region:</strong> Moving data between availability zones (AZs) for redundancy. Training a model in Zone A with data in Zone B incurs massive costs.</li>
</ul>

<h2 id="high-level-architecture-the-cost-aware-pipeline">High-Level Architecture: The Cost-Aware Pipeline</h2>

<pre><code class="language-ascii">+----------------+       +------------------+       +-------------------+
|  User Request  | ----&gt; |  Load Balancer   | ----&gt; |  Inference Router |
+----------------+       +------------------+       +-------------------+
                                                            |
                                         +------------------+------------------+
                                         |                                     |
                                 (High Confidence?)                     (Low Confidence?)
                                         |                                     |
                                         v                                     v
                                +----------------+                    +----------------+
                                |  CPU Cluster   |                    |  GPU Cluster   |
                                | (DistilBERT)   |                    | (BERT-Large)   |
                                +----------------+                    +----------------+
                                         |                                     |
                                         +------------------+------------------+
                                                            |
                                                            v
                                                   +----------------+
                                                   |    Response    |
                                                   +----------------+
</code></pre>

<h2 id="strategy-1-the-spot-instance-revolution">Strategy 1: The Spot Instance Revolution</h2>

<p>Cloud providers have excess capacity. They sell this spare capacity at a massive discount (60-90% off) called <strong>Spot Instances</strong> (AWS) or <strong>Preemptible VMs</strong> (GCP). The catch? They can take it back with a 30-second to 2-minute warning.</p>

<h3 id="how-to-tame-the-spot-beast">How to Tame the Spot Beast</h3>

<p>You cannot run a standard web server on Spot instances without risk. But for ML, they are a goldmine.</p>

<h4 id="for-training">For Training</h4>
<p>Training is long-running but often checkpointable.</p>
<ol>
  <li><strong>Checkpoint Frequently:</strong> Save your model weights to S3 every 10-15 minutes.</li>
  <li><strong>Auto-Resume:</strong> Use a job orchestrator (like Ray, Slurm, or Kubernetes Jobs) that detects when a node dies and automatically spins up a new one, loading the last checkpoint.</li>
  <li><strong>Mixed Clusters:</strong> Use a small “On-Demand” head node (manager) and a fleet of “Spot” worker nodes. If workers die, the manager survives and requests new workers.</li>
</ol>

<h4 id="for-inference">For Inference</h4>
<p>This is trickier because you can’t drop user requests.</p>
<ol>
  <li><strong>Over-provisioning:</strong> Run 20% more replicas than you need. If some get preempted, the others handle the load while new ones spin up.</li>
  <li><strong>Graceful Shutdown:</strong> Listen for the “Preemption Notice” (a signal sent by the cloud provider). When received:
    <ul>
      <li>Stop accepting new requests (update Load Balancer health check to ‘fail’).</li>
      <li>Finish processing current requests.</li>
      <li>Upload logs.</li>
      <li>Die peacefully.</li>
    </ul>
  </li>
</ol>

<h2 id="strategy-2-model-optimization-make-it-smaller">Strategy 2: Model Optimization (Make it Smaller)</h2>

<p>The most effective way to save compute is to do less math.</p>

<h3 id="1-quantization">1. Quantization</h3>
<p>Standard ML models use <strong>FP32</strong> (32-bit floating point numbers).</p>
<ul>
  <li><strong>FP16 (Half Precision):</strong> Most modern GPUs run faster on FP16. It cuts memory usage in half.</li>
  <li><strong>INT8 (8-bit Integer):</strong> This is the game changer. It reduces model size by 4x and speeds up inference by 2-4x on CPUs.</li>
</ul>

<p><strong>Types of Quantization:</strong></p>
<ul>
  <li><strong>Post-Training Quantization (PTQ):</strong> Take a trained model and convert weights to INT8. Simple, but can drop accuracy.</li>
  <li><strong>Quantization-Aware Training (QAT):</strong> Simulate low-precision during training. The model learns to be robust to rounding errors. Higher accuracy, but more complex.</li>
</ul>

<h3 id="2-pruning">2. Pruning</h3>
<p>Neural networks are over-parameterized. Many weights are close to zero and contribute nothing.</p>
<ul>
  <li><strong>Unstructured Pruning:</strong> Set individual weights to zero. Makes the matrix “sparse.” Requires specialized hardware to see speedups.</li>
  <li><strong>Structured Pruning:</strong> Remove entire neurons, channels, or layers. This shrinks the matrix dimensions, leading to immediate speedups on all hardware.</li>
</ul>

<h3 id="3-knowledge-distillation">3. Knowledge Distillation</h3>
<p>Train a massive “Teacher” model (e.g., BERT-Large) to get high accuracy. Then, train a tiny “Student” model (e.g., DistilBERT) to mimic the Teacher’s output probabilities.</p>
<ul>
  <li><strong>Result:</strong> The Student is 40% smaller and 60% faster, retaining 97% of the Teacher’s accuracy.</li>
</ul>

<h2 id="strategy-3-hardware-selection-deep-dive">Strategy 3: Hardware Selection Deep Dive</h2>

<p>Don’t default to the most expensive GPU. Let’s look at the physics.</p>

<h3 id="nvidia-gpus-the-workhorses">NVIDIA GPUs: The Workhorses</h3>
<ul>
  <li><strong>A100 (Ampere):</strong> The king of training. 40GB/80GB VRAM. Massive memory bandwidth (1.6TB/s). Use this for training LLMs. Cost: ~$3-4/hr.</li>
  <li><strong>H100 (Hopper):</strong> The new king. Specialized Transformer Engine. 3x faster than A100 for LLMs. Cost: ~$4-5/hr (if you can find one).</li>
  <li><strong>T4 (Turing):</strong> The inference workhorse. 16GB VRAM. Cheap, widely available, supports INT8 well. Cost: ~$0.35-0.50/hr.</li>
  <li><strong>L4 (Ada Lovelace):</strong> The successor to T4. 24GB VRAM. Much faster ray tracing and video encoding. Great for generative AI (Stable Diffusion). Cost: ~$0.50-0.70/hr.</li>
  <li><strong>A10G:</strong> A middle ground. 24GB VRAM. Good for fine-tuning smaller models (7B params). Cost: ~$1.00/hr.</li>
</ul>

<h3 id="google-tpus-tensor-processing-units">Google TPUs (Tensor Processing Units)</h3>
<ul>
  <li><strong>Architecture:</strong> Systolic Arrays. Data flows through the chip like a heartbeat.</li>
  <li><strong>Pros:</strong> Massive throughput for large matrix math (perfect for Transformers).</li>
  <li><strong>Cons:</strong> Harder to debug than GPUs. Tightly coupled with XLA compiler.</li>
  <li><strong>Versions:</strong> TPUv4, TPUv5e (Efficiency focused).</li>
</ul>

<h3 id="aws-inferentia--trainium">AWS Inferentia / Trainium</h3>
<ul>
  <li><strong>Custom Silicon:</strong> Built by AWS specifically for cost.</li>
  <li><strong>Pros:</strong> Up to 40% cheaper than comparable GPU instances.</li>
  <li><strong>Cons:</strong> Requires recompiling models using AWS Neuron SDK.</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Hardware</th>
      <th style="text-align: left">Best For</th>
      <th style="text-align: left">Cost Profile</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>NVIDIA A100/H100</strong></td>
      <td style="text-align: left">Training massive LLMs</td>
      <td style="text-align: left">Very High ($3-4/hr)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>NVIDIA T4/L4</strong></td>
      <td style="text-align: left">Inference, Fine-tuning small models</td>
      <td style="text-align: left">Medium ($0.50/hr)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>CPU (Intel/AMD)</strong></td>
      <td style="text-align: left">Classical ML (XGBoost), Small DL models</td>
      <td style="text-align: left">Low ($0.05-0.10/hr)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>AWS Inferentia</strong></td>
      <td style="text-align: left">Specialized DL Inference</td>
      <td style="text-align: left">Very Low (High performance/$)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Google TPU</strong></td>
      <td style="text-align: left">Massive Training/Inference</td>
      <td style="text-align: left">Varies (Great for TensorFlow/JAX)</td>
    </tr>
  </tbody>
</table>

<p><strong>Rule of Thumb:</strong> Always try to run inference on CPU first. If it’s too slow, try a T4. Only use A100 for training.</p>

<h2 id="strategy-4-kubernetes-cost-optimization">Strategy 4: Kubernetes Cost Optimization</h2>

<p>Most ML runs on Kubernetes (K8s). Here is how to configure it for cost.</p>

<h3 id="1-node-pools">1. Node Pools</h3>
<p>Create separate pools for different workloads.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">cpu-pool</code>: For the API server, logging, monitoring. (Cheap instances).</li>
  <li><code class="language-plaintext highlighter-rouge">gpu-pool</code>: For inference pods. (Expensive instances).</li>
  <li><code class="language-plaintext highlighter-rouge">spot-gpu-pool</code>: For batch jobs. (Cheap, risky instances).</li>
</ul>

<h3 id="2-taints-and-tolerations">2. Taints and Tolerations</h3>
<p>Prevent non-critical pods from stealing expensive GPU nodes.</p>

<p><strong>Node Configuration:</strong></p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># On the GPU node</span>
<span class="na">taints</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s2">"</span><span class="s">accelerator"</span>
    <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">nvidia-tesla-t4"</span>
    <span class="na">effect</span><span class="pi">:</span> <span class="s2">"</span><span class="s">NoSchedule"</span>
</code></pre></div></div>

<p><strong>Pod Configuration:</strong></p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In your Inference Deployment</span>
<span class="na">tolerations</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">key</span><span class="pi">:</span> <span class="s2">"</span><span class="s">accelerator"</span>
    <span class="na">operator</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Equal"</span>
    <span class="na">value</span><span class="pi">:</span> <span class="s2">"</span><span class="s">nvidia-tesla-t4"</span>
    <span class="na">effect</span><span class="pi">:</span> <span class="s2">"</span><span class="s">NoSchedule"</span>
</code></pre></div></div>

<h3 id="3-resource-requests--limits">3. Resource Requests &amp; Limits</h3>
<p>If you don’t set these, one pod can eat the whole node.</p>
<ul>
  <li><strong>Requests:</strong> “I need at least this much.” K8s uses this for scheduling.</li>
  <li><strong>Limits:</strong> “Kill me if I use more than this.” K8s uses this for throttling/OOMKill.</li>
</ul>

<p><strong>Best Practice:</strong> Set Requests = Limits for memory (to avoid OOM kills). Set Requests &lt; Limits for CPU (to allow bursting).</p>

<h2 id="detailed-case-study-the-expensive-classifier">Detailed Case Study: The “Expensive Classifier”</h2>

<p><strong>Scenario:</strong>
You work at a startup. You have a sentiment analysis model (BERT-Base) that processes 1 million user reviews per day.</p>
<ul>
  <li><strong>Current Setup:</strong> 5 x <code class="language-plaintext highlighter-rouge">g4dn.xlarge</code> (NVIDIA T4) instances running 24/7.</li>
  <li><strong>Cost:</strong> $0.526/hr * 24 hrs * 30 days * 5 instances = <strong>$1,893 / month</strong>.</li>
</ul>

<p><strong>The Junior Engineer’s Optimization Plan:</strong></p>

<p><strong>Step 1: Auto-scaling (HPA)</strong>
Traffic isn’t constant. It peaks at 9 AM and drops at 2 AM.</p>
<ul>
  <li>You implement Kubernetes HPA.</li>
  <li>Average instance count drops from 5 to 3.</li>
  <li><strong>New Cost:</strong> $1,135 / month. (<strong>Saved $758</strong>)</li>
</ul>

<p><strong>Step 2: Spot Instances</strong>
You switch the node pool to Spot instances.</p>
<ul>
  <li>Spot price for <code class="language-plaintext highlighter-rouge">g4dn.xlarge</code> is ~$0.15/hr (approx 70% discount).</li>
  <li><strong>New Cost:</strong> $0.15 * 24 * 30 * 3 = $324 / month. (<strong>Saved $811</strong>)</li>
</ul>

<p><strong>Step 3: Quantization &amp; CPU Migration</strong>
You quantize the model to INT8 using ONNX Runtime. It now runs fast enough on a CPU!</p>
<ul>
  <li>You switch to <code class="language-plaintext highlighter-rouge">c6i.large</code> (Compute Optimized CPU) instances.</li>
  <li>Spot price for <code class="language-plaintext highlighter-rouge">c6i.large</code> is ~$0.03/hr.</li>
  <li>Because CPU is slower than GPU, you need 6 instances instead of 3 to handle the load.</li>
  <li><strong>New Cost:</strong> $0.03 * 24 * 30 * 6 = $129 / month. (<strong>Saved $195</strong>)</li>
</ul>

<p><strong>Total Savings:</strong>
From <strong>$1,893</strong> to <strong>$129</strong> per month. That is a <strong>93% reduction</strong> in cost.
This is the power of system design.</p>

<h2 id="implementation-cost-aware-router">Implementation: Cost-Aware Router</h2>

<p>Let’s look at code for a “Cascade” router. This is a pattern where you try a cheap model first, and only call the expensive model if the cheap one is unsure.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>

<span class="k">class</span> <span class="nc">ModelCascade</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cheap_model_url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://cpu-service/predict</span><span class="sh">"</span>
        <span class="n">self</span><span class="p">.</span><span class="n">expensive_model_url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://gpu-service/predict</span><span class="sh">"</span>
        <span class="n">self</span><span class="p">.</span><span class="n">confidence_threshold</span> <span class="o">=</span> <span class="mf">0.85</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_text</span><span class="p">):</span>
        <span class="c1"># Step 1: Call the Cheap Model (DistilBERT on CPU)
</span>        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">cheap_model_url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="n">input_text</span><span class="p">})</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()</span>
        
        <span class="n">confidence</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">confidence</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span>
        
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Cheap Model Confidence: </span><span class="si">{</span><span class="n">confidence</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Step 2: Check Confidence
</span>        <span class="k">if</span> <span class="n">confidence</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">confidence_threshold</span><span class="p">:</span>
            <span class="c1"># Good enough! Return early.
</span>            <span class="k">return</span> <span class="n">prediction</span>
        
        <span class="c1"># Step 3: Fallback to Expensive Model (GPT-4 / Large BERT on GPU)
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Confidence too low. Calling Expensive Model...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">expensive_model_url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="n">input_text</span><span class="p">})</span>
        <span class="k">return</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># Usage
</span><span class="n">cascade</span> <span class="o">=</span> <span class="nc">ModelCascade</span><span class="p">()</span>
<span class="c1"># "I love this product!" -&gt; Cheap model is 99% sure. Returns. Cost: $0.0001
# "The nuance of the texture was..." -&gt; Cheap model is 60% sure. Calls GPU. Cost: $0.01
</span></code></pre></div></div>

<h2 id="monitoring--metrics-the-finops-dashboard">Monitoring &amp; Metrics: The FinOps Dashboard</h2>

<p>You cannot optimize what you cannot measure. You need a dashboard.</p>

<p><strong>Tools:</strong></p>
<ul>
  <li><strong>Prometheus:</strong> Scrapes metrics from your pods.</li>
  <li><strong>Grafana:</strong> Visualizes the metrics.</li>
  <li><strong>Kubecost:</strong> A specialized tool that tells you exactly how much each namespace/deployment costs.</li>
</ul>

<p><strong>Key Metrics to Track:</strong></p>
<ol>
  <li><strong>Cost per Inference:</strong> Total Cost / Total Requests. (Goal: Drive this down).</li>
  <li><strong>GPU Utilization:</strong> If average utilization is &lt; 30%, you are wasting money. Scale down or bin-pack more models.</li>
  <li><strong>Spot Interruption Rate:</strong> How often are your nodes dying? If &gt; 5%, your reliability might suffer.</li>
</ol>

<h2 id="vendor-comparison-aws-vs-gcp-vs-azure">Vendor Comparison: AWS vs GCP vs Azure</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Feature</th>
      <th style="text-align: left">AWS (SageMaker)</th>
      <th style="text-align: left">GCP (Vertex AI)</th>
      <th style="text-align: left">Azure (ML Studio)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Spot Instances</strong></td>
      <td style="text-align: left">“Spot Instances” (Deep pools, reliable)</td>
      <td style="text-align: left">“Preemptible VMs” (Cheaper, but hard 24h limit)</td>
      <td style="text-align: left">“Spot VMs” (Variable eviction policy)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Inference Hardware</strong></td>
      <td style="text-align: left">Inferentia (Custom cheap chips)</td>
      <td style="text-align: left">TPUs (Fastest for massive models)</td>
      <td style="text-align: left">Strong partnership with OpenAI/NVIDIA</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Serverless</strong></td>
      <td style="text-align: left">Lambda (Good support)</td>
      <td style="text-align: left">Cloud Run (Excellent container support)</td>
      <td style="text-align: left">Azure Functions</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Pricing</strong></td>
      <td style="text-align: left">Complex, many hidden fees</td>
      <td style="text-align: left">Per-second billing (very friendly)</td>
      <td style="text-align: left">Enterprise-focused, bundled deals</td>
    </tr>
  </tbody>
</table>

<p><strong>Verdict:</strong></p>
<ul>
  <li><strong>GCP</strong> is often the cheapest for pure compute and easiest to use (K8s native).</li>
  <li><strong>AWS</strong> has the most mature ecosystem and hardware options (Inferentia).</li>
  <li><strong>Azure</strong> is best if you are already a Microsoft shop.</li>
</ul>

<h2 id="green-ai-the-hidden-cost">Green AI: The Hidden Cost</h2>

<p>Cost isn’t just money. It’s carbon.
Training a single large Transformer model can emit as much CO2 as 5 cars in their lifetimes.</p>
<ul>
  <li><strong>Measure:</strong> Use tools like <code class="language-plaintext highlighter-rouge">CodeCarbon</code> to estimate your emissions.</li>
  <li><strong>Optimize:</strong> Train in regions with green energy (e.g., Montreal, Oregon) where electricity comes from hydro/wind.</li>
  <li><strong>Impact:</strong> Cost optimization usually leads to carbon optimization. Using fewer GPUs means burning less coal.</li>
</ul>

<h2 id="future-trends">Future Trends</h2>

<p>Where is this field going?</p>
<ol>
  <li><strong>Neuromorphic Computing:</strong> Chips that mimic the human brain (Spiking Neural Networks). They consume milliwatts instead of watts.</li>
  <li><strong>Optical Computing:</strong> Using light (photons) instead of electricity (electrons) for matrix multiplication. Potentially 1000x faster and cheaper.</li>
  <li><strong>Federated Learning:</strong> Training models on user devices (phones) instead of central servers. Shifts the cost from you to the user (and preserves privacy).</li>
</ol>

<h2 id="checklist-for-junior-engineers">Checklist for Junior Engineers</h2>

<p>Before you deploy, ask yourself:</p>
<ol class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Do I really need a GPU?</strong> Have I benchmarked on a modern CPU?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Is my model quantized?</strong> Can I use INT8?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Am I using Spot instances?</strong> If not, why?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Is auto-scaling enabled?</strong> Or am I paying for idle time?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Are my logs optimized?</strong> Am I logging huge tensors to CloudWatch/Datadog? (This is a hidden cost killer!)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Is the data in the same region?</strong> Check for cross-region transfer fees.</li>
</ol>

<h2 id="appendix-a-system-design-interview-transcript">Appendix A: System Design Interview Transcript</h2>

<p><strong>Interviewer:</strong> “Design a cost-efficient training platform for a startup.”</p>

<p><strong>Candidate:</strong> “Okay, let’s start with requirements. How many users? What kind of models?”</p>

<p><strong>Interviewer:</strong> “50 data scientists. Training BERT and ResNet models. Budget is tight.”</p>

<p><strong>Candidate:</strong> “Understood. I propose a Kubernetes-based architecture on AWS.</p>
<ol>
  <li><strong>Compute:</strong> We will use a mixed cluster.
    <ul>
      <li><strong>Head Node:</strong> On-Demand <code class="language-plaintext highlighter-rouge">m5.large</code> for the K8s control plane.</li>
      <li><strong>Notebooks:</strong> Spot <code class="language-plaintext highlighter-rouge">t3.medium</code> instances. If they die, we lose the kernel but data is on EFS.</li>
      <li><strong>Training:</strong> Spot <code class="language-plaintext highlighter-rouge">g4dn.xlarge</code> instances. We will use <code class="language-plaintext highlighter-rouge">Volcano</code> scheduler for batch scheduling.</li>
    </ul>
  </li>
  <li><strong>Storage:</strong>
    <ul>
      <li><strong>Data:</strong> S3 Standard-IA (Infrequent Access) to save money.</li>
      <li><strong>Checkpoints:</strong> S3 Intelligent-Tiering.</li>
      <li><strong>Scratch Space:</strong> Amazon FSx for Lustre (expensive but needed for speed) or just local NVMe on the instances.</li>
    </ul>
  </li>
  <li><strong>Networking:</strong>
    <ul>
      <li>Keep everything in <code class="language-plaintext highlighter-rouge">us-east-1</code> to avoid data transfer fees.</li>
      <li>Use VPC Endpoints for S3 to avoid NAT Gateway charges.”</li>
    </ul>
  </li>
</ol>

<p><strong>Interviewer:</strong> “How do you handle Spot interruptions during training?”</p>

<p><strong>Candidate:</strong> “We will use <code class="language-plaintext highlighter-rouge">TorchElastic</code> or <code class="language-plaintext highlighter-rouge">Ray Train</code>. These frameworks support fault tolerance. When a Spot node is reclaimed, the job pauses. The K8s autoscaler requests a new Spot node. Once it joins, the job resumes from the last checkpoint stored in S3.”</p>

<p><strong>Interviewer:</strong> “What if Spot capacity is unavailable for hours?”</p>

<p><strong>Candidate:</strong> “We can implement a ‘Fallback to On-Demand’ policy. If a job is pending for &gt; 1 hour, we spin up an On-Demand instance. It costs more, but it unblocks the team.”</p>

<h2 id="appendix-b-faq">Appendix B: FAQ</h2>

<p><strong>Q: Is Serverless always cheaper?</strong>
A: No. If you have constant high traffic (e.g., 100 requests/sec 24/7), a dedicated instance is cheaper. Serverless is cheaper for “spiky” or low-volume traffic.</p>

<p><strong>Q: Does quantization hurt accuracy?</strong>
A: Usually &lt; 1% drop for INT8. If you go to INT4, the drop is significant unless you use advanced techniques like QLoRA.</p>

<p><strong>Q: Why is data transfer so expensive?</strong>
A: Cloud providers charge a premium for “Egress” (data leaving their network). It’s a lock-in mechanism.</p>

<p><strong>Q: What is the best region for cost?</strong>
A: <code class="language-plaintext highlighter-rouge">us-east-1</code> (N. Virginia), <code class="language-plaintext highlighter-rouge">us-west-2</code> (Oregon), and <code class="language-plaintext highlighter-rouge">eu-west-1</code> (Ireland) are usually the cheapest and have the most capacity.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Cost optimization is not about being “cheap.” It’s about being <strong>efficient</strong>. It’s about maximizing the business value extracted from every compute cycle.</p>

<p>By mastering these techniques—Spot instances, quantization, architectural patterns like Cascading—you become a force multiplier for your team. You allow your company to run more experiments, serve more users, and build better products with the same budget.</p>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Spot Instances</strong> are your best friend for batch workloads.</li>
  <li><strong>Quantization</strong> (INT8) is the easiest way to slash inference costs.</li>
  <li><strong>Right-sizing</strong> hardware (CPU vs GPU) is critical.</li>
  <li><strong>FinOps</strong> is an engineering discipline, not just accounting.</li>
</ul>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0022-cost-optimization-for-ml/">arunbaby.com/ml-system-design/0022-cost-optimization-for-ml</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#cloud-computing" class="page__taxonomy-item p-category" rel="tag">cloud-computing</a><span class="sep">, </span>
    
      <a href="/tags/#distillation" class="page__taxonomy-item p-category" rel="tag">distillation</a><span class="sep">, </span>
    
      <a href="/tags/#finops" class="page__taxonomy-item p-category" rel="tag">finops</a><span class="sep">, </span>
    
      <a href="/tags/#pruning" class="page__taxonomy-item p-category" rel="tag">pruning</a><span class="sep">, </span>
    
      <a href="/tags/#quantization" class="page__taxonomy-item p-category" rel="tag">quantization</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0022-minimum-path-sum/" rel="permalink">Minimum Path Sum
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          21 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The classic grid optimization problem that bridges the gap between simple recursion and 2D Dynamic Programming.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0022-cost-efficient-speech-systems/" rel="permalink">Cost-efficient Speech Systems
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Strategies for building profitable speech recognition systems by optimizing the entire pipeline from signal processing to hardware.
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Cost+Optimization+for+ML%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0022-cost-optimization-for-ml%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0022-cost-optimization-for-ml%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0022-cost-optimization-for-ml/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0021-neural-architecture-search/" class="pagination--pager" title="Neural Architecture Search">Previous</a>
    
    
      <a href="/ml-system-design/0023-beam-search-decoding/" class="pagination--pager" title="Beam Search Decoding">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
