<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Neural Architecture Search - Arun Baby</title>
<meta name="description" content="Design neural architecture search systems that automatically discover optimal model architectures using dynamic programming and path optimization—the same principles from grid path counting scaled to exponential search spaces.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Neural Architecture Search">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0021-neural-architecture-search/">


  <meta property="og:description" content="Design neural architecture search systems that automatically discover optimal model architectures using dynamic programming and path optimization—the same principles from grid path counting scaled to exponential search spaces.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Neural Architecture Search">
  <meta name="twitter:description" content="Design neural architecture search systems that automatically discover optimal model architectures using dynamic programming and path optimization—the same principles from grid path counting scaled to exponential search spaces.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0021-neural-architecture-search/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0021-neural-architecture-search/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Neural Architecture Search">
    <meta itemprop="description" content="Design neural architecture search systems that automatically discover optimal model architectures using dynamic programming and path optimization—the same principles from grid path counting scaled to exponential search spaces.">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0021-neural-architecture-search/" itemprop="url">Neural Architecture Search
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem-statement">Problem Statement</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li></ul></li><li><a href="#understanding-the-requirements">Understanding the Requirements</a><ul><li><a href="#why-nas">Why NAS?</a></li><li><a href="#scale-of-the-problem">Scale of the Problem</a></li><li><a href="#the-path-optimization-connection">The Path Optimization Connection</a></li></ul></li><li><a href="#high-level-architecture">High-Level Architecture</a><ul><li><a href="#key-components">Key Components</a></li></ul></li><li><a href="#component-deep-dives">Component Deep-Dives</a><ul><li><a href="#1-search-space-definition">1. Search Space Definition</a></li><li><a href="#2-search-strategy---reinforcement-learning">2. Search Strategy - Reinforcement Learning</a></li><li><a href="#3-search-strategy---differentiable-nas-darts">3. Search Strategy - Differentiable NAS (DARTS)</a></li><li><a href="#4-performance-estimation-strategies">4. Performance Estimation Strategies</a></li></ul></li><li><a href="#data-flow">Data Flow</a><ul><li><a href="#nas-pipeline">NAS Pipeline</a></li></ul></li><li><a href="#scaling-strategies">Scaling Strategies</a><ul><li><a href="#distributed-architecture-evaluation">Distributed Architecture Evaluation</a></li></ul></li><li><a href="#monitoring--metrics">Monitoring &amp; Metrics</a><ul><li><a href="#key-metrics">Key Metrics</a></li><li><a href="#visualization">Visualization</a></li></ul></li><li><a href="#failure-modes--mitigations">Failure Modes &amp; Mitigations</a></li><li><a href="#real-world-case-study-googles-efficientnet">Real-World Case Study: Google’s EfficientNet</a><ul><li><a href="#googles-nas-approach">Google’s NAS Approach</a></li><li><a href="#key-lessons">Key Lessons</a></li></ul></li><li><a href="#cost-analysis">Cost Analysis</a><ul><li><a href="#nas-vs-manual-design">NAS vs Manual Design</a></li></ul></li><li><a href="#advanced-topics">Advanced Topics</a><ul><li><a href="#1-once-for-all-networks">1. Once-For-All Networks</a></li><li><a href="#2-hardware-aware-nas">2. Hardware-Aware NAS</a></li><li><a href="#3-transfer-nas">3. Transfer NAS</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a><ul><li><a href="#connection-to-thematic-link-dynamic-programming-and-path-optimization">Connection to Thematic Link: Dynamic Programming and Path Optimization</a></li></ul></li></ul>
            </nav>
          </aside>
        
        <p><strong>Design neural architecture search systems that automatically discover optimal model architectures using dynamic programming and path optimization—the same principles from grid path counting scaled to exponential search spaces.</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p>Design a <strong>Neural Architecture Search (NAS) System</strong> that:</p>

<ol>
  <li><strong>Automatically discovers</strong> neural network architectures that outperform hand-designed models</li>
  <li><strong>Searches efficiently</strong> through exponentially large search spaces</li>
  <li><strong>Optimizes multiple objectives</strong> (accuracy, latency, memory, FLOPS)</li>
  <li><strong>Scales to production</strong> (finds models deployable on mobile/edge devices)</li>
  <li><strong>Supports different domains</strong> (vision, NLP, speech, multi-modal)</li>
</ol>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Search space definition:</strong>
    <ul>
      <li>Define architecture space (layers, operations, connections)</li>
      <li>Support modular search (cells, blocks, stages)</li>
      <li>Enable constrained search (max latency, max params)</li>
    </ul>
  </li>
  <li><strong>Search strategy:</strong>
    <ul>
      <li>Reinforcement learning (controller RNN)</li>
      <li>Evolutionary algorithms (mutation, crossover)</li>
      <li>Gradient-based (DARTS, differentiable NAS)</li>
      <li>Random search (baseline)</li>
      <li>Bayesian optimization</li>
    </ul>
  </li>
  <li><strong>Performance estimation:</strong>
    <ul>
      <li>Train architectures to evaluate quality</li>
      <li>Early stopping for bad candidates</li>
      <li>Weight sharing / one-shot models (ENAS, DARTS)</li>
      <li>Performance predictors (surrogate models)</li>
    </ul>
  </li>
  <li><strong>Multi-objective optimization:</strong>
    <ul>
      <li>Accuracy vs latency</li>
      <li>Accuracy vs model size</li>
      <li>Accuracy vs FLOPS</li>
      <li>Pareto frontier identification</li>
    </ul>
  </li>
  <li><strong>Distributed search:</strong>
    <ul>
      <li>Parallel architecture evaluation</li>
      <li>Distributed training of candidates</li>
      <li>Efficient resource allocation</li>
    </ul>
  </li>
  <li><strong>Transfer and reuse:</strong>
    <ul>
      <li>Transfer architectures across tasks</li>
      <li>Re-use components from previous searches</li>
      <li>Meta-learning for search initialization</li>
    </ul>
  </li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Efficiency:</strong> Find good architecture in &lt;100 GPU days (vs manual design months)</li>
  <li><strong>Quality:</strong> Discovered models competitive with hand-designed ones</li>
  <li><strong>Generalizability:</strong> Architectures transfer across datasets</li>
  <li><strong>Interpretability:</strong> Understand why architecture works</li>
  <li><strong>Reproducibility:</strong> Same search produces same results</li>
</ol>

<h2 id="understanding-the-requirements">Understanding the Requirements</h2>

<h3 id="why-nas">Why NAS?</h3>

<p><strong>Manual architecture design is:</strong></p>
<ul>
  <li>Time-consuming (months of expert effort)</li>
  <li>Limited by human intuition and expertise</li>
  <li>Hard to optimize for specific constraints (mobile latency, memory)</li>
  <li>Difficult to explore unconventional designs</li>
</ul>

<p><strong>NAS automates:</strong></p>
<ul>
  <li>Architecture discovery</li>
  <li>Multi-objective optimization</li>
  <li>Hardware-aware design</li>
  <li>Cross-domain transfer</li>
</ul>

<h3 id="scale-of-the-problem">Scale of the Problem</h3>

<p><strong>Search space size:</strong></p>
<ul>
  <li>A simple NAS space with 6 layers, 5 operations per layer: 5^6 = 15,625 architectures</li>
  <li>NASNet search space: ~10^18 possible architectures</li>
  <li>Without smart search, infeasible to evaluate all</li>
</ul>

<p><strong>Computational cost:</strong></p>
<ul>
  <li>Training one model: 1-10 GPU days</li>
  <li>Naive search (10K architectures): 10K-100K GPU days</li>
  <li>Smart search (NAS): 100-1000 GPU days</li>
  <li><strong>Goal:</strong> Reduce by 10-100x through efficient search</li>
</ul>

<h3 id="the-path-optimization-connection">The Path Optimization Connection</h3>

<p>Just like <strong>Unique Paths</strong> counts paths through a grid using DP:</p>

<table>
  <thead>
    <tr>
      <th>Unique Paths</th>
      <th>Neural Architecture Search</th>
      <th>Speech Arch Search</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>m×n grid</td>
      <td>Layer×operation space</td>
      <td>Encoder×decoder configs</td>
    </tr>
    <tr>
      <td>Count all paths</td>
      <td>Count/evaluate architectures</td>
      <td>Evaluate speech models</td>
    </tr>
    <tr>
      <td>DP optimization</td>
      <td>DP/RL search</td>
      <td>DP search</td>
    </tr>
    <tr>
      <td>O(m×n) vs O(2^(m+n))</td>
      <td>Smart search vs brute force</td>
      <td>Efficient vs exhaustive</td>
    </tr>
    <tr>
      <td>Path reconstruction</td>
      <td>Architecture construction</td>
      <td>Model construction</td>
    </tr>
  </tbody>
</table>

<p>Both use <strong>dynamic programming / smart search</strong> to navigate exponentially large spaces efficiently.</p>

<h2 id="high-level-architecture">High-Level Architecture</h2>

<p>``
┌─────────────────────────────────────────────────────────────────┐
│ Neural Architecture Search System │
└─────────────────────────────────────────────────────────────────┘</p>

<p>Search Controller
 ┌──────────────────────────────────┐
 │ Strategy: RL / EA / DARTS │
 │ - Propose architectures │
 │ - Update based on performance │
 └──────────────┬───────────────────┘
 │
 ↓
 ┌────────────────┐
 │ Search Space │
 │ - Layers │
 │ - Operations │
 │ - Connections │
 └────────┬───────┘
 │
 ┌──────────────────┼──────────────────┐
 │ │ │
┌───────▼────────┐ ┌──────▼──────┐ ┌────────▼────────┐
│ Architecture │ │ Performance │ │ Multi-obj │
│ Evaluator │ │ Predictor │ │ Optimizer │
│ │ │ │ │ │
│ - Train model │ │ - Surrogate │ │ - Pareto │
│ - Measure acc │ │ - Skip bad │ │ - Constraints │
│ - Measure lat │ │ candidates│ │ - Trade-offs │
└───────┬────────┘ └──────┬──────┘ └────────┬────────┘
 │ │ │
 └──────────────────┼──────────────────┘
 │
 ┌────────▼────────┐
 │ Distributed │
 │ Training │
 │ - Worker pool │
 │ - GPU cluster │
 └────────┬────────┘
 │
 ┌────────▼────────┐
 │ Results Store │
 │ - Architectures│
 │ - Metrics │
 │ - Models │
 └─────────────────┘
``</p>

<h3 id="key-components">Key Components</h3>

<ol>
  <li><strong>Search Controller:</strong> Proposes new architectures to try</li>
  <li><strong>Search Space:</strong> Defines valid architecture configurations</li>
  <li><strong>Architecture Evaluator:</strong> Trains and evaluates architectures</li>
  <li><strong>Performance Predictor:</strong> Estimates performance without full training</li>
  <li><strong>Multi-objective Optimizer:</strong> Balances accuracy, latency, size</li>
  <li><strong>Distributed Training:</strong> Parallel evaluation of architectures</li>
  <li><strong>Results Store:</strong> Tracks all evaluated architectures</li>
</ol>

<h2 id="component-deep-dives">Component Deep-Dives</h2>

<h3 id="1-search-space-definition">1. Search Space Definition</h3>

<p>Define what architectures are possible:</p>

<p>``python
from typing import List, Dict
from dataclasses import dataclass</p>

<p>@dataclass
class Operation:
 “"”A single operation in the search space.”””
 name: str
 params: Dict</p>

<p>@dataclass
class SearchSpace:
 “””
 NAS search space definition.</p>

<p>Similar to Unique Paths grid:</p>
<ul>
  <li>Grid dimensions → num_layers, ops_per_layer</li>
  <li>Paths through grid → architectures through search space
 “””
 num_layers: int
 operations: List[Operation]
 connections: str # “sequential”, “skip”, “dense”</li>
</ul>

<p>def count_architectures(self) -&gt; int:
 “””
 Count total possible architectures.</p>

<p>Like counting paths in Unique Paths:</p>
<ul>
  <li>If sequential: ops_per_layer ^ num_layers</li>
  <li>If with skip connections: much larger
 “””
 if self.connections == “sequential”:
 return len(self.operations) ** self.num_layers
 else:
 # With skip connections, combinatorially larger
 return -1 # Too many to count exactly</li>
</ul>

<h1 id="example-search-space">Example search space</h1>
<p>MOBILENET_SEARCH_SPACE = SearchSpace(
 num_layers=20,
 operations=[
 Operation(“conv3x3”, {“kernel_size”: 3, “stride”: 1}),
 Operation(“conv5x5”, {“kernel_size”: 5, “stride”: 1}),
 Operation(“depthwise_conv3x3”, {“kernel_size”: 3}),
 Operation(“depthwise_conv5x5”, {“kernel_size”: 5}),
 Operation(“maxpool3x3”, {“kernel_size”: 3, “stride”: 1}),
 Operation(“skip”, {}),
 ],
 connections=”skip”
)</p>

<p>def encode_architecture(arch_ops: List[str], search_space: SearchSpace) -&gt; str:
 “””
 Encode architecture as string.</p>

<p>Args:
 arch_ops: List of operation names per layer</p>

<p>Returns:
 String encoding (for hashing/caching)
 “””
 return “-“.join(arch_ops)</p>

<p>def decode_architecture(arch_string: str) -&gt; List[str]:
 “"”Decode architecture string to operation list.”””
 return arch_string.split(“-“)
``</p>

<h3 id="2-search-strategy---reinforcement-learning">2. Search Strategy - Reinforcement Learning</h3>

<p>Use RL controller to generate architectures:</p>

<p>``python
import torch
import torch.nn as nn</p>

<p>class NASController(nn.Module):
 “””
 RNN controller that generates architectures.</p>

<p>Similar to DP in Unique Paths:</p>
<ul>
  <li>Build architecture layer-by-layer</li>
  <li>Use previous decisions to inform next</li>
  <li>Optimize for high reward (accuracy)
 “””</li>
</ul>

<p>def <strong>init</strong>(
 self,
 num_layers: int,
 num_operations: int,
 hidden_size: int = 100
 ):
 super().<strong>init</strong>()</p>

<p>self.num_layers = num_layers
 self.num_operations = num_operations</p>

<p># RNN to track state across layers
 self.rnn = nn.LSTM(
 input_size=num_operations,
 hidden_size=hidden_size,
 num_layers=1
 )</p>

<p># Output layer: predict operation for next layer
 self.fc = nn.Linear(hidden_size, num_operations)</p>

<p>def forward(self):
 “””
 Generate an architecture.</p>

<p>Returns:
 architecture: List of operation indices
 log_probs: Log probabilities for REINFORCE
 “””
 architecture = []
 log_probs = []</p>

<p># Initial input
 inputs = torch.zeros(1, 1, self.num_operations)
 hidden = None</p>

<p># Generate layer-by-layer (like DP building solution)
 for layer in range(self.num_layers):
 # RNN step
 output, hidden = self.rnn(inputs, hidden)</p>

<p># Predict operation for this layer
 logits = self.fc(output.squeeze(0))
 probs = torch.softmax(logits, dim=-1)</p>

<p># Sample operation
 dist = torch.distributions.Categorical(probs)
 action = dist.sample()</p>

<p>architecture.append(action.item())
 log_probs.append(dist.log_prob(action))</p>

<p># Next input is one-hot of chosen operation
 inputs = torch.zeros(1, 1, self.num_operations)
 inputs[0, 0, action] = 1.0</p>

<p>return architecture, log_probs</p>

<p>def update(self, log_probs: List[torch.Tensor], reward: float, optimizer):
 “””
 Update controller using REINFORCE.</p>

<p>Args:
 log_probs: Log probabilities of sampled actions
 reward: Accuracy of generated architecture
 optimizer: Controller optimizer
 “””
 # REINFORCE loss: -sum(log_prob * reward)
 policy_loss = []
 for log_prob in log_probs:
 policy_loss.append(-log_prob * reward)</p>

<p>loss = torch.stack(policy_loss).sum()</p>

<p># Update controller
 optimizer.zero_grad()
 loss.backward()
 optimizer.step()</p>

<h1 id="training-loop">Training loop</h1>
<p>def train_nas_controller(
 controller: NASController,
 search_space: SearchSpace,
 num_iterations: int = 1000
):
 “””
 Train NAS controller to generate good architectures.
 “””
 optimizer = torch.optim.Adam(controller.parameters(), lr=0.001)</p>

<p>for iteration in range(num_iterations):
 # Generate architecture
 arch, log_probs = controller()</p>

<p># Evaluate architecture (train small model)
 reward = evaluate_architecture(arch, search_space)</p>

<p># Update controller
 controller.update(log_probs, reward, optimizer)</p>

<p>if iteration % 100 == 0:
 print(f”Iteration {iteration}: Best reward = {reward:.3f}”)
``</p>

<h3 id="3-search-strategy---differentiable-nas-darts">3. Search Strategy - Differentiable NAS (DARTS)</h3>

<p>DARTS makes architecture search differentiable:</p>

<p>``python
class DARTSSearchSpace(nn.Module):
 “””
 Differentiable architecture search.</p>

<p>Key idea: Instead of discrete choice, use weighted combination.
 Learn weights (architecture parameters) via gradient descent.
 “””</p>

<p>def <strong>init</strong>(self, num_layers: int, operations: List[nn.Module]):
 super().<strong>init</strong>()</p>

<p>self.num_layers = num_layers
 self.operations = nn.ModuleList(operations)</p>

<p># Architecture parameters (learnable weights for each operation)
 self.alpha = nn.Parameter(
 torch.randn(num_layers, len(operations))
 )</p>

<p>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
 “””
 Forward pass with weighted operations.</p>

<p>Each layer computes weighted sum of all operations.
 “””
 for layer in range(self.num_layers):
 # Get architecture weights for this layer
 weights = torch.softmax(self.alpha[layer], dim=0)</p>

<p># Compute weighted sum of operations
 layer_output = sum(
 w * op(x)
 for w, op in zip(weights, self.operations)
 )</p>

<p>x = layer_output</p>

<p>return x</p>

<p>def get_best_architecture(self) -&gt; List[int]:
 “””
 Extract discrete architecture from learned weights.</p>

<p>Choose operation with highest weight per layer.
 “””
 architecture = []</p>

<p>for layer in range(self.num_layers):
 weights = torch.softmax(self.alpha[layer], dim=0)
 best_op = torch.argmax(weights).item()
 architecture.append(best_op)</p>

<p>return architecture</p>

<h1 id="darts-training-bi-level-optimization">DARTS training (bi-level optimization)</h1>
<p>def train_darts(search_space: DARTSSearchSpace, train_data, val_data, epochs: int = 50):
 “””
 Train DARTS to find optimal architecture.</p>

<p>Bi-level optimization:</p>
<ul>
  <li>Inner loop: optimize model weights</li>
  <li>Outer loop: optimize architecture parameters
 “””
 # Model weights optimizer
 model_optimizer = torch.optim.SGD(
 search_space.parameters(),
 lr=0.025,
 momentum=0.9
 )</li>
</ul>

<p># Architecture parameters optimizer
 arch_optimizer = torch.optim.Adam(
 [search_space.alpha],
 lr=0.001
 )</p>

<p>for epoch in range(epochs):
 # Update model weights on train data
 for batch in train_data:
 model_optimizer.zero_grad()
 loss = compute_loss(search_space(batch[‘x’]), batch[‘y’])
 loss.backward()
 model_optimizer.step()</p>

<p># Update architecture parameters on val data
 for batch in val_data:
 arch_optimizer.zero_grad()
 loss = compute_loss(search_space(batch[‘x’]), batch[‘y’])
 loss.backward()
 arch_optimizer.step()</p>

<p># Extract final architecture
 best_arch = search_space.get_best_architecture()
 return best_arch
``</p>

<h3 id="4-performance-estimation-strategies">4. Performance Estimation Strategies</h3>

<p><strong>Problem:</strong> Training every architecture fully is too expensive.</p>

<p><strong>Solutions:</strong></p>

<p><strong>a) Early stopping:</strong></p>

<p>``python
def evaluate_with_early_stopping(
 arch: List[int],
 train_data,
 val_data,
 max_epochs: int = 50,
 patience: int = 5
):
 “””
 Train architecture with early stopping.</p>

<p>Stop if validation accuracy doesn’t improve for <code class="language-plaintext highlighter-rouge">patience</code> epochs.
 “””
 model = build_model_from_arch(arch)
 optimizer = torch.optim.Adam(model.parameters())</p>

<p>best_val_acc = 0.0
 epochs_without_improvement = 0</p>

<p>for epoch in range(max_epochs):
 # Train
 train_one_epoch(model, train_data, optimizer)</p>

<p># Validate
 val_acc = evaluate(model, val_data)</p>

<p>if val_acc &gt; best_val_acc:
 best_val_acc = val_acc
 epochs_without_improvement = 0
 else:
 epochs_without_improvement += 1</p>

<p># Early stop
 if epochs_without_improvement &gt;= patience:
 break</p>

<p>return best_val_acc
``</p>

<p><strong>b) Weight sharing (ENAS):</strong></p>

<p>``python
class SuperNet(nn.Module):
 “””
 Super-network containing all possible operations.</p>

<p>Different architectures share weights.
 Train once, evaluate many architectures quickly.
 “””</p>

<p>def <strong>init</strong>(self, search_space: SearchSpace):
 super().<strong>init</strong>()</p>

<p># Create all operations (shared across architectures)
 self.ops = nn.ModuleList([
 create_operation(op)
 for op in search_space.operations
 ])</p>

<p>def forward(self, x: torch.Tensor, architecture: List[int]) -&gt; torch.Tensor:
 “””
 Forward pass for specific architecture.</p>

<p>Args:
 x: Input
 architecture: List of operation indices per layer
 “””
 for layer, op_idx in enumerate(architecture):
 x = self.ops<a href="x">op_idx</a></p>

<p>return x</p>

<p>def evaluate_architecture(self, arch: List[int], val_data) -&gt; float:
 “””
 Evaluate architecture without training.</p>

<p>Uses shared weights - much faster than training from scratch.
 “””
 self.eval()
 total_correct = 0
 total_samples = 0</p>

<p>with torch.no_grad():
 for batch in val_data:
 outputs = self.forward(batch[‘x’], arch)
 preds = outputs.argmax(dim=1)
 total_correct += (preds == batch[‘y’]).sum().item()
 total_samples += len(batch[‘y’])</p>

<p>return total_correct / total_samples
``</p>

<p><strong>c) Performance prediction:</strong></p>

<p>``python
from sklearn.ensemble import RandomForestRegressor</p>

<p>class PerformancePredictor:
 “””
 Predict architecture performance without training.</p>

<p>Train a surrogate model: architecture features → accuracy.
 “””</p>

<p>def <strong>init</strong>(self):
 self.model = RandomForestRegressor(n_estimators=100)
 self.trained = False</p>

<p>def extract_features(self, arch: List[int]) -&gt; np.ndarray:
 “””
 Extract features from architecture.</p>

<p>Features:</p>
<ul>
  <li>Number of each operation type</li>
  <li>Depth (number of layers)</li>
  <li>Estimated FLOPs</li>
  <li>Estimated params
 “””
 features = []</li>
</ul>

<p># Count each operation type
 from collections import Counter
 op_counts = Counter(arch)
 for op_idx in range(max(arch) + 1):
 features.append(op_counts.get(op_idx, 0))</p>

<p># Add depth
 features.append(len(arch))</p>

<p>return np.array(features)</p>

<p>def train(self, architectures: List[List[int]], accuracies: List[float]):
 “"”Train predictor on evaluated architectures.”””
 X = np.array([self.extract_features(arch) for arch in architectures])
 y = np.array(accuracies)</p>

<p>self.model.fit(X, y)
 self.trained = True</p>

<p>def predict(self, arch: List[int]) -&gt; float:
 “"”Predict accuracy for architecture.”””
 if not self.trained:
 raise ValueError(“Predictor not trained”)</p>

<p>features = self.extract_features(arch).reshape(1, -1)
 return self.model.predict(features)[0]
``</p>

<h2 id="data-flow">Data Flow</h2>

<h3 id="nas-pipeline">NAS Pipeline</h3>

<p>``</p>
<ol>
  <li>
    <p>Initialize search
 └─&gt; Define search space
 └─&gt; Initialize controller (RL/EA/DARTS)
 └─&gt; Set up distributed workers</p>
  </li>
  <li>Search loop (1000-10000 iterations)
 └─&gt; Controller proposes architecture
 └─&gt; (Optional) Performance predictor filters bad candidates
 └─&gt; Evaluate architecture:
    <ul>
      <li>Train on subset of data</li>
      <li>Measure accuracy, latency, size
 └─&gt; Update controller based on reward
 └─&gt; Store results</li>
    </ul>
  </li>
  <li>
    <p>Post-processing
 └─&gt; Identify Pareto frontier (best accuracy-latency trade-offs)
 └─&gt; Retrain top candidates on full data
 └─&gt; Final evaluation on test set</p>
  </li>
  <li>Deployment
 └─&gt; Export best architecture
 └─&gt; Optimize for target hardware
 └─&gt; Deploy to production
``</li>
</ol>

<h2 id="scaling-strategies">Scaling Strategies</h2>

<h3 id="distributed-architecture-evaluation">Distributed Architecture Evaluation</h3>

<p>``python
import ray</p>

<p>@ray.remote(num_gpus=1)
class ArchitectureWorker:
 “"”Worker that evaluates architectures on GPU.”””</p>

<p>def <strong>init</strong>(self, search_space: SearchSpace):
 self.search_space = search_space</p>

<p>def evaluate(self, arch: List[int], train_subset, val_subset) -&gt; Dict:
 “””
 Evaluate architecture.</p>

<p>Returns:
 Dictionary with accuracy, latency, params, flops
 “””
 # Build model
 model = build_model_from_arch(arch, self.search_space)</p>

<p># Train briefly
 train_model(model, train_subset, epochs=10)</p>

<p># Evaluate
 accuracy = evaluate(model, val_subset)
 latency = measure_latency(model)
 params = count_parameters(model)
 flops = estimate_flops(model)</p>

<p>return {
 “accuracy”: accuracy,
 “latency_ms”: latency,
 “params”: params,
 “flops”: flops
 }</p>

<p>class DistributedNAS:
 “"”Distributed NAS system.”””</p>

<p>def <strong>init</strong>(self, search_space: SearchSpace, num_workers: int = 8):
 self.search_space = search_space</p>

<p># Create worker pool
 self.workers = [
 ArchitectureWorker.remote(search_space)
 for _ in range(num_workers)
 ]</p>

<p>self.num_workers = num_workers</p>

<p>def search(self, controller: NASController, num_iterations: int = 1000):
 “””
 Distributed NAS search.</p>

<p>Args:
 controller: Architecture generator
 num_iterations: Number of architectures to try
 “””
 results = []</p>

<p># Process in batches (parallel evaluation)
 batch_size = self.num_workers</p>

<p>for iteration in range(0, num_iterations, batch_size):
 # Generate batch of architectures
 architectures = []
 log_probs_batch = []</p>

<p>for _ in range(batch_size):
 arch, log_probs = controller()
 architectures.append(arch)
 log_probs_batch.append(log_probs)</p>

<p># Evaluate in parallel
 futures = [
 self.workers[i % self.num_workers].evaluate.remote(
 architectures[i],
 get_train_subset(),
 get_val_subset()
 )
 for i in range(batch_size)
 ]</p>

<p>eval_results = ray.get(futures)</p>

<p># Update controller with rewards
 for arch, log_probs, result in zip(architectures, log_probs_batch, eval_results):
 reward = result[‘accuracy’]
 controller.update(log_probs, reward, controller_optimizer)
 results.append((arch, result))</p>

<p>return results
``</p>

<h2 id="monitoring--metrics">Monitoring &amp; Metrics</h2>

<h3 id="key-metrics">Key Metrics</h3>

<p><strong>Search Progress:</strong></p>
<ul>
  <li>Best accuracy found so far</li>
  <li>Number of architectures evaluated</li>
  <li>Search efficiency (good arch per GPU day)</li>
  <li>Diversity of architectures explored</li>
</ul>

<p><strong>Architecture Quality:</strong></p>
<ul>
  <li>Accuracy vs latency scatter plot</li>
  <li>Pareto frontier (optimal trade-offs)</li>
  <li>Architecture complexity distribution (params, FLOPs)</li>
</ul>

<p><strong>Resource Usage:</strong></p>
<ul>
  <li>GPU utilization</li>
  <li>Training time per architecture</li>
  <li>Total GPU hours consumed</li>
</ul>

<h3 id="visualization">Visualization</h3>

<ul>
  <li>Architecture topology graphs</li>
  <li>Performance over search iterations</li>
  <li>Pareto frontier (accuracy vs latency/size)</li>
  <li>Operation frequency (which ops are most common in good models)</li>
</ul>

<h2 id="failure-modes--mitigations">Failure Modes &amp; Mitigations</h2>

<table>
  <thead>
    <tr>
      <th>Failure Mode</th>
      <th>Impact</th>
      <th>Mitigation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Search collapse</strong></td>
      <td>Controller generates same arch repeatedly</td>
      <td>Entropy regularization, exploration bonus</td>
    </tr>
    <tr>
      <td><strong>Overfitting to search</strong></td>
      <td>Arch good on val, bad on test</td>
      <td>Proper val/test splits, cross-validation</td>
    </tr>
    <tr>
      <td><strong>Poor weight sharing</strong></td>
      <td>ENAS/supernet gives misleading results</td>
      <td>Standalone training for top candidates</td>
    </tr>
    <tr>
      <td><strong>Hardware mismatch</strong></td>
      <td>Arch fast on A100, slow on mobile</td>
      <td>Include target hardware in eval</td>
    </tr>
    <tr>
      <td><strong>Expensive search</strong></td>
      <td>1000s of GPU days</td>
      <td>Early stopping, predictor, weight sharing</td>
    </tr>
  </tbody>
</table>

<h2 id="real-world-case-study-googles-efficientnet">Real-World Case Study: Google’s EfficientNet</h2>

<h3 id="googles-nas-approach">Google’s NAS Approach</h3>

<p><strong>Goal:</strong> Find architectures that are both accurate and efficient (mobile-friendly).</p>

<p><strong>Method:</strong></p>
<ul>
  <li>Multi-objective NAS optimizing accuracy and FLOPS</li>
  <li>Compound scaling (depth, width, resolution)</li>
  <li>Progressive search (coarse → fine)</li>
</ul>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Search space: MobileNetV2-based</li>
  <li>Search strategy: Reinforcement learning</li>
  <li>Evaluation: Early stopping + supernet</li>
  <li>Scale: 1000 architectures evaluated, 100 GPU days</li>
</ul>

<p><strong>Results:</strong></p>
<ul>
  <li><strong>EfficientNet-B0:</strong> 77.1% top-1 on ImageNet, 390M FLOPs</li>
  <li><strong>10x more efficient</strong> than previous SOTA (at same accuracy)</li>
  <li><strong>Transfer learning:</strong> Worked across domains (detection, segmentation)</li>
</ul>

<h3 id="key-lessons">Key Lessons</h3>

<ol>
  <li><strong>Multi-objective is crucial:</strong> Accuracy alone isn’t enough</li>
  <li><strong>Progressive search:</strong> Start coarse, refine best candidates</li>
  <li><strong>Transfer across tasks:</strong> Good architecture for ImageNet → good for other vision tasks</li>
  <li><strong>Hardware-aware:</strong> Include latency/FLOPS in objective</li>
  <li><strong>Compound scaling:</strong> After finding base arch, scale systematically</li>
</ol>

<h2 id="cost-analysis">Cost Analysis</h2>

<h3 id="nas-vs-manual-design">NAS vs Manual Design</h3>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Time</th>
      <th>GPU Cost</th>
      <th>Quality</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Manual design</td>
      <td>3-6 months</td>
      <td>100 GPU days</td>
      <td>Good</td>
      <td>Expert-dependent</td>
    </tr>
    <tr>
      <td>Random search</td>
      <td>N/A</td>
      <td>1000 GPU days</td>
      <td>Poor</td>
      <td>Baseline</td>
    </tr>
    <tr>
      <td>RL-based NAS</td>
      <td>1 month</td>
      <td>200 GPU days</td>
      <td>Better</td>
      <td>EfficientNet-style</td>
    </tr>
    <tr>
      <td>DARTS</td>
      <td>1 week</td>
      <td>4 GPU days</td>
      <td>Good</td>
      <td>Fast but less stable</td>
    </tr>
    <tr>
      <td>Transfer + fine-tune</td>
      <td>1 week</td>
      <td>10 GPU days</td>
      <td>Good</td>
      <td>Use existing NAS results</td>
    </tr>
  </tbody>
</table>

<p><strong>ROI Calculation:</strong></p>
<ul>
  <li>Manual design: 3 months engineer time (<code class="language-plaintext highlighter-rouge">60K) + 100 GPU days (</code>30K) = $90K</li>
  <li>NAS: 1 month engineer time (<code class="language-plaintext highlighter-rouge">20K) + 200 GPU days (</code>60K) = $80K</li>
  <li><strong>Savings:</strong> $10K + better model</li>
</ul>

<h2 id="advanced-topics">Advanced Topics</h2>

<h3 id="1-once-for-all-networks">1. Once-For-All Networks</h3>

<p>Train a single super-network that contains many sub-networks:</p>

<p>``python
class OnceForAllNetwork:
 “””
 Train once, deploy many architectures.</p>

<p>Enables instant architecture selection without retraining.
 “””</p>

<p>def <strong>init</strong>(self):
 self.supernet = create_supernet()
 self.trained = False</p>

<p>def train_supernet(self, train_data):
 “””
 Train supernet to support all sub-architectures.</p>

<p>Progressive shrinking strategy:</p>
<ul>
  <li>Train largest network first</li>
  <li>Progressively train smaller sub-networks
 “””
 # Implementation details…
 pass</li>
</ul>

<p>def extract_subnet(self, target_latency_ms: float):
 “””
 Extract sub-network meeting latency constraint.</p>

<p>No training needed!
 “””
 # Search for subnet with latency &lt; target
 # Use efficiency predictor
 pass
``</p>

<h3 id="2-hardware-aware-nas">2. Hardware-Aware NAS</h3>

<p>Include hardware metrics in search:</p>

<p>``python
def hardware_aware_nas(search_space, target_hardware: str):
 “””
 Search for architectures optimized for specific hardware.</p>

<p>Args:
 target_hardware: “mobile”, “edge_tpu”, “nvidia_t4”, etc.
 “””
 # Measure latency on target hardware
 def measure_latency_on_target(arch):
 model = build_model(arch)
 # Deploy to target, measure
 return measure_on_hardware(model, target_hardware)</p>

<p># Multi-objective: accuracy + latency on target
 def fitness(arch):
 acc = evaluate_accuracy(arch)
 lat = measure_latency_on_target(arch)</p>

<p># Combine (accuracy high, latency low)
 return acc - 0.01 * lat # Weight latency penalty
``</p>

<h3 id="3-transfer-nas">3. Transfer NAS</h3>

<p>Transfer architectures across tasks:</p>

<p>``python
def transfer_nas(source_task: str, target_task: str):
 “””
 Transfer NAS results from source to target task.</p>

<p>Example: ImageNet → COCO detection
 “””
 # Load architectures found on source task
 source_archs = load_nas_results(source_task)</p>

<p># Top-K from source
 top_archs = sorted(source_archs, key=lambda x: x[‘accuracy’], reverse=True)[:10]</p>

<p># Fine-tune on target task
 target_results = []
 for arch in top_archs:
 # Build model with source architecture
 model = build_model_from_arch(arch[‘architecture’])</p>

<p># Fine-tune on target task
 fine_tune(model, target_task_data)</p>

<p># Evaluate
 target_acc = evaluate(model, target_task_test_data)
 target_results.append((arch, target_acc))</p>

<p># Best transferred architecture
 best = max(target_results, key=lambda x: x[1])
 return best
``</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>NAS automates architecture design</strong> - discovers models competitive with or better than hand-designed ones</p>

<p>✅ <strong>Search space is exponential</strong> - like paths in a grid, exponentially many architectures</p>

<p>✅ <strong>DP and smart search</strong> reduce complexity - from infeasible to practical</p>

<p>✅ <strong>Multiple search strategies</strong> - RL (flexible), DARTS (fast), evolutionary (robust)</p>

<p>✅ <strong>Weight sharing critical</strong> - enables evaluating 1000s of architectures efficiently</p>

<p>✅ <strong>Multi-objective optimization</strong> - accuracy vs latency vs size</p>

<p>✅ <strong>Hardware-aware NAS</strong> - optimize for target deployment platform</p>

<p>✅ <strong>Transfer learning works</strong> - architectures transfer across tasks</p>

<p>✅ <strong>Same DP principles</strong> as Unique Paths - break into subproblems, build optimal solution</p>

<p>✅ <strong>Production deployment</strong> - once-for-all networks, progressive search, cost-aware</p>

<h3 id="connection-to-thematic-link-dynamic-programming-and-path-optimization">Connection to Thematic Link: Dynamic Programming and Path Optimization</h3>

<p>All three topics use <strong>DP for path optimization</strong>:</p>

<p><strong>DSA (Unique Paths):</strong></p>
<ul>
  <li>Count paths in m×n grid using DP</li>
  <li>Recurrence: paths(i,j) = paths(i-1,j) + paths(i,j-1)</li>
  <li>Reduces O(2^(m+n)) to O(m×n)</li>
</ul>

<p><strong>ML System Design (Neural Architecture Search):</strong></p>
<ul>
  <li>Search through exponential architecture space</li>
  <li>Use DP/RL/gradient-based methods to find optimal</li>
  <li>Build architectures from optimal sub-architectures</li>
</ul>

<p><strong>Speech Tech (Speech Architecture Search):</strong></p>
<ul>
  <li>Search encoder/decoder configurations</li>
  <li>Use DP to evaluate speech model paths</li>
  <li>Find optimal ASR/TTS architectures</li>
</ul>

<p>The <strong>unifying principle</strong>: navigate exponentially large search spaces by breaking into subproblems, solving optimally, and building up the final solution—whether counting grid paths, finding neural architectures, or designing speech models.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0021-neural-architecture-search/">arunbaby.com/ml-system-design/0021-neural-architecture-search</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#automl" class="page__taxonomy-item p-category" rel="tag">automl</a><span class="sep">, </span>
    
      <a href="/tags/#model-design" class="page__taxonomy-item p-category" rel="tag">model-design</a><span class="sep">, </span>
    
      <a href="/tags/#nas" class="page__taxonomy-item p-category" rel="tag">nas</a><span class="sep">, </span>
    
      <a href="/tags/#neural-architecture-search" class="page__taxonomy-item p-category" rel="tag">neural-architecture-search</a><span class="sep">, </span>
    
      <a href="/tags/#optimization" class="page__taxonomy-item p-category" rel="tag">optimization</a><span class="sep">, </span>
    
      <a href="/tags/#reinforcement-learning" class="page__taxonomy-item p-category" rel="tag">reinforcement-learning</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0021-unique-paths/" rel="permalink">Unique Paths
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master grid path counting with dynamic programming—the same optimization technique used in neural architecture search and speech model design.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0021-speech-architecture-search/" rel="permalink">Speech Architecture Search
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Design neural architecture search systems for speech models that automatically discover optimal ASR/TTS architectures—using dynamic programming and path opti...</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0021-vision-agent-fundamentals/" rel="permalink">Vision Agent Fundamentals
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Giving eyes to the brain: How Agents see the world.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Neural+Architecture+Search%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0021-neural-architecture-search%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0021-neural-architecture-search%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0021-neural-architecture-search/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0020-online-learning-systems/" class="pagination--pager" title="Online Learning Systems">Previous</a>
    
    
      <a href="/ml-system-design/0022-cost-optimization-for-ml/" class="pagination--pager" title="Cost Optimization for ML">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
