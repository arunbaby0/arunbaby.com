<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Recommendation System: Candidate Retrieval - Arun Baby</title>
<meta name="description" content="How do you narrow down 10 million items to 1000 candidates in under 50ms? The art of fast retrieval at scale.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Recommendation System: Candidate Retrieval">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0001-recommendation-system/">


  <meta property="og:description" content="How do you narrow down 10 million items to 1000 candidates in under 50ms? The art of fast retrieval at scale.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Recommendation System: Candidate Retrieval">
  <meta name="twitter:description" content="How do you narrow down 10 million items to 1000 candidates in under 50ms? The art of fast retrieval at scale.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0001-recommendation-system/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0001-recommendation-system/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Recommendation System: Candidate Retrieval">
    <meta itemprop="description" content="How do you narrow down 10 million items to 1000 candidates in under 50ms? The art of fast retrieval at scale.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0001-recommendation-system/" itemprop="url">Recommendation System: Candidate Retrieval
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          29 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#problem-definition">Problem Definition</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li><li><a href="#out-of-scope-clarify-these">Out of Scope (Clarify These)</a></li></ul></li><li><a href="#high-level-architecture">High-Level Architecture</a><ul><li><a href="#component-architecture">Component Architecture</a></li></ul></li><li><a href="#core-component-1-user-and-item-embeddings">Core Component 1: User and Item Embeddings</a><ul><li><a href="#what-are-embeddings">What are Embeddings?</a></li><li><a href="#two-tower-architecture">Two-Tower Architecture</a></li><li><a href="#training-the-model">Training the Model</a></li><li><a href="#why-two-tower-works">Why Two-Tower Works</a></li></ul></li><li><a href="#core-component-2-approximate-nearest-neighbor-ann-search">Core Component 2: Approximate Nearest Neighbor (ANN) Search</a><ul><li><a href="#the-problem">The Problem</a></li><li><a href="#approximate-nearest-neighbor-ann">Approximate Nearest Neighbor (ANN)</a></li><li><a href="#hnsw-hierarchical-navigable-small-world">HNSW (Hierarchical Navigable Small World)</a></li><li><a href="#parameter-tuning">Parameter Tuning</a></li><li><a href="#alternative-product-quantization">Alternative: Product Quantization</a></li></ul></li><li><a href="#core-component-3-multiple-retrieval-strategies">Core Component 3: Multiple Retrieval Strategies</a><ul><li><a href="#strategy-1-collaborative-filtering-40-of-candidates">Strategy 1: Collaborative Filtering (40% of candidates)</a></li><li><a href="#strategy-2-content-based-filtering-30-of-candidates">Strategy 2: Content-Based Filtering (30% of candidates)</a></li><li><a href="#strategy-3-trending-20-of-candidates">Strategy 3: Trending (20% of candidates)</a></li><li><a href="#strategy-4-social-10-of-candidates">Strategy 4: Social (10% of candidates)</a></li><li><a href="#merging-strategies">Merging Strategies</a></li></ul></li><li><a href="#core-component-4-caching-strategy">Core Component 4: Caching Strategy</a><ul><li><a href="#three-level-cache-architecture">Three-Level Cache Architecture</a></li><li><a href="#implementation">Implementation</a></li><li><a href="#cache-warming-strategy">Cache Warming Strategy</a></li><li><a href="#cache-invalidation">Cache Invalidation</a></li><li><a href="#cache-hit-rate-monitoring">Cache Hit Rate Monitoring</a></li></ul></li><li><a href="#handling-cold-start">Handling Cold Start</a><ul><li><a href="#new-user-problem">New User Problem</a></li><li><a href="#new-item-problem">New Item Problem</a></li></ul></li><li><a href="#real-world-examples">Real-World Examples</a><ul><li><a href="#youtube-recommendations">YouTube Recommendations</a></li><li><a href="#pinterest-pinsage">Pinterest (PinSage)</a></li><li><a href="#spotify-recommendations">Spotify Recommendations</a></li></ul></li><li><a href="#monitoring-and-evaluation">Monitoring and Evaluation</a><ul><li><a href="#online-metrics">Online Metrics</a></li><li><a href="#offline-metrics">Offline Metrics</a></li><li><a href="#ab-testing">A/B Testing</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li><li><a href="#further-reading">Further Reading</a></li><li><a href="#conclusion">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>How do you narrow down 10 million items to 1000 candidates in under 50ms? The art of fast retrieval at scale.</strong></p>

<h2 id="introduction">Introduction</h2>

<p>Every day, you interact with recommendation systems dozens of times: YouTube suggests videos, Netflix recommends shows, Amazon suggests products, Spotify curates playlists, and Instagram fills your feed. Behind each recommendation is a sophisticated system that must:</p>

<ul>
  <li>Search through <strong>millions</strong> of items in <strong>milliseconds</strong></li>
  <li>Personalize results for <strong>hundreds of millions</strong> of users</li>
  <li>Balance relevance, diversity, and freshness</li>
  <li>Handle new users and new content gracefully</li>
  <li>Scale horizontally to serve billions of requests per day</li>
</ul>

<p>The naive approach computing scores for all items for each user is mathematically impossible at scale. If we have 100M users and 10M items, that’s 1 quadrillion (10^15) combinations to score. Even at 1 billion computations per second, this would take <strong>11+ days per request</strong>.</p>

<p>This post focuses on the <strong>candidate generation</strong> (or retrieval) stage: how we efficiently narrow millions of items down to hundreds of candidates that might interest a user. This is the first and most critical stage of any recommendation system, as it determines the maximum possible quality of recommendations while constraining latency and cost.</p>

<p><strong>What you’ll learn:</strong></p>
<ul>
  <li>Why most recommendation systems use a funnel architecture</li>
  <li>How embedding-based retrieval enables personalization at scale</li>
  <li>Approximate nearest neighbor (ANN) search algorithms</li>
  <li>Multiple retrieval strategies and how to combine them</li>
  <li>Caching patterns for sub-50ms latency</li>
  <li>Cold start problem solutions</li>
  <li>Real production architectures from YouTube, Pinterest, and Spotify</li>
</ul>

<hr />

<h2 id="problem-definition">Problem Definition</h2>

<p>Design the <strong>candidate generation stage</strong> of a recommendation system that:</p>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Personalized Retrieval</strong>
    <ul>
      <li>Different candidates for each user based on their preferences</li>
      <li>Not just “popular items for everyone”</li>
      <li>Must capture user’s interests, behavior patterns, and context</li>
    </ul>
  </li>
  <li><strong>Multiple Retrieval Strategies</strong>
    <ul>
      <li>Collaborative filtering (users with similar taste)</li>
      <li>Content-based filtering (items similar to what user liked)</li>
      <li>Trending/popular items (what’s hot right now)</li>
      <li>Social signals (what friends are engaging with)</li>
    </ul>
  </li>
  <li><strong>Diversity</strong>
    <ul>
      <li>Avoid filter bubbles (all items too similar)</li>
      <li>Show variety of content types, topics, creators</li>
      <li>Enable exploration (help users discover new interests)</li>
    </ul>
  </li>
  <li><strong>Freshness</strong>
    <ul>
      <li>New items should appear within minutes of publication</li>
      <li>System should adapt to changing user interests</li>
      <li>Handle trending topics and viral content</li>
    </ul>
  </li>
  <li><strong>Cold Start Handling</strong>
    <ul>
      <li>New users with no history</li>
      <li>New items with no engagement data</li>
      <li>Graceful degradation when data is sparse</li>
    </ul>
  </li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Latency</strong>
    <ul>
      <li>p50 &lt; 20ms (median request)</li>
      <li>p95 &lt; 40ms (95th percentile)</li>
      <li>p99 &lt; 50ms (99th percentile)</li>
      <li>Why so strict? Candidate generation is just one stage; ranking, re-ranking, and other processing add more latency</li>
    </ul>
  </li>
  <li><strong>Throughput</strong>
    <ul>
      <li>100M daily active users</li>
      <li>Assume 100 requests per user per day (feed refreshes, scrolls)</li>
      <li>10 billion requests per day</li>
      <li>~115k QPS average, ~500k QPS peak</li>
    </ul>
  </li>
  <li><strong>Scale</strong>
    <ul>
      <li>100M+ active users</li>
      <li>10M+ active items (videos, posts, products)</li>
      <li>Billions of historical interactions</li>
      <li>Petabytes of training data</li>
    </ul>
  </li>
  <li><strong>Availability</strong>
    <ul>
      <li>99.9% uptime (43 minutes downtime per month)</li>
      <li>Graceful degradation when components fail</li>
      <li>No single points of failure</li>
    </ul>
  </li>
  <li><strong>Cost Efficiency</strong>
    <ul>
      <li>Minimize compute costs (GPU/CPU)</li>
      <li>Optimize storage (embeddings, features)</li>
      <li>Reduce data transfer (network bandwidth)</li>
    </ul>
  </li>
</ol>

<h3 id="out-of-scope-clarify-these">Out of Scope (Clarify These)</h3>

<ul>
  <li>Ranking stage (scoring the 1000 candidates to get top 20)</li>
  <li>Re-ranking and diversity post-processing</li>
  <li>A/B testing infrastructure</li>
  <li>Training pipeline and data collection</li>
  <li>Content moderation and safety</li>
  <li>Business logic (e.g., promoted content, ads)</li>
</ul>

<hr />

<h2 id="high-level-architecture">High-Level Architecture</h2>

<p>The recommendation system follows a <strong>funnel architecture</strong>:</p>

<p><code class="language-plaintext highlighter-rouge">
10M Items
 ↓ Candidate Generation (This Post)
1000 Candidates
 ↓ Ranking (Lightweight Model)
100 Candidates
 ↓ Re-ranking (Heavy Model + Business Logic)
20 Final Results
</code></p>

<p><strong>Why a funnel?</strong></p>
<ul>
  <li><strong>Cannot score all items:</strong> 10M items × 50ms per item = 5.8 days per request</li>
  <li><strong>Quality vs. Speed tradeoff:</strong> Fast approximate methods first, expensive accurate methods last</li>
  <li><strong>Resource optimization:</strong> Apply expensive computations only to promising candidates</li>
</ul>

<p>Our focus: <strong>10M → 1000 in &lt; 50ms</strong></p>

<h3 id="component-architecture">Component Architecture</h3>

<p><code class="language-plaintext highlighter-rouge">
User Request
 ├─ user_id: 12345
 ├─ context: {device: mobile, time: evening, location: US-CA}
 └─ num_candidates: 1000
 ↓
┌─────────────────────────────────────────────┐
│ Feature Lookup (5ms) │
│ • User Embedding (Redis) │
│ • User Profile (Cassandra) │
│ • Recent Activity (Redis Stream) │
└──────────────┬──────────────────────────────┘
 ↓
┌─────────────────────────────────────────────┐
│ Retrieval Strategies (Parallel, 30ms) │
│ ┌────────────────┐ ┌──────────────────┐ │
│ │ Collaborative │ │ Content-Based │ │
│ │ Filtering │ │ Filtering │ │
│ │ (ANN Search) │ │ (Tag Matching) │ │
│ │ 400 items │ │ 300 items │ │
│ └────────────────┘ └──────────────────┘ │
│ ┌────────────────┐ ┌──────────────────┐ │
│ │ Trending │ │ Social │ │
│ │ (Sorted) │ │ (Friends' Feed) │ │
│ │ 200 items │ │ 100 items │ │
│ └────────────────┘ └──────────────────┘ │
└──────────────┬──────────────────────────────┘
 ↓
┌─────────────────────────────────────────────┐
│ Merge &amp; Deduplicate (5ms) │
│ • Combine all sources │
│ • Remove duplicates │
│ • Basic filtering (already seen, blocked) │
└──────────────┬──────────────────────────────┘
 ↓
 Return ~1000 candidates
</code></p>

<p><strong>Latency Budget (50ms total):</strong>
<code class="language-plaintext highlighter-rouge">
Feature lookup: 5ms
Retrieval (parallel): 30ms
Merge/dedup: 5ms
Network overhead: 10ms
Total: 50ms ✓
</code></p>

<hr />

<h2 id="core-component-1-user-and-item-embeddings">Core Component 1: User and Item Embeddings</h2>

<h3 id="what-are-embeddings">What are Embeddings?</h3>

<p><strong>Embeddings</strong> are dense vector representations that capture semantic meaning in a continuous space.</p>

<p><strong>Example:</strong>
``python</p>
<h1 id="user-embedding-128-dimensions">User embedding (128 dimensions)</h1>
<p>user_12345 = [0.23, -0.45, 0.67, …, 0.12] # 128 numbers</p>

<h1 id="item-embeddings">Item embeddings</h1>
<p>item_5678 = [0.19, -0.41, 0.72, …, 0.15] # Similar to user!
item_9999 = [-0.78, 0.92, -0.34, …, -0.88] # Very different</p>

<h1 id="similarity--dot-product">Similarity = dot product</h1>
<p>similarity = sum(u * i for u, i in zip(user_12345, item_5678))</p>
<h1 id="high-similarity--good-recommendation">High similarity → good recommendation!</h1>
<p>``</p>

<p><strong>Why embeddings work:</strong></p>
<ul>
  <li><strong>Semantic similarity:</strong> Similar users/items have similar vectors</li>
  <li><strong>Efficient computation:</strong> Dot product is fast (O(d) for d dimensions)</li>
  <li><strong>Learned representations:</strong> Neural networks learn meaningful patterns</li>
  <li><strong>Dense vs. sparse:</strong> 128 floats vs. millions of categorical features</li>
</ul>

<h3 id="two-tower-architecture">Two-Tower Architecture</h3>

<p>The most common architecture for retrieval is the <strong>two-tower model</strong>:</p>

<p><code class="language-plaintext highlighter-rouge">
User Features Item Features
 ├─ Demographics ├─ Title/Description
 ├─ Historical Behavior ├─ Category/Tags
 ├─ Recent Activity ├─ Creator Info
 └─ Context └─ Metadata
 ↓ ↓
 ┌─────────┐ ┌─────────┐
 │ User │ │ Item │
 │ Tower │ │ Tower │
 │ (NN) │ │ (NN) │
 └────┬────┘ └────┬────┘
 │ │
 └───────────┬───────────┘
 ↓
 Dot Product
 ↓
 Similarity Score
</code></p>

<p><strong>Implementation:</strong></p>

<p>``python
import torch
import torch.nn as nn</p>

<p>class TwoTowerModel(nn.Module):
 def <strong>init</strong>(self, user_feature_dim=100, item_feature_dim=80, embedding_dim=128):
 super().<strong>init</strong>()</p>

<p># User tower: transform user features to embedding
 self.user_tower = nn.Sequential(
 nn.Linear(user_feature_dim, 256),
 nn.ReLU(),
 nn.Dropout(0.2),
 nn.Linear(256, 256),
 nn.ReLU(),
 nn.Dropout(0.2),
 nn.Linear(256, embedding_dim)
 )</p>

<p># Item tower: transform item features to embedding
 self.item_tower = nn.Sequential(
 nn.Linear(item_feature_dim, 256),
 nn.ReLU(),
 nn.Dropout(0.2),
 nn.Linear(256, 256),
 nn.ReLU(),
 nn.Dropout(0.2),
 nn.Linear(256, embedding_dim)
 )</p>

<p># L2 normalization layer
 self.normalize = lambda x: x / (torch.norm(x, dim=1, keepdim=True) + 1e-6)</p>

<p>def forward(self, user_features, item_features):
 # Generate embeddings
 user_emb = self.user_tower(user_features) # (batch, 128)
 item_emb = self.item_tower(item_features) # (batch, 128)</p>

<p># Normalize to unit vectors (cosine similarity = dot product)
 user_emb = self.normalize(user_emb)
 item_emb = self.normalize(item_emb)</p>

<p># Compute similarity (dot product)
 score = (user_emb * item_emb).sum(dim=1) # (batch,)</p>

<p>return score, user_emb, item_emb</p>

<p>def get_user_embedding(self, user_features):
 “"”Get just the user embedding (for serving)”””
 with torch.no_grad():
 user_emb = self.user_tower(user_features)
 user_emb = self.normalize(user_emb)
 return user_emb</p>

<p>def get_item_embedding(self, item_features):
 “"”Get just the item embedding (for indexing)”””
 with torch.no_grad():
 item_emb = self.item_tower(item_features)
 item_emb = self.normalize(item_emb)
 return item_emb
``</p>

<h3 id="training-the-model">Training the Model</h3>

<p><strong>Training Data:</strong></p>
<ul>
  <li><strong>Positive examples:</strong> (user, item) pairs where user engaged with item (click, watch, purchase)</li>
  <li><strong>Negative examples:</strong> (user, item) pairs where user didn’t engage</li>
</ul>

<p><strong>Loss Function:</strong></p>

<p>``python
def contrastive_loss(positive_scores, negative_scores, margin=0.5):
 “””
 Encourage positive pairs to have high scores,
 negative pairs to have low scores
 “””
 # Positive examples should have score &gt; 0
 positive_loss = torch.relu(margin - positive_scores).mean()</p>

<p># Negative examples should have score &lt; 0
 negative_loss = torch.relu(margin + negative_scores).mean()</p>

<p>return positive_loss + negative_loss</p>

<p>def triplet_loss(anchor_emb, positive_emb, negative_emb, margin=0.5):
 “””
 Distance to positive should be less than distance to negative
 “””
 pos_distance = torch.norm(anchor_emb - positive_emb, dim=1)
 neg_distance = torch.norm(anchor_emb - negative_emb, dim=1)</p>

<p>loss = torch.relu(pos_distance - neg_distance + margin)
 return loss.mean()</p>

<p>def batch_softmax_loss(user_emb, item_emb_positive, item_emb_negatives):
 “””
 Treat as multi-class classification: which item did user engage with?</p>

<p>user_emb: (batch, dim)
 item_emb_positive: (batch, dim)
 item_emb_negatives: (batch, num_negatives, dim)
 “””
 # Positive score
 pos_score = (user_emb * item_emb_positive).sum(dim=1) # (batch,)</p>

<p># Negative scores
 # user_emb: (batch, 1, dim), item_emb_negatives: (batch, num_neg, dim)
 neg_scores = torch.bmm(
 item_emb_negatives, 
 user_emb.unsqueeze(-1)
 ).squeeze(-1) # (batch, num_neg)</p>

<p># Concatenate: first column is positive, rest are negatives
 all_scores = torch.cat([pos_score.unsqueeze(1), neg_scores], dim=1) # (batch, 1+num_neg)</p>

<p># Target: index 0 (positive item)
 targets = torch.zeros(all_scores.size(0), dtype=torch.long, device=all_scores.device)</p>

<p># Cross-entropy loss
 loss = nn.CrossEntropyLoss()(all_scores, targets)
 return loss
``</p>

<p><strong>Training Loop:</strong></p>

<p>``python
def train_two_tower_model(model, train_loader, num_epochs=10, lr=0.001):
 optimizer = torch.optim.Adam(model.parameters(), lr=lr)</p>

<p>for epoch in range(num_epochs):
 model.train()
 total_loss = 0</p>

<p>for batch in train_loader:
 # Unpack batch
 user_features = batch[‘user_features’]
 positive_item_features = batch[‘positive_item_features’]
 negative_item_features = batch[‘negative_item_features’] # (batch, num_neg, dim)</p>

<p># Forward pass
 _, user_emb, pos_item_emb = model(user_features, positive_item_features)</p>

<p># Get negative embeddings
 batch_size, num_negatives, feature_dim = negative_item_features.shape
 neg_item_features_flat = negative_item_features.view(-1, feature_dim)
 neg_item_emb_flat = model.get_item_embedding(neg_item_features_flat)
 neg_item_emb = neg_item_emb_flat.view(batch_size, num_negatives, -1)</p>

<p># Compute loss
 loss = batch_softmax_loss(user_emb, pos_item_emb, neg_item_emb)</p>

<p># Backward pass
 optimizer.zero_grad()
 loss.backward()
 optimizer.step()</p>

<p>total_loss += loss.item()</p>

<p>avg_loss = total_loss / len(train_loader)
 print(f”Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}”)</p>

<p>return model
``</p>

<p><strong>Negative Sampling Strategies:</strong></p>

<ol>
  <li><strong>Random Negatives:</strong> Sample random items user didn’t interact with
    <ul>
      <li>Pro: Simple, covers broad space</li>
      <li>Con: Often too easy (user clearly not interested)</li>
    </ul>
  </li>
  <li><strong>Hard Negatives:</strong> Sample items user almost engaged with (scrolled past, clicked but didn’t purchase)
    <ul>
      <li>Pro: More informative, improves model discrimination</li>
      <li>Con: Harder to obtain, may need separate model to identify</li>
    </ul>
  </li>
  <li><strong>Batch Negatives:</strong> Use positive items from other users in batch as negatives
    <ul>
      <li>Pro: No additional sampling needed, efficient</li>
      <li>Con: Not truly negative (another user liked it)</li>
    </ul>
  </li>
  <li><strong>Mixed Strategy:</strong> Combine all three
 <code class="language-plaintext highlighter-rouge">python
 negatives = []
 negatives.extend(sample_random(user, k=10))
 negatives.extend(sample_hard(user, k=5))
 negatives.extend(batch_negatives(batch, exclude=user))
</code></li>
</ol>

<h3 id="why-two-tower-works">Why Two-Tower Works</h3>

<p><strong>Key advantage:</strong> User and item embeddings are <strong>decoupled</strong>.</p>

<p>``
Traditional approach:
 user × item → score
 Problem: Need to compute for all 10M items online</p>

<p>Two-tower approach:
 user → user_embedding (online, 1ms)
 item → item_embedding (offline, precompute for all items)
 Retrieval: Find items with embeddings similar to user_embedding (ANN, 20ms)
``</p>

<p><strong>Precomputation:</strong>
``python</p>
<h1 id="offline-compute-all-item-embeddings-once">Offline: Compute all item embeddings once</h1>
<p>all_item_embeddings = {}
for item in all_items:
 item_features = get_item_features(item.id)
 item_emb = model.get_item_embedding(item_features)
 all_item_embeddings[item.id] = item_emb</p>

<h1 id="online-just-compute-user-embedding-and-search">Online: Just compute user embedding and search</h1>
<p>user_features = get_user_features(user_id)
user_emb = model.get_user_embedding(user_features)
similar_item_ids = ann_search(user_emb, all_item_embeddings, k=400)
``</p>

<hr />

<h2 id="core-component-2-approximate-nearest-neighbor-ann-search">Core Component 2: Approximate Nearest Neighbor (ANN) Search</h2>

<h3 id="the-problem">The Problem</h3>

<p>Given a user embedding, find the top-k items with most similar embeddings.</p>

<p><strong>Naive approach (exact search):</strong>
``python
def exact_nearest_neighbors(query, all_embeddings, k=1000):
 similarities = []
 for item_id, item_emb in all_embeddings.items():
 similarity = dot_product(query, item_emb)
 similarities.append((item_id, similarity))</p>

<p>similarities.sort(key=lambda x: x[1], reverse=True)
 return similarities[:k]
``</p>

<p><strong>Problem:</strong> O(n) where n = 10M items</p>
<ul>
  <li>10M dot products × 128 dimensions = 1.28B operations</li>
  <li>At 1B ops/sec: 1.28 seconds per query</li>
  <li>Way too slow for 50ms latency target!</li>
</ul>

<h3 id="approximate-nearest-neighbor-ann">Approximate Nearest Neighbor (ANN)</h3>

<p><strong>Trade accuracy for speed:</strong> Find items that are <em>approximately</em> nearest, not <em>exactly</em> nearest.</p>

<p><strong>Typical tradeoff:</strong></p>
<ul>
  <li>Exact search: 100% recall, 1000ms latency</li>
  <li>ANN search: 95% recall, 20ms latency</li>
</ul>

<p><strong>Key algorithms:</strong></p>
<ol>
  <li><strong>HNSW</strong> (Hierarchical Navigable Small World) - Best overall</li>
  <li><strong>ScaNN</strong> (Google) - Excellent for large scale</li>
  <li><strong>FAISS</strong> (Facebook) - Multiple algorithms, well-optimized</li>
  <li><strong>Annoy</strong> (Spotify) - Simple, good for smaller datasets</li>
</ol>

<h3 id="hnsw-hierarchical-navigable-small-world">HNSW (Hierarchical Navigable Small World)</h3>

<p><strong>Core idea:</strong> Build a multi-layer graph where:</p>
<ul>
  <li>Top layers: Long-range connections (coarse search)</li>
  <li>Bottom layers: Short-range connections (fine search)</li>
</ul>

<p><strong>Visualization:</strong>
``
Layer 2: •─────────────• (Sparse, long jumps)</p>

<p>Layer 1: •──•──•────•──•──• (Medium density)</p>

<p>Layer 0: •─•─•─•─•─•─•─•─•─• (Dense, precise)
``</p>

<p><strong>Search algorithm:</strong></p>
<ol>
  <li>Start at top layer</li>
  <li>Greedily move to closest neighbor</li>
  <li>When can’t improve, descend to lower layer</li>
  <li>Repeat until bottom layer</li>
  <li>Return k nearest neighbors</li>
</ol>

<p><strong>Implementation with FAISS:</strong></p>

<p>``python
import faiss
import numpy as np</p>

<p>class HNSWIndex:
 def <strong>init</strong>(self, dimension=128, M=32, ef_construction=200):
 “””
 Args:
 dimension: Embedding dimension
 M: Number of bi-directional links per layer (higher = more accurate, more memory)
 ef_construction: Size of dynamic candidate list during construction (higher = better quality, slower build)
 “””
 self.dimension = dimension
 self.index = faiss.IndexHNSWFlat(dimension, M)
 self.index.hnsw.efConstruction = ef_construction
 self.item_ids = []</p>

<p>def add(self, item_ids, embeddings):
 “””
 Add items to index</p>

<p>Args:
 item_ids: List of item IDs
 embeddings: numpy array of shape (n, dimension)
 “””
 # FAISS requires float32
 embeddings = embeddings.astype(‘float32’)</p>

<p># Add to index
 self.index.add(embeddings)
 self.item_ids.extend(item_ids)</p>

<p>print(f”Index now contains {self.index.ntotal} items”)</p>

<p>def search(self, query_embedding, k=1000, ef_search=100):
 “””
 Search for k nearest neighbors</p>

<p>Args:
 query_embedding: numpy array of shape (dimension,) or (1, dimension)
 k: Number of neighbors to return
 ef_search: Size of dynamic candidate list during search (higher = more accurate, slower)</p>

<p>Returns:
 item_ids: List of k item IDs
 distances: List of k distances
 “””
 # Set search parameter
 self.index.hnsw.efSearch = ef_search</p>

<p># Reshape query
 if query_embedding.ndim == 1:
 query_embedding = query_embedding.reshape(1, -1)</p>

<p>query_embedding = query_embedding.astype(‘float32’)</p>

<p># Search
 distances, indices = self.index.search(query_embedding, k)</p>

<p># Map indices to item IDs
 item_ids = [self.item_ids[idx] for idx in indices[0]]</p>

<p>return item_ids, distances[0]</p>

<p>def save(self, filepath):
 “"”Save index to disk”””
 faiss.write_index(self.index, filepath)</p>

<p>def load(self, filepath):
 “"”Load index from disk”””
 self.index = faiss.read_index(filepath)</p>

<h1 id="usage">Usage</h1>
<p>index = HNSWIndex(dimension=128, M=32, ef_construction=200)</p>

<h1 id="build-index-offline">Build index offline</h1>
<p>item_embeddings = get_all_item_embeddings() # Shape: (10M, 128)
item_ids = list(range(10_000_000))
index.add(item_ids, item_embeddings)
index.save(“item_index.faiss”)</p>

<h1 id="search-online">Search online</h1>
<p>user_embedding = get_user_embedding(user_id) # Shape: (128,)
candidate_ids, distances = index.search(user_embedding, k=400, ef_search=100)</p>
<h1 id="20ms-for-10m-items">~20ms for 10M items!</h1>
<p>``</p>

<h3 id="parameter-tuning">Parameter Tuning</h3>

<p><strong>Build-time parameters (offline):</strong></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Effect</th>
      <th>Recommendation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>M</td>
      <td>Connections per node</td>
      <td>16-64 (32 is good default)</td>
    </tr>
    <tr>
      <td>ef_construction</td>
      <td>Build quality</td>
      <td>200-400 for production</td>
    </tr>
  </tbody>
</table>

<p><strong>Search-time parameters (online):</strong></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Effect</th>
      <th>Recommendation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ef_search</td>
      <td>Search quality</td>
      <td>1.5-2× k for good recall</td>
    </tr>
  </tbody>
</table>

<p><strong>Tuning process:</strong>
``python
def tune_ann_parameters(index, queries, ground_truth, k=1000):
 “””
 Find optimal ef_search that balances recall and latency
 “””
 results = []</p>

<p>for ef_search in [50, 100, 200, 400, 800]:
 start_time = time.time()
 recalls = []</p>

<p>for query, truth in zip(queries, ground_truth):
 results_ids, _ = index.search(query, k=k, ef_search=ef_search)
 results_set = set(results_ids)
 truth_set = set(truth)
 recall = len(results_set &amp; truth_set) / len(truth_set)
 recalls.append(recall)</p>

<p>avg_recall = np.mean(recalls)
 latency = (time.time() - start_time) / len(queries) * 1000 # ms</p>

<p>results.append({
 ‘ef_search’: ef_search,
 ‘recall’: avg_recall,
 ‘latency_ms’: latency
 })</p>

<p>print(f”ef_search={ef_search}: recall={avg_recall:.3f}, latency={latency:.1f}ms”)</p>

<p>return results</p>

<h1 id="example-output">Example output:</h1>
<h1 id="ef_search50-recall0850-latency123ms">ef_search=50: recall=0.850, latency=12.3ms</h1>
<h1 id="ef_search100-recall0920-latency187ms--good-balance">ef_search=100: recall=0.920, latency=18.7ms ← Good balance</h1>
<h1 id="ef_search200-recall0960-latency312ms">ef_search=200: recall=0.960, latency=31.2ms</h1>
<h1 id="ef_search400-recall0985-latency548ms--diminishing-returns">ef_search=400: recall=0.985, latency=54.8ms ← Diminishing returns</h1>
<p>``</p>

<p><strong>Production choice:</strong> ef_search=100 gives 92% recall @ 20ms</p>

<h3 id="alternative-product-quantization">Alternative: Product Quantization</h3>

<p>For even larger scale, use <strong>product quantization</strong> to compress embeddings:</p>

<p>``python</p>
<h1 id="reduce-memory-footprint-128-floats-512-bytes--64-bytes">Reduce memory footprint: 128 floats (512 bytes) → 64 bytes</h1>
<h1 id="10m-items-5gb--640mb">10M items: 5GB → 640MB</h1>

<p>index = faiss.IndexIVFPQ(
 faiss.IndexFlatL2(dimension),
 dimension,
 nlist=1000, # Number of clusters
 M=64, # Number of subquantizers
 nbits=8 # Bits per subquantizer
)</p>

<h1 id="train-quantizer">Train quantizer</h1>
<p>index.train(training_embeddings)</p>

<h1 id="add-items">Add items</h1>
<p>index.add(item_embeddings)</p>

<h1 id="search-slightly-less-accurate-much-more-memory-efficient">Search (slightly less accurate, much more memory-efficient)</h1>
<p>distances, indices = index.search(query, k=400)
``</p>

<hr />

<h2 id="core-component-3-multiple-retrieval-strategies">Core Component 3: Multiple Retrieval Strategies</h2>

<p>Relying on a single retrieval method limits quality. <strong>Diversify sources:</strong></p>

<h3 id="strategy-1-collaborative-filtering-40-of-candidates">Strategy 1: Collaborative Filtering (40% of candidates)</h3>

<p><strong>Idea:</strong> “Users who liked X also liked Y”</p>

<p>``python
def collaborative_filtering_retrieval(user_id, k=400):
 # Get user embedding
 user_emb = get_user_embedding(user_id)</p>

<p># ANN search in item embedding space
 candidate_ids = ann_index.search(user_emb, k=k)</p>

<p>return candidate_ids
``</p>

<p><strong>Pros:</strong></p>
<ul>
  <li>Captures implicit patterns</li>
  <li>Discovers non-obvious connections</li>
  <li>Scales well with data</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>Cold start for new users/items</li>
  <li>Popularity bias (recommends popular items disproportionately)</li>
</ul>

<h3 id="strategy-2-content-based-filtering-30-of-candidates">Strategy 2: Content-Based Filtering (30% of candidates)</h3>

<p><strong>Idea:</strong> Recommend items similar to what user liked before</p>

<p>``python
def content_based_retrieval(user_id, k=300):
 # Get user’s liked items
 liked_items = get_user_history(user_id, limit=50)</p>

<p># For each liked item, find similar items
 candidates = set()
 for item_id in liked_items:
 # Find items with similar tags, categories, creators
 similar = find_similar_content(item_id, k=10)
 candidates.update(similar)</p>

<p>if len(candidates) &gt;= k:
 break</p>

<p>return list(candidates)[:k]</p>

<p>def find_similar_content(item_id, k=10):
 item = get_item(item_id)</p>

<p># Match by tags
 similar_by_tags = query_database(
 f”SELECT item_id FROM items WHERE tags &amp;&amp; {item.tags} ORDER BY similarity DESC LIMIT {k}”
 )</p>

<p>return similar_by_tags
``</p>

<p><strong>Pros:</strong></p>
<ul>
  <li>Explainable (“because you liked X”)</li>
  <li>Works for new users with stated preferences</li>
  <li>No popularity bias</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>Limited discovery (filter bubble)</li>
  <li>Requires good item metadata</li>
  <li>May over-specialize</li>
</ul>

<h3 id="strategy-3-trending-20-of-candidates">Strategy 3: Trending (20% of candidates)</h3>

<p><strong>Idea:</strong> What’s popular right now</p>

<p>``python
def trending_retrieval(k=200, time_window_hours=24):
 # Redis sorted set by engagement score
 trending_items = redis.zrevrange(
 f”trending:{time_window_hours}h”,
 start=0,
 end=k-1,
 withscores=True
 )</p>

<p>return [item_id for item_id, score in trending_items]</p>

<p>def update_trending_scores():
 “"”Background job runs every 5 minutes”””
 now = time.time()
 window = 24 * 3600 # 24 hours</p>

<p>for item_id, engagement_data in recent_engagements():
 # Weighted by recency and engagement type
 score = (
 engagement_data[‘views’] * 1.0 +
 engagement_data[‘clicks’] * 2.0 +
 engagement_data[‘likes’] * 3.0 +
 engagement_data[‘shares’] * 5.0
 ) * math.exp(-(now - engagement_data[‘timestamp’]) / (6 * 3600)) # Decay over 6 hours</p>

<p>redis.zadd(f”trending:24h”, {item_id: score})
``</p>

<p><strong>Pros:</strong></p>
<ul>
  <li>Discovers viral content</li>
  <li>No cold start</li>
  <li>High CTR (users like trending items)</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>Same for all users (not personalized)</li>
  <li>Can amplify low-quality viral content</li>
  <li>Rich-get-richer effect</li>
</ul>

<h3 id="strategy-4-social-10-of-candidates">Strategy 4: Social (10% of candidates)</h3>

<p><strong>Idea:</strong> What are my friends engaging with</p>

<p>``python
def social_retrieval(user_id, k=100):
 # Get user’s friends
 friends = get_friends(user_id, limit=100)</p>

<p># Get their recent activity
 recent_engagements = {}
 for friend_id in friends:
 activities = get_recent_activities(friend_id, hours=24, limit=10)
 for activity in activities:
 item_id = activity[‘item_id’]
 recent_engagements[item_id] = recent_engagements.get(item_id, 0) + 1</p>

<p># Sort by frequency
 sorted_items = sorted(
 recent_engagements.items(),
 key=lambda x: x[1],
 reverse=True
 )</p>

<p>return [item_id for item_id, count in sorted_items[:k]]
``</p>

<p><strong>Pros:</strong></p>
<ul>
  <li>Highly relevant (social proof)</li>
  <li>Encourages engagement/sharing</li>
  <li>Natural diversity</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>Requires social graph</li>
  <li>Privacy concerns</li>
  <li>Cold start for users with few friends</li>
</ul>

<h3 id="merging-strategies">Merging Strategies</h3>

<p>``python
def retrieve_candidates(user_id, total_k=1000):
 # Run all strategies in parallel
 with ThreadPoolExecutor() as executor:
 cf_future = executor.submit(collaborative_filtering_retrieval, user_id, k=400)
 cb_future = executor.submit(content_based_retrieval, user_id, k=300)
 tr_future = executor.submit(trending_retrieval, k=200)
 sc_future = executor.submit(social_retrieval, user_id, k=100)</p>

<p># Wait for all to complete
 cf_candidates = cf_future.result()
 cb_candidates = cb_future.result()
 tr_candidates = tr_future.result()
 sc_candidates = sc_future.result()</p>

<p># Merge and deduplicate
 all_candidates = []
 seen = set()</p>

<p>for candidate in cf_candidates + cb_candidates + tr_candidates + sc_candidates:
 if candidate not in seen:
 all_candidates.append(candidate)
 seen.add(candidate)</p>

<p>if len(all_candidates) &gt;= total_k:
 break</p>

<p>return all_candidates
``</p>

<p><strong>Weighting sources:</strong>
Instead of fixed counts, use probability-based sampling:</p>

<p>``python
def weighted_merge(sources, weights, total_k=1000):
 “””
 sources: {
 ‘cf’: [item1, item2, …],
 ‘cb’: [item3, item4, …],
 …
 }
 weights: {‘cf’: 0.4, ‘cb’: 0.3, ‘tr’: 0.2, ‘sc’: 0.1}
 “””
 merged = []
 seen = set()</p>

<p># For each position, sample a source based on weights
 for _ in range(total_k * 2): # Oversample to account for duplicates
 # Sample source
 source = np.random.choice(
 list(weights.keys()),
 p=list(weights.values())
 )</p>

<p># Pop next item from that source
 if sources[source]:
 item = sources[source].pop(0)
 if item not in seen:
 merged.append(item)
 seen.add(item)</p>

<p>if len(merged) &gt;= total_k:
 break</p>

<p>return merged
``</p>

<hr />

<h2 id="core-component-4-caching-strategy">Core Component 4: Caching Strategy</h2>

<p>To achieve &lt; 50ms latency, aggressive caching is essential.</p>

<h3 id="three-level-cache-architecture">Three-Level Cache Architecture</h3>

<p><code class="language-plaintext highlighter-rouge">
Request
 ↓
L1: Candidate Cache (Redis, TTL=5min)
 ├─ Hit → Return cached candidates (5ms)
 └─ Miss ↓
L2: User Embedding Cache (Redis, TTL=1hour)
 ├─ Hit → Skip embedding computation (3ms saved)
 └─ Miss ↓
L3: Precomputed Candidates (Redis, TTL=10min, top 10% users only)
 ├─ Hit → Return precomputed (2ms)
 └─ Miss → Full computation (40ms)
</code></p>

<h3 id="implementation">Implementation</h3>

<p>``python
class CandidateCache:
 def <strong>init</strong>(self, redis_client):
 self.redis = redis_client</p>

<p># TTLs
 self.candidate_ttl = 300 # 5 minutes
 self.embedding_ttl = 3600 # 1 hour
 self.precomputed_ttl = 600 # 10 minutes</p>

<p>def get_candidates(self, user_id, k=1000):
 “””
 Try L1 → L2 → L3 → Compute
 “””
 # L1: Candidate cache
 cache_key = f”candidates:{user_id}:{k}”
 cached = self.redis.get(cache_key)
 if cached:
 print(“[L1 HIT] Returning cached candidates”)
 return json.loads(cached)</p>

<p># L2: Embedding cache
 emb_key = f”user_emb:{user_id}”
 user_emb_cached = self.redis.get(emb_key)</p>

<p>if user_emb_cached:
 print(“[L2 HIT] Using cached embedding”)
 user_emb = np.frombuffer(user_emb_cached, dtype=np.float32)
 else:
 print(“[L2 MISS] Computing embedding”)
 user_features = get_user_features(user_id)
 user_emb = compute_user_embedding(user_features)
 # Cache embedding
 self.redis.setex(emb_key, self.embedding_ttl, user_emb.tobytes())</p>

<p># Retrieve candidates
 candidates = retrieve_candidates_with_embedding(user_emb, k)</p>

<p># Cache candidates
 self.redis.setex(cache_key, self.candidate_ttl, json.dumps(candidates))</p>

<p>return candidates</p>

<p>def precompute_for_active_users(self, user_ids):
 “””
 Background job: precompute candidates for top 10% active users
 Runs every 10 minutes
 “””
 for user_id in user_ids:
 candidates = self.get_candidates(user_id)</p>

<p>precomp_key = f”precomputed:{user_id}”
 self.redis.setex(
 precomp_key,
 self.precomputed_ttl,
 json.dumps(candidates)
 )</p>

<p>print(f”Precomputed candidates for {len(user_ids)} active users”)
``</p>

<h3 id="cache-warming-strategy">Cache Warming Strategy</h3>

<p>``python
def identify_active_users(lookback_hours=24):
 “””
 Find top 10% active users for precomputation
 “””
 # Query analytics database
 query = f”””
 SELECT user_id, COUNT(*) as activity_count
 FROM user_activities
 WHERE timestamp &gt; NOW() - INTERVAL ‘{lookback_hours}’ HOUR
 GROUP BY user_id
 ORDER BY activity_count DESC
 LIMIT {int(total_users * 0.1)}
 “””</p>

<p>active_users = execute_query(query)
 return [row[‘user_id’] for row in active_users]</p>

<p>def warm_cache_scheduler():
 “””
 Runs every 10 minutes
 “””
 while True:
 active_users = identify_active_users()
 cache.precompute_for_active_users(active_users)</p>

<p>time.sleep(600) # 10 minutes
``</p>

<h3 id="cache-invalidation">Cache Invalidation</h3>

<p><strong>Problem:</strong> When should we invalidate cached candidates?</p>

<p><strong>Triggers:</strong></p>
<ol>
  <li><strong>User action:</strong> User engages with item → invalidate their candidates</li>
  <li><strong>Time-based:</strong> Fixed TTL (5 minutes)</li>
  <li><strong>New item published:</strong> Invalidate trending cache</li>
  <li><strong>Model update:</strong> Invalidate all embeddings and candidates</li>
</ol>

<p>``python
def on_user_engagement(user_id, item_id, action):
 “””
 Called when user clicks/likes/shares item
 “””
 # Invalidate candidate cache (stale now)
 # Redis DEL does not support globs; use SCAN + DEL for safety
 cursor = 0
 pattern = f”candidates:{user_id}:<em>”
 while True:
 cursor, keys = redis.scan(cursor=cursor, match=pattern, count=1000)
 if keys:
 redis.delete(</em>keys)
 if cursor == 0:
 break</p>

<p># Don’t invalidate embedding cache (more stable)
 # Will naturally expire after 1 hour</p>

<p># Log event for retraining
 log_engagement_event(user_id, item_id, action)
``</p>

<h3 id="cache-hit-rate-monitoring">Cache Hit Rate Monitoring</h3>

<p>``python
class CacheMetrics:
 def <strong>init</strong>(self):
 self.hits = {‘L1’: 0, ‘L2’: 0, ‘L3’: 0}
 self.misses = {‘L1’: 0, ‘L2’: 0, ‘L3’: 0}</p>

<p>def record_hit(self, level):
 self.hits[level] += 1</p>

<p>def record_miss(self, level):
 self.misses[level] += 1</p>

<p>def get_stats(self):
 stats = {}
 for level in [‘L1’, ‘L2’, ‘L3’]:
 total = self.hits[level] + self.misses[level]
 hit_rate = self.hits[level] / total if total &gt; 0 else 0
 stats[level] = {
 ‘hit_rate’: hit_rate,
 ‘hits’: self.hits[level],
 ‘misses’: self.misses[level]
 }
 return stats</p>

<h1 id="expected-hit-rates">Expected hit rates:</h1>
<h1 id="l1-candidates-60-70-users-refresh-feed-multiple-times">L1 (candidates): 60-70% (users refresh feed multiple times)</h1>
<h1 id="l2-embeddings-80-90-embeddings-stable-for-1-hour">L2 (embeddings): 80-90% (embeddings stable for ~1 hour)</h1>
<h1 id="l3-precomputed-10-15-only-for-top-10-users">L3 (precomputed): 10-15% (only for top 10% users)</h1>
<p>``</p>

<hr />

<h2 id="handling-cold-start">Handling Cold Start</h2>

<h3 id="new-user-problem">New User Problem</h3>

<p><strong>Challenge:</strong> User with no history → no personalization signals</p>

<p><strong>Solution Hierarchy:</strong></p>

<p><strong>Level 1: Onboarding Survey</strong>
``python
def handle_new_user_onboarding(user_id, selected_interests):
 “””
 User selects 3-5 interests during signup
 “””
 # Map interests to item tags
 interest_tags = map_interests_to_tags(selected_interests)</p>

<p># Find items matching these tags
 candidates = query_items_by_tags(interest_tags, k=1000)</p>

<p># Cache for fast retrieval
 redis.setex(f”new_user_candidates:{user_id}”, 3600, json.dumps(candidates))</p>

<p>return candidates
``</p>

<p><strong>Level 2: Demographic-based Defaults</strong>
``python
def get_demographic_defaults(user_id):
 user = get_user_profile(user_id)</p>

<p># Lookup popular items for this demographic
 cache_key = f”popular_items:{user.age_group}:{user.location}:{user.language}”</p>

<p>cached = redis.get(cache_key)
 if cached:
 return json.loads(cached)</p>

<p># Query most popular items for similar users
 popular = query_popular_items(
 age_group=user.age_group,
 location=user.location,
 language=user.language,
 k=1000
 )</p>

<p>redis.setex(cache_key, 3600, json.dumps(popular))
 return popular
``</p>

<p><strong>Level 3: Explore-Heavy Mix</strong>
``python
def new_user_retrieval(user_id):
 “””
 For new users, use more exploration
 “””
 # 50% popular items (safe choices)
 popular = get_popular_items(k=500)</p>

<p># 30% based on stated interests
 interests = get_user_interests(user_id)
 interest_based = get_items_by_interests(interests, k=300)</p>

<p># 20% random exploration
 random_items = sample_random_items(k=200)</p>

<p>return merge_and_shuffle(popular, interest_based, random_items)
``</p>

<p><strong>Rapid Learning:</strong>
``python
def update_new_user_preferences(user_id, engagement):
 “””
 Weight early engagements heavily to quickly build profile
 “””
 engagement_count = get_engagement_count(user_id)</p>

<p>if engagement_count &lt; 10:
 # First 10 engagements: 5x weight
 weight = 5.0
 elif engagement_count &lt; 50:
 # Next 40 engagements: 2x weight
 weight = 2.0
 else:
 # Normal weight
 weight = 1.0</p>

<p>update_user_profile(user_id, engagement, weight=weight)
``</p>

<h3 id="new-item-problem">New Item Problem</h3>

<p><strong>Challenge:</strong> Item with no engagement history → no collaborative signal</p>

<p><strong>Solution 1: Content-Based Features</strong>
``python
def get_new_item_candidates_for_users(item_id):
 “””
 Find users who might like this new item based on content
 “””
 item = get_item(item_id)</p>

<p># Extract content features
 tags = item.tags
 category = item.category
 creator = item.creator_id</p>

<p># Find users interested in these features
 candidate_users = []</p>

<p># Users who liked similar tags
 candidate_users.extend(
 get_users_by_tag_preferences(tags, k=10000)
 )</p>

<p># Users who follow this creator
 candidate_users.extend(
 get_creator_followers(creator)
 )</p>

<p>return list(set(candidate_users))
``</p>

<p><strong>Solution 2: Small-Scale Exploration</strong>
``python
def bootstrap_new_item(item_id):
 “””
 Show new item to small random sample to gather initial signals
 “””
 # Sample 1% of users randomly
 sample_size = int(total_users * 0.01)
 sampled_users = random.sample(all_users, sample_size)</p>

<p># Add this item to their candidate pools with high position
 for user_id in sampled_users:
 inject_item_into_candidates(user_id, item_id, position=50)</p>

<p># Monitor for 1 hour
 # If engagement rate &gt; threshold, continue showing
 # If engagement rate &lt; threshold, reduce exposure
``</p>

<p><strong>Solution 3: Multi-Armed Bandit</strong>
``python
class ThompsonSamplingBandit:
 “””
 Balance exploration (new items) vs exploitation (proven items)
 “””
 def <strong>init</strong>(self):
 self.successes = {} # item_id -&gt; success count
 self.failures = {} # item_id -&gt; failure count</p>

<p>def select_item(self, candidate_items, k=20):
 “””
 Sample items based on estimated CTR with uncertainty
 “””
 selected = []</p>

<p>for item_id in candidate_items:
 alpha = self.successes.get(item_id, 1) # Prior: 1 success
 beta = self.failures.get(item_id, 1) # Prior: 1 failure</p>

<p># Sample from Beta distribution
 theta = np.random.beta(alpha, beta)</p>

<p>selected.append((item_id, theta))</p>

<p># Sort by sampled theta and return top k
 selected.sort(key=lambda x: x[1], reverse=True)
 return [item_id for item_id, _ in selected[:k]]</p>

<p>def update(self, item_id, success):
 “””
 Update counts after showing item
 “””
 if success:
 self.successes[item_id] = self.successes.get(item_id, 0) + 1
 else:
 self.failures[item_id] = self.failures.get(item_id, 0) + 1
``</p>

<hr />

<h2 id="real-world-examples">Real-World Examples</h2>

<h3 id="youtube-recommendations">YouTube Recommendations</h3>

<p><strong>Architecture (circa 2016):</strong></p>
<ul>
  <li>Two-stage: Candidate generation → Ranking</li>
  <li>Candidate generation: Deep neural network with collaborative filtering</li>
  <li>Features: Watch history, search history, demographics</li>
  <li>800k candidates → Hundreds for ranking</li>
  <li>Uses TensorFlow for training</li>
</ul>

<p><strong>Key innovations:</strong></p>
<ul>
  <li>“Example age” feature (prefer fresh content)</li>
  <li>Normalized watch time (account for video length)</li>
  <li>Asymmetric co-watch (A→B doesn’t mean B→A)</li>
</ul>

<h3 id="pinterest-pinsage">Pinterest (PinSage)</h3>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Graph neural network (GNN) on Pin-Board graph</li>
  <li>3 billion nodes, 18 billion edges</li>
  <li>Random walk sampling for neighborhoods</li>
  <li>Two-tower model: Pin embeddings, User embeddings</li>
  <li>Production deployment on GPUs</li>
</ul>

<p><strong>Key innovations:</strong></p>
<ul>
  <li>Importance pooling (weight neighbors by importance)</li>
  <li>Hard negative sampling (visually similar but topically different)</li>
  <li>Multi-task learning (save, click, hide)</li>
</ul>

<h3 id="spotify-recommendations">Spotify Recommendations</h3>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Collaborative filtering (matrix factorization)</li>
  <li>Content-based (audio features via CNNs)</li>
  <li>Natural language processing (playlist names, song metadata)</li>
  <li>Reinforcement learning (sequential recommendations)</li>
</ul>

<p><strong>Key innovations:</strong></p>
<ul>
  <li>Audio embedding from raw waveforms</li>
  <li>Contextual bandits for playlist curation</li>
  <li>Session-based recommendations</li>
</ul>

<hr />

<h2 id="monitoring-and-evaluation">Monitoring and Evaluation</h2>

<h3 id="online-metrics">Online Metrics</h3>

<p><strong>User Engagement:</strong></p>
<ul>
  <li>Click-through rate (CTR)</li>
  <li>Watch time / Dwell time</li>
  <li>Like / Share rate</li>
  <li>Session length</li>
  <li>Return rate (DAU / MAU)</li>
</ul>

<p><strong>Diversity Metrics:</strong></p>
<ul>
  <li>Intra-list diversity (avg pairwise distance)</li>
  <li>Coverage (% of catalog recommended)</li>
  <li>Concentration (Gini coefficient)</li>
</ul>

<p><strong>System Metrics:</strong></p>
<ul>
  <li>Candidate generation latency (p50, p95, p99)</li>
  <li>Cache hit rates (L1, L2, L3)</li>
  <li>ANN recall@k</li>
  <li>QPS per server</li>
</ul>

<h3 id="offline-metrics">Offline Metrics</h3>

<p><strong>Retrieval Quality:</strong>
``python
def evaluate_retrieval(model, test_set):
 “””
 Evaluate on held-out test set
 “””
 recalls = []
 precisions = []</p>

<p>for user_id, ground_truth_items in test_set:
 # Generate candidates
 candidates = retrieve_candidates(user_id, k=1000)</p>

<p># Recall: What % of ground truth items were retrieved?
 recall = len(set(candidates) &amp; set(ground_truth_items)) / len(ground_truth_items)
 recalls.append(recall)</p>

<p># Precision: What % of candidates are relevant?
 precision = len(set(candidates) &amp; set(ground_truth_items)) / len(candidates)
 precisions.append(precision)</p>

<p>print(f”Recall@1000: {np.mean(recalls):.3f}”)
 print(f”Precision@1000: {np.mean(precisions):.3f}”)
``</p>

<p><strong>Target: Recall@1000 &gt; 0.90</strong> (retrieve 90% of items user would engage with)</p>

<h3 id="ab-testing">A/B Testing</h3>

<p>``python
class ABExperiment:
 def <strong>init</strong>(self, name, control_config, treatment_config, traffic_split=0.05):
 self.name = name
 self.control = control_config
 self.treatment = treatment_config
 self.traffic_split = traffic_split</p>

<p>def assign_variant(self, user_id):
 “””
 Consistent hashing for stable assignment
 “””
 hash_val = hashlib.md5(f”{user_id}:{self.name}”.encode()).hexdigest()
 hash_int = int(hash_val, 16)</p>

<p>if (hash_int % 100) &lt; (self.traffic_split * 100):
 return ‘treatment’
 return ‘control’</p>

<p>def get_config(self, user_id):
 variant = self.assign_variant(user_id)
 return self.treatment if variant == ‘treatment’ else self.control</p>

<h1 id="example-test-new-retrieval-mix">Example: Test new retrieval mix</h1>
<p>experiment = ABExperiment(
 name=”retrieval_mix_v2”,
 control_config={‘cf’: 0.4, ‘cb’: 0.3, ‘tr’: 0.2, ‘sc’: 0.1},
 treatment_config={‘cf’: 0.5, ‘cb’: 0.2, ‘tr’: 0.2, ‘sc’: 0.1}, # More CF, less CB
 traffic_split=0.05 # 5% treatment, 95% control
)</p>

<h1 id="usage-1">Usage</h1>
<p>config = experiment.get_config(user_id)
candidates = retrieve_with_mix(user_id, weights=config)</p>

<h1 id="measure">Measure:</h1>
<h1 id="--ctr-improvement-23-">- CTR improvement: +2.3% ✓</h1>
<h1 id="--diversity--12-acceptable">- Diversity: -1.2% (acceptable)</h1>
<h1 id="--latency-no-change">- Latency: No change</h1>
<h1 id="decision-ship-to-100">Decision: Ship to 100%</h1>
<p>``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Funnel architecture</strong> (millions → thousands → dozens) is essential for scale 
✅ <strong>Two-tower models</strong> decouple user/item embeddings for efficient retrieval 
✅ <strong>ANN search</strong> (HNSW, ScaNN) provides 95%+ recall @ 20ms vs 1000ms exact search 
✅ <strong>Multiple retrieval strategies</strong> (CF, content, trending, social) improve diversity 
✅ <strong>Aggressive caching</strong> (3-level) achieves sub-50ms latency 
✅ <strong>Cold start</strong> requires explicit strategies (onboarding, demographics, exploration) 
✅ <strong>Monitoring</strong> both online metrics (CTR, diversity) and offline metrics (recall@k)</p>

<hr />

<h2 id="further-reading">Further Reading</h2>

<p><strong>Papers:</strong></p>
<ul>
  <li><a href="https://research.google/pubs/pub45530/">Deep Neural Networks for YouTube Recommendations</a></li>
  <li><a href="https://arxiv.org/abs/1806.01973">PinSage: Graph Convolutional Neural Networks</a></li>
  <li><a href="https://arxiv.org/abs/1603.09320">HNSW: Efficient and Robust Approximate Nearest Neighbor Search</a></li>
</ul>

<p><strong>Libraries:</strong></p>
<ul>
  <li><a href="https://github.com/facebookresearch/faiss">Faiss (Facebook)</a></li>
  <li><a href="https://github.com/google-research/google-research/tree/master/scann">ScaNN (Google)</a></li>
  <li><a href="https://github.com/spotify/annoy">Annoy (Spotify)</a></li>
</ul>

<p><strong>Books:</strong></p>
<ul>
  <li><em>Recommender Systems Handbook</em> (Ricci et al.)</li>
  <li><em>Practical Recommender Systems</em> (Kim Falk)</li>
</ul>

<p><strong>Courses:</strong></p>
<ul>
  <li><a href="http://web.stanford.edu/class/cs246/">Stanford CS246: Mining Massive Datasets</a></li>
  <li><a href="https://recsys.acm.org/">RecSys Conference Tutorials</a></li>
</ul>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>Recommendation systems are one of the most impactful applications of machine learning, directly affecting user experience for billions of people daily. The candidate generation stage is where the magic begins efficiently narrowing millions of possibilities to a manageable set of high-quality candidates.</p>

<p>The key insights:</p>
<ol>
  <li><strong>Embeddings</strong> capture semantic similarity in continuous space</li>
  <li><strong>ANN search</strong> makes similarity search practical at scale</li>
  <li><strong>Diversity</strong> in retrieval strategies prevents filter bubbles</li>
  <li><strong>Caching</strong> is not optional it’s essential for latency</li>
  <li><strong>Cold start</strong> requires thoughtful product and engineering solutions</li>
</ol>

<p>As you build recommendation systems, remember: the best system balances multiple objectives (relevance, diversity, freshness, serendipity) while maintaining the strict latency and cost constraints of production environments.</p>

<p>Now go build something that helps users discover content they’ll love! 🚀</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0001-recommendation-system/">arunbaby.com/ml-system-design/0001-recommendation-system</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#embeddings" class="page__taxonomy-item p-category" rel="tag">embeddings</a><span class="sep">, </span>
    
      <a href="/tags/#recommendation-systems" class="page__taxonomy-item p-category" rel="tag">recommendation-systems</a><span class="sep">, </span>
    
      <a href="/tags/#retrieval" class="page__taxonomy-item p-category" rel="tag">retrieval</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0001-two-sum/" rel="permalink">Two Sum
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          28 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The hash table trick that makes O(n²) become O(n) and why this pattern appears everywhere from feature stores to embedding lookups.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0001-streaming-asr/" rel="permalink">Streaming ASR Architecture
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Why batch ASR won’t work for voice assistants, and how streaming models transcribe speech as you speak in under 200ms.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0001-what-are-ai-agents/" rel="permalink">What are AI Agents?
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“From Passive Tools to Active Assistants: The Cognitive Revolution in Software.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Recommendation+System%3A+Candidate+Retrieval%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0001-recommendation-system%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0001-recommendation-system%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0001-recommendation-system/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="#" class="pagination--pager disabled">Previous</a>
    
    
      <a href="/ml-system-design/0002-classification-pipeline/" class="pagination--pager" title="Classification Pipeline Design">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
