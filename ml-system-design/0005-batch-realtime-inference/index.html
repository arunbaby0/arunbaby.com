<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Batch vs Real-Time Inference - Arun Baby</title>
<meta name="description" content="How to choose between batch and real-time inference, the architectural decision that shapes your entire ML serving infrastructure.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Batch vs Real-Time Inference">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0005-batch-realtime-inference/">


  <meta property="og:description" content="How to choose between batch and real-time inference, the architectural decision that shapes your entire ML serving infrastructure.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Batch vs Real-Time Inference">
  <meta name="twitter:description" content="How to choose between batch and real-time inference, the architectural decision that shapes your entire ML serving infrastructure.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0005-batch-realtime-inference/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0005-batch-realtime-inference/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Batch vs Real-Time Inference">
    <meta itemprop="description" content="How to choose between batch and real-time inference, the architectural decision that shapes your entire ML serving infrastructure.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0005-batch-realtime-inference/" itemprop="url">Batch vs Real-Time Inference
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#problem-definition">Problem Definition</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li></ul></li><li><a href="#batch-inference">Batch Inference</a><ul><li><a href="#architecture">Architecture</a></li><li><a href="#implementation">Implementation</a></li><li><a href="#spark-based-batch-inference">Spark-based Batch Inference</a></li></ul></li><li><a href="#real-time-inference">Real-Time Inference</a><ul><li><a href="#architecture-1">Architecture</a></li><li><a href="#implementation-1">Implementation</a></li><li><a href="#tensorflow-serving">TensorFlow Serving</a></li></ul></li><li><a href="#hybrid-approach">Hybrid Approach</a><ul><li><a href="#architecture-2">Architecture</a></li><li><a href="#implementation-2">Implementation</a></li></ul></li><li><a href="#decision-framework">Decision Framework</a><ul><li><a href="#use-batch-inference-when">Use Batch Inference When:</a></li><li><a href="#use-real-time-inference-when">Use Real-Time Inference When:</a></li><li><a href="#use-hybrid-when">Use Hybrid When:</a></li></ul></li><li><a href="#cost-comparison">Cost Comparison</a></li><li><a href="#advanced-patterns">Advanced Patterns</a><ul><li><a href="#multi-tier-caching">Multi-Tier Caching</a></li><li><a href="#prediction-warming">Prediction Warming</a></li><li><a href="#conditional-batch-updates">Conditional Batch Updates</a></li><li><a href="#graceful-degradation">Graceful Degradation</a></li></ul></li><li><a href="#real-world-case-studies">Real-World Case Studies</a><ul><li><a href="#netflix-hybrid-recommendations">Netflix: Hybrid Recommendations</a></li><li><a href="#uber-real-time-eta-prediction">Uber: Real-Time ETA Prediction</a></li><li><a href="#linkedin-people-you-may-know">LinkedIn: People You May Know</a></li></ul></li><li><a href="#monitoring--observability">Monitoring &amp; Observability</a><ul><li><a href="#key-metrics-to-track">Key Metrics to Track</a></li><li><a href="#sla-definition">SLA Definition</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>How to choose between batch and real-time inference, the architectural decision that shapes your entire ML serving infrastructure.</strong></p>

<h2 id="introduction">Introduction</h2>

<p>After training a model, you need to <strong>serve predictions</strong>. Two fundamental approaches:</p>

<ol>
  <li><strong>Batch Inference:</strong> Precompute predictions for all users/items periodically</li>
  <li><strong>Real-Time Inference:</strong> Compute predictions on-demand when requested</li>
</ol>

<p><strong>Why this matters:</strong></p>
<ul>
  <li><strong>Different latency requirements</strong> → Different architectures</li>
  <li><strong>Cost implications</strong> → Batch can be 10-100x cheaper</li>
  <li><strong>System complexity</strong> → Real-time requires more infrastructure</li>
  <li><strong>Feature freshness</strong> → Real-time uses latest data</li>
</ul>

<p><strong>What you’ll learn:</strong></p>
<ul>
  <li>When to use batch vs real-time</li>
  <li>Architecture for each approach</li>
  <li>Hybrid systems combining both</li>
  <li>Trade-offs and decision framework</li>
  <li>Production implementation patterns</li>
</ul>

<hr />

<h2 id="problem-definition">Problem Definition</h2>

<p>Design an ML inference system that serves predictions efficiently.</p>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Prediction Serving</strong>
    <ul>
      <li>Batch: Generate predictions for all entities periodically</li>
      <li>Real-time: Serve predictions on-demand with low latency</li>
      <li>Hybrid: Combine both approaches</li>
    </ul>
  </li>
  <li><strong>Data Freshness</strong>
    <ul>
      <li>Access to latest features</li>
      <li>Handle feature staleness</li>
      <li>Feature computation strategy</li>
    </ul>
  </li>
  <li><strong>Scalability</strong>
    <ul>
      <li>Handle millions of predictions</li>
      <li>Scale horizontally</li>
      <li>Handle traffic spikes</li>
    </ul>
  </li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Latency</strong>
    <ul>
      <li>Batch: Minutes to hours acceptable</li>
      <li>Real-time: &lt; 100ms for most applications</li>
    </ul>
  </li>
  <li><strong>Throughput</strong>
    <ul>
      <li>Batch: Process millions of predictions in one run</li>
      <li>Real-time: 1000s of requests/second</li>
    </ul>
  </li>
  <li><strong>Cost</strong>
    <ul>
      <li>Optimize compute resources</li>
      <li>Minimize infrastructure costs</li>
    </ul>
  </li>
  <li><strong>Reliability</strong>
    <ul>
      <li>99.9%+ uptime for real-time</li>
      <li>Graceful degradation</li>
      <li>Fallback mechanisms</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="batch-inference">Batch Inference</h2>

<p>Precompute predictions periodically (daily, hourly, etc.).</p>

<h3 id="architecture">Architecture</h3>

<p>``
┌─────────────────────────────────────────────────────────┐
│ Batch Inference Pipeline │
├─────────────────────────────────────────────────────────┤
│ │
│ ┌──────────────┐ ┌──────────────┐ │
│ │ Data Lake │ │ Feature │ │
│ │ (HDFS/S3) │─────▶│ Engineering │ │
│ └──────────────┘ └──────────────┘ │
│ │ │
│ ▼ │
│ ┌──────────────┐ │
│ │ Batch Job │ │
│ │ (Spark/Ray) │ │
│ │ - Load model│ │
│ │ - Predict │ │
│ └──────────────┘ │
│ │ │
│ ▼ │
│ ┌──────────────┐ │
│ │ Write to │ │
│ │ Cache/DB │ │
│ │ (Redis/DDB) │ │
│ └──────────────┘ │
│ │ │
│ ┌──────────────┐ │ │
│ │ Application │◀───────────┘ │
│ │ Server │ Lookup predictions │
│ └──────────────┘ │
│ │
└─────────────────────────────────────────────────────────┘</p>

<p>Flow:</p>
<ol>
  <li>Extract features from data warehouse</li>
  <li>Run batch prediction job</li>
  <li>Store predictions in fast lookup store</li>
  <li>Application does simple lookup
``</li>
</ol>

<h3 id="implementation">Implementation</h3>

<p>``python
from typing import List, Dict
import numpy as np
import redis
import json
import time</p>

<p>class BatchInferenceSystem:
 “””
 Batch inference system</p>

<p>Precomputes predictions for all users/items
 “””</p>

<p>def <strong>init</strong>(self, model, redis_client):
 self.model = model
 self.redis = redis_client
 self.batch_size = 1000</p>

<p>def run_batch_prediction(self, entity_ids: List[str], features_df):
 “””
 Run batch prediction for all entities</p>

<p>Args:
 entity_ids: List of user/item IDs
 features_df: DataFrame with features for all entities</p>

<p>Returns:
 Number of predictions generated
 “””
 num_predictions = 0</p>

<p># Process in batches for memory efficiency
 for i in range(0, len(entity_ids), self.batch_size):
 batch_ids = entity_ids[i:i+self.batch_size]
 batch_features = features_df.iloc[i:i+self.batch_size]</p>

<p># Predict
 predictions = self.model.predict(batch_features.values)</p>

<p># Store in Redis
 self._store_predictions(batch_ids, predictions)</p>

<p>num_predictions += len(batch_ids)</p>

<p>if num_predictions % 10000 == 0:
 print(f”Processed {num_predictions} predictions…”)</p>

<p>return num_predictions</p>

<p>def _store_predictions(self, entity_ids: List[str], predictions: np.ndarray):
 “"”Store predictions in Redis with TTL”””
 pipeline = self.redis.pipeline()</p>

<p>ttl_seconds = 24 * 3600 # 24 hours</p>

<p>for entity_id, prediction in zip(entity_ids, predictions):
 # Store as JSON
 key = f”pred:{entity_id}”
 value = json.dumps({
 ‘prediction’: float(prediction),
 ‘timestamp’: time.time()
 })</p>

<p>pipeline.setex(key, ttl_seconds, value)</p>

<p>pipeline.execute()</p>

<p>def get_prediction(self, entity_id: str) -&gt; float:
 “””
 Lookup precomputed prediction</p>

<p>Fast O(1) lookup
 “””
 key = f”pred:{entity_id}”
 value = self.redis.get(key)</p>

<p>if value is None:
 # Prediction not found or expired
 return None</p>

<p>data = json.loads(value)
 return data[‘prediction’]</p>

<h1 id="usage">Usage</h1>
<p>import pandas as pd
import time</p>

<h1 id="initialize">Initialize</h1>
<p>redis_client = redis.Redis(host=’localhost’, port=6379, db=0)
model = load_trained_model() # Your trained model</p>

<p>batch_system = BatchInferenceSystem(model, redis_client)</p>

<h1 id="run-batch-prediction-eg-daily-cron-job">Run batch prediction (e.g., daily cron job)</h1>
<p>user_ids = fetch_all_user_ids() # Get all users
features_df = fetch_user_features(user_ids) # Get features</p>

<p>num_preds = batch_system.run_batch_prediction(user_ids, features_df)
print(f”Generated {num_preds} predictions”)</p>

<h1 id="later-application-looks-up-prediction">Later, application looks up prediction</h1>
<p>prediction = batch_system.get_prediction(“user_12345”)
print(f”Prediction: {prediction}”)
``</p>

<h3 id="spark-based-batch-inference">Spark-based Batch Inference</h3>

<p>For large-scale batch processing:</p>

<p>``python
from pyspark.sql import SparkSession
from pyspark.sql.functions import pandas_udf, PandasUDFType
import pandas as pd</p>

<p>class SparkBatchInference:
 “””
 Distributed batch inference using PySpark</p>

<p>Scales to billions of predictions
 “””</p>

<p>def <strong>init</strong>(self, model_path):
 self.spark = SparkSession.builder <br />
 .appName(“BatchInference”) <br />
 .getOrCreate()</p>

<p>self.model_path = model_path</p>

<p>def predict_spark(self, features_df_spark):
 “””
 Distribute prediction across cluster</p>

<p>Args:
 features_df_spark: Spark DataFrame with features</p>

<p>Returns:
 Spark DataFrame with predictions
 “””
 model_path = self.model_path</p>

<p># Define pandas UDF for prediction
 @pandas_udf(“double”, PandasUDFType.SCALAR)
 def predict_udf(*features):
 # Load model once per executor
 import joblib
 model = joblib.load(model_path)</p>

<p># Create feature matrix
 X = pd.DataFrame({
 f’feature_{i}’: features[i]
 for i in range(len(features))
 })</p>

<p># Predict
 predictions = model.predict(X.values)
 return pd.Series(predictions)</p>

<p># Apply UDF
 feature_cols = [col for col in features_df_spark.columns if col.startswith(‘feature_’)]</p>

<p>result_df = features_df_spark.withColumn(
 ‘prediction’,
 predict_udf(*feature_cols)
 )</p>

<p>return result_df</p>

<p>def run_batch_job(self, input_path, output_path):
 “””
 Full batch inference pipeline</p>

<p>Args:
 input_path: S3/HDFS path to input data
 output_path: S3/HDFS path to save predictions
 “””
 # Read input
 df = self.spark.read.parquet(input_path)</p>

<p># Predict
 predictions_df = self.predict_spark(df)</p>

<p># Write output
 predictions_df.write.parquet(output_path, mode=’overwrite’)</p>

<p>print(f”Batch prediction complete. Output: {output_path}”)</p>

<h1 id="usage-1">Usage</h1>
<p>spark_batch = SparkBatchInference(model_path=’s3://models/my_model.pkl’)</p>

<p>spark_batch.run_batch_job(
 input_path=’s3://data/user_features/’,
 output_path=’s3://predictions/daily/2025-01-15/’
)
``</p>

<hr />

<h2 id="real-time-inference">Real-Time Inference</h2>

<p>Compute predictions on-demand when requested.</p>

<h3 id="architecture-1">Architecture</h3>

<p>``
┌─────────────────────────────────────────────────────────┐
│ Real-Time Inference System │
├─────────────────────────────────────────────────────────┤
│ │
│ ┌──────────────┐ │
│ │ Load │◀─── Model Registry │
│ │ Balancer │ │
│ └──────┬───────┘ │
│ │ │
│ ┌────▼─────────────────────────────┐ │
│ │ Model Serving Instances │ │
│ │ ┌─────────┐ ┌─────────┐ │ │
│ │ │ Model 1 │ │ Model 2 │ … │ │
│ │ │ (GPU) │ │ (GPU) │ │ │
│ │ └─────────┘ └─────────┘ │ │
│ └────┬─────────────────────────────┘ │
│ │ │
│ ┌────▼──────────┐ ┌──────────────┐ │
│ │ Feature │────▶│ Feature │ │
│ │ Service │ │ Store │ │
│ │ - Online │ │ (Redis) │ │
│ │ features │ └──────────────┘ │
│ └───────────────┘ │
│ │
└─────────────────────────────────────────────────────────┘</p>

<p>Flow:</p>
<ol>
  <li>Request arrives with user/item ID</li>
  <li>Fetch features from feature store</li>
  <li>Compute additional online features</li>
  <li>Model predicts</li>
  <li>Return prediction
``</li>
</ol>

<h3 id="implementation-1">Implementation</h3>

<p>``python
from fastapi import FastAPI
import numpy as np
from typing import Dict
import torch</p>

<p>app = FastAPI()</p>

<p>class RealTimeInferenceService:
 “””
 Real-time inference service</p>

<p>Serves predictions with low latency
 “””</p>

<p>def <strong>init</strong>(self, model, feature_store):
 self.model = model
 self.feature_store = feature_store</p>

<p># Warm up model
 self._warmup()</p>

<p>def _warmup(self):
 “"”Warm up model with dummy prediction”””
 dummy_features = np.random.randn(1, self.model.input_dim)
 _ = self.model.predict(dummy_features)</p>

<p>def get_features(self, entity_id: str) -&gt; Dict:
 “””
 Fetch features for entity</p>

<p>Combines precomputed + real-time features
 “””
 # Fetch precomputed features from Redis
 precomputed_raw = self.feature_store.get(f”features:{entity_id}”)
 precomputed = {}
 if precomputed_raw:
 try:
 precomputed = json.loads(precomputed_raw)
 except Exception:
 precomputed = {}</p>

<p>if precomputed is None:
 # Fallback: compute features on-the-fly
 precomputed = self._compute_features_fallback(entity_id)</p>

<p># Add real-time features
 realtime_features = self._compute_realtime_features(entity_id)</p>

<p># Combine
 features = {**precomputed, **realtime_features}</p>

<p>return features</p>

<p>def _compute_realtime_features(self, entity_id: str) -&gt; Dict:
 “””
 Compute features that must be fresh</p>

<p>E.g., time of day, user’s current session, etc.
 “””
 import datetime</p>

<p>now = datetime.datetime.now()</p>

<p>return {
 ‘hour_of_day’: now.hour,
 ‘day_of_week’: now.weekday(),
 ‘is_weekend’: 1 if now.weekday() &gt;= 5 else 0
 }</p>

<p>def _compute_features_fallback(self, entity_id: str) -&gt; Dict:
 “"”Fallback feature computation”””
 # Query database, compute on-the-fly
 # This is slower but ensures we can always serve
 return {}</p>

<p>def predict(self, entity_id: str) -&gt; float:
 “””
 Real-time prediction</p>

<p>Returns:
 Prediction score
 “””
 # Get features
 features = self.get_features(entity_id)</p>

<p># Convert to numpy array (assuming fixed feature order)
 feature_vector = np.array([
 features.get(f’feature_{i}’, 0.0)
 for i in range(self.model.input_dim)
 ]).reshape(1, -1)</p>

<p># Predict
 prediction = self.model.predict(feature_vector)[0]</p>

<p>return float(prediction)</p>

<h1 id="fastapi-endpoints">FastAPI endpoints</h1>
<p>realtime_service = RealTimeInferenceService(model, redis_client)</p>

<p>@app.get(“/predict/{entity_id}”)
async def predict_endpoint(entity_id: str):
 “””
 Real-time prediction endpoint</p>

<p>GET /predict/user_12345
 “””
 try:
 prediction = realtime_service.predict(entity_id)</p>

<p>return {
 ‘entity_id’: entity_id,
 ‘prediction’: prediction,
 ‘timestamp’: time.time()
 }</p>

<p>except Exception as e:
 from fastapi import HTTPException
 raise HTTPException(status_code=500, detail={‘error’: str(e), ‘entity_id’: entity_id})</p>

<h1 id="run-with-uvicorn-appapp-host-0000-port-8000">Run with: uvicorn app:app –host 0.0.0.0 –port 8000</h1>
<p>``</p>

<h3 id="tensorflow-serving">TensorFlow Serving</h3>

<p>Production-grade model serving:</p>

<p>``python
import requests
import json</p>

<p>class TensorFlowServingClient:
 “””
 Client for TensorFlow Serving</p>

<p>High-performance model serving
 “””</p>

<p>def <strong>init</strong>(self, server_url, model_name, model_version=None):
 self.server_url = server_url
 self.model_name = model_name
 self.model_version = model_version or ‘latest’</p>

<p># Endpoint
 if self.model_version == ‘latest’:
 self.endpoint = f”{server_url}/v1/models/{model_name}:predict”
 else:
 self.endpoint = f”{server_url}/v1/models/{model_name}/versions/{model_version}:predict”</p>

<p>def predict(self, instances: List[List[float]]) -&gt; List[float]:
 “””
 Send prediction request to TF Serving</p>

<p>Args:
 instances: List of feature vectors</p>

<p>Returns:
 List of predictions
 “””
 # Prepare request
 payload = {
 “signature_name”: “serving_default”,
 “instances”: instances
 }</p>

<p># Send request
 response = requests.post(
 self.endpoint,
 data=json.dumps(payload),
 headers={‘Content-Type’: ‘application/json’}
 )</p>

<p>if response.status_code != 200:
 raise Exception(f”Prediction failed: {response.text}”)</p>

<p># Parse response
 result = response.json()
 predictions = result[‘predictions’]</p>

<p>return predictions</p>

<h1 id="usage-2">Usage</h1>
<p>tf_client = TensorFlowServingClient(
 server_url=’http://localhost:8501’,
 model_name=’recommendation_model’,
 model_version=’3’
)</p>

<h1 id="predict">Predict</h1>
<p>features = [[0.1, 0.5, 0.3, 0.9]]
predictions = tf_client.predict(features)
print(f”Prediction: {predictions[0]}”)
``</p>

<hr />

<h2 id="hybrid-approach">Hybrid Approach</h2>

<p>Combine batch and real-time for optimal performance.</p>

<h3 id="architecture-2">Architecture</h3>

<p><code class="language-plaintext highlighter-rouge">
┌────────────────────────────────────────────────────────┐
│ Hybrid Inference System │
├────────────────────────────────────────────────────────┤
│ │
│ ┌────────────────┐ ┌────────────────┐ │
│ │ Batch Pipeline │ │ Real-Time API │ │
│ │ (Daily) │ │ │ │
│ └───────┬────────┘ └───────┬────────┘ │
│ │ │ │
│ ▼ ▼ │
│ ┌──────────────────────────────────────────┐ │
│ │ Prediction Cache (Redis) │ │
│ │ ┌────────────┐ ┌────────────┐ │ │
│ │ │ Batch │ │ Real-time │ │ │
│ │ │ Predictions│ │ Predictions│ │ │
│ │ │ (TTL: 24h) │ │ (TTL: 1h) │ │ │
│ │ └────────────┘ └────────────┘ │ │
│ └──────────────────────────────────────────┘ │
│ ▲ │
│ │ │
│ ┌──────┴────────┐ │
│ │ Application │ │
│ │ 1. Check cache│ │
│ │ 2. Fallback to│ │
│ │ real-time │ │
│ └───────────────┘ │
│ │
└────────────────────────────────────────────────────────┘
</code></p>

<h3 id="implementation-2">Implementation</h3>

<p>``python
class HybridInferenceSystem:
 “””
 Hybrid system: batch + real-time</p>

<ul>
  <li>Fast path: Use batch predictions if available</li>
  <li>Slow path: Compute real-time if needed
 “””</li>
</ul>

<p>def <strong>init</strong>(self, batch_system, realtime_system):
 self.batch = batch_system
 self.realtime = realtime_system
 self.cache_hit_counter = 0
 self.cache_miss_counter = 0</p>

<p>def predict(self, entity_id: str, max_staleness_hours: int = 24) -&gt; Dict:
 “””
 Get prediction with automatic fallback</p>

<p>Args:
 entity_id: Entity to predict for
 max_staleness_hours: Maximum age of batch prediction</p>

<p>Returns:
 {
 ‘prediction’: float,
 ‘source’: ‘batch’ | ‘realtime’,
 ‘timestamp’: float
 }
 “””
 # Try batch prediction first
 batch_pred_value = self.batch.get_prediction(entity_id)</p>

<p>if batch_pred_value is not None:
 # If batch system returns only a float, treat as fresh within TTL of Redis
 self.cache_hit_counter += 1
 return {
 ‘prediction’: batch_pred_value,
 ‘source’: ‘batch’,
 ‘timestamp’: time.time(),
 ‘cache_hit’: True
 }</p>

<p># Fallback to real-time
 self.cache_miss_counter += 1</p>

<p>realtime_pred = self.realtime.predict(entity_id)</p>

<p>return {
 ‘prediction’: realtime_pred,
 ‘source’: ‘realtime’,
 ‘timestamp’: time.time(),
 ‘cache_hit’: False
 }</p>

<p>def get_cache_hit_rate(self) -&gt; float:
 “"”Calculate cache hit rate”””
 total = self.cache_hit_counter + self.cache_miss_counter
 if total == 0:
 return 0.0
 return self.cache_hit_counter / total</p>

<h1 id="usage-3">Usage</h1>
<p>hybrid = HybridInferenceSystem(batch_system, realtime_service)</p>

<h1 id="predict-for-user">Predict for user</h1>
<p>result = hybrid.predict(‘user_12345’, max_staleness_hours=12)</p>

<p>print(f”Prediction: {result[‘prediction’]}”)
print(f”Source: {result[‘source’]}”)
print(f”Cache hit rate: {hybrid.get_cache_hit_rate():.2%}”)
``</p>

<hr />

<h2 id="decision-framework">Decision Framework</h2>

<p>When to use which approach:</p>

<h3 id="use-batch-inference-when">Use Batch Inference When:</h3>

<p>✅ <strong>Latency is not critical</strong> (recommendations, email campaigns) 
✅ <strong>Predictions needed for all entities</strong> (e.g., all users) 
✅ <strong>Features are expensive to compute</strong> 
✅ <strong>Model is large/slow</strong> 
✅ <strong>Cost optimization is priority</strong> 
✅ <strong>Predictions don’t change frequently</strong></p>

<p><strong>Examples:</strong></p>
<ul>
  <li>Daily email recommendations</li>
  <li>Product catalog rankings</li>
  <li>Weekly personalized content</li>
  <li>Batch fraud scoring</li>
</ul>

<h3 id="use-real-time-inference-when">Use Real-Time Inference When:</h3>

<p>✅ <strong>Low latency required</strong> (&lt; 100ms) 
✅ <strong>Fresh features critical</strong> (current context) 
✅ <strong>Predictions for small subset</strong> (active users) 
✅ <strong>Immediate user feedback</strong> (search, ads) 
✅ <strong>High-value decisions</strong> (fraud detection)</p>

<p><strong>Examples:</strong></p>
<ul>
  <li>Search ranking</li>
  <li>Ad serving</li>
  <li>Real-time fraud detection</li>
  <li>Live recommendation widgets</li>
</ul>

<h3 id="use-hybrid-when">Use Hybrid When:</h3>

<p>✅ <strong>Mix of latency requirements</strong> 
✅ <strong>Want cost + performance</strong> 
✅ <strong>Can tolerate some staleness</strong> 
✅ <strong>Variable traffic patterns</strong> 
✅ <strong>Graceful degradation needed</strong></p>

<p><strong>Examples:</strong></p>
<ul>
  <li>Homepage recommendations (batch) + search (real-time)</li>
  <li>Social feed (batch) + stories (real-time)</li>
  <li>Product pages (batch) + checkout (real-time)</li>
</ul>

<hr />

<h2 id="cost-comparison">Cost Comparison</h2>

<p>``python
class CostAnalyzer:
 “””
 Estimate costs for batch vs real-time
 “””</p>

<p>def estimate_batch_cost(
 self,
 num_entities: int,
 predictions_per_day: int,
 cost_per_compute_hour: float = 3.0
 ) -&gt; Dict:
 “"”Estimate daily batch inference cost”””</p>

<p># Assume 10K predictions/second throughput
 throughput = 10_000</p>

<p># Total predictions
 total_preds = num_entities * predictions_per_day</p>

<p># Compute time needed
 compute_seconds = total_preds / throughput
 compute_hours = compute_seconds / 3600</p>

<p># Cost
 compute_cost = compute_hours * cost_per_compute_hour</p>

<p># Storage cost (Redis/DDB)
 storage_gb = total_preds * 100 / 1e9 # 100 bytes per prediction
 storage_cost = storage_gb * 0.25 # $0.25/GB/month</p>

<p>total_cost = compute_cost + storage_cost</p>

<p>return {
 ‘compute_hours’: compute_hours,
 ‘compute_cost’: compute_cost,
 ‘storage_cost’: storage_cost,
 ‘total_daily_cost’: total_cost,
 ‘cost_per_prediction’: total_cost / total_preds
 }</p>

<p>def estimate_realtime_cost(
 self,
 requests_per_second: int,
 cost_per_instance_hour: float = 5.0,
 requests_per_instance: int = 100
 ) -&gt; Dict:
 “"”Estimate real-time serving cost”””</p>

<p># Number of instances needed
 num_instances = requests_per_second / requests_per_instance
 num_instances = int(np.ceil(num_instances * 1.5)) # 50% headroom</p>

<p># Daily cost
 daily_hours = 24
 daily_cost = num_instances * cost_per_instance_hour * daily_hours</p>

<p># Predictions per day
 daily_requests = requests_per_second * 86400</p>

<p>return {
 ‘num_instances’: num_instances,
 ‘daily_cost’: daily_cost,
 ‘cost_per_prediction’: daily_cost / daily_requests
 }</p>

<h1 id="compare-costs">Compare costs</h1>
<p>analyzer = CostAnalyzer()</p>

<h1 id="batch-1m-users-predict-onceday">Batch: 1M users, predict once/day</h1>
<p>batch_cost = analyzer.estimate_batch_cost(
 num_entities=1_000_000,
 predictions_per_day=1
)</p>

<p>print(“Batch Inference:”)
print(f” Daily cost: ${batch_cost[‘total_daily_cost’]:.2f}”)
print(f” Cost per prediction: ${batch_cost[‘cost_per_prediction’]:.6f}”)</p>

<h1 id="real-time-100-qps-average">Real-time: 100 QPS average</h1>
<p>realtime_cost = analyzer.estimate_realtime_cost(
 requests_per_second=100
)</p>

<p>print(“\nReal-Time Inference:”)
print(f” Daily cost: ${realtime_cost[‘daily_cost’]:.2f}”)
print(f” Cost per prediction: ${realtime_cost[‘cost_per_prediction’]:.6f}”)</p>

<h1 id="compare">Compare</h1>
<p>savings = (realtime_cost[‘daily_cost’] - batch_cost[‘total_daily_cost’]) / realtime_cost[‘daily_cost’] * 100
print(f”\nBatch is {savings:.1f}% cheaper!”)
``</p>

<hr />

<h2 id="advanced-patterns">Advanced Patterns</h2>

<h3 id="multi-tier-caching">Multi-Tier Caching</h3>

<p>Layer multiple caches for optimal performance.</p>

<p>``python
class MultiTierInferenceSystem:
 “””
 Multi-tier caching: Memory → Redis → Compute</p>

<p>Optimizes for different latency/cost profiles
 “””</p>

<p>def <strong>init</strong>(self, model, redis_client):
 self.model = model
 self.redis = redis_client</p>

<p># In-memory cache (fastest)
 self.memory_cache = {}
 self.memory_cache_size = 10000</p>

<p># Statistics
 self.stats = {
 ‘memory_hits’: 0,
 ‘redis_hits’: 0,
 ‘compute’: 0,
 ‘total_requests’: 0
 }</p>

<p>def predict(self, entity_id: str) -&gt; float:
 “””
 Predict with multi-tier caching</p>

<p>Tier 1: In-memory cache (~1ms)
 Tier 2: Redis cache (~5ms)
 Tier 3: Compute prediction (~50ms)
 “””
 self.stats[‘total_requests’] += 1</p>

<p># Tier 1: Memory cache
 if entity_id in self.memory_cache:
 self.stats[‘memory_hits’] += 1
 return self.memory_cache[entity_id]</p>

<p># Tier 2: Redis cache
 redis_key = f”pred:{entity_id}”
 cached = self.redis.get(redis_key)</p>

<p>if cached is not None:
 self.stats[‘redis_hits’] += 1
 prediction = float(cached)</p>

<p># Promote to memory cache
 self._add_to_memory_cache(entity_id, prediction)</p>

<p>return prediction</p>

<p># Tier 3: Compute
 self.stats[‘compute’] += 1
 prediction = self._compute_prediction(entity_id)</p>

<p># Write to both caches
 self.redis.setex(redis_key, 3600, str(prediction)) # 1 hour TTL
 self._add_to_memory_cache(entity_id, prediction)</p>

<p>return prediction</p>

<p>def _add_to_memory_cache(self, entity_id: str, prediction: float):
 “"”Add to memory cache with LRU eviction”””
 if len(self.memory_cache) &gt;= self.memory_cache_size:
 # Simple eviction: remove first item
 # In production, use LRU cache
 self.memory_cache.pop(next(iter(self.memory_cache)))</p>

<p>self.memory_cache[entity_id] = prediction</p>

<p>def _compute_prediction(self, entity_id: str) -&gt; float:
 “"”Compute prediction from model”””
 # Fetch features
 features = self._get_features(entity_id)</p>

<p># Predict
 prediction = self.model.predict([features])[0]</p>

<p>return float(prediction)</p>

<p>def _get_features(self, entity_id: str):
 “"”Fetch features for entity”””
 # Placeholder
 return [0.1, 0.2, 0.3, 0.4, 0.5]</p>

<p>def get_cache_stats(self) -&gt; dict:
 “"”Get cache performance statistics”””
 total = self.stats[‘total_requests’]</p>

<p>if total == 0:
 return self.stats</p>

<p>return {
 **self.stats,
 ‘memory_hit_rate’: self.stats[‘memory_hits’] / total * 100,
 ‘redis_hit_rate’: self.stats[‘redis_hits’] / total * 100,
 ‘compute_rate’: self.stats[‘compute’] / total * 100,
 ‘overall_cache_hit_rate’: 
 (self.stats[‘memory_hits’] + self.stats[‘redis_hits’]) / total * 100
 }</p>

<h1 id="usage-4">Usage</h1>
<p>system = MultiTierInferenceSystem(model, redis_client)</p>

<h1 id="make-predictions">Make predictions</h1>
<p>for entity_id in [‘user_1’, ‘user_2’, ‘user_1’, ‘user_3’, ‘user_1’]:
 prediction = system.predict(entity_id)
 print(f”{entity_id}: {prediction:.4f}”)</p>

<p>stats = system.get_cache_stats()
print(f”\nCache hit rate: {stats[‘overall_cache_hit_rate’]:.1f}%”)
print(f”Memory: {stats[‘memory_hit_rate’]:.1f}%, Redis: {stats[‘redis_hit_rate’]:.1f}%, Compute: {stats[‘compute_rate’]:.1f}%”)
``</p>

<h3 id="prediction-warming">Prediction Warming</h3>

<p>Precompute predictions for likely requests.</p>

<p>``python
class PredictionWarmer:
 “””
 Warm cache with predictions for likely-to-be-requested entities</p>

<p>Use case: Preload predictions for active users
 “””</p>

<p>def <strong>init</strong>(self, model, cache):
 self.model = model
 self.cache = cache</p>

<p>def warm_predictions(
 self,
 entity_ids: List[str],
 batch_size: int = 100
 ):
 “””
 Warm cache for list of entities</p>

<p>Args:
 entity_ids: Entities to warm
 batch_size: Batch size for efficient computation
 “””
 num_warmed = 0</p>

<p>for i in range(0, len(entity_ids), batch_size):
 batch_ids = entity_ids[i:i+batch_size]</p>

<p># Batch feature fetching
 features = self._batch_get_features(batch_ids)</p>

<p># Batch prediction
 predictions = self.model.predict(features)</p>

<p># Write to cache
 for entity_id, prediction in zip(batch_ids, predictions):
 self.cache.set(f”pred:{entity_id}”, float(prediction), ex=3600)
 num_warmed += 1</p>

<p>return num_warmed</p>

<p>def _batch_get_features(self, entity_ids: List[str]):
 “"”Fetch features for multiple entities”””
 # In production: Batch query to feature store
 return [[0.1] * 5 for _ in entity_ids]</p>

<p>def warm_by_activity(
 self,
 lookback_hours: int = 24,
 top_k: int = 10000
 ):
 “””
 Warm cache for most active entities</p>

<p>Args:
 lookback_hours: Look back this many hours for activity
 top_k: Warm top K most active entities
 “””
 # Query activity logs
 active_entities = self._get_active_entities(lookback_hours, top_k)</p>

<p># Warm predictions
 num_warmed = self.warm_predictions(active_entities)</p>

<p>return {
 ‘num_warmed’: num_warmed,
 ‘lookback_hours’: lookback_hours,
 ‘timestamp’: time.time()
 }</p>

<p>def <em>get_active_entities(self, lookback_hours: int, top_k: int) -&gt; List[str]:
 “"”Get most active entities from activity logs”””
 # Placeholder: Query activity database
 return [f’user</em>{i}’ for i in range(top_k)]</p>

<h1 id="usage-warm-cache-every-hour-for-active-users">Usage: Warm cache every hour for active users</h1>
<p>warmer = PredictionWarmer(model, redis_client)</p>

<h1 id="warm-cache-for-top-10k-active-users">Warm cache for top 10K active users</h1>
<p>result = warmer.warm_by_activity(lookback_hours=1, top_k=10000)
print(f”Warmed {result[‘num_warmed’]} predictions”)
``</p>

<h3 id="conditional-batch-updates">Conditional Batch Updates</h3>

<p>Update batch predictions conditionally based on staleness/changes.</p>

<p>``python
class ConditionalBatchUpdater:
 “””
 Update batch predictions only when necessary</p>

<p>Strategies:</p>
<ul>
  <li>Update only if features changed significantly</li>
  <li>Update only if prediction is stale</li>
  <li>Update only for active entities
 “””</li>
</ul>

<p>def <strong>init</strong>(self, model, cache, feature_store):
 self.model = model
 self.cache = cache
 self.feature_store = feature_store</p>

<p>def update_if_changed(
 self,
 entity_ids: List[str],
 change_threshold: float = 0.1
 ) -&gt; dict:
 “””
 Update predictions only if features changed significantly</p>

<p>Args:
 entity_ids: Entities to check
 change_threshold: Update if features changed by this much</p>

<p>Returns:
 Statistics on updates
 “””
 num_checked = 0
 num_updated = 0</p>

<p>for entity_id in entity_ids:
 num_checked += 1</p>

<p># Get current features
 current_features = self.feature_store.get(f”features:{entity_id}”)</p>

<p># Get cached features (when prediction was made)
 cached_features = self.feature_store.get(f”cached_features:{entity_id}”)</p>

<p># Check if features changed significantly
 if self._features_changed(cached_features, current_features, change_threshold):
 # Recompute prediction
 prediction = self.model.predict([current_features])[0]</p>

<p># Update cache
 self.cache.set(f”pred:{entity_id}”, float(prediction), ex=3600)
 self.feature_store.set(f”cached_features:{entity_id}”, current_features)</p>

<p>num_updated += 1</p>

<p>return {
 ‘num_checked’: num_checked,
 ‘num_updated’: num_updated,
 ‘update_rate’: num_updated / num_checked * 100 if num_checked &gt; 0 else 0
 }</p>

<p>def _features_changed(
 self,
 old_features,
 new_features,
 threshold: float
 ) -&gt; bool:
 “"”Check if features changed significantly”””
 if old_features is None or new_features is None:
 return True</p>

<p># Compute L2 distance
 diff = np.linalg.norm(np.array(new_features) - np.array(old_features))</p>

<p>return diff &gt; threshold
``</p>

<h3 id="graceful-degradation">Graceful Degradation</h3>

<p>Handle failures gracefully with fallback strategies.</p>

<p>``python
class GracefulDegradationSystem:
 “””
 Inference system with graceful degradation</p>

<p>Fallback chain:</p>
<ol>
  <li>Try real-time prediction</li>
  <li>Fallback to batch prediction (if available)</li>
  <li>Fallback to default/fallback prediction
 “””</li>
</ol>

<p>def <strong>init</strong>(
 self,
 realtime_service,
 batch_cache,
 default_prediction: float = 0.5
 ):
 self.realtime = realtime_service
 self.batch_cache = batch_cache
 self.default_prediction = default_prediction</p>

<p># Monitoring
 self.degradation_stats = {
 ‘realtime’: 0,
 ‘batch_fallback’: 0,
 ‘default_fallback’: 0
 }</p>

<p>def predict_with_fallback(
 self,
 entity_id: str,
 max_latency_ms: int = 100
 ) -&gt; dict:
 “””
 Predict with fallback strategies</p>

<p>Args:
 entity_id: Entity to predict for
 max_latency_ms: Maximum acceptable latency</p>

<p>Returns:
 {
 ‘prediction’: float,
 ‘source’: str,
 ‘latency_ms’: float
 }
 “””
 start = time.perf_counter()</p>

<p># Try real-time prediction
 try:
 prediction = self.realtime.predict(entity_id)
 elapsed_ms = (time.perf_counter() - start) * 1000</p>

<p>if elapsed_ms &lt;= max_latency_ms:
 self.degradation_stats[‘realtime’] += 1
 return {
 ‘prediction’: prediction,
 ‘source’: ‘realtime’,
 ‘latency_ms’: elapsed_ms
 }
 except Exception as e:
 print(f”Real-time prediction failed: {e}”)</p>

<p># Fallback 1: Batch cache
 try:
 batch_pred = self.batch_cache.get(f”pred:{entity_id}”)</p>

<p>if batch_pred is not None:
 elapsed_ms = (time.perf_counter() - start) * 1000
 self.degradation_stats[‘batch_fallback’] += 1</p>

<p>return {
 ‘prediction’: float(batch_pred),
 ‘source’: ‘batch_fallback’,
 ‘latency_ms’: elapsed_ms,
 ‘warning’: ‘Using stale batch prediction’
 }
 except Exception as e:
 print(f”Batch fallback failed: {e}”)</p>

<p># Fallback 2: Default prediction
 elapsed_ms = (time.perf_counter() - start) * 1000
 self.degradation_stats[‘default_fallback’] += 1</p>

<p>return {
 ‘prediction’: self.default_prediction,
 ‘source’: ‘default_fallback’,
 ‘latency_ms’: elapsed_ms,
 ‘warning’: ‘Using default prediction - service degraded’
 }</p>

<p>def get_health_status(self) -&gt; dict:
 “"”Get system health metrics”””
 total = sum(self.degradation_stats.values())</p>

<p>if total == 0:
 return {‘status’: ‘no_traffic’}</p>

<p>realtime_rate = self.degradation_stats[‘realtime’] / total * 100</p>

<p>if realtime_rate &gt; 95:
 status = ‘healthy’
 elif realtime_rate &gt; 80:
 status = ‘degraded’
 else:
 status = ‘critical’</p>

<p>return {
 ‘status’: status,
 ‘realtime_rate’: realtime_rate,
 ‘batch_fallback_rate’: self.degradation_stats[‘batch_fallback’] / total * 100,
 ‘default_fallback_rate’: self.degradation_stats[‘default_fallback’] / total * 100,
 ‘total_requests’: total
 }
``</p>

<hr />

<h2 id="real-world-case-studies">Real-World Case Studies</h2>

<h3 id="netflix-hybrid-recommendations">Netflix: Hybrid Recommendations</h3>

<p><strong>Challenge:</strong> Personalized recommendations for 200M+ users</p>

<p><strong>Solution:</strong></p>
<ul>
  <li><strong>Batch:</strong> Precompute top-N recommendations for all users daily</li>
  <li><strong>Real-time:</strong> Rerank based on current session context</li>
  <li><strong>Result:</strong> &lt; 100ms latency with personalized results</li>
</ul>

<p><strong>Architecture:</strong>
``
Daily Batch Job (Spark)
 ↓
Precompute Top 1000 movies per user
 ↓
Store in Cassandra
 ↓
Real-time API fetches top 1000 + reranks based on:</p>
<ul>
  <li>Current time of day</li>
  <li>Device type</li>
  <li>Recent viewing history
 ↓
Return Top 20 to UI
``</li>
</ul>

<h3 id="uber-real-time-eta-prediction">Uber: Real-Time ETA Prediction</h3>

<p><strong>Challenge:</strong> Predict arrival time for millions of rides</p>

<p><strong>Solution:</strong></p>
<ul>
  <li><strong>Real-time only:</strong> ETA must reflect current traffic</li>
  <li><strong>Strategy:</strong> Fast model (&lt; 50ms inference)</li>
  <li><strong>Features:</strong> Current location, traffic data, historical patterns</li>
</ul>

<p><strong>Why not batch:</strong></p>
<ul>
  <li>Traffic changes rapidly</li>
  <li>Each ride is unique</li>
  <li>Requires current GPS coordinates</li>
</ul>

<h3 id="linkedin-people-you-may-know">LinkedIn: People You May Know</h3>

<p><strong>Challenge:</strong> Suggest connections for 800M+ users</p>

<p><strong>Solution:</strong></p>
<ul>
  <li><strong>Batch:</strong> Graph algorithms compute connection candidates (weekly)</li>
  <li><strong>Real-time:</strong> Scoring based on user activity</li>
  <li><strong>Result:</strong> Balance compute cost with personalization</li>
</ul>

<p><strong>Hybrid Strategy:</strong>
``
Weekly Batch:</p>
<ul>
  <li>Graph traversal (2nd, 3rd degree connections)</li>
  <li>Identify ~1000 candidates per user</li>
  <li>Store in candidate DB</li>
</ul>

<p>Real-time (on page load):</p>
<ul>
  <li>Fetch candidates from DB</li>
  <li>Score based on:</li>
  <li>Recent profile views</li>
  <li>Shared groups/companies</li>
  <li>Mutual connections</li>
  <li>Return top 10
``</li>
</ul>

<hr />

<h2 id="monitoring--observability">Monitoring &amp; Observability</h2>

<h3 id="key-metrics-to-track">Key Metrics to Track</h3>

<p>``python
class InferenceMetrics:
 “””
 Track comprehensive inference metrics
 “””</p>

<p>def <strong>init</strong>(self):
 self.metrics = {
 ‘latency_p50’: [],
 ‘latency_p95’: [],
 ‘latency_p99’: [],
 ‘cache_hit_rate’: [],
 ‘error_rate’: [],
 ‘throughput’: [],
 ‘cost_per_prediction’: []
 }</p>

<p>def record_prediction(
 self,
 latency_ms: float,
 cache_hit: bool,
 error: bool,
 cost: float
 ):
 “"”Record single prediction metrics”””
 pass # Implementation details</p>

<p>def get_dashboard_metrics(self) -&gt; dict:
 “””
 Get metrics for monitoring dashboard</p>

<p>Returns:
 Key metrics for alerting
 “””
 return {
 ‘latency_p50_ms’: np.median(self.metrics[‘latency_p50’]),
 ‘latency_p99_ms’: np.percentile(self.metrics[‘latency_p99’], 99),
 ‘cache_hit_rate’: np.mean(self.metrics[‘cache_hit_rate’]) * 100,
 ‘error_rate’: np.mean(self.metrics[‘error_rate’]) * 100,
 ‘qps’: np.mean(self.metrics[‘throughput’]),
 ‘cost_per_1k_predictions’: np.mean(self.metrics[‘cost_per_prediction’]) * 1000
 }
``</p>

<h3 id="sla-definition">SLA Definition</h3>

<p>``python
class InferenceSLA:
 “””
 Define and monitor SLA for inference service
 “””</p>

<p>def <strong>init</strong>(self):
 self.sla_targets = {
 ‘p99_latency_ms’: 100,
 ‘availability’: 99.9,
 ‘error_rate’: 0.1 # 0.1%
 }</p>

<p>def check_sla_compliance(self, metrics: dict) -&gt; dict:
 “””
 Check if current metrics meet SLA</p>

<p>Returns:
 SLA compliance report
 “””
 compliance = {}</p>

<p>for metric, target in self.sla_targets.items():
 actual = metrics.get(metric, 0)</p>

<p>if metric == ‘error_rate’:
 # Lower is better
 meets_sla = actual &lt;= target
 else:
 # Check if within range (e.g., latency or availability)
 meets_sla = actual &lt;= target if ‘latency’ in metric else actual &gt;= target</p>

<p>compliance[metric] = {
 ‘target’: target,
 ‘actual’: actual,
 ‘meets_sla’: meets_sla,
 ‘margin’: target - actual if ‘latency’ in metric or ‘error’ in metric else actual - target
 }</p>

<p>return compliance
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Batch inference</strong> precomputes predictions, cheaper, higher latency 
✅ <strong>Real-time inference</strong> computes on-demand, expensive, lower latency 
✅ <strong>Hybrid approach</strong> combines both for optimal cost/performance 
✅ <strong>Multi-tier caching</strong> (memory → Redis → compute) optimizes latency 
✅ <strong>Prediction warming</strong> preloads cache for likely requests 
✅ <strong>Conditional updates</strong> reduce unnecessary recomputation 
✅ <strong>Graceful degradation</strong> ensures reliability via fallback strategies 
✅ <strong>Latency vs cost</strong> is the fundamental trade-off 
✅ <strong>Feature freshness</strong> often determines the choice 
✅ <strong>Most systems</strong> use hybrid: batch for bulk, real-time for edge cases 
✅ <strong>Cache hit rate</strong> critical metric for hybrid systems 
✅ <strong>SLA monitoring</strong> ensures service quality</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0005-batch-realtime-inference/">arunbaby.com/ml-system-design/0005-batch-realtime-inference</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#architecture" class="page__taxonomy-item p-category" rel="tag">architecture</a><span class="sep">, </span>
    
      <a href="/tags/#batch-processing" class="page__taxonomy-item p-category" rel="tag">batch-processing</a><span class="sep">, </span>
    
      <a href="/tags/#inference" class="page__taxonomy-item p-category" rel="tag">inference</a><span class="sep">, </span>
    
      <a href="/tags/#model-serving" class="page__taxonomy-item p-category" rel="tag">model-serving</a><span class="sep">, </span>
    
      <a href="/tags/#real-time" class="page__taxonomy-item p-category" rel="tag">real-time</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0005-maximum-subarray/" rel="permalink">Maximum Subarray (Kadane’s Algorithm)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master the pattern behind online algorithms, streaming analytics, and dynamic programming, a single elegant idea powering countless production systems.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0005-speaker-recognition/" rel="permalink">Speaker Recognition &amp; Verification
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How voice assistants recognize who’s speaking, the biometric authentication powering “Hey Alexa” and personalized experiences.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0005-memory-architectures/" rel="permalink">Memory Architectures
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The difference between a Chatbot and a Partner is Memory.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Batch+vs+Real-Time+Inference%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0005-batch-realtime-inference%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0005-batch-realtime-inference%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0005-batch-realtime-inference/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0004-ab-testing-systems/" class="pagination--pager" title="A/B Testing Systems for ML">Previous</a>
    
    
      <a href="/ml-system-design/0006-model-evaluation-metrics/" class="pagination--pager" title="Model Evaluation Metrics">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
