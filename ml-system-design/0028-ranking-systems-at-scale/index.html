<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Ranking Systems at Scale - Arun Baby</title>
<meta name="description" content="How does Google search 50 billion pages in 0.1 seconds? The answer is the “Ranking Funnel”.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Ranking Systems at Scale">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0028-ranking-systems-at-scale/">


  <meta property="og:description" content="How does Google search 50 billion pages in 0.1 seconds? The answer is the “Ranking Funnel”.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Ranking Systems at Scale">
  <meta name="twitter:description" content="How does Google search 50 billion pages in 0.1 seconds? The answer is the “Ranking Funnel”.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0028-ranking-systems-at-scale/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-26T21:44:08+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0028-ranking-systems-at-scale/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Ranking Systems at Scale">
    <meta itemprop="description" content="How does Google search 50 billion pages in 0.1 seconds? The answer is the “Ranking Funnel”.">
    <meta itemprop="datePublished" content="2025-12-26T21:44:08+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0028-ranking-systems-at-scale/" itemprop="url">Ranking Systems at Scale
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-problem-definition">1. Problem Definition</a></li><li><a href="#2-high-level-architecture-the-funnel">2. High-Level Architecture: The Funnel</a></li><li><a href="#3-stage-1-retrieval-candidate-generation">3. Stage 1: Retrieval (Candidate Generation)</a></li><li><a href="#4-stage-2-l1-ranking-the-two-tower-model">4. Stage 2: L1 Ranking (The Two-Tower Model)</a></li><li><a href="#5-stage-3-l2-ranking-the-heavy-lifter">5. Stage 3: L2 Ranking (The Heavy Lifter)</a></li><li><a href="#6-stage-4-re-ranking--blending">6. Stage 4: Re-Ranking &amp; Blending</a></li><li><a href="#deep-dive-approximate-nearest-neighbors-ann">Deep Dive: Approximate Nearest Neighbors (ANN)</a><ul><li><a href="#1-hnsw-hierarchical-navigable-small-world">1. HNSW (Hierarchical Navigable Small World)</a></li><li><a href="#2-ivf-inverted-file-index">2. IVF (Inverted File Index)</a></li><li><a href="#3-scann-google">3. ScaNN (Google)</a></li></ul></li><li><a href="#deep-dive-training-the-two-tower-model">Deep Dive: Training the Two-Tower Model</a></li><li><a href="#deep-dive-learning-to-rank-ltr-loss-functions">Deep Dive: Learning to Rank (LTR) Loss Functions</a><ul><li><a href="#1-pointwise-rmse--sigmoid">1. Pointwise (RMSE / Sigmoid)</a></li><li><a href="#2-pairwise-ranknet--lambdarank">2. Pairwise (RankNet / LambdaRank)</a></li><li><a href="#3-listwise-lambdamart">3. Listwise (LambdaMART)</a></li></ul></li><li><a href="#deep-dive-dlrm-deep-learning-recommendation-model">Deep Dive: DLRM (Deep Learning Recommendation Model)</a></li><li><a href="#deep-dive-the-feature-store-feast">Deep Dive: The Feature Store (Feast)</a></li><li><a href="#deep-dive-ab-testing-for-ranking">Deep Dive: A/B Testing for Ranking</a></li><li><a href="#deep-dive-real-time-inference-triton">Deep Dive: Real-time Inference (Triton)</a></li><li><a href="#deep-dive-dlrm-deep-learning-recommendation-model-1">Deep Dive: DLRM (Deep Learning Recommendation Model)</a></li><li><a href="#deep-dive-the-feature-store-feast-1">Deep Dive: The Feature Store (Feast)</a></li><li><a href="#deep-dive-ab-testing-for-ranking-1">Deep Dive: A/B Testing for Ranking</a></li><li><a href="#deep-dive-real-time-inference-triton-1">Deep Dive: Real-time Inference (Triton)</a></li><li><a href="#deep-dive-dlrm-deep-learning-recommendation-model-2">Deep Dive: DLRM (Deep Learning Recommendation Model)</a></li><li><a href="#deep-dive-the-feature-store-feast-2">Deep Dive: The Feature Store (Feast)</a></li><li><a href="#deep-dive-ab-testing-for-ranking-2">Deep Dive: A/B Testing for Ranking</a></li><li><a href="#deep-dive-real-time-inference-triton-2">Deep Dive: Real-time Inference (Triton)</a></li><li><a href="#deep-dive-dlrm-deep-learning-recommendation-model-3">Deep Dive: DLRM (Deep Learning Recommendation Model)</a></li><li><a href="#deep-dive-the-feature-store-feast-3">Deep Dive: The Feature Store (Feast)</a></li><li><a href="#deep-dive-ab-testing-for-ranking-3">Deep Dive: A/B Testing for Ranking</a></li><li><a href="#deep-dive-real-time-inference-triton-3">Deep Dive: Real-time Inference (Triton)</a></li><li><a href="#deep-dive-dlrm-deep-learning-recommendation-model-4">Deep Dive: DLRM (Deep Learning Recommendation Model)</a></li><li><a href="#deep-dive-the-feature-store-feast-4">Deep Dive: The Feature Store (Feast)</a></li><li><a href="#deep-dive-ab-testing-for-ranking-4">Deep Dive: A/B Testing for Ranking</a></li><li><a href="#deep-dive-real-time-inference-triton-4">Deep Dive: Real-time Inference (Triton)</a></li><li><a href="#deep-dive-graph-neural-networks-gnns-for-ranking">Deep Dive: Graph Neural Networks (GNNs) for Ranking</a></li><li><a href="#deep-dive-session-based-recommendation">Deep Dive: Session-Based Recommendation</a></li><li><a href="#deep-dive-calibration-why-probabilities-matter">Deep Dive: Calibration (Why Probabilities Matter)</a></li><li><a href="#deep-dive-real-time-bidding-rtb">Deep Dive: Real-time Bidding (RTB)</a></li><li><a href="#deep-dive-multi-task-learning-mtl">Deep Dive: Multi-Task Learning (MTL)</a></li><li><a href="#deep-dive-causal-inference-in-ranking">Deep Dive: Causal Inference in Ranking</a><ul><li><a href="#1-epsilon-greedy">1. Epsilon-Greedy</a></li><li><a href="#2-upper-confidence-bound-ucb">2. Upper Confidence Bound (UCB)</a></li><li><a href="#3-thompson-sampling">3. Thompson Sampling</a></li></ul></li><li><a href="#deep-dive-bias-and-fairness-in-ranking">Deep Dive: Bias and Fairness in Ranking</a></li><li><a href="#deep-dive-online-learning-ftrl">Deep Dive: Online Learning (FTRL)</a></li><li><a href="#deep-dive-evaluation-metrics-math-ndcg">Deep Dive: Evaluation Metrics Math (NDCG)</a></li><li><a href="#deep-dive-feature-engineering-for-ranking">Deep Dive: Feature Engineering for Ranking</a><ul><li><a href="#1-counting-features">1. Counting Features</a></li><li><a href="#2-crossing-features">2. Crossing Features</a></li><li><a href="#3-sequence-features">3. Sequence Features</a></li></ul></li><li><a href="#system-design-the-latency-budget">System Design: The Latency Budget</a></li><li><a href="#top-interview-questions">Top Interview Questions</a></li><li><a href="#key-takeaways--features">Key Takeaways &amp; Features</a></li><li><a href="#8-evaluation-metrics">8. Evaluation Metrics</a></li><li><a href="#9-failure-modes">9. Failure Modes</a></li><li><a href="#10-summary">10. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>How does Google search 50 billion pages in 0.1 seconds? The answer is the “Ranking Funnel”.</strong></p>

<h2 id="1-problem-definition">1. Problem Definition</h2>

<p><strong>Goal:</strong> Given a user query (or context) and a massive corpus of items (1B+), return the top (K) most relevant items sorted by relevance.</p>

<p><strong>Constraints:</strong></p>
<ul>
  <li><strong>Latency:</strong> &lt; 200ms (P99).</li>
  <li><strong>Throughput:</strong> 100k QPS.</li>
  <li><strong>Freshness:</strong> New items should be searchable within minutes.</li>
</ul>

<h2 id="2-high-level-architecture-the-funnel">2. High-Level Architecture: The Funnel</h2>

<p>We cannot score 1 billion items with a heavy BERT model. We use a <strong>Multi-Stage Cascade</strong>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Corpus: 1 Billion Items]
       |
       v
[Stage 1: Retrieval / Candidate Generation] -&gt; Selects Top 10,000
       |  (Fast, Simple, High Recall)
       v
[Stage 2: L1 Ranking (Lightweight)] --------&gt; Selects Top 500
       |  (Two-Tower, Logistic Regression)
       v
[Stage 3: L2 Ranking (Heavyweight)] --------&gt; Selects Top 10
       |  (BERT, LambdaMART, DLRM)
       v
[Stage 4: Re-Ranking / Blending] -----------&gt; Final Output
          (Diversity, Business Logic)
</code></pre></div></div>

<h2 id="3-stage-1-retrieval-candidate-generation">3. Stage 1: Retrieval (Candidate Generation)</h2>

<p><strong>Objective:</strong> High Recall. Don’t miss the relevant items.
<strong>Methods:</strong></p>
<ol>
  <li><strong>Inverted Index (BM25):</strong> Standard keyword search.</li>
  <li><strong>Vector Search (ANN):</strong> Embed query and items. Use Faiss/ScaNN to find nearest neighbors.</li>
  <li><strong>Collaborative Filtering:</strong> “Users who bought X also bought Y”.</li>
</ol>

<h2 id="4-stage-2-l1-ranking-the-two-tower-model">4. Stage 2: L1 Ranking (The Two-Tower Model)</h2>

<p><strong>Objective:</strong> Filter 10k -&gt; 500.
<strong>Model:</strong> <strong>Two-Tower Neural Network (Bi-Encoder)</strong>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   [User Features]          [Item Features]
         |                        |
    [Deep Net]               [Deep Net]
         |                        |
    [User Vector U]          [Item Vector V]
         \                      /
          \                    /
           \                  /
            Dot Product (U . V)
                    |
                  Score
</code></pre></div></div>

<p><strong>Why Two-Tower?</strong></p>
<ul>
  <li><strong>Cacheable:</strong> Item vectors (V) can be precomputed and stored in a Vector DB.</li>
  <li><strong>Fast:</strong> Inference is just a Dot Product.</li>
</ul>

<h2 id="5-stage-3-l2-ranking-the-heavy-lifter">5. Stage 3: L2 Ranking (The Heavy Lifter)</h2>

<p><strong>Objective:</strong> Precision. Order the top 500 perfectly.
<strong>Model:</strong> <strong>Cross-Encoder (BERT)</strong> or <strong>LambdaMART (GBDT)</strong>.</p>

<p><strong>Feature Interaction:</strong>
Unlike Two-Tower, Cross-Encoders feed User and Item features <em>together</em> into the network.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Input: [User_Age, Item_Price, User_History, Item_Title]</code></li>
  <li>The network learns interactions: “Young users like cheap items”.</li>
</ul>

<p><strong>Loss Functions:</strong></p>
<ul>
  <li><strong>Pointwise:</strong> RMSE / LogLoss. (Predict “Will click?”).</li>
  <li><strong>Pairwise:</strong> RankNet / LambdaRank. (Predict “Is A better than B?”).</li>
  <li><strong>Listwise:</strong> LambdaMART / Softmax. (Optimize NDCG directly).</li>
</ul>

<h2 id="6-stage-4-re-ranking--blending">6. Stage 4: Re-Ranking &amp; Blending</h2>

<p>The raw scores aren’t enough. We need:</p>
<ol>
  <li><strong>Diversity:</strong> Don’t show 10 shoes. Show shoes, socks, and laces. (MMR Algorithm).</li>
  <li><strong>Freshness:</strong> Boost new items.</li>
  <li><strong>Business Logic:</strong> Boost sponsored items (Ads).</li>
</ol>

<h2 id="deep-dive-approximate-nearest-neighbors-ann">Deep Dive: Approximate Nearest Neighbors (ANN)</h2>

<p>In Stage 1 (Retrieval), we need to find the top 1000 items closest to query vector (Q) among 1 billion item vectors (V).
Brute force scan is (O(N)). Too slow.
We use <strong>ANN</strong> algorithms.</p>

<h3 id="1-hnsw-hierarchical-navigable-small-world">1. HNSW (Hierarchical Navigable Small World)</h3>
<ul>
  <li><strong>Structure:</strong> A multi-layered graph.</li>
  <li><strong>Top Layer:</strong> Sparse long-range links (Highways).</li>
  <li><strong>Bottom Layer:</strong> Dense short-range links.</li>
  <li><strong>Search:</strong> Start at the top, greedily move closer to the target, drop down a layer, repeat.</li>
  <li><strong>Pros:</strong> Extremely fast, high recall.</li>
  <li><strong>Cons:</strong> High memory usage (stores the graph).</li>
</ul>

<h3 id="2-ivf-inverted-file-index">2. IVF (Inverted File Index)</h3>
<ul>
  <li><strong>Training:</strong> Cluster the vector space into (C) centroids (using K-Means).</li>
  <li><strong>Indexing:</strong> Assign each item to its closest centroid.</li>
  <li><strong>Search:</strong> Find the closest centroids to (Q), then scan only the items in those clusters.</li>
  <li><strong>Pros:</strong> Low memory (can be compressed with Product Quantization).</li>
</ul>

<h3 id="3-scann-google">3. ScaNN (Google)</h3>
<ul>
  <li>Optimizes the Anisotropic Quantization loss.</li>
  <li>State-of-the-art performance for inner-product search (which is what we need for Dot Product).</li>
</ul>

<h2 id="deep-dive-training-the-two-tower-model">Deep Dive: Training the Two-Tower Model</h2>

<p>How do we train the L1 Ranker?
We don’t have explicit “negative” labels (items the user <em>didn’t</em> like). We only have “clicks” (positives).</p>

<p><strong>Negative Sampling:</strong>
For every positive pair ((U, I^+)), we sample (K) negative items (I^-).</p>
<ol>
  <li><strong>Random Negatives:</strong> Pick random items from the catalog. (Easy, but too easy for the model).</li>
  <li><strong>Hard Negatives:</strong> Pick items that the model <em>thought</em> were good but the user didn’t click. (Harder, learns better boundaries).</li>
  <li><strong>In-Batch Negatives:</strong> Use the positives from <em>other</em> users in the same batch as negatives for the current user. (Efficient, GPU friendly).</li>
</ol>

<p><strong>Loss Function (Softmax Cross-Entropy):</strong>
[ L = -\log \frac{\exp(U \cdot I^+)}{\exp(U \cdot I^+) + \sum \exp(U \cdot I^-)} ]</p>

<h2 id="deep-dive-learning-to-rank-ltr-loss-functions">Deep Dive: Learning to Rank (LTR) Loss Functions</h2>

<p>In Stage 3 (L2 Ranking), we care about the <em>order</em>.</p>

<h3 id="1-pointwise-rmse--sigmoid">1. Pointwise (RMSE / Sigmoid)</h3>
<ul>
  <li>Treats each item independently.</li>
  <li>“Predict the probability of click for Item A”.</li>
  <li><strong>Problem:</strong> Doesn’t care if Item A &gt; Item B, only about the absolute score.</li>
</ul>

<h3 id="2-pairwise-ranknet--lambdarank">2. Pairwise (RankNet / LambdaRank)</h3>
<ul>
  <li>Takes pairs ((A, B)) where A is clicked and B is not.</li>
  <li>Minimizes loss if (Score(A) &lt; Score(B)).</li>
  <li><strong>Problem:</strong> Optimizes the number of inversions, not the position (NDCG). An error at rank 100 is treated same as error at rank 1.</li>
</ul>

<h3 id="3-listwise-lambdamart">3. Listwise (LambdaMART)</h3>
<ul>
  <li>LambdaMART is a Gradient Boosted Decision Tree (GBDT) method.</li>
  <li>It modifies the gradients of the pairwise loss by weighting them with the change in NDCG ((\Delta NDCG)).</li>
  <li><strong>Intuition:</strong> If swapping A and B causes a huge drop in NDCG, the gradient should be huge.</li>
  <li><strong>Status:</strong> Still the SOTA for tabular features in L2 ranking.</li>
</ul>

<h2 id="deep-dive-dlrm-deep-learning-recommendation-model">Deep Dive: DLRM (Deep Learning Recommendation Model)</h2>

<p>Facebook open-sourced DLRM. It’s the standard for L2 Ranking.</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Sparse Features (Categorical):</strong> Mapped to dense embeddings.</li>
  <li><strong>Dense Features (Numerical):</strong> Processed by an MLP (Bottom MLP).</li>
  <li><strong>Interaction:</strong> Dot Product between embeddings and MLP output.</li>
  <li><strong>Top MLP:</strong> Takes the interaction output and predicts CTR.</li>
</ol>

<p><strong>Why DLRM?</strong>
It explicitly models the interaction between sparse features (User ID, Item ID) and dense features (Age, Price).
It is optimized for training on massive clusters (Model Parallelism for embeddings, Data Parallelism for MLPs).</p>

<h2 id="deep-dive-the-feature-store-feast">Deep Dive: The Feature Store (Feast)</h2>

<p>In Stage 4 (Feature Fetching), we need low latency.
We cannot run SQL queries on a Data Warehouse (Snowflake/BigQuery) in real-time.</p>

<p><strong>The Feature Store Solution:</strong></p>
<ol>
  <li><strong>Offline Store:</strong> (S3/Parquet). Used for training. Contains months of history.</li>
  <li><strong>Online Store:</strong> (Redis/DynamoDB). Used for inference. Contains only the <em>latest</em> values.</li>
  <li><strong>Consistency:</strong> The Feature Store ensures that the Offline and Online stores are in sync (Point-in-time correctness).</li>
</ol>

<p><strong>Workflow:</strong></p>
<ul>
  <li>Data Engineering pipeline computes <code class="language-plaintext highlighter-rouge">user_last_50_clicks</code>.</li>
  <li>Pushes to Offline Store (for training tomorrow’s model).</li>
  <li>Pushes to Online Store (for serving today’s traffic).</li>
</ul>

<h2 id="deep-dive-ab-testing-for-ranking">Deep Dive: A/B Testing for Ranking</h2>

<p>How do we know the new model is better?
We run an <strong>Interleaving Experiment</strong>.</p>

<p><strong>Standard A/B Test:</strong></p>
<ul>
  <li>Group A sees Model A results.</li>
  <li>Group B sees Model B results.</li>
  <li>Compare CTR.</li>
  <li><strong>Problem:</strong> High variance. Users in Group A might just be happier people.</li>
</ul>

<p><strong>Interleaving:</strong></p>
<ul>
  <li>Show <em>every</em> user a mix of Model A and Model B results.</li>
  <li><code class="language-plaintext highlighter-rouge">Result = [A1, B1, A2, B2, ...]</code></li>
  <li>If user clicks <code class="language-plaintext highlighter-rouge">A1</code>, Model A gets a point.</li>
  <li>If user clicks <code class="language-plaintext highlighter-rouge">B1</code>, Model B gets a point.</li>
  <li><strong>Pros:</strong> Removes user variance. 100x more sensitive than standard A/B tests.</li>
</ul>

<h2 id="deep-dive-real-time-inference-triton">Deep Dive: Real-time Inference (Triton)</h2>

<p>Serving a BERT model in 50ms is hard.
<strong>NVIDIA Triton Inference Server:</strong></p>
<ul>
  <li><strong>Dynamic Batching:</strong> Groups incoming requests into a batch to maximize GPU utilization.</li>
  <li><strong>Model Ensembling:</strong> Runs Preprocessing (Python) -&gt; Model (TensorRT) -&gt; Postprocessing (Python) in a single pipeline.</li>
  <li><strong>Concurrent Execution:</strong> Runs multiple models on the same GPU.</li>
</ul>

<h2 id="deep-dive-dlrm-deep-learning-recommendation-model-1">Deep Dive: DLRM (Deep Learning Recommendation Model)</h2>

<p>Facebook open-sourced DLRM. It’s the standard for L2 Ranking.</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Sparse Features (Categorical):</strong> Mapped to dense embeddings.</li>
  <li><strong>Dense Features (Numerical):</strong> Processed by an MLP (Bottom MLP).</li>
  <li><strong>Interaction:</strong> Dot Product between embeddings and MLP output.</li>
  <li><strong>Top MLP:</strong> Takes the interaction output and predicts CTR.</li>
</ol>

<p><strong>Why DLRM?</strong>
It explicitly models the interaction between sparse features (User ID, Item ID) and dense features (Age, Price).
It is optimized for training on massive clusters (Model Parallelism for embeddings, Data Parallelism for MLPs).</p>

<h2 id="deep-dive-the-feature-store-feast-1">Deep Dive: The Feature Store (Feast)</h2>

<p>In Stage 4 (Feature Fetching), we need low latency.
We cannot run SQL queries on a Data Warehouse (Snowflake/BigQuery) in real-time.</p>

<p><strong>The Feature Store Solution:</strong></p>
<ol>
  <li><strong>Offline Store:</strong> (S3/Parquet). Used for training. Contains months of history.</li>
  <li><strong>Online Store:</strong> (Redis/DynamoDB). Used for inference. Contains only the <em>latest</em> values.</li>
  <li><strong>Consistency:</strong> The Feature Store ensures that the Offline and Online stores are in sync (Point-in-time correctness).</li>
</ol>

<p><strong>Workflow:</strong></p>
<ul>
  <li>Data Engineering pipeline computes <code class="language-plaintext highlighter-rouge">user_last_50_clicks</code>.</li>
  <li>Pushes to Offline Store (for training tomorrow’s model).</li>
  <li>Pushes to Online Store (for serving today’s traffic).</li>
</ul>

<h2 id="deep-dive-ab-testing-for-ranking-1">Deep Dive: A/B Testing for Ranking</h2>

<p>How do we know the new model is better?
We run an <strong>Interleaving Experiment</strong>.</p>

<p><strong>Standard A/B Test:</strong></p>
<ul>
  <li>Group A sees Model A results.</li>
  <li>Group B sees Model B results.</li>
  <li>Compare CTR.</li>
  <li><strong>Problem:</strong> High variance. Users in Group A might just be happier people.</li>
</ul>

<p><strong>Interleaving:</strong></p>
<ul>
  <li>Show <em>every</em> user a mix of Model A and Model B results.</li>
  <li><code class="language-plaintext highlighter-rouge">Result = [A1, B1, A2, B2, ...]</code></li>
  <li>If user clicks <code class="language-plaintext highlighter-rouge">A1</code>, Model A gets a point.</li>
  <li>If user clicks <code class="language-plaintext highlighter-rouge">B1</code>, Model B gets a point.</li>
  <li><strong>Pros:</strong> Removes user variance. 100x more sensitive than standard A/B tests.</li>
</ul>

<h2 id="deep-dive-real-time-inference-triton-1">Deep Dive: Real-time Inference (Triton)</h2>

<p>Serving a BERT model in 50ms is hard.
<strong>NVIDIA Triton Inference Server:</strong></p>
<ul>
  <li><strong>Dynamic Batching:</strong> Groups incoming requests into a batch to maximize GPU utilization.</li>
  <li><strong>Model Ensembling:</strong> Runs Preprocessing (Python) -&gt; Model (TensorRT) -&gt; Postprocessing (Python) in a single pipeline.</li>
  <li><strong>Concurrent Execution:</strong> Runs multiple models on the same GPU.</li>
</ul>

<h2 id="deep-dive-dlrm-deep-learning-recommendation-model-2">Deep Dive: DLRM (Deep Learning Recommendation Model)</h2>

<p>Facebook open-sourced DLRM. It’s the standard for L2 Ranking.</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Sparse Features (Categorical):</strong> Mapped to dense embeddings.</li>
  <li><strong>Dense Features (Numerical):</strong> Processed by an MLP (Bottom MLP).</li>
  <li><strong>Interaction:</strong> Dot Product between embeddings and MLP output.</li>
  <li><strong>Top MLP:</strong> Takes the interaction output and predicts CTR.</li>
</ol>

<p><strong>Why DLRM?</strong>
It explicitly models the interaction between sparse features (User ID, Item ID) and dense features (Age, Price).
It is optimized for training on massive clusters (Model Parallelism for embeddings, Data Parallelism for MLPs).</p>

<h2 id="deep-dive-the-feature-store-feast-2">Deep Dive: The Feature Store (Feast)</h2>

<p>In Stage 4 (Feature Fetching), we need low latency.
We cannot run SQL queries on a Data Warehouse (Snowflake/BigQuery) in real-time.</p>

<p><strong>The Feature Store Solution:</strong></p>
<ol>
  <li><strong>Offline Store:</strong> (S3/Parquet). Used for training. Contains months of history.</li>
  <li><strong>Online Store:</strong> (Redis/DynamoDB). Used for inference. Contains only the <em>latest</em> values.</li>
  <li><strong>Consistency:</strong> The Feature Store ensures that the Offline and Online stores are in sync (Point-in-time correctness).</li>
</ol>

<p><strong>Workflow:</strong></p>
<ul>
  <li>Data Engineering pipeline computes <code class="language-plaintext highlighter-rouge">user_last_50_clicks</code>.</li>
  <li>Pushes to Offline Store (for training tomorrow’s model).</li>
  <li>Pushes to Online Store (for serving today’s traffic).</li>
</ul>

<h2 id="deep-dive-ab-testing-for-ranking-2">Deep Dive: A/B Testing for Ranking</h2>

<p>How do we know the new model is better?
We run an <strong>Interleaving Experiment</strong>.</p>

<p><strong>Standard A/B Test:</strong></p>
<ul>
  <li>Group A sees Model A results.</li>
  <li>Group B sees Model B results.</li>
  <li>Compare CTR.</li>
  <li><strong>Problem:</strong> High variance. Users in Group A might just be happier people.</li>
</ul>

<p><strong>Interleaving:</strong></p>
<ul>
  <li>Show <em>every</em> user a mix of Model A and Model B results.</li>
  <li><code class="language-plaintext highlighter-rouge">Result = [A1, B1, A2, B2, ...]</code></li>
  <li>If user clicks <code class="language-plaintext highlighter-rouge">A1</code>, Model A gets a point.</li>
  <li>If user clicks <code class="language-plaintext highlighter-rouge">B1</code>, Model B gets a point.</li>
  <li><strong>Pros:</strong> Removes user variance. 100x more sensitive than standard A/B tests.</li>
</ul>

<h2 id="deep-dive-real-time-inference-triton-2">Deep Dive: Real-time Inference (Triton)</h2>

<p>Serving a BERT model in 50ms is hard.
<strong>NVIDIA Triton Inference Server:</strong></p>
<ul>
  <li><strong>Dynamic Batching:</strong> Groups incoming requests into a batch to maximize GPU utilization.</li>
  <li><strong>Model Ensembling:</strong> Runs Preprocessing (Python) -&gt; Model (TensorRT) -&gt; Postprocessing (Python) in a single pipeline.</li>
  <li><strong>Concurrent Execution:</strong> Runs multiple models on the same GPU.</li>
</ul>

<h2 id="deep-dive-dlrm-deep-learning-recommendation-model-3">Deep Dive: DLRM (Deep Learning Recommendation Model)</h2>

<p>Facebook open-sourced DLRM. It’s the standard for L2 Ranking.</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Sparse Features (Categorical):</strong> Mapped to dense embeddings.</li>
  <li><strong>Dense Features (Numerical):</strong> Processed by an MLP (Bottom MLP).</li>
  <li><strong>Interaction:</strong> Dot Product between embeddings and MLP output.</li>
  <li><strong>Top MLP:</strong> Takes the interaction output and predicts CTR.</li>
</ol>

<p><strong>Why DLRM?</strong>
It explicitly models the interaction between sparse features (User ID, Item ID) and dense features (Age, Price).
It is optimized for training on massive clusters (Model Parallelism for embeddings, Data Parallelism for MLPs).</p>

<h2 id="deep-dive-the-feature-store-feast-3">Deep Dive: The Feature Store (Feast)</h2>

<p>In Stage 4 (Feature Fetching), we need low latency.
We cannot run SQL queries on a Data Warehouse (Snowflake/BigQuery) in real-time.</p>

<p><strong>The Feature Store Solution:</strong></p>
<ol>
  <li><strong>Offline Store:</strong> (S3/Parquet). Used for training. Contains months of history.</li>
  <li><strong>Online Store:</strong> (Redis/DynamoDB). Used for inference. Contains only the <em>latest</em> values.</li>
  <li><strong>Consistency:</strong> The Feature Store ensures that the Offline and Online stores are in sync (Point-in-time correctness).</li>
</ol>

<p><strong>Workflow:</strong></p>
<ul>
  <li>Data Engineering pipeline computes <code class="language-plaintext highlighter-rouge">user_last_50_clicks</code>.</li>
  <li>Pushes to Offline Store (for training tomorrow’s model).</li>
  <li>Pushes to Online Store (for serving today’s traffic).</li>
</ul>

<h2 id="deep-dive-ab-testing-for-ranking-3">Deep Dive: A/B Testing for Ranking</h2>

<p>How do we know the new model is better?
We run an <strong>Interleaving Experiment</strong>.</p>

<p><strong>Standard A/B Test:</strong></p>
<ul>
  <li>Group A sees Model A results.</li>
  <li>Group B sees Model B results.</li>
  <li>Compare CTR.</li>
  <li><strong>Problem:</strong> High variance. Users in Group A might just be happier people.</li>
</ul>

<p><strong>Interleaving:</strong></p>
<ul>
  <li>Show <em>every</em> user a mix of Model A and Model B results.</li>
  <li><code class="language-plaintext highlighter-rouge">Result = [A1, B1, A2, B2, ...]</code></li>
  <li>If user clicks <code class="language-plaintext highlighter-rouge">A1</code>, Model A gets a point.</li>
  <li>If user clicks <code class="language-plaintext highlighter-rouge">B1</code>, Model B gets a point.</li>
  <li><strong>Pros:</strong> Removes user variance. 100x more sensitive than standard A/B tests.</li>
</ul>

<h2 id="deep-dive-real-time-inference-triton-3">Deep Dive: Real-time Inference (Triton)</h2>

<p>Serving a BERT model in 50ms is hard.
<strong>NVIDIA Triton Inference Server:</strong></p>
<ul>
  <li><strong>Dynamic Batching:</strong> Groups incoming requests into a batch to maximize GPU utilization.</li>
  <li><strong>Model Ensembling:</strong> Runs Preprocessing (Python) -&gt; Model (TensorRT) -&gt; Postprocessing (Python) in a single pipeline.</li>
  <li><strong>Concurrent Execution:</strong> Runs multiple models on the same GPU.</li>
</ul>

<h2 id="deep-dive-dlrm-deep-learning-recommendation-model-4">Deep Dive: DLRM (Deep Learning Recommendation Model)</h2>

<p>Facebook open-sourced DLRM. It’s the standard for L2 Ranking.</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Sparse Features (Categorical):</strong> Mapped to dense embeddings.</li>
  <li><strong>Dense Features (Numerical):</strong> Processed by an MLP (Bottom MLP).</li>
  <li><strong>Interaction:</strong> Dot Product between embeddings and MLP output.</li>
  <li><strong>Top MLP:</strong> Takes the interaction output and predicts CTR.</li>
</ol>

<p><strong>Why DLRM?</strong>
It explicitly models the interaction between sparse features (User ID, Item ID) and dense features (Age, Price).
It is optimized for training on massive clusters (Model Parallelism for embeddings, Data Parallelism for MLPs).</p>

<h2 id="deep-dive-the-feature-store-feast-4">Deep Dive: The Feature Store (Feast)</h2>

<p>In Stage 4 (Feature Fetching), we need low latency.
We cannot run SQL queries on a Data Warehouse (Snowflake/BigQuery) in real-time.</p>

<p><strong>The Feature Store Solution:</strong></p>
<ol>
  <li><strong>Offline Store:</strong> (S3/Parquet). Used for training. Contains months of history.</li>
  <li><strong>Online Store:</strong> (Redis/DynamoDB). Used for inference. Contains only the <em>latest</em> values.</li>
  <li><strong>Consistency:</strong> The Feature Store ensures that the Offline and Online stores are in sync (Point-in-time correctness).</li>
</ol>

<p><strong>Workflow:</strong></p>
<ul>
  <li>Data Engineering pipeline computes <code class="language-plaintext highlighter-rouge">user_last_50_clicks</code>.</li>
  <li>Pushes to Offline Store (for training tomorrow’s model).</li>
  <li>Pushes to Online Store (for serving today’s traffic).</li>
</ul>

<h2 id="deep-dive-ab-testing-for-ranking-4">Deep Dive: A/B Testing for Ranking</h2>

<p>How do we know the new model is better?
We run an <strong>Interleaving Experiment</strong>.</p>

<p><strong>Standard A/B Test:</strong></p>
<ul>
  <li>Group A sees Model A results.</li>
  <li>Group B sees Model B results.</li>
  <li>Compare CTR.</li>
  <li><strong>Problem:</strong> High variance. Users in Group A might just be happier people.</li>
</ul>

<p><strong>Interleaving:</strong></p>
<ul>
  <li>Show <em>every</em> user a mix of Model A and Model B results.</li>
  <li><code class="language-plaintext highlighter-rouge">Result = [A1, B1, A2, B2, ...]</code></li>
  <li>If user clicks <code class="language-plaintext highlighter-rouge">A1</code>, Model A gets a point.</li>
  <li>If user clicks <code class="language-plaintext highlighter-rouge">B1</code>, Model B gets a point.</li>
  <li><strong>Pros:</strong> Removes user variance. 100x more sensitive than standard A/B tests.</li>
</ul>

<h2 id="deep-dive-real-time-inference-triton-4">Deep Dive: Real-time Inference (Triton)</h2>

<p>Serving a BERT model in 50ms is hard.
<strong>NVIDIA Triton Inference Server:</strong></p>
<ul>
  <li><strong>Dynamic Batching:</strong> Groups incoming requests into a batch to maximize GPU utilization.</li>
  <li><strong>Model Ensembling:</strong> Runs Preprocessing (Python) -&gt; Model (TensorRT) -&gt; Postprocessing (Python) in a single pipeline.</li>
  <li><strong>Concurrent Execution:</strong> Runs multiple models on the same GPU.</li>
</ul>

<h2 id="deep-dive-graph-neural-networks-gnns-for-ranking">Deep Dive: Graph Neural Networks (GNNs) for Ranking</h2>

<p>Pinterest uses <strong>PinSage</strong> (GraphSAGE).</p>
<ul>
  <li><strong>Graph:</strong> Users and Items are nodes. Interactions (Clicks/Pins) are edges.</li>
  <li><strong>Idea:</strong> An item is defined by the users who pinned it. A user is defined by the items they pinned.</li>
  <li><strong>Convolution:</strong> Aggregate features from neighbors (and neighbors of neighbors).</li>
  <li><strong>Benefit:</strong> Captures higher-order structure. “Users who bought X also bought Y” is 1-hop. GNNs capture 2-hop and 3-hop signals.</li>
</ul>

<h2 id="deep-dive-session-based-recommendation">Deep Dive: Session-Based Recommendation</h2>

<p>What if the user is anonymous (incognito)? We have no history.
We only have the current session: <code class="language-plaintext highlighter-rouge">[Item A, Item B, Item C, ...]</code>.
<strong>Model:</strong> GRU4Rec (Gated Recurrent Unit).</p>
<ul>
  <li>Input: Sequence of item embeddings.</li>
  <li>Output: Predicted next item.</li>
  <li><strong>Mechanism:</strong> The hidden state evolves with each click, capturing the “Current Intent” (e.g., “Shopping for shoes”).</li>
  <li><strong>Loss:</strong> Pairwise Ranking Loss (BPR).</li>
</ul>

<h2 id="deep-dive-calibration-why-probabilities-matter">Deep Dive: Calibration (Why Probabilities Matter)</h2>

<p>In Ranking, we often just care about the <em>order</em>.
But in <strong>Ads Ranking</strong>, the <em>absolute probability</em> matters.
<strong>Why?</strong></p>
<ul>
  <li><strong>Bid:</strong> Advertiser pays $1.00 per click.</li>
  <li><strong>Expected Revenue:</strong> ( \text{Bid} \times P(\text{Click}) ).</li>
  <li>If model predicts 0.2 but real probability is 0.1, we over-estimate revenue and show the wrong ad.</li>
</ul>

<p><strong>Calibration Techniques:</strong></p>
<ol>
  <li><strong>Platt Scaling:</strong> Fit a Logistic Regression on the output logits.</li>
  <li><strong>Isotonic Regression:</strong> Fit a non-decreasing free-form line. (More flexible, requires more data).</li>
  <li><strong>Reliability Diagram:</strong> Plot “Predicted Probability” vs “Actual Frequency”. Ideally, it’s a diagonal line (y=x).</li>
</ol>

<h2 id="deep-dive-real-time-bidding-rtb">Deep Dive: Real-time Bidding (RTB)</h2>

<p>In Ad Tech, ranking happens in an auction.</p>
<ol>
  <li><strong>User visits page.</strong></li>
  <li><strong>Request sent to Ad Exchange.</strong></li>
  <li><strong>Exchange asks DSPs (Demand Side Platforms):</strong> “How much for this user?”</li>
  <li><strong>DSP runs Ranking Model:</strong> Predicts CTR and CVR (Conversion Rate).</li>
  <li><strong>Bid Calculation:</strong> (\text{Bid} = \text{CVR} \times \text{Value} \times \text{Pacing_Factor}).</li>
  <li><strong>Auction:</strong> Highest bid wins.</li>
  <li><strong>Latency:</strong> All this must happen in &lt; 100ms.</li>
</ol>

<h2 id="deep-dive-multi-task-learning-mtl">Deep Dive: Multi-Task Learning (MTL)</h2>

<p>We often want to optimize multiple objectives:</p>
<ul>
  <li><strong>CTR:</strong> Click-Through Rate.</li>
  <li><strong>CVR:</strong> Conversion Rate (Purchase).</li>
  <li><strong>Dwell Time:</strong> Time spent.</li>
</ul>

<p><strong>Shared-Bottom Architecture:</strong></p>
<ul>
  <li><strong>Shared Layers:</strong> Learn generic features (User embedding, Item embedding).</li>
  <li><strong>Tower Layers:</strong> Specific to each task.
    <ul>
      <li>CTR Tower -&gt; Sigmoid.</li>
      <li>CVR Tower -&gt; Sigmoid.</li>
    </ul>
  </li>
  <li><strong>Loss:</strong> ( L = w_1 L_{CTR} + w_2 L_{CVR} ).</li>
  <li><strong>Benefit:</strong> Transfer learning. Learning to predict clicks helps predict conversions (and vice versa).</li>
</ul>

<h2 id="deep-dive-causal-inference-in-ranking">Deep Dive: Causal Inference in Ranking</h2>

<p>The “Cold Start” problem is real. If we never show a new item, we never get data on it.
We treat this as a <strong>Multi-Armed Bandit</strong> problem.</p>

<h3 id="1-epsilon-greedy">1. Epsilon-Greedy</h3>
<ul>
  <li>With probability (1-\epsilon), show the best item (Exploit).</li>
  <li>With probability (\epsilon), show a random item (Explore).</li>
  <li><strong>Pros:</strong> Simple.</li>
  <li><strong>Cons:</strong> “Random” items might be terrible.</li>
</ul>

<h3 id="2-upper-confidence-bound-ucb">2. Upper Confidence Bound (UCB)</h3>
<ul>
  <li>Calculate the mean CTR (\mu) and the variance (\sigma).</li>
  <li>Score = (\mu + \alpha \cdot \sigma).</li>
  <li><strong>Intuition:</strong> Boost items we are uncertain about (high variance).</li>
  <li>As we get more data, (\sigma) decreases, and we rely more on (\mu).</li>
</ul>

<h3 id="3-thompson-sampling">3. Thompson Sampling</h3>
<ul>
  <li>Model the CTR as a Beta distribution (Beta(\alpha, \beta)).</li>
  <li>Sample a value from this distribution for each item. Rank by sampled value.</li>
  <li><strong>Pros:</strong> Mathematically optimal for minimizing regret.</li>
</ul>

<h2 id="deep-dive-bias-and-fairness-in-ranking">Deep Dive: Bias and Fairness in Ranking</h2>

<p>Ranking systems can amplify bias.</p>
<ul>
  <li><strong>Popularity Bias:</strong> The rich get richer. Popular items get shown more, get more clicks, and become even more popular.</li>
  <li><strong>Position Bias:</strong> Top items get clicked because they are top.</li>
</ul>

<p><strong>Mitigation:</strong></p>
<ol>
  <li><strong>Inverse Propensity Weighting (IPW):</strong>
    <ul>
      <li>Weight clicks by the inverse probability of the user seeing the item.</li>
      <li>If item X was at rank 10 (low probability of being seen) but got clicked, it’s a <em>very</em> strong signal.</li>
    </ul>
  </li>
  <li><strong>Fairness Constraints:</strong>
    <ul>
      <li>Ensure that the top 10 results contain at least 2 items from “Small Creators”.</li>
      <li>This is a constrained optimization problem.</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-online-learning-ftrl">Deep Dive: Online Learning (FTRL)</h2>

<p>In fast-moving domains (News, Ads), a model trained yesterday is stale.
We need <strong>Online Learning</strong>.</p>

<p><strong>FTRL (Follow The Regularized Leader):</strong></p>
<ul>
  <li>An optimization algorithm designed for sparse data and online updates.</li>
  <li>It updates the weights after <em>every</em> batch of clicks.</li>
  <li>Used heavily in Ad Click Prediction (Logistic Regression).</li>
  <li><strong>Architecture:</strong>
    <ul>
      <li><strong>Wide Part:</strong> FTRL (Memorization).</li>
      <li><strong>Deep Part:</strong> DNN (Generalization).</li>
      <li><strong>Wide &amp; Deep Learning (Google).</strong></li>
    </ul>
  </li>
</ul>

<h2 id="deep-dive-evaluation-metrics-math-ndcg">Deep Dive: Evaluation Metrics Math (NDCG)</h2>

<p>Why do we use NDCG instead of Accuracy?
Because order matters.</p>

<p><strong>CG (Cumulative Gain):</strong>
[ CG_p = \sum_{i=1}^p rel_i ]
Sum of relevance scores. (Does not care about order).</p>

<p><strong>DCG (Discounted Cumulative Gain):</strong>
[ DCG_p = \sum_{i=1}^p \frac{rel_i}{\log_2(i+1)} ]
Penalizes relevant items appearing lower in the list.</p>

<p><strong>IDCG (Ideal DCG):</strong>
The DCG of the <em>perfect</em> ordering.</p>

<p><strong>NDCG (Normalized DCG):</strong>
[ NDCG_p = \frac{DCG_p}{IDCG_p} ]
Values are between 0 and 1. Comparable across queries.</p>

<h2 id="deep-dive-feature-engineering-for-ranking">Deep Dive: Feature Engineering for Ranking</h2>

<p>Features are the lifeblood of ranking.</p>

<h3 id="1-counting-features">1. Counting Features</h3>
<ul>
  <li>“How many times has this user clicked this category?”</li>
  <li>“How many times has this item been bought in the last hour?”</li>
  <li><strong>Implementation:</strong> Count-Min Sketch or Redis counters.</li>
</ul>

<h3 id="2-crossing-features">2. Crossing Features</h3>
<ul>
  <li><code class="language-plaintext highlighter-rouge">User_Country x Item_Country</code>: (Match vs Mismatch).</li>
  <li><code class="language-plaintext highlighter-rouge">User_Gender x Item_Category</code>.</li>
</ul>

<h3 id="3-sequence-features">3. Sequence Features</h3>
<ul>
  <li>“Last 50 items viewed”.</li>
  <li>Processed by a Transformer or GRU to generate a dynamic user embedding.</li>
</ul>

<h2 id="system-design-the-latency-budget">System Design: The Latency Budget</h2>

<p>We have 200ms total. How do we spend it?</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Component</th>
      <th style="text-align: left">Budget</th>
      <th style="text-align: left">Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Network Overhead</strong></td>
      <td style="text-align: left">30ms</td>
      <td style="text-align: left">Round trip to client.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Query Understanding</strong></td>
      <td style="text-align: left">20ms</td>
      <td style="text-align: left">BERT is slow; use DistilBERT or caching.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Retrieval (ANN)</strong></td>
      <td style="text-align: left">20ms</td>
      <td style="text-align: left">Parallelize across shards.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>L1 Ranking</strong></td>
      <td style="text-align: left">30ms</td>
      <td style="text-align: left">Dot products are fast.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Feature Fetching</strong></td>
      <td style="text-align: left">40ms</td>
      <td style="text-align: left">The bottleneck! Fetching features for 500 items from Feature Store (Cassandra/Redis).</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>L2 Ranking</strong></td>
      <td style="text-align: left">50ms</td>
      <td style="text-align: left">Heavy model inference.</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Re-Ranking</strong></td>
      <td style="text-align: left">10ms</td>
      <td style="text-align: left">Logic heavy, compute light.</td>
    </tr>
  </tbody>
</table>

<p><strong>Optimization:</strong></p>
<ul>
  <li><strong>Parallelism:</strong> Run Retrieval and L1 for different sources (Ads, Organic) in parallel.</li>
  <li><strong>Feature Caching:</strong> Cache hot item features in local memory.</li>
</ul>

<h2 id="top-interview-questions">Top Interview Questions</h2>

<p><strong>Q1: How do you handle “Position Bias” in training data?</strong>
<em>Answer:</em>
Users click the top result because it’s top.</p>
<ol>
  <li><strong>Randomization:</strong> Shuffle the top 3 results for 1% of traffic.</li>
  <li><strong>Counterfactual Model:</strong> Add <code class="language-plaintext highlighter-rouge">Position</code> as a feature during training. During inference, set <code class="language-plaintext highlighter-rouge">Position = 0</code> (or <code class="language-plaintext highlighter-rouge">Position = 1</code>) for all items. This asks the model: “How relevant would this be if it were at the top?”</li>
</ol>

<p><strong>Q2: Why use Dot Product for L1 and Trees/BERT for L2?</strong>
<em>Answer:</em></p>
<ul>
  <li><strong>L1:</strong> Needs to scan millions of items. Dot product allows using ANN indices (MIPS). Trees/BERT cannot be indexed easily.</li>
  <li><strong>L2:</strong> Needs precision on a small set (500). Trees/BERT capture non-linear feature interactions (e.g., “User likes Sci-Fi AND Item is Star Wars”) better than a simple Dot Product.</li>
</ul>

<p><strong>Q3: How do you evaluate a ranking system offline?</strong>
<em>Answer:</em>
We use <strong>Replay Evaluation</strong>.
Take a historical log: User saw <code class="language-plaintext highlighter-rouge">[A, B, C]</code>, clicked <code class="language-plaintext highlighter-rouge">B</code>.
Run the new model on the context.
If the new model ranks <code class="language-plaintext highlighter-rouge">B</code> at position 1, it wins.
<strong>Metric:</strong> NDCG (Normalized Discounted Cumulative Gain) or MRR (Mean Reciprocal Rank).</p>

<h2 id="key-takeaways--features">Key Takeaways &amp; Features</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Feature Type</th>
      <th style="text-align: left">Examples</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>User</strong></td>
      <td style="text-align: left">Age, Gender, Past Clicks, Search History</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Item</strong></td>
      <td style="text-align: left">Title, Price, Category, CTR (Click-Through Rate)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Context</strong></td>
      <td style="text-align: left">Time of Day, Device, Location</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Interaction</strong></td>
      <td style="text-align: left">User-Category Affinity, Last-Viewed-Item Similarity</td>
    </tr>
  </tbody>
</table>

<p><strong>Positional Bias:</strong>
Users click the top result simply because it’s at the top.
<strong>Fix:</strong> Train with “Position” as a feature, but set <code class="language-plaintext highlighter-rouge">Position=0</code> during inference (Counterfactual Inference).</p>

<h2 id="8-evaluation-metrics">8. Evaluation Metrics</h2>

<ul>
  <li><strong>Offline:</strong>
    <ul>
      <li><strong>NDCG@K:</strong> Normalized Discounted Cumulative Gain. (Gold Standard).</li>
      <li><strong>MRR:</strong> Mean Reciprocal Rank.</li>
    </ul>
  </li>
  <li><strong>Online:</strong>
    <ul>
      <li><strong>CTR:</strong> Click-Through Rate.</li>
      <li><strong>Conversion Rate:</strong> Purchases / Clicks.</li>
      <li><strong>Dwell Time:</strong> Time spent on the page.</li>
    </ul>
  </li>
</ul>

<h2 id="9-failure-modes">9. Failure Modes</h2>

<ol>
  <li><strong>The Cold Start Problem:</strong> New items have no interaction data.
    <ul>
      <li><em>Fix:</em> Use Content-based retrieval (Embeddings) + Exploration (Bandits).</li>
    </ul>
  </li>
  <li><strong>Feedback Loops:</strong> The model only learns from what it shows. It never learns about items it <em>didn’t</em> show.
    <ul>
      <li><em>Fix:</em> Epsilon-Greedy Exploration (Show random items 1% of the time).</li>
    </ul>
  </li>
</ol>

<h2 id="10-summary">10. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Stage</th>
      <th style="text-align: left">Model</th>
      <th style="text-align: left">Input Size</th>
      <th style="text-align: left">Latency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Retrieval</strong></td>
      <td style="text-align: left">ANN / Inverted Index</td>
      <td style="text-align: left">1 Billion</td>
      <td style="text-align: left">10ms</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>L1 Ranker</strong></td>
      <td style="text-align: left">Two-Tower / LR</td>
      <td style="text-align: left">10,000</td>
      <td style="text-align: left">20ms</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>L2 Ranker</strong></td>
      <td style="text-align: left">BERT / GBDT</td>
      <td style="text-align: left">500</td>
      <td style="text-align: left">50ms</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Re-Ranker</strong></td>
      <td style="text-align: left">Heuristics</td>
      <td style="text-align: left">50</td>
      <td style="text-align: left">5ms</td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0028-ranking-systems-at-scale/">arunbaby.com/ml-system-design/0028-ranking-systems-at-scale</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#ltr" class="page__taxonomy-item p-category" rel="tag">ltr</a><span class="sep">, </span>
    
      <a href="/tags/#ranking" class="page__taxonomy-item p-category" rel="tag">ranking</a><span class="sep">, </span>
    
      <a href="/tags/#recommender-systems" class="page__taxonomy-item p-category" rel="tag">recommender_systems</a><span class="sep">, </span>
    
      <a href="/tags/#system-design" class="page__taxonomy-item p-category" rel="tag">system_design</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml_system_design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0028-kth-smallest-in-bst/" rel="permalink">Kth Smallest Element in a BST
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Finding the median or the 99th percentile is easy in a sorted array. Can we do it in a tree?
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0028-voice-search-ranking/" rel="permalink">Voice Search Ranking
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Play Call Me Maybe”. Did you mean the song, the video, or the contact named ‘Maybe’?
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0028-database-interaction-agents/" rel="permalink">Database Interaction Agents: Mastering Text-to-SQL &amp; Beyond
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Democratizing data access through natural language.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Ranking+Systems+at+Scale%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0028-ranking-systems-at-scale%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0028-ranking-systems-at-scale%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0028-ranking-systems-at-scale/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0027-model-architecture-design/" class="pagination--pager" title="Model Architecture Design">Previous</a>
    
    
      <a href="/ml-system-design/0029-hierarchical-classification/" class="pagination--pager" title="Hierarchical Classification Systems">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
