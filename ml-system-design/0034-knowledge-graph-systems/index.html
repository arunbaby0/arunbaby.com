<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Knowledge Graph Systems - Arun Baby</title>
<meta name="description" content="“Structuring the world’s information into connected entities and relationships.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Knowledge Graph Systems">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0034-knowledge-graph-systems/">


  <meta property="og:description" content="“Structuring the world’s information into connected entities and relationships.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Knowledge Graph Systems">
  <meta name="twitter:description" content="“Structuring the world’s information into connected entities and relationships.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0034-knowledge-graph-systems/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:07:50+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0034-knowledge-graph-systems/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Knowledge Graph Systems">
    <meta itemprop="description" content="“Structuring the world’s information into connected entities and relationships.”">
    <meta itemprop="datePublished" content="2025-12-31T10:07:50+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0034-knowledge-graph-systems/" itemprop="url">Knowledge Graph Systems
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-what-is-a-knowledge-graph-kg">1. What is a Knowledge Graph (KG)?</a></li><li><a href="#2-data-model-rdf-vs-labeled-property-graph">2. Data Model: RDF vs. Labeled Property Graph</a><ul><li><a href="#1-resource-description-framework-rdf">1. Resource Description Framework (RDF)</a></li><li><a href="#2-labeled-property-graph-lpg">2. Labeled Property Graph (LPG)</a></li></ul></li><li><a href="#3-knowledge-graph-construction-pipeline">3. Knowledge Graph Construction Pipeline</a><ul><li><a href="#step-1-named-entity-recognition-ner">Step 1: Named Entity Recognition (NER)</a></li><li><a href="#step-2-entity-linking-resolution">Step 2: Entity Linking (Resolution)</a></li><li><a href="#step-3-relation-extraction-re">Step 3: Relation Extraction (RE)</a></li><li><a href="#step-4-knowledge-fusion">Step 4: Knowledge Fusion</a></li></ul></li><li><a href="#4-storage-architecture">4. Storage Architecture</a><ul><li><a href="#1-native-graph-databases-neo4j-amazon-neptune">1. Native Graph Databases (Neo4j, Amazon Neptune)</a></li><li><a href="#2-relational-backends-facebook-tao-linkedin-liquid">2. Relational Backends (Facebook TAO, LinkedIn Liquid)</a></li><li><a href="#3-distributed-key-value-stores-google-knowledge-graph">3. Distributed Key-Value Stores (Google Knowledge Graph)</a></li></ul></li><li><a href="#5-knowledge-graph-inference">5. Knowledge Graph Inference</a><ul><li><a href="#1-link-prediction-knowledge-graph-completion">1. Link Prediction (Knowledge Graph Completion)</a></li><li><a href="#2-knowledge-graph-embeddings-kge">2. Knowledge Graph Embeddings (KGE)</a></li><li><a href="#3-graph-neural-networks-gnns">3. Graph Neural Networks (GNNs)</a></li></ul></li><li><a href="#6-real-world-case-studies">6. Real-World Case Studies</a><ul><li><a href="#case-study-1-google-knowledge-graph">Case Study 1: Google Knowledge Graph</a></li><li><a href="#case-study-2-linkedin-economic-graph">Case Study 2: LinkedIn Economic Graph</a></li><li><a href="#case-study-3-pinterest-taste-graph">Case Study 3: Pinterest Taste Graph</a></li></ul></li><li><a href="#7-deep-dive-graph-rag-retrieval-augmented-generation">7. Deep Dive: Graph RAG (Retrieval Augmented Generation)</a></li><li><a href="#8-deep-dive-scaling-graph-databases">8. Deep Dive: Scaling Graph Databases</a></li><li><a href="#9-system-design-interview-design-a-kg-for-movies">9. System Design Interview: Design a KG for Movies</a></li><li><a href="#10-top-interview-questions">10. Top Interview Questions</a></li><li><a href="#11-summary">11. Summary</a></li><li><a href="#12-deep-dive-graph-neural-networks-gnns">12. Deep Dive: Graph Neural Networks (GNNs)</a></li><li><a href="#13-deep-dive-knowledge-graph-embeddings-kge-math">13. Deep Dive: Knowledge Graph Embeddings (KGE) Math</a></li><li><a href="#14-deep-dive-entity-linking-at-scale">14. Deep Dive: Entity Linking at Scale</a></li><li><a href="#15-deep-dive-graph-rag-implementation">15. Deep Dive: Graph RAG Implementation</a></li><li><a href="#16-deep-dive-temporal-knowledge-graphs">16. Deep Dive: Temporal Knowledge Graphs</a></li><li><a href="#17-deep-dive-quality-assurance-in-kgs">17. Deep Dive: Quality Assurance in KGs</a></li><li><a href="#18-deep-dive-federated-knowledge-graphs">18. Deep Dive: Federated Knowledge Graphs</a></li><li><a href="#19-system-design-real-time-fraud-detection-with-kg">19. System Design: Real-Time Fraud Detection with KG</a></li><li><a href="#20-advanced-neuro-symbolic-ai">20. Advanced: Neuro-Symbolic AI</a></li><li><a href="#21-summary">21. Summary</a></li><li><a href="#22-deep-dive-graph-databases-vs-relational-databases">22. Deep Dive: Graph Databases vs. Relational Databases</a></li><li><a href="#23-deep-dive-ontology-design">23. Deep Dive: Ontology Design</a></li><li><a href="#24-deep-dive-reasoning-engines">24. Deep Dive: Reasoning Engines</a></li><li><a href="#25-deep-dive-graph-visualization-tools">25. Deep Dive: Graph Visualization Tools</a></li><li><a href="#26-code-loading-data-into-neo4j">26. Code: Loading Data into Neo4j</a></li><li><a href="#27-summary">27. Summary</a></li><li><a href="#28-deep-dive-graph-partitioning-algorithms">28. Deep Dive: Graph Partitioning Algorithms</a></li><li><a href="#29-deep-dive-graph-query-optimization">29. Deep Dive: Graph Query Optimization</a></li><li><a href="#30-deep-dive-graph-analytics-algorithms">30. Deep Dive: Graph Analytics Algorithms</a></li><li><a href="#31-deep-dive-hardware-acceleration-for-graphs">31. Deep Dive: Hardware Acceleration for Graphs</a></li><li><a href="#32-deep-dive-semantic-web-vs-knowledge-graphs">32. Deep Dive: Semantic Web vs. Knowledge Graphs</a></li><li><a href="#33-deep-dive-hyper-relational-knowledge-graphs">33. Deep Dive: Hyper-Relational Knowledge Graphs</a></li><li><a href="#34-deep-dive-inductive-vs-transductive-learning">34. Deep Dive: Inductive vs. Transductive Learning</a></li><li><a href="#35-case-study-amazon-product-graph">35. Case Study: Amazon Product Graph</a></li><li><a href="#36-future-trends-large-knowledge-models-lkms">36. Future Trends: Large Knowledge Models (LKMs)</a></li><li><a href="#37-ethical-considerations-in-knowledge-graphs">37. Ethical Considerations in Knowledge Graphs</a></li><li><a href="#38-further-reading">38. Further Reading</a></li><li><a href="#39-conclusion">39. Conclusion</a></li><li><a href="#40-summary">40. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Structuring the world’s information into connected entities and relationships.”</strong></p>

<h2 id="1-what-is-a-knowledge-graph-kg">1. What is a Knowledge Graph (KG)?</h2>

<p>A Knowledge Graph is a structured representation of facts, consisting of <strong>entities</strong> (nodes) and <strong>relationships</strong> (edges).</p>

<ul>
  <li><strong>Entities:</strong> Real-world objects (e.g., “Barack Obama”, “Hawaii”, “President”).</li>
  <li><strong>Relationships:</strong> Connections between them (e.g., “born_in”, “role”).</li>
  <li><strong>Fact:</strong> <code class="language-plaintext highlighter-rouge">(Barack Obama, born_in, Hawaii)</code></li>
</ul>

<p><strong>Why use KGs?</strong></p>
<ul>
  <li><strong>Search:</strong> “Who is the wife of the 44th president?” requires traversing relationships.</li>
  <li><strong>Recommendations:</strong> “Users who liked ‘Inception’ also liked movies directed by ‘Christopher Nolan’”.</li>
  <li><strong>Q&amp;A Systems:</strong> Providing direct answers instead of just blue links.</li>
</ul>

<h2 id="2-data-model-rdf-vs-labeled-property-graph">2. Data Model: RDF vs. Labeled Property Graph</h2>

<p>There are two main ways to model KGs:</p>

<h3 id="1-resource-description-framework-rdf">1. Resource Description Framework (RDF)</h3>
<ul>
  <li><strong>Standard:</strong> W3C standard for semantic web.</li>
  <li><strong>Structure:</strong> Triples <code class="language-plaintext highlighter-rouge">(Subject, Predicate, Object)</code>.</li>
  <li><strong>Example:</strong></li>
  <li><code class="language-plaintext highlighter-rouge">(DaVinci, painted, MonaLisa)</code></li>
  <li><code class="language-plaintext highlighter-rouge">(MonaLisa, located_in, Louvre)</code></li>
  <li><strong>Query Language:</strong> SPARQL.</li>
  <li><strong>Pros:</strong> Great for interoperability, public datasets (DBpedia, Wikidata).</li>
  <li><strong>Cons:</strong> Verbose, hard to attach properties to edges (requires reification).</li>
</ul>

<h3 id="2-labeled-property-graph-lpg">2. Labeled Property Graph (LPG)</h3>
<ul>
  <li><strong>Structure:</strong> Nodes and edges have internal key-value properties.</li>
  <li><strong>Example:</strong></li>
  <li>Node: <code class="language-plaintext highlighter-rouge">Person {name: "DaVinci", born: 1452}</code></li>
  <li>Edge: <code class="language-plaintext highlighter-rouge">PAINTED {year: 1503}</code></li>
  <li>Node: <code class="language-plaintext highlighter-rouge">Artwork {title: "Mona Lisa"}</code></li>
  <li><strong>Query Language:</strong> Cypher (Neo4j), Gremlin.</li>
  <li><strong>Pros:</strong> Intuitive, efficient for traversal, flexible schema.</li>
  <li><strong>Cons:</strong> Less standardized than RDF.</li>
</ul>

<p><strong>Industry Choice:</strong> Most tech companies (LinkedIn, Airbnb, Uber) use <strong>LPG</strong> for internal applications due to performance and flexibility.</p>

<h2 id="3-knowledge-graph-construction-pipeline">3. Knowledge Graph Construction Pipeline</h2>

<p>Building a KG from unstructured text (web pages, documents) is a massive NLP challenge.</p>

<h3 id="step-1-named-entity-recognition-ner">Step 1: Named Entity Recognition (NER)</h3>
<p>Identify entities in text.</p>
<ul>
  <li><strong>Input:</strong> “Elon Musk founded SpaceX in 2002.”</li>
  <li><strong>Output:</strong> <code class="language-plaintext highlighter-rouge">[Elon Musk] (PERSON)</code>, <code class="language-plaintext highlighter-rouge">[SpaceX] (ORG)</code>, <code class="language-plaintext highlighter-rouge">[2002] (DATE)</code>.</li>
  <li><strong>Model:</strong> BERT-NER, BiLSTM-CRF.</li>
</ul>

<h3 id="step-2-entity-linking-resolution">Step 2: Entity Linking (Resolution)</h3>
<p>Map the extracted mention to a unique ID in the KG.</p>
<ul>
  <li><strong>Challenge:</strong> “Michael Jordan” -&gt; Basketball player or ML researcher?</li>
  <li><strong>Solution:</strong> Contextual embeddings. Compare the context of the mention with the description of the candidate entities.</li>
</ul>

<h3 id="step-3-relation-extraction-re">Step 3: Relation Extraction (RE)</h3>
<p>Identify the relationship between entities.</p>
<ul>
  <li><strong>Input:</strong> “Elon Musk” and “SpaceX”.</li>
  <li><strong>Context:</strong> “…founded…”</li>
  <li><strong>Output:</strong> <code class="language-plaintext highlighter-rouge">founded_by(SpaceX, Elon Musk)</code>.</li>
  <li><strong>Model:</strong> Relation Classification heads on BERT.</li>
</ul>

<h3 id="step-4-knowledge-fusion">Step 4: Knowledge Fusion</h3>
<p>Merge facts from multiple sources.</p>
<ul>
  <li>Source A: <code class="language-plaintext highlighter-rouge">(Obama, born, Hawaii)</code></li>
  <li>Source B: <code class="language-plaintext highlighter-rouge">(Obama, birth_place, Honolulu)</code></li>
  <li><strong>Resolution:</strong> “Honolulu” is part of “Hawaii”. Merge or create hierarchy.</li>
</ul>

<h2 id="4-storage-architecture">4. Storage Architecture</h2>

<p>How do we store billions of nodes and trillions of edges?</p>

<h3 id="1-native-graph-databases-neo4j-amazon-neptune">1. Native Graph Databases (Neo4j, Amazon Neptune)</h3>
<ul>
  <li><strong>Storage:</strong> Index-free adjacency. Each node physically stores pointers to its neighbors.</li>
  <li><strong>Pros:</strong> O(1) traversal per hop. Fast for deep queries.</li>
  <li><strong>Cons:</strong> Hard to shard (graph partitioning is NP-hard).</li>
</ul>

<h3 id="2-relational-backends-facebook-tao-linkedin-liquid">2. Relational Backends (Facebook TAO, LinkedIn Liquid)</h3>
<ul>
  <li><strong>Storage:</strong> MySQL/PostgreSQL sharded by ID.</li>
  <li><strong>Schema:</strong> <code class="language-plaintext highlighter-rouge">(id, type, data)</code> and <code class="language-plaintext highlighter-rouge">(id1, type, id2, time)</code>.</li>
  <li><strong>Pros:</strong> Extremely scalable, leverages existing DB infra.</li>
  <li><strong>Cons:</strong> Multi-hop queries require multiple DB lookups (higher latency).</li>
</ul>

<h3 id="3-distributed-key-value-stores-google-knowledge-graph">3. Distributed Key-Value Stores (Google Knowledge Graph)</h3>
<ul>
  <li><strong>Storage:</strong> BigTable / HBase.</li>
  <li><strong>Key:</strong> Subject ID.</li>
  <li><strong>Value:</strong> List of (Predicate, Object).</li>
  <li><strong>Pros:</strong> Massive write throughput.</li>
</ul>

<h2 id="5-knowledge-graph-inference">5. Knowledge Graph Inference</h2>

<p>We don’t just store facts; we infer <strong>new</strong> facts.</p>

<h3 id="1-link-prediction-knowledge-graph-completion">1. Link Prediction (Knowledge Graph Completion)</h3>
<p>Predict missing edges.</p>
<ul>
  <li><strong>Query:</strong> <code class="language-plaintext highlighter-rouge">(Tom Hanks, acted_in, ?)</code></li>
  <li><strong>Task:</strong> Rank all movies by probability.</li>
</ul>

<h3 id="2-knowledge-graph-embeddings-kge">2. Knowledge Graph Embeddings (KGE)</h3>
<p>Map entities and relations to vector space.</p>
<ul>
  <li><strong>TransE:</strong> <code class="language-plaintext highlighter-rouge">h + r \approx t</code>. The translation of head <code class="language-plaintext highlighter-rouge">h</code> by relation <code class="language-plaintext highlighter-rouge">r</code> should land near tail <code class="language-plaintext highlighter-rouge">t</code>.</li>
  <li><strong>RotatE:</strong> Models relations as rotations in complex space (handles symmetry/antisymmetry).</li>
  <li><strong>DistMult:</strong> Uses bilinear product.</li>
</ul>

<h3 id="3-graph-neural-networks-gnns">3. Graph Neural Networks (GNNs)</h3>
<ul>
  <li><strong>GraphSAGE / GAT:</strong> Aggregate information from neighbors to generate node embeddings.</li>
  <li><strong>Use Case:</strong> Node classification (is this account a bot?), Link prediction (friend recommendation).</li>
</ul>

<h2 id="6-real-world-case-studies">6. Real-World Case Studies</h2>

<h3 id="case-study-1-google-knowledge-graph">Case Study 1: Google Knowledge Graph</h3>
<ul>
  <li><strong>Scale:</strong> 500B+ facts.</li>
  <li><strong>Use:</strong> “Things, not strings.” Powers the info box on the right side of search results.</li>
  <li><strong>Innovation:</strong> Massive scale entity disambiguation using search logs.</li>
</ul>

<h3 id="case-study-2-linkedin-economic-graph">Case Study 2: LinkedIn Economic Graph</h3>
<ul>
  <li><strong>Entities:</strong> Members, Companies, Skills, Jobs, Schools.</li>
  <li><strong>Edges:</strong> <code class="language-plaintext highlighter-rouge">employed_by</code>, <code class="language-plaintext highlighter-rouge">has_skill</code>, <code class="language-plaintext highlighter-rouge">alumni_of</code>.</li>
  <li><strong>Use:</strong> “People You May Know”, Job Recommendations, Skill Gap Analysis.</li>
  <li><strong>Tech:</strong> “Liquid” (Graph DB built on top of relational sharding).</li>
</ul>

<h3 id="case-study-3-pinterest-taste-graph">Case Study 3: Pinterest Taste Graph</h3>
<ul>
  <li><strong>Entities:</strong> Users, Pins (Images), Boards.</li>
  <li><strong>Edges:</strong> <code class="language-plaintext highlighter-rouge">saved_to</code>, <code class="language-plaintext highlighter-rouge">clicked_on</code>.</li>
  <li><strong>Model:</strong> <strong>PinSage</strong> (GNN).</li>
  <li><strong>Innovation:</strong> Random-walk based sampling to train GNNs on billions of nodes.</li>
</ul>

<h2 id="7-deep-dive-graph-rag-retrieval-augmented-generation">7. Deep Dive: Graph RAG (Retrieval Augmented Generation)</h2>

<p>LLMs hallucinate. KGs provide ground truth.</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>User Query:</strong> “What drugs interact with Aspirin?”</li>
  <li><strong>KG Lookup:</strong> Query KG for <code class="language-plaintext highlighter-rouge">(Aspirin, interacts_with, ?)</code>.</li>
  <li><strong>Context Retrieval:</strong> Get subgraph: <code class="language-plaintext highlighter-rouge">(Aspirin, interacts_with, Warfarin)</code>, <code class="language-plaintext highlighter-rouge">(Aspirin, interacts_with, Ibuprofen)</code>.</li>
  <li><strong>Prompt Augmentation:</strong> “Context: Aspirin interacts with Warfarin and Ibuprofen. Question: What drugs interact with Aspirin?”</li>
  <li><strong>LLM Generation:</strong> “Aspirin interacts with Warfarin and Ibuprofen…”</li>
</ol>

<p><strong>Pros:</strong> Factual accuracy, explainability (can cite the KG edge).</p>

<h2 id="8-deep-dive-scaling-graph-databases">8. Deep Dive: Scaling Graph Databases</h2>

<p><strong>Sharding Problem:</strong>
Cutting a graph cuts edges. Queries that traverse cuts are slow (network calls).</p>

<p><strong>Solution 1: Hash Partitioning</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ShardID = hash(NodeID) % N</code>.</li>
  <li><strong>Pros:</strong> Even distribution.</li>
  <li><strong>Cons:</strong> Random cuts. A 3-hop query might hit 4 shards.</li>
</ul>

<p><strong>Solution 2: METIS (Graph Partitioning)</strong></p>
<ul>
  <li>Minimize edge cuts. Keep communities on the same shard.</li>
  <li><strong>Pros:</strong> Faster local traversals.</li>
  <li><strong>Cons:</strong> Hard to maintain as graph changes dynamically.</li>
</ul>

<p><strong>Solution 3: Replication (Facebook TAO)</strong></p>
<ul>
  <li>Cache the entire “social graph” in RAM across thousands of memcache nodes.</li>
  <li>Read-heavy workload optimization.</li>
</ul>

<h2 id="9-system-design-interview-design-a-kg-for-movies">9. System Design Interview: Design a KG for Movies</h2>

<p><strong>Requirements:</strong></p>
<ul>
  <li>Store Movies, Actors, Directors.</li>
  <li>Query: “Movies directed by Nolan starring DiCaprio”.</li>
  <li>Scale: 1M movies, 10M people.</li>
</ul>

<p><strong>Schema:</strong></p>
<ul>
  <li>Nodes: <code class="language-plaintext highlighter-rouge">Movie</code>, <code class="language-plaintext highlighter-rouge">Person</code>.</li>
  <li>Edges: <code class="language-plaintext highlighter-rouge">DIRECTED</code>, <code class="language-plaintext highlighter-rouge">ACTED_IN</code>.</li>
</ul>

<p><strong>Storage:</strong></p>
<ul>
  <li><strong>Neo4j</strong> (Single instance fits in RAM). 11M nodes is small.</li>
  <li>If 10B nodes -&gt; <strong>JanusGraph</strong> on Cassandra.</li>
</ul>

<p><strong>API:</strong></p>
<ul>
  <li>GraphQL is perfect for hierarchical graph queries.
<code class="language-plaintext highlighter-rouge">graphql
query {
 director(name: "Christopher Nolan") {
 movies {
 title
 actors(name: "Leonardo DiCaprio") {
 name
 }
 }
 }
}
</code></li>
</ul>

<h2 id="10-top-interview-questions">10. Top Interview Questions</h2>

<p><strong>Q1: How do you handle entity resolution at scale?</strong>
<em>Answer:</em> Blocking (LSH) to find candidates, then pairwise classification (XGBoost/BERT) to verify.</p>

<p><strong>Q2: TransE vs GNNs?</strong>
<em>Answer:</em> TransE is shallow (lookup table). GNNs are deep (aggregate features). GNNs generalize to unseen nodes (inductive), TransE is transductive.</p>

<p><strong>Q3: How to update a KG in real-time?</strong>
<em>Answer:</em> Lambda architecture. Batch pipeline re-builds the high-quality graph nightly. Streaming pipeline adds temporary edges from Kafka events.</p>

<h2 id="11-summary">11. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Component</th>
      <th style="text-align: left">Technology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Data Model</strong></td>
      <td style="text-align: left">Labeled Property Graph (LPG)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Storage</strong></td>
      <td style="text-align: left">Neo4j, JanusGraph, Amazon Neptune</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Query</strong></td>
      <td style="text-align: left">Cypher, Gremlin, GraphQL</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Inference</strong></td>
      <td style="text-align: left">GraphSAGE, TransE</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Use Cases</strong></td>
      <td style="text-align: left">Search, RecSys, Fraud Detection</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Scale</strong></td>
      <td style="text-align: left">Billions of nodes, Trillions of edges</td>
    </tr>
  </tbody>
</table>

<h2 id="12-deep-dive-graph-neural-networks-gnns">12. Deep Dive: Graph Neural Networks (GNNs)</h2>

<p>Traditional embeddings (TransE) are “shallow” — they learn a unique vector for every node. They cannot generalize to new nodes without retraining. <strong>GNNs</strong> solve this.</p>

<p><strong>GraphSAGE (Graph Sample and Aggregate):</strong></p>
<ul>
  <li><strong>Idea:</strong> Generate node embeddings by sampling and aggregating features from a node’s local neighborhood.</li>
  <li><strong>Inductive:</strong> Can generate embeddings for unseen nodes if we know their features and neighbors.</li>
</ul>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Sample:</strong> For each node, sample a fixed number of neighbors (e.g., 10).</li>
  <li><strong>Aggregate:</strong> Combine neighbor embeddings (Mean, LSTM, or Max Pooling).</li>
  <li><strong>Update:</strong> Concatenate self-embedding with aggregated neighbor embedding and pass through a Neural Network.</li>
  <li><strong>Repeat:</strong> Do this for <code class="language-plaintext highlighter-rouge">K</code> layers (hops).</li>
</ol>

<p><strong>Code Snippet (PyTorch Geometric):</strong>
``python
import torch
from torch_geometric.nn import SAGEConv</p>

<p>class GraphSAGE(torch.nn.Module):
 def <strong>init</strong>(self, in_channels, hidden_channels, out_channels):
 super().<strong>init</strong>()
 self.conv1 = SAGEConv(in_channels, hidden_channels)
 self.conv2 = SAGEConv(hidden_channels, out_channels)</p>

<p>def forward(self, x, edge_index):
 # x: Node feature matrix
 # edge_index: Graph connectivity</p>

<p>x = self.conv1(x, edge_index)
 x = x.relu()
 x = torch.nn.functional.dropout(x, p=0.5, training=self.training)</p>

<p>x = self.conv2(x, edge_index)
 return x
``</p>

<p><strong>Graph Attention Networks (GAT):</strong></p>
<ul>
  <li><strong>Idea:</strong> Not all neighbors are equal. Learn <strong>attention weights</strong> to prioritize important neighbors.</li>
  <li><strong>Mechanism:</strong> Compute attention coefficient <code class="language-plaintext highlighter-rouge">\alpha_{ij}</code> for edge <code class="language-plaintext highlighter-rouge">i \to j</code>.</li>
  <li><strong>Benefit:</strong> Better performance on noisy graphs.</li>
</ul>

<h2 id="13-deep-dive-knowledge-graph-embeddings-kge-math">13. Deep Dive: Knowledge Graph Embeddings (KGE) Math</h2>

<p>Let’s look at the math behind <strong>TransE</strong> and <strong>RotatE</strong>.</p>

<p><strong>TransE (Translating Embeddings):</strong></p>
<ul>
  <li><strong>Score Function:</strong> <code class="language-plaintext highlighter-rouge">f(h, r, t) = -||h + r - t||</code></li>
  <li><strong>Objective:</strong> Minimize margin-based ranking loss.
 <code class="language-plaintext highlighter-rouge">L = \sum_{(h,r,t) \in S} \sum_{(h',r,t') \in S'} [\gamma + f(h, r, t) - f(h', r, t')]_+</code></li>
  <li><strong>Limitation:</strong> Cannot model 1-to-N relations (e.g., <code class="language-plaintext highlighter-rouge">Teacher -&gt; Student</code>). If <code class="language-plaintext highlighter-rouge">h+r \approx t_1</code> and <code class="language-plaintext highlighter-rouge">h+r \approx t_2</code>, then <code class="language-plaintext highlighter-rouge">t_1 \approx t_2</code>, forcing all students to be identical.</li>
</ul>

<p><strong>RotatE (Rotation Embeddings):</strong></p>
<ul>
  <li><strong>Idea:</strong> Map entities to the complex plane <code class="language-plaintext highlighter-rouge">\mathbb{C}</code>.</li>
  <li><strong>Relation:</strong> Rotation in complex space. <code class="language-plaintext highlighter-rouge">t = h \circ r</code>, where <code class="language-plaintext highlighter-rouge">|r_i| = 1</code>.</li>
  <li><strong>Capability:</strong> Can model:</li>
  <li><strong>Symmetry:</strong> <code class="language-plaintext highlighter-rouge">r \circ r = 1</code> (e.g., <code class="language-plaintext highlighter-rouge">spouse</code>).</li>
  <li><strong>Antisymmetry:</strong> <code class="language-plaintext highlighter-rouge">r \circ r \neq 1</code> (e.g., <code class="language-plaintext highlighter-rouge">parent</code>).</li>
  <li><strong>Inversion:</strong> <code class="language-plaintext highlighter-rouge">r_2 = r_1^{-1}</code> (e.g., <code class="language-plaintext highlighter-rouge">hypernym</code> vs <code class="language-plaintext highlighter-rouge">hyponym</code>).</li>
</ul>

<h2 id="14-deep-dive-entity-linking-at-scale">14. Deep Dive: Entity Linking at Scale</h2>

<p>How do you link “MJ” to “Michael Jordan” (Basketball) vs “Michael Jackson” (Singer) when you have 100M entities?</p>

<p><strong>Two-Stage Pipeline:</strong></p>

<p><strong>Stage 1: Blocking / Candidate Generation (Recall)</strong></p>
<ul>
  <li><strong>Goal:</strong> Retrieve top-K (e.g., 100) candidates quickly.</li>
  <li><strong>Technique:</strong></li>
  <li><strong>Inverted Index:</strong> Map surface forms (“MJ”, “Mike”) to Entity IDs.</li>
  <li><strong>Dense Retrieval:</strong> Encode mention context and entity description into vectors. Use FAISS to find nearest neighbors.</li>
</ul>

<p><strong>Stage 2: Re-Ranking (Precision)</strong></p>
<ul>
  <li><strong>Goal:</strong> Select the best match from candidates.</li>
  <li><strong>Model:</strong> Cross-Encoder (BERT).</li>
  <li>Input: <code class="language-plaintext highlighter-rouge">[CLS] Mention Context [SEP] Entity Description [SEP]</code></li>
  <li>Output: Probability of match.</li>
  <li><strong>Features:</strong></li>
  <li><strong>String Similarity:</strong> Edit distance.</li>
  <li><strong>Prior Probability:</strong> <code class="language-plaintext highlighter-rouge">P(Entity)</code>. “Michael Jordan” usually means the basketball player.</li>
  <li><strong>Coherence:</strong> Does this entity fit with other entities in the document? (e.g., “Chicago Bulls” is nearby).</li>
</ul>

<h2 id="15-deep-dive-graph-rag-implementation">15. Deep Dive: Graph RAG Implementation</h2>

<p><strong>Retrieval Augmented Generation</strong> with Graphs is powerful for multi-hop reasoning.</p>

<p><strong>Scenario:</strong> “Who is the CEO of the company that acquired GitHub?”</p>

<p><strong>Vector RAG Failure:</strong></p>
<ul>
  <li>Vector search might find docs about “GitHub acquisition” and “Microsoft CEO”.</li>
  <li>But it might miss the connection if they are in separate documents.</li>
</ul>

<p><strong>Graph RAG Success:</strong></p>
<ol>
  <li><strong>Entity Linking:</strong> Extract “GitHub”. Link to <code class="language-plaintext highlighter-rouge">GitHub (Company)</code>.</li>
  <li><strong>Graph Traversal (1-hop):</strong> <code class="language-plaintext highlighter-rouge">GitHub -[ACQUIRED_BY]-&gt; Microsoft</code>.</li>
  <li><strong>Graph Traversal (2-hop):</strong> <code class="language-plaintext highlighter-rouge">Microsoft -[CEO_IS]-&gt; Satya Nadella</code>.</li>
  <li><strong>Context Construction:</strong> “GitHub was acquired by Microsoft. Microsoft’s CEO is Satya Nadella.”</li>
  <li><strong>LLM Answer:</strong> “Satya Nadella.”</li>
</ol>

<p><strong>Implementation Steps:</strong></p>
<ol>
  <li><strong>Ingest:</strong> Parse documents into triples <code class="language-plaintext highlighter-rouge">(Subject, Predicate, Object)</code>.</li>
  <li><strong>Index:</strong> Store triples in Neo4j.</li>
  <li><strong>Query:</strong>
    <ul>
      <li>Use LLM to generate Cypher query.</li>
      <li><code class="language-plaintext highlighter-rouge">MATCH (c:Company {name: "GitHub"})-[:ACQUIRED_BY]-&gt;(parent)-[:CEO]-&gt;(ceo) RETURN ceo.name</code></li>
    </ul>
  </li>
  <li><strong>Generate:</strong> Pass result to LLM for natural language response.</li>
</ol>

<h2 id="16-deep-dive-temporal-knowledge-graphs">16. Deep Dive: Temporal Knowledge Graphs</h2>

<p>Facts change over time.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">(Obama, role, President)</code> is true only for <code class="language-plaintext highlighter-rouge">[2009, 2017]</code>.</li>
</ul>

<p><strong>Modeling Time:</strong></p>
<ol>
  <li><strong>Reification:</strong> Turn the edge into a node.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">(Obama) -&gt; [Term] -&gt; (President)</code></li>
      <li><code class="language-plaintext highlighter-rouge">[Term]</code> has property <code class="language-plaintext highlighter-rouge">start: 2009</code>, <code class="language-plaintext highlighter-rouge">end: 2017</code>.</li>
    </ul>
  </li>
  <li><strong>Quadruples:</strong> Store <code class="language-plaintext highlighter-rouge">(Subject, Predicate, Object, Timestamp)</code>.</li>
  <li><strong>Temporal Embeddings:</strong> <code class="language-plaintext highlighter-rouge">f(h, r, t, \tau)</code>. The embedding evolves over time.</li>
</ol>

<h2 id="17-deep-dive-quality-assurance-in-kgs">17. Deep Dive: Quality Assurance in KGs</h2>

<p>Garbage In, Garbage Out. How to ensure KG quality?</p>

<p><strong>1. Schema Constraints (SHACL):</strong></p>
<ul>
  <li>Define rules: <code class="language-plaintext highlighter-rouge">Person</code> can only <code class="language-plaintext highlighter-rouge">marry</code> another <code class="language-plaintext highlighter-rouge">Person</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">BirthDate</code> must be a valid date.</li>
</ul>

<p><strong>2. Consistency Checking:</strong></p>
<ul>
  <li>Logic rules: <code class="language-plaintext highlighter-rouge">born_in(X, Y) AND located_in(Y, Z) -&gt; born_in(X, Z)</code>.</li>
  <li>If KG says <code class="language-plaintext highlighter-rouge">born_in(Obama, Kenya)</code> but also <code class="language-plaintext highlighter-rouge">born_in(Obama, Hawaii)</code> and <code class="language-plaintext highlighter-rouge">Hawaii != Kenya</code>, flag contradiction.</li>
</ul>

<p><strong>3. Human-in-the-Loop:</strong></p>
<ul>
  <li>High-confidence facts -&gt; Auto-merge.</li>
  <li>Low-confidence facts -&gt; Send to human annotators (crowdsourcing).</li>
</ul>

<h2 id="18-deep-dive-federated-knowledge-graphs">18. Deep Dive: Federated Knowledge Graphs</h2>

<p>Enterprises often have data silos.</p>
<ul>
  <li><strong>HR Graph:</strong> Employees, Roles.</li>
  <li><strong>Sales Graph:</strong> Customers, Deals.</li>
  <li><strong>Product Graph:</strong> SKUs, Specs.</li>
</ul>

<p><strong>Challenge:</strong> Query across silos. “Which sales rep sold Product X to Customer Y?”</p>

<p><strong>Solution: Data Fabric / Virtual Graph</strong></p>
<ul>
  <li>Leave data where it is (SQL, NoSQL, APIs).</li>
  <li>Create a <strong>Virtual Semantic Layer</strong> on top.</li>
  <li>Map local schemas to a global ontology.</li>
  <li>Query Federation engine (e.g., Starogard) decomposes SPARQL/GraphQL query into sub-queries for each backend.</li>
</ul>

<h2 id="19-system-design-real-time-fraud-detection-with-kg">19. System Design: Real-Time Fraud Detection with KG</h2>

<p><strong>Problem:</strong> Detect credit card fraud.
<strong>Insight:</strong> Fraudsters often share attributes (same phone, same IP, same device) forming “rings”.</p>

<p><strong>Design:</strong></p>
<ol>
  <li><strong>Ingestion:</strong> Kafka stream of transactions.</li>
  <li><strong>Graph Update:</strong> Add node <code class="language-plaintext highlighter-rouge">Transaction</code>. Link to <code class="language-plaintext highlighter-rouge">User</code>, <code class="language-plaintext highlighter-rouge">Device</code>, <code class="language-plaintext highlighter-rouge">IP</code>.</li>
  <li><strong>Feature Extraction (Real-time):</strong>
    <ul>
      <li>Count connected components size.</li>
      <li>Cycle detection (User A -&gt; Card B -&gt; User C -&gt; Card A).</li>
      <li>PageRank (guilt by association).</li>
    </ul>
  </li>
  <li><strong>Inference:</strong> Pass graph features to XGBoost model.</li>
  <li><strong>Latency:</strong> &lt; 200ms.
    <ul>
      <li>Use in-memory graph (RedisGraph or Neo4j Causal Cluster).</li>
    </ul>
  </li>
</ol>

<h2 id="20-advanced-neuro-symbolic-ai">20. Advanced: Neuro-Symbolic AI</h2>

<p>Combining the learning capability of Neural Networks with the reasoning of Symbolic Logic (KGs).</p>

<p><strong>Concept:</strong></p>
<ul>
  <li><strong>Neural:</strong> Good at perception (images, text).</li>
  <li><strong>Symbolic:</strong> Good at reasoning, math, consistency.</li>
</ul>

<p><strong>Application:</strong></p>
<ul>
  <li><strong>Visual Question Answering (VQA):</strong></li>
  <li>Image: “A red cube on a blue cylinder.”</li>
  <li>Neural: Detect objects (Cube, Cylinder) and attributes (Red, Blue).</li>
  <li>Symbolic: Build scene graph. Query <code class="language-plaintext highlighter-rouge">on(Cube, Cylinder)</code>.</li>
</ul>

<h2 id="21-summary">21. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Component</th>
      <th style="text-align: left">Technology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Data Model</strong></td>
      <td style="text-align: left">Labeled Property Graph (LPG)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Storage</strong></td>
      <td style="text-align: left">Neo4j, JanusGraph, Amazon Neptune</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Query</strong></td>
      <td style="text-align: left">Cypher, Gremlin, GraphQL</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Inference</strong></td>
      <td style="text-align: left">GraphSAGE, TransE</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Use Cases</strong></td>
      <td style="text-align: left">Search, RecSys, Fraud Detection</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Scale</strong></td>
      <td style="text-align: left">Billions of nodes, Trillions of edges</td>
    </tr>
  </tbody>
</table>

<h2 id="22-deep-dive-graph-databases-vs-relational-databases">22. Deep Dive: Graph Databases vs. Relational Databases</h2>

<p>When should you use a Graph DB over Postgres?</p>

<p><strong>Relational (SQL):</strong></p>
<ul>
  <li><strong>Data Model:</strong> Tables, Rows, Foreign Keys.</li>
  <li><strong>Join:</strong> Computed at query time. O(N \log N) or O(N^2).</li>
  <li><strong>Use Case:</strong> Structured data, transactions, aggregations.</li>
  <li><strong>Query:</strong> “Find all users who bought item X.” (1 Join).</li>
</ul>

<p><strong>Graph (Neo4j):</strong></p>
<ul>
  <li><strong>Data Model:</strong> Nodes, Edges.</li>
  <li><strong>Join:</strong> Pre-computed (edges are pointers). O(1) per hop.</li>
  <li><strong>Use Case:</strong> Highly connected data, pathfinding.</li>
  <li><strong>Query:</strong> “Find all users who bought item X, and their friends who bought item Y.” (Multi-hop).</li>
</ul>

<p><strong>Benchmark:</strong>
For a 5-hop query on a social network:</p>
<ul>
  <li><strong>SQL:</strong> 10+ seconds (5 joins).</li>
  <li><strong>Graph:</strong> &lt; 100ms (pointer traversal).</li>
</ul>

<h2 id="23-deep-dive-ontology-design">23. Deep Dive: Ontology Design</h2>

<p>An <strong>Ontology</strong> is the schema of your Knowledge Graph.</p>

<p><strong>Components:</strong></p>
<ol>
  <li><strong>Classes:</strong> <code class="language-plaintext highlighter-rouge">Person</code>, <code class="language-plaintext highlighter-rouge">Company</code>, <code class="language-plaintext highlighter-rouge">City</code>.</li>
  <li><strong>Properties:</strong> <code class="language-plaintext highlighter-rouge">name</code> (string), <code class="language-plaintext highlighter-rouge">age</code> (int).</li>
  <li><strong>Relationships:</strong> <code class="language-plaintext highlighter-rouge">WORKS_AT</code> (Person -&gt; Company).</li>
  <li><strong>Inheritance:</strong> <code class="language-plaintext highlighter-rouge">Employee</code> is a subclass of <code class="language-plaintext highlighter-rouge">Person</code>.</li>
</ol>

<p><strong>Design Patterns:</strong></p>
<ul>
  <li><strong>Reification:</strong> Don’t just link <code class="language-plaintext highlighter-rouge">Actor -&gt; Movie</code>. Link <code class="language-plaintext highlighter-rouge">Actor -&gt; Role -&gt; Movie</code> to store “character name”.</li>
  <li><strong>Hierarchy:</strong> Use <code class="language-plaintext highlighter-rouge">subClassOf</code> sparingly. Too deep hierarchies make inference slow.</li>
</ul>

<p><strong>Example (OWL/Turtle):</strong>
<code class="language-plaintext highlighter-rouge">turtle
:Person a owl:Class .
:Employee a owl:Class ;
 rdfs:subClassOf :Person .
:worksAt a owl:ObjectProperty ;
 rdfs:domain :Employee ;
 rdfs:range :Company .
</code></p>

<h2 id="24-deep-dive-reasoning-engines">24. Deep Dive: Reasoning Engines</h2>

<p>Reasoning allows inferring implicit facts.</p>

<p><strong>Types of Reasoning:</strong></p>
<ol>
  <li><strong>RDFS Reasoning:</strong>
    <ul>
      <li>Rule: <code class="language-plaintext highlighter-rouge">Employee subClassOf Person</code>.</li>
      <li>Fact: <code class="language-plaintext highlighter-rouge">John is Employee</code>.</li>
      <li>Inference: <code class="language-plaintext highlighter-rouge">John is Person</code>.</li>
    </ul>
  </li>
  <li><strong>Transitive Reasoning:</strong>
    <ul>
      <li>Rule: <code class="language-plaintext highlighter-rouge">partOf</code> is transitive.</li>
      <li>Fact: <code class="language-plaintext highlighter-rouge">Finger partOf Hand</code>, <code class="language-plaintext highlighter-rouge">Hand partOf Arm</code>.</li>
      <li>Inference: <code class="language-plaintext highlighter-rouge">Finger partOf Arm</code>.</li>
    </ul>
  </li>
  <li><strong>Inverse Reasoning:</strong>
    <ul>
      <li>Rule: <code class="language-plaintext highlighter-rouge">parentOf</code> inverseOf <code class="language-plaintext highlighter-rouge">childOf</code>.</li>
      <li>Fact: <code class="language-plaintext highlighter-rouge">A parentOf B</code>.</li>
      <li>Inference: <code class="language-plaintext highlighter-rouge">B childOf A</code>.</li>
    </ul>
  </li>
</ol>

<p><strong>Tools:</strong></p>
<ul>
  <li><strong>Jena Inference Engine</strong> (Java).</li>
  <li><strong>GraphDB</strong> (Ontotext).</li>
</ul>

<h2 id="25-deep-dive-graph-visualization-tools">25. Deep Dive: Graph Visualization Tools</h2>

<p>Visualizing 1B nodes is impossible. We need tools to explore subgraphs.</p>

<p><strong>Tools:</strong></p>
<ol>
  <li><strong>Gephi:</strong> Desktop tool. Good for static analysis of medium graphs (100k nodes).</li>
  <li><strong>Cytoscape:</strong> Bio-informatics focus. Good for protein interaction networks.</li>
  <li><strong>Neo4j Bloom:</strong> Interactive exploration. “Show me the shortest path between X and Y.”</li>
  <li><strong>KeyLines / ReGraph:</strong> JavaScript libraries for building web-based graph visualizers.</li>
</ol>

<p><strong>Visualization Techniques:</strong></p>
<ul>
  <li><strong>Force-Directed Layout:</strong> Simulates physics (nodes repel, edges attract).</li>
  <li><strong>Community Detection coloring:</strong> Color nodes by Louvain community.</li>
  <li><strong>Ego-Network:</strong> Only show node X and its immediate neighbors.</li>
</ul>

<h2 id="26-code-loading-data-into-neo4j">26. Code: Loading Data into Neo4j</h2>

<p>How to ingest data programmatically.</p>

<p>``python
from neo4j import GraphDatabase</p>

<p>class KnowledgeGraphLoader:
 def <strong>init</strong>(self, uri, user, password):
 self.driver = GraphDatabase.driver(uri, auth=(user, password))</p>

<p>def close(self):
 self.driver.close()</p>

<p>def add_person(self, name, age):
 with self.driver.session() as session:
 session.run(
 “MERGE (p:Person {name: <code class="language-plaintext highlighter-rouge">name}) SET p.age = </code>age”,
 name=name, age=age
 )</p>

<p>def add_friendship(self, name1, name2):
 with self.driver.session() as session:
 session.run(
 “””
 MATCH (a:Person {name: $name1})
 MATCH (b:Person {name: $name2})
 MERGE (a)-[:FRIEND]-&gt;(b)
 “””,
 name1=name1, name2=name2
 )</p>

<h1 id="usage">Usage</h1>
<p>loader = KnowledgeGraphLoader(“bolt://localhost:7687”, “neo4j”, “password”)
loader.add_person(“Alice”, 30)
loader.add_person(“Bob”, 32)
loader.add_friendship(“Alice”, “Bob”)
loader.close()
``</p>

<h2 id="27-summary">27. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Component</th>
      <th style="text-align: left">Technology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Data Model</strong></td>
      <td style="text-align: left">Labeled Property Graph (LPG)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Storage</strong></td>
      <td style="text-align: left">Neo4j, JanusGraph, Amazon Neptune</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Query</strong></td>
      <td style="text-align: left">Cypher, Gremlin, GraphQL</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Inference</strong></td>
      <td style="text-align: left">GraphSAGE, TransE</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Use Cases</strong></td>
      <td style="text-align: left">Search, RecSys, Fraud Detection</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Scale</strong></td>
      <td style="text-align: left">Billions of nodes, Trillions of edges</td>
    </tr>
  </tbody>
</table>

<h2 id="28-deep-dive-graph-partitioning-algorithms">28. Deep Dive: Graph Partitioning Algorithms</h2>

<p>To scale to billions of nodes, we must shard the graph.</p>

<p><strong>Problem:</strong> Minimizing “edge cuts” (edges that span across shards) to reduce network latency.</p>

<p><strong>Algorithm 1: METIS (Multilevel Graph Partitioning)</strong></p>
<ul>
  <li><strong>Phase 1 (Coarsening):</strong> Collapse adjacent nodes into super-nodes to create a smaller graph.</li>
  <li><strong>Phase 2 (Partitioning):</strong> Partition the coarse graph.</li>
  <li><strong>Phase 3 (Uncoarsening):</strong> Project the partition back to the original graph and refine.</li>
  <li><strong>Pros:</strong> High quality partitions.</li>
  <li><strong>Cons:</strong> Slow, requires global graph view (offline).</li>
</ul>

<p><strong>Algorithm 2: Fennel (Streaming Partitioning)</strong></p>
<ul>
  <li>Assign nodes to shards as they arrive in a stream.</li>
  <li><strong>Heuristic:</strong> Place node <code class="language-plaintext highlighter-rouge">v</code> in shard <code class="language-plaintext highlighter-rouge">i</code> that maximizes:
 <code class="language-plaintext highlighter-rouge">Score(v, i) = |N(v) \cap S_i| - \alpha (|S_i|)^{\gamma}</code></li>
  <li>Term 1: Attraction (place where neighbors are).</li>
  <li>Term 2: Repulsion (load balancing).</li>
  <li><strong>Pros:</strong> Fast, scalable, works for dynamic graphs.</li>
</ul>

<h2 id="29-deep-dive-graph-query-optimization">29. Deep Dive: Graph Query Optimization</h2>

<p>Just like SQL optimizers, Graph DBs need to plan queries.</p>

<p><strong>Query:</strong> <code class="language-plaintext highlighter-rouge">MATCH (p:Person)-[:LIVES_IN]-&gt;(c:City {name: 'London'})-[:HAS_RESTAURANT]-&gt;(r:Restaurant)</code></p>

<p><strong>Execution Plans:</strong></p>
<ol>
  <li><strong>Scan Person:</strong> Find all people, check if they live in London… (Bad, 1B people).</li>
  <li><strong>Index Scan City:</strong> Find ‘London’ (1 node). Traverse out to <code class="language-plaintext highlighter-rouge">Person</code> (8M nodes). Traverse out to <code class="language-plaintext highlighter-rouge">Restaurant</code> (20k nodes).</li>
  <li><strong>Bi-directional:</strong> Start at ‘London’, traverse both ways.</li>
</ol>

<p><strong>Cost-Based Optimizer:</strong></p>
<ul>
  <li>Uses statistics (node counts, degree distribution).</li>
  <li>“City” has cardinality 10,000. “Person” has 1B.</li>
  <li>Start with the most selective filter (<code class="language-plaintext highlighter-rouge">name='London'</code>).</li>
</ul>

<h2 id="30-deep-dive-graph-analytics-algorithms">30. Deep Dive: Graph Analytics Algorithms</h2>

<p>Beyond simple queries, we run global algorithms.</p>

<p><strong>1. PageRank:</strong></p>
<ul>
  <li>Measure node importance.</li>
  <li><strong>Use Case:</strong> Search ranking, finding influential Twitter users.</li>
  <li><strong>Update Rule:</strong> <code class="language-plaintext highlighter-rouge">PR(u) = (1-d) + d \sum_{v \in N_{in}(u)} \frac{PR(v)}{OutDegree(v)}</code>.</li>
</ul>

<p><strong>2. Louvain Modularity (Community Detection):</strong></p>
<ul>
  <li>Detect clusters of densely connected nodes.</li>
  <li><strong>Use Case:</strong> Fraud rings, topic detection.</li>
</ul>

<p><strong>3. Betweenness Centrality:</strong></p>
<ul>
  <li>Number of shortest paths passing through a node.</li>
  <li><strong>Use Case:</strong> Identifying bottlenecks in a supply chain or network router.</li>
</ul>

<h2 id="31-deep-dive-hardware-acceleration-for-graphs">31. Deep Dive: Hardware Acceleration for Graphs</h2>

<p>CPUs are bad at graph processing (random memory access = cache misses).</p>

<p><strong>Graphcore IPU (Intelligence Processing Unit):</strong></p>
<ul>
  <li><strong>Architecture:</strong> Massive MIMD (Multiple Instruction, Multiple Data).</li>
  <li><strong>Memory:</strong> In-processor memory (SRAM) instead of HBM.</li>
  <li><strong>Benefit:</strong> 10x-100x speedup for GNN training and random walks.</li>
</ul>

<p><strong>Cerebras Wafer-Scale Engine:</strong></p>
<ul>
  <li>A single chip the size of a wafer.</li>
  <li>Holds the entire graph in SRAM.</li>
  <li>
    <p>Zero latency communication between cores.</p>
  </li>
  <li>Zero latency communication between cores.</li>
</ul>

<h2 id="32-deep-dive-semantic-web-vs-knowledge-graphs">32. Deep Dive: Semantic Web vs. Knowledge Graphs</h2>

<p><strong>History:</strong></p>
<ul>
  <li><strong>Semantic Web (2001):</strong> Tim Berners-Lee’s vision. A web of data readable by machines.</li>
  <li><strong>Standards:</strong> RDF, OWL, SPARQL.</li>
  <li><strong>Failure:</strong> Too complex, academic, and rigid. “Ontology engineering” was too hard.</li>
</ul>

<p><strong>Knowledge Graphs (2012):</strong></p>
<ul>
  <li><strong>Google’s Rebranding:</strong> “Things, not strings.”</li>
  <li><strong>Pragmatism:</strong> Focus on utility (Search, RecSys) rather than strict logical correctness.</li>
  <li><strong>Shift:</strong> From “Reasoning” to “Embedding”. From “XML” to “JSON/LPG”.</li>
</ul>

<p><strong>Key Difference:</strong></p>
<ul>
  <li><strong>Semantic Web:</strong> Open world assumption. If it’s not in the DB, it might still be true.</li>
  <li><strong>Enterprise KG:</strong> Closed world assumption. If it’s not in the DB, it’s false (usually).</li>
</ul>

<h2 id="33-deep-dive-hyper-relational-knowledge-graphs">33. Deep Dive: Hyper-Relational Knowledge Graphs</h2>

<p>Standard KGs are triples: <code class="language-plaintext highlighter-rouge">(Obama, President, USA)</code>.
But reality is complex: <code class="language-plaintext highlighter-rouge">(Obama, President, USA, Start:2009, End:2017, Source:Wikipedia)</code>.</p>

<p><strong>Modeling Qualifiers:</strong></p>
<ol>
  <li><strong>Star-Schema (LPG):</strong> Add properties to the edge.</li>
  <li><strong>N-ary Relations (RDF):</strong> Create an intermediate node.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">(Obama) -&gt; [Presidency] -&gt; (USA)</code></li>
      <li><code class="language-plaintext highlighter-rouge">[Presidency] -&gt; (Start: 2009)</code></li>
    </ul>
  </li>
</ol>

<p><strong>StarE (Hyper-Relational Embedding):</strong></p>
<ul>
  <li>Extends TransE/RotatE to handle qualifiers.</li>
  <li>Embedding depends on <code class="language-plaintext highlighter-rouge">(h, r, t)</code> AND <code class="language-plaintext highlighter-rouge">(key_1, value_1), (key_2, value_2)</code>.</li>
  <li><strong>Benefit:</strong> Better link prediction accuracy by using context.</li>
</ul>

<h2 id="34-deep-dive-inductive-vs-transductive-learning">34. Deep Dive: Inductive vs. Transductive Learning</h2>

<p><strong>Transductive (TransE, RotatE):</strong></p>
<ul>
  <li>Learn an embedding for every node in the training set.</li>
  <li><strong>Problem:</strong> If a new user joins, we have no embedding. We must retrain the whole model.</li>
</ul>

<p><strong>Inductive (GraphSAGE, GAT):</strong></p>
<ul>
  <li>Learn a <strong>function</strong> to generate embeddings from features.</li>
  <li><code class="language-plaintext highlighter-rouge">f(features, neighbors)</code>.</li>
  <li><strong>Benefit:</strong> Can handle dynamic graphs (new nodes arriving constantly) without retraining.</li>
</ul>

<h2 id="35-case-study-amazon-product-graph">35. Case Study: Amazon Product Graph</h2>

<p><strong>Scale:</strong> Billions of products, users, reviews.</p>

<p><strong>Entities:</strong> <code class="language-plaintext highlighter-rouge">Product</code>, <code class="language-plaintext highlighter-rouge">Brand</code>, <code class="language-plaintext highlighter-rouge">Category</code>, <code class="language-plaintext highlighter-rouge">User</code>.
<strong>Edges:</strong> <code class="language-plaintext highlighter-rouge">bought_together</code>, <code class="language-plaintext highlighter-rouge">viewed_together</code>, <code class="language-plaintext highlighter-rouge">is_compatible_with</code> (Lens -&gt; Camera).</p>

<p><strong>Application: “Complementary Product Recommendation”</strong></p>
<ul>
  <li>If user buys “Canon EOS R5”, recommend “RF 24-70mm Lens”.</li>
  <li><strong>Challenge:</strong> Compatibility is strict. A Nikon lens won’t fit.</li>
  <li><strong>Solution:</strong> KG explicitly models <code class="language-plaintext highlighter-rouge">compatible_mount</code> relationships. LLMs/Embeddings might guess “Lens fits Camera” generally, but KG ensures <em>exact</em> fit.</li>
</ul>

<h2 id="36-future-trends-large-knowledge-models-lkms">36. Future Trends: Large Knowledge Models (LKMs)</h2>

<p><strong>KG + LLM Convergence:</strong></p>
<ol>
  <li><strong>KG-Enhanced LLM:</strong> RAG (Retrieval Augmented Generation).</li>
  <li><strong>LLM-Enhanced KG:</strong> Use LLM to clean, populate, and reason over KG.</li>
  <li><strong>LKM (Large Knowledge Model):</strong> A single transformer trained on both Text (Common Crawl) and Subgraphs (Wikidata).
    <ul>
      <li>Can output text OR graph structures.</li>
      <li>“Draw the family tree of the Targaryens” -&gt; Outputs JSON graph.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>“Draw the family tree of the Targaryens” -&gt; Outputs JSON graph.</li>
</ul>

<h2 id="37-ethical-considerations-in-knowledge-graphs">37. Ethical Considerations in Knowledge Graphs</h2>

<p><strong>1. Bias in Entity Representation:</strong></p>
<ul>
  <li>KGs built from Wikipedia over-represent Western entities.</li>
  <li><strong>Example:</strong> “Scientist” nodes are 90% male, 80% Western.</li>
  <li><strong>Impact:</strong> Recommendation systems amplify this bias.</li>
  <li><strong>Fix:</strong> Actively source diverse data (non-English Wikipedia, local databases).</li>
</ul>

<p><strong>2. Privacy:</strong></p>
<ul>
  <li>KGs can link disparate data sources to de-anonymize individuals.</li>
  <li><strong>Example:</strong> <code class="language-plaintext highlighter-rouge">(User123, lives_in, ZIP_12345) + (User123, age, 34) + (User123, disease, Rare_Condition)</code> → Uniquely identifies a person.</li>
  <li><strong>Mitigation:</strong> Differential Privacy on graph queries. Add noise to aggregate statistics.</li>
</ul>

<p><strong>3. Misinformation:</strong></p>
<ul>
  <li>Automated KG construction from web data ingests false facts.</li>
  <li><strong>Example:</strong> Conspiracy theories (“Vaccine causes autism”) might appear as edges if they’re widely discussed online.</li>
  <li><strong>Fix:</strong> Source credibility scoring. Prioritize facts from .gov, .edu, peer-reviewed sources.</li>
</ul>

<h2 id="38-further-reading">38. Further Reading</h2>

<ol>
  <li><strong>“Knowledge Graphs” (Hogan et al., 2021):</strong> Comprehensive survey covering all aspects.</li>
  <li><strong>“Translating Embeddings for Modeling Multi-relational Data” (Bordes et al., 2013):</strong> The TransE paper.</li>
  <li><strong>“Inductive Representation Learning on Large Graphs” (Hamilton et al., 2017):</strong> The GraphSAGE paper.</li>
  <li><strong>“Google’s Knowledge Graph: Serving Billions of Queries” (Singhal, 2012):</strong> The original blog post.</li>
  <li><strong>“PinSage: Graph Convolutional Neural Networks for Web-Scale Recommender Systems” (Ying et al., 2018):</strong> Pinterest’s production GNN.</li>
</ol>

<ul>
  <li><strong>“PinSage: Graph Convolutional Neural Networks for Web-Scale Recommender Systems” (Ying et al., 2018):</strong> Pinterest’s production GNN.</li>
</ul>

<h2 id="39-conclusion">39. Conclusion</h2>

<p>Knowledge Graphs represent a fundamental shift from “documents” to “facts.” They power the world’s most sophisticated AI systems—from Google Search to LinkedIn’s Economic Graph to fraud detection at banks. The convergence of symbolic reasoning (graphs) and neural learning (embeddings, GNNs) is creating a new generation of <strong>Neuro-Symbolic AI</strong> that combines the best of both worlds: the interpretability and structure of graphs with the learning power of deep learning. As we move toward Large Knowledge Models (LKMs), the boundary between “structured data” and “unstructured text” will blur, enabling AI systems that can reason, learn, and explain.</p>

<h2 id="40-summary">40. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Component</th>
      <th style="text-align: left">Technology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Data Model</strong></td>
      <td style="text-align: left">Labeled Property Graph (LPG)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Storage</strong></td>
      <td style="text-align: left">Neo4j, JanusGraph, Amazon Neptune</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Query</strong></td>
      <td style="text-align: left">Cypher, Gremlin, GraphQL</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Inference</strong></td>
      <td style="text-align: left">GraphSAGE, TransE</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Use Cases</strong></td>
      <td style="text-align: left">Search, RecSys, Fraud Detection</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Scale</strong></td>
      <td style="text-align: left">Billions of nodes, Trillions of edges</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Future</strong></td>
      <td style="text-align: left">Graph RAG, Neuro-Symbolic AI</td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0034-knowledge-graph-systems/">arunbaby.com/ml-system-design/0034-knowledge-graph-systems</a></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#database" class="page__taxonomy-item p-category" rel="tag">database</a><span class="sep">, </span>
    
      <a href="/tags/#graph-neural-networks" class="page__taxonomy-item p-category" rel="tag">graph-neural-networks</a><span class="sep">, </span>
    
      <a href="/tags/#knowledge-graph" class="page__taxonomy-item p-category" rel="tag">knowledge-graph</a><span class="sep">, </span>
    
      <a href="/tags/#nlp" class="page__taxonomy-item p-category" rel="tag">nlp</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml_system_design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0034-evaluate-division/" rel="permalink">Evaluate Division (Graph/Union-Find)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Modeling algebraic equations as graph path problems.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0034-dialog-state-tracking/" rel="permalink">Dialog State Tracking (DST)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          20 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The brain of a task-oriented dialogue system: remembering what the user wants.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0034-observability-tracing/" rel="permalink">Observability and Tracing
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Inside the mind of the machine: Mastering agentic observability.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Knowledge+Graph+Systems%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0034-knowledge-graph-systems%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0034-knowledge-graph-systems%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0034-knowledge-graph-systems/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0033-model-replication-systems/" class="pagination--pager" title="Model Replication Systems">Previous</a>
    
    
      <a href="/ml-system-design/0035-boundary-detection-in-ml/" class="pagination--pager" title="Boundary Detection in ML">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
