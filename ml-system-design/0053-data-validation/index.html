<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Data Validation - Arun Baby</title>
<meta name="description" content="“Most ML failures aren’t model bugs—they’re invalid data quietly passing through.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Data Validation">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0053-data-validation/">


  <meta property="og:description" content="“Most ML failures aren’t model bugs—they’re invalid data quietly passing through.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Data Validation">
  <meta name="twitter:description" content="“Most ML failures aren’t model bugs—they’re invalid data quietly passing through.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0053-data-validation/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0053-data-validation/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Data Validation">
    <meta itemprop="description" content="“Most ML failures aren’t model bugs—they’re invalid data quietly passing through.”">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0053-data-validation/" itemprop="url">Data Validation
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-problem-statement">1. Problem Statement</a></li><li><a href="#2-understanding-the-requirements">2. Understanding the Requirements</a><ul><li><a href="#21-functional-requirements">2.1 Functional requirements</a></li><li><a href="#22-non-functional-requirements">2.2 Non-functional requirements</a></li></ul></li><li><a href="#3-high-level-architecture">3. High-Level Architecture</a></li><li><a href="#4-component-deep-dives">4. Component Deep-Dives</a><ul><li><a href="#41-expectations-what-do-we-validate">4.1 Expectations: what do we validate?</a></li><li><a href="#42-where-validation-runs-batch-vs-streaming">4.2 Where validation runs (batch vs streaming)</a></li><li><a href="#421-validation-in-the-feature-store-the-last-mile">4.2.1 Validation in the feature store (the “last mile”)</a></li><li><a href="#422-validation-during-model-serving-online-data-contracts">4.2.2 Validation during model serving (online “data contracts”)</a></li><li><a href="#43-policy-engine-what-happens-when-validation-fails">4.3 Policy engine: what happens when validation fails?</a></li><li><a href="#431-quarantine-design-how-to-store-bad-data-safely">4.3.1 Quarantine design (how to store “bad data” safely)</a></li><li><a href="#44-ownership-and-change-management">4.4 Ownership and change management</a></li><li><a href="#441-a-practical-ownership-model-who-owns-expectations">4.4.1 A practical ownership model (who owns expectations?)</a></li></ul></li><li><a href="#5-data-flow-training-vs-serving">5. Data Flow (Training vs Serving)</a><ul><li><a href="#51-training-data-validation">5.1 Training data validation</a></li><li><a href="#52-serving-data-validation-online">5.2 Serving data validation (online)</a></li></ul></li><li><a href="#6-implementation-core-logic-in-python">6. Implementation (Core Logic in Python)</a><ul><li><a href="#60-a-great-expectations--tfdv-mental-mapping">6.0 A Great Expectations / TFDV mental mapping</a></li><li><a href="#61-schema-validation-dict-based">6.1 Schema validation (dict-based)</a></li><li><a href="#62-range-and-enum-checks">6.2 Range and enum checks</a></li><li><a href="#63-drift-check-on-histograms-l1-distance">6.3 Drift check on histograms (L1 distance)</a></li><li><a href="#64-data-completeness-and-freshness-checks-table-level">6.4 Data completeness and freshness checks (table-level)</a></li><li><a href="#65-a-minimal-policy-engine-passwarnblock">6.5 A minimal policy engine (pass/warn/block)</a></li></ul></li><li><a href="#7-scaling-strategies">7. Scaling Strategies</a><ul><li><a href="#71-sampling-vs-full-scans">7.1 Sampling vs full scans</a></li><li><a href="#72-sketches-for-distributions">7.2 Sketches for distributions</a></li><li><a href="#73-cardinality-governance">7.3 Cardinality governance</a></li></ul></li><li><a href="#8-monitoring--metrics">8. Monitoring &amp; Metrics</a><ul><li><a href="#81-rca-first-reporting-what-a-good-failure-report-contains">8.1 RCA-first reporting (what a good failure report contains)</a></li><li><a href="#82-distribution-drift-at-scale-histograms-vs-sketches-vs-embeddings">8.2 Distribution drift at scale: histograms vs sketches vs embeddings</a></li><li><a href="#83-when-validation-should-page-avoid-alert-fatigue">8.3 When validation should page (avoid alert fatigue)</a></li></ul></li><li><a href="#9-failure-modes-and-mitigations">9. Failure Modes (and Mitigations)</a><ul><li><a href="#91-overly-strict-checks-false-positives">9.1 Overly strict checks (false positives)</a></li><li><a href="#92-too-loose-checks-false-negatives">9.2 Too loose checks (false negatives)</a></li><li><a href="#93-unit-changes-and-schema-drift">9.3 Unit changes and schema drift</a></li><li><a href="#94-pii-and-privacy-failures-validation-can-leak">9.4 PII and privacy failures (validation can leak!)</a></li><li><a href="#95-the-validator-outage-problem">9.5 The “validator outage” problem</a></li></ul></li><li><a href="#10-real-world-case-study">10. Real-World Case Study</a><ul><li><a href="#101-another-case-study-silent-label-leakage-and-too-good-to-be-true">10.1 Another case study: silent label leakage and “too good to be true”</a></li><li><a href="#102-a-case-study-category-explosion-cardinality-drift">10.2 A case study: category explosion (cardinality drift)</a></li></ul></li><li><a href="#11-cost-analysis">11. Cost Analysis</a><ul><li><a href="#111-why-data-validation-becomes-a-platform-org-scale-evolution">11.1 Why data validation becomes a platform (org-scale evolution)</a></li><li><a href="#112-training-vs-serving-cost-trade-offs">11.2 Training vs serving cost trade-offs</a></li><li><a href="#113-what-to-do-when-you-cant-block-graceful-degradation">11.3 What to do when you can’t block (graceful degradation)</a></li></ul></li><li><a href="#12-key-takeaways">12. Key Takeaways</a><ul><li><a href="#121-appendix-a-minimal-expectations-checklist-for-new-datasets">12.1 Appendix: a minimal “expectations checklist” for new datasets</a></li><li><a href="#122-appendix-howconnects-across-tracks">12.2 Appendix: howconnects across tracks</a></li><li><a href="#123-appendix-a-minimal-triage-packet-template-for-validators">12.3 Appendix: a minimal “triage packet” template for validators</a></li><li><a href="#124-appendix-quick-interview-framing">12.4 Appendix: quick interview framing</a></li><li><a href="#125-appendix-stop-the-line-vs-degrade-gracefully-a-crisp-rule">12.5 Appendix: “stop the line” vs “degrade gracefully” (a crisp rule)</a></li><li><a href="#126-appendix-dataset-lineage-and-blast-radius-estimation">12.6 Appendix: dataset lineage and “blast radius” estimation</a></li><li><a href="#127-appendix-validation-maturity-model">12.7 Appendix: validation maturity model</a></li><li><a href="#128-appendix-join-validation-the-silent-correctness-killer">12.8 Appendix: join validation (the silent correctness killer)</a></li><li><a href="#129-appendix-skew-vs-drift-offline-vs-online-mismatch">12.9 Appendix: skew vs drift (offline vs online mismatch)</a></li><li><a href="#1210-appendix-a-concrete-expectation-suite-example">12.10 Appendix: a concrete expectation suite example</a></li><li><a href="#1211-appendix-streaming-validation-patterns-fast-gates">12.11 Appendix: streaming validation patterns (fast gates)</a></li></ul></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Most ML failures aren’t model bugs—they’re invalid data quietly passing through.”</strong></p>

<h2 id="1-problem-statement">1. Problem Statement</h2>

<p>Production ML systems consume data from many upstream sources:</p>
<ul>
  <li>event streams (clicks, purchases, searches)</li>
  <li>batch ETL tables (user profiles, catalogs)</li>
  <li>logs and metrics (latency, errors)</li>
  <li>third-party feeds</li>
</ul>

<p>Data changes constantly:</p>
<ul>
  <li>schemas evolve</li>
  <li>units change (ms → s)</li>
  <li>new categories appear</li>
  <li>null rates drift</li>
  <li>pipelines partially fail</li>
</ul>

<p><strong>Data validation</strong> is the system that ensures:</p>
<ul>
  <li>data conforms to expectations (schema, ranges, types)</li>
  <li>anomalies are detected early (before training/serving)</li>
  <li>quality regressions are gated (stop the line)</li>
  <li>teams can debug quickly with clear evidence</li>
</ul>

<p>The shared theme today is <strong>data validation and edge case handling</strong>:
like the “First Missing Positive” algorithm, you define the valid domain and ignore/flag everything outside it.</p>

<hr />

<h2 id="2-understanding-the-requirements">2. Understanding the Requirements</h2>

<h3 id="21-functional-requirements">2.1 Functional requirements</h3>

<ul>
  <li>Validate <strong>schema</strong>: required columns, types, nested structures.</li>
  <li>Validate <strong>constraints</strong>: ranges, enums, monotonicity, uniqueness.</li>
  <li>Validate <strong>distribution</strong>: drift, skew, outliers, rare category explosions.</li>
  <li>Validate <strong>freshness/completeness</strong>: partitions present, record counts, latency.</li>
  <li>Produce <strong>actions</strong>: pass, warn, quarantine, block, rollback.</li>
  <li>Produce <strong>artifacts</strong>: validation reports, dashboards, alerts, lineage links.</li>
</ul>

<h3 id="22-non-functional-requirements">2.2 Non-functional requirements</h3>

<ul>
  <li><strong>Scale</strong>: TB/day, high-cardinality features, many tenants.</li>
  <li><strong>Low latency gates</strong>: must run before training jobs / feature publishing.</li>
  <li><strong>Reliability</strong>: validators must not become the bottleneck.</li>
  <li><strong>Explainability</strong>: “what failed and why” must be human-friendly.</li>
  <li><strong>Governance</strong>: who owns expectations? who can change them? audit trails.</li>
  <li><strong>Safety</strong>: avoid both false negatives (bad data slips) and false positives (blocks everything).</li>
</ul>

<hr />

<h2 id="3-high-level-architecture">3. High-Level Architecture</h2>

<p><code class="language-plaintext highlighter-rouge">
 +------------------------+
 | Upstream Sources |
 | (streams + batch) |
 +-----------+------------+
 |
 v
 +-------------------+
 | Ingestion Layer |
 | (Kafka/S3/HDFS) |
 +---------+---------+
 |
 +------------+-------------+
 | |
 v v
 +-------------------+ +-------------------+
 | Validation (Fast) | | Validation (Deep) |
 | schema + counts | | dist + drift |
 +---------+---------+ +---------+---------+
 | |
 +------------+-------------+
 |
 v
 +-------------------+
 | Policy Engine |
 | (pass/warn/block) |
 +---------+---------+
 |
 +------------+-------------+
 | |
 v v
 +-------------------+ +-------------------+
 | Downstream: | | Quarantine Store |
 | feature store, | | + RCA reports |
 | training, serving | +-------------------+
 +-------------------+
</code></p>

<p>Key idea:</p>
<ul>
  <li><strong>Fast checks</strong> run on every partition quickly (schema, counts, null rates).</li>
  <li><strong>Deep checks</strong> run on sampled or aggregated data (distribution drift, correlations).</li>
  <li>A <strong>policy engine</strong> decides what to do based on severity and ownership.</li>
</ul>

<hr />

<h2 id="4-component-deep-dives">4. Component Deep-Dives</h2>

<h3 id="41-expectations-what-do-we-validate">4.1 Expectations: what do we validate?</h3>

<p>Expectations come in tiers:</p>

<p><strong>Tier 0: schema and contracts</strong></p>
<ul>
  <li>required columns exist</li>
  <li>types are correct (int/float/string)</li>
  <li>nested schema matches (JSON/Avro/Proto)</li>
</ul>

<p><strong>Tier 1: basic quality</strong></p>
<ul>
  <li>null rates under thresholds</li>
  <li>ranges sane (age in [0, 120])</li>
  <li>enum values known (country in ISO list)</li>
</ul>

<p><strong>Tier 2: distribution and drift</strong></p>
<ul>
  <li>feature distributions stable (histogram distance)</li>
  <li>category proportions stable</li>
  <li>embedding stats stable (mean/variance)</li>
</ul>

<p><strong>Tier 3: semantic / business constraints</strong></p>
<ul>
  <li>revenue &gt;= 0</li>
  <li>timestamps monotonic per entity</li>
  <li>joins don’t explode (fanout constraints)</li>
</ul>

<p>The key lesson:</p>
<blockquote>
  <p>You can’t validate everything deeply at all times; choose expectations that catch real failures.</p>
</blockquote>

<h3 id="42-where-validation-runs-batch-vs-streaming">4.2 Where validation runs (batch vs streaming)</h3>

<ul>
  <li><strong>Batch validation</strong></li>
  <li>validates partitions (daily tables)</li>
  <li>good for training data and offline features</li>
  <li><strong>Streaming validation</strong></li>
  <li>validates events in near real time (schema, rate, missing fields)</li>
  <li>good for online features and real-time inference pipelines</li>
</ul>

<p>Most orgs end up with both.</p>

<h3 id="421-validation-in-the-feature-store-the-last-mile">4.2.1 Validation in the feature store (the “last mile”)</h3>

<p>Even if batch pipelines are perfect, the <strong>feature store</strong> is where validation failures become user-facing.
Common feature-store validations:</p>
<ul>
  <li><strong>freshness</strong>: feature timestamps within TTL</li>
  <li><strong>availability</strong>: fraction of requests with missing features</li>
  <li><strong>default rate</strong>: how often a feature is defaulted due to missing/invalid values</li>
  <li><strong>range guards</strong>: clamp or null-out impossible values</li>
</ul>

<p>A strong pattern:</p>
<blockquote>
  <p>Treat validation as a continuous contract between producers and consumers.</p>
</blockquote>

<p>If a producer changes a feature distribution, the feature store should detect it and route it as an incident or a planned evolution (via schema/expectation update).</p>

<h3 id="422-validation-during-model-serving-online-data-contracts">4.2.2 Validation during model serving (online “data contracts”)</h3>

<p>Online model serving has different constraints than training:</p>
<ul>
  <li>you can’t block traffic (you must respond)</li>
  <li>you must be robust to missing or corrupted features</li>
</ul>

<p>So serving validation usually results in:</p>
<ul>
  <li><strong>fallback behavior</strong> (defaults, last-known-good features, simpler model)</li>
  <li><strong>logging and alerting</strong></li>
  <li><strong>traffic shaping</strong> (route to safe model variant)</li>
</ul>

<p>A useful mental model:</p>
<blockquote>
  <p>Training validation prevents bad learning. Serving validation prevents bad decisions.</p>
</blockquote>

<h3 id="43-policy-engine-what-happens-when-validation-fails">4.3 Policy engine: what happens when validation fails?</h3>

<p>Possible actions:</p>
<ul>
  <li><strong>pass</strong>: everything ok</li>
  <li><strong>warn</strong>: log + notify owner, allow pipeline</li>
  <li><strong>block</strong>: prevent publish/train/deploy</li>
  <li><strong>quarantine</strong>: store bad partition for RCA</li>
  <li><strong>rollback</strong>: revert to last known good snapshot</li>
</ul>

<p>This turns validation from “metrics” into “control”.</p>

<h3 id="431-quarantine-design-how-to-store-bad-data-safely">4.3.1 Quarantine design (how to store “bad data” safely)</h3>

<p>When validation fails, teams often want to inspect the bad partition.
But you need to design quarantine carefully:</p>
<ul>
  <li>quarantine should preserve raw inputs (for RCA) <strong>and</strong> derived summaries (for fast diagnosis)</li>
  <li>quarantine access may involve sensitive data (PII), so it needs RBAC and audit logs</li>
  <li>quarantine data should have TTLs to avoid indefinite retention</li>
</ul>

<p>Common quarantine artifacts:</p>
<ul>
  <li>a small sample of failing records (where policy allows)</li>
  <li>aggregated histograms and null maps</li>
  <li>top failing fields and rules</li>
  <li>the producing pipeline version and change log references</li>
</ul>

<p>This is the “RCA-first” principle: every failure should come with enough evidence to fix it quickly.</p>

<h3 id="44-ownership-and-change-management">4.4 Ownership and change management</h3>

<p>Expectations must be owned.
Otherwise teams either:</p>
<ul>
  <li>never update thresholds (false positives forever), or</li>
  <li>loosen everything to stop pages (false negatives forever)</li>
</ul>

<p>Good patterns:</p>
<ul>
  <li>expectation files in Git, PR-reviewed</li>
  <li>a “break glass” path with audit logs</li>
  <li>a change log correlation panel for failures</li>
</ul>

<h3 id="441-a-practical-ownership-model-who-owns-expectations">4.4.1 A practical ownership model (who owns expectations?)</h3>

<p>Expectations should usually be owned by:</p>
<ul>
  <li><strong>producer teams</strong> for schema and basic constraints (Tier 0/1)</li>
  <li><strong>consumer teams</strong> (ML teams) for distribution and semantic expectations (Tier 2/3)</li>
</ul>

<p>Why split ownership:</p>
<ul>
  <li>producers understand correctness of raw fields</li>
  <li>consumers understand what the model relies on</li>
</ul>

<p>In practice, high-performing orgs establish:</p>
<ul>
  <li>a shared “data contract” repo</li>
  <li>an on-call rotation for critical datasets</li>
  <li>a lightweight review process for expectation changes (avoid bottlenecks)</li>
</ul>

<hr />

<h2 id="5-data-flow-training-vs-serving">5. Data Flow (Training vs Serving)</h2>

<h3 id="51-training-data-validation">5.1 Training data validation</h3>

<p>Typical flow:</p>
<ol>
  <li>ETL writes a daily partition</li>
  <li>validation runs (schema + counts + nulls)</li>
  <li>deep drift checks compare against recent partitions</li>
  <li>policy decides pass/warn/block</li>
  <li>training starts only if gates pass</li>
</ol>

<h3 id="52-serving-data-validation-online">5.2 Serving data validation (online)</h3>

<p>Online inference uses:</p>
<ul>
  <li>streaming features</li>
  <li>feature store reads</li>
</ul>

<p>Validations include:</p>
<ul>
  <li>schema validation at ingestion (drop invalid events)</li>
  <li>online feature freshness (TTL checks)</li>
  <li>feature value range checks (clamp or default)</li>
</ul>

<p>In serving, “blocking” is rarely acceptable. Instead you:</p>
<ul>
  <li>fall back to defaults</li>
  <li>degrade gracefully</li>
  <li>alert for investigation</li>
</ul>

<hr />

<h2 id="6-implementation-core-logic-in-python">6. Implementation (Core Logic in Python)</h2>

<p>Below are minimal examples of validation primitives. In production you’d use Great Expectations/TFDV, but the logic is universal.</p>

<h3 id="60-a-great-expectations--tfdv-mental-mapping">6.0 A Great Expectations / TFDV mental mapping</h3>

<p>If you’re familiar with common libraries:</p>

<ul>
  <li><strong>Great Expectations</strong></li>
  <li>“expectation suite” = a set of rules for a dataset</li>
  <li>“checkpoint” = validation run that produces a report</li>
  <li>
    <p>good fit for: batch tables, SQL/Spark pipelines</p>
  </li>
  <li><strong>TFDV (TensorFlow Data Validation)</strong></li>
  <li>“schema” + “statistics” = baseline + constraints</li>
  <li>drift/skew detection = compare stats between datasets/slices</li>
  <li>good fit for: TF pipelines and feature-heavy datasets</li>
</ul>

<p>Even if you don’t use these libraries, the primitives are the same:</p>
<ul>
  <li>schema checks</li>
  <li>range checks</li>
  <li>distribution checks</li>
  <li>policy decisions</li>
</ul>

<h3 id="61-schema-validation-dict-based">6.1 Schema validation (dict-based)</h3>

<p>``python
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple</p>

<p>@dataclass
class ValidationError:
 field: str
 message: str</p>

<p>def validate_schema(record: Dict[str, Any], required: Dict[str, type]) -&gt; List[ValidationError]:
 errors: List[ValidationError] = []
 for field, t in required.items():
 if field not in record:
 errors.append(ValidationError(field, “missing”))
 continue
 if record[field] is None:
 errors.append(ValidationError(field, “null”))
 continue
 if not isinstance(record[field], t):
 errors.append(ValidationError(field, f”type_mismatch: expected {t.<strong>name</strong>}”))
 return errors
``</p>

<h3 id="62-range-and-enum-checks">6.2 Range and enum checks</h3>

<p>``python
def validate_range(x: float, lo: float, hi: float) -&gt; bool:
 return lo &lt;= x &lt;= hi</p>

<p>def validate_enum(x: str, allowed: set[str]) -&gt; bool:
 return x in allowed
``</p>

<h3 id="63-drift-check-on-histograms-l1-distance">6.3 Drift check on histograms (L1 distance)</h3>

<p>``python
import numpy as np</p>

<p>def l1_hist_distance(h1: np.ndarray, h2: np.ndarray) -&gt; float:
 h1 = h1 / (np.sum(h1) + 1e-9)
 h2 = h2 / (np.sum(h2) + 1e-9)
 return float(np.sum(np.abs(h1 - h2)))
``</p>

<p>This is a lightweight baseline:</p>
<ul>
  <li>cheap</li>
  <li>interpretable</li>
  <li>works well for many numeric features when you bucket them</li>
</ul>

<h3 id="64-data-completeness-and-freshness-checks-table-level">6.4 Data completeness and freshness checks (table-level)</h3>

<p>Most catastrophic failures are “missing data” masquerading as “valid zeros”.
Two simple but high-signal checks:</p>
<ul>
  <li>expected record count bounds</li>
  <li>partition freshness bounds</li>
</ul>

<p>In practice:</p>
<ul>
  <li>training data: block if partition missing or count drops sharply</li>
  <li>serving: alert and fall back to last-known-good</li>
</ul>

<h3 id="65-a-minimal-policy-engine-passwarnblock">6.5 A minimal policy engine (pass/warn/block)</h3>

<p>``python
from dataclasses import dataclass
from typing import List</p>

<p>@dataclass
class PolicyDecision:
 status: str # “pass” | “warn” | “block”
 reasons: List[str]</p>

<p>def decide_policy(errors: List[str], warn_only: set[str]) -&gt; PolicyDecision:
 “””
 Very simple policy:</p>
<ul>
  <li>if any error is not in warn_only -&gt; block</li>
  <li>else if any errors exist -&gt; warn</li>
  <li>else pass
 “””
 if not errors:
 return PolicyDecision(“pass”, [])</li>
</ul>

<p>hard = [e for e in errors if e not in warn_only]
 if hard:
 return PolicyDecision(“block”, hard)
 return PolicyDecision(“warn”, errors)
``</p>

<p>In real systems, policies are per-dataset and per-severity tier.
But this skeleton captures the core idea: validation results must be translated into actions.</p>

<hr />

<h2 id="7-scaling-strategies">7. Scaling Strategies</h2>

<h3 id="71-sampling-vs-full-scans">7.1 Sampling vs full scans</h3>

<p>Deep validation on 100TB/day can’t scan everything.
Common approach:</p>
<ul>
  <li>run Tier 0/1 validations on full partitions (cheap aggregates)</li>
  <li>run Tier 2/3 on samples or sketches</li>
</ul>

<h3 id="72-sketches-for-distributions">7.2 Sketches for distributions</h3>

<p>Use streaming-friendly summaries:</p>
<ul>
  <li>Count-Min Sketch for frequencies</li>
  <li>HyperLogLog for cardinality</li>
  <li>t-digest / KLL for quantiles</li>
</ul>

<p>This enables:</p>
<ul>
  <li>drift detection without storing raw data</li>
  <li>low-latency dashboards</li>
</ul>

<h3 id="73-cardinality-governance">7.3 Cardinality governance</h3>

<p>High-cardinality features can DOS your validation and storage.
Guardrails:</p>
<ul>
  <li>label allowlists/denylists</li>
  <li>bucketing strategies (device model buckets)</li>
  <li>top-k heavy hitter tracking</li>
</ul>

<hr />

<h2 id="8-monitoring--metrics">8. Monitoring &amp; Metrics</h2>

<p>Monitor validators like production services:</p>
<ul>
  <li>validation latency and throughput</li>
  <li>percent partitions passing/warning/blocking</li>
  <li>top failing expectations</li>
  <li>“expectation churn” (how often thresholds change)</li>
  <li>quarantine volume</li>
</ul>

<p>Also monitor downstream impact:</p>
<ul>
  <li>training job failure rates due to gates</li>
  <li>serving fallback rates</li>
</ul>

<h3 id="81-rca-first-reporting-what-a-good-failure-report-contains">8.1 RCA-first reporting (what a good failure report contains)</h3>

<p>A validation failure should come with a “triage packet”:</p>
<ul>
  <li><strong>What failed</strong>: rule name, field, threshold</li>
  <li><strong>How bad</strong>: current value vs baseline, severity tier</li>
  <li><strong>Where</strong>: top segments affected (if applicable)</li>
  <li><strong>When</strong>: start time, detection time, duration</li>
  <li><strong>Why likely</strong>: correlated change events (deploys, schema changes)</li>
  <li><strong>Next</strong>: owner, runbook, rollback instructions, quarantine link</li>
</ul>

<p>This is the difference between:</p>
<ul>
  <li>“we saw drift” (noise)</li>
  <li>and “we know exactly what to fix” (actionable)</li>
</ul>

<h3 id="82-distribution-drift-at-scale-histograms-vs-sketches-vs-embeddings">8.2 Distribution drift at scale: histograms vs sketches vs embeddings</h3>

<p>Drift detection can be done with different representations:</p>

<ul>
  <li><strong>Histograms</strong></li>
  <li>easy to interpret</li>
  <li>good for numeric features with stable ranges</li>
  <li>
    <p>costs grow with number of bins × number of features</p>
  </li>
  <li><strong>Sketches</strong></li>
  <li>compact</li>
  <li>streaming-friendly</li>
  <li>great for cardinality and heavy hitters</li>
  <li>
    <p>less interpretable than histograms but still useful</p>
  </li>
  <li><strong>Embedding statistics</strong></li>
  <li>for high-dimensional features (text embeddings, image embeddings)</li>
  <li>track mean/variance, PCA projections, or cluster assignments</li>
  <li>useful for catching “silent changes” upstream (new encoder version)</li>
</ul>

<p>Practical platform approach:</p>
<ul>
  <li>use cheap, interpretable drift checks for “core” features</li>
  <li>use embedding stats and sketches for high-dimensional/high-cardinality features</li>
</ul>

<h3 id="83-when-validation-should-page-avoid-alert-fatigue">8.3 When validation should page (avoid alert fatigue)</h3>

<p>Validation alerts can easily become noisy. A robust policy:</p>
<ul>
  <li><strong>page</strong> only when user impact is likely (serving) or training integrity is at risk (training gates)</li>
  <li><strong>notify</strong> for moderate drift or new category growth</li>
  <li><strong>log-only</strong> for exploratory detectors</li>
</ul>

<p>This aligns with reality: teams have limited attention, and “too many alerts” is a failure mode of the system.</p>

<hr />

<h2 id="9-failure-modes-and-mitigations">9. Failure Modes (and Mitigations)</h2>

<h3 id="91-overly-strict-checks-false-positives">9.1 Overly strict checks (false positives)</h3>
<p>Mitigation:</p>
<ul>
  <li>severity tiers</li>
  <li>gradual rollout of new expectations (shadow mode)</li>
  <li>auto-suggest threshold updates with human review</li>
</ul>

<h3 id="92-too-loose-checks-false-negatives">9.2 Too loose checks (false negatives)</h3>
<p>Mitigation:</p>
<ul>
  <li>incident postmortems feed new expectations</li>
  <li>add drift checks for silent failures</li>
</ul>

<h3 id="93-unit-changes-and-schema-drift">9.3 Unit changes and schema drift</h3>
<p>Mitigation:</p>
<ul>
  <li>schema registry</li>
  <li>explicit unit metadata</li>
  <li>canary validations on new producer versions</li>
</ul>

<h3 id="94-pii-and-privacy-failures-validation-can-leak">9.4 PII and privacy failures (validation can leak!)</h3>

<p>Validation systems often process raw records, which may contain PII.
A common “oops” is that validation logs accidentally store:</p>
<ul>
  <li>raw emails/phone numbers</li>
  <li>full JSON payloads</li>
  <li>user IDs and sensitive attributes</li>
</ul>

<p>Mitigations:</p>
<ul>
  <li>treat validation reports as sensitive artifacts</li>
  <li>redact or hash PII in logs</li>
  <li>store only aggregated statistics by default</li>
  <li>gate sample extraction behind explicit access controls and audit logs</li>
</ul>

<p>This is particularly important if your validation system is used across many teams (multi-tenant).</p>

<h3 id="95-the-validator-outage-problem">9.5 The “validator outage” problem</h3>

<p>If validators are required gates, they become critical infrastructure.
Failure modes:</p>
<ul>
  <li>validation service down blocks training</li>
  <li>schema registry outage blocks ingestion</li>
  <li>state store lag causes false failures</li>
</ul>

<p>Mitigations:</p>
<ul>
  <li>redundancy and caching (serve last-known schema)</li>
  <li>graceful degradation (warn-only mode temporarily)</li>
  <li>explicit “break glass” workflows with audit logs</li>
</ul>

<p>In other words: the data validator must be operated like an SRE-owned service.</p>

<hr />

<h2 id="10-real-world-case-study">10. Real-World Case Study</h2>

<p>A classic incident:</p>
<ul>
  <li>upstream changes <code class="language-plaintext highlighter-rouge">duration_ms</code> to <code class="language-plaintext highlighter-rouge">duration_s</code></li>
  <li>models train fine but behave incorrectly</li>
</ul>

<p>Validation catches this via:</p>
<ul>
  <li>range checks (duration now too small)</li>
  <li>distribution shift (histogram moves)</li>
  <li>change-log correlation (producer version bump)</li>
</ul>

<p>The value is not the detection alone; it’s the fast RCA path.</p>

<h3 id="101-another-case-study-silent-label-leakage-and-too-good-to-be-true">10.1 Another case study: silent label leakage and “too good to be true”</h3>

<p>A different class of data validation failure is <strong>leakage</strong>:</p>
<ul>
  <li>a feature accidentally contains future information</li>
  <li>a join accidentally includes the label itself</li>
  <li>an ID leaks the target (e.g., “refund_status” used to predict refunds)</li>
</ul>

<p>Symptoms:</p>
<ul>
  <li>offline metrics jump dramatically</li>
  <li>training loss collapses unexpectedly fast</li>
  <li>online performance does not improve (or gets worse)</li>
</ul>

<p>Validation signals to add:</p>
<ul>
  <li>correlation checks between features and labels (suspiciously high correlation)</li>
  <li>“future timestamp” checks (feature timestamps after label timestamps)</li>
  <li>join key audits (prevent joining on future tables)</li>
</ul>

<p>This is why validation is not only about “schema correctness”; it’s also about <strong>semantic correctness</strong>.</p>

<h3 id="102-a-case-study-category-explosion-cardinality-drift">10.2 A case study: category explosion (cardinality drift)</h3>

<p>Example:</p>
<ul>
  <li>upstream starts logging raw URLs as a feature</li>
  <li>cardinality explodes from 1k to 10M</li>
</ul>

<p>Impact:</p>
<ul>
  <li>feature store storage blows up</li>
  <li>model quality degrades (rare categories, sparse embeddings)</li>
  <li>validators and dashboards become slow</li>
</ul>

<p>Validation mitigations:</p>
<ul>
  <li>enforce cardinality budgets</li>
  <li>bucket rare categories into “OTHER”</li>
  <li>allowlist canonical categories (domain restriction)</li>
</ul>

<p>This is the same domain restriction pattern as the DSA problem: decide what values are “valid” for downstream use.</p>

<hr />

<h2 id="11-cost-analysis">11. Cost Analysis</h2>

<p>Cost drivers:</p>
<ul>
  <li>compute for deep validations</li>
  <li>storage for quarantine and reports</li>
  <li>operational overhead (ownership, governance)</li>
</ul>

<p>Cost levers:</p>
<ul>
  <li>do cheap checks everywhere, deep checks selectively</li>
  <li>use sketches and aggregates</li>
  <li>reuse shared platform components</li>
</ul>

<h3 id="111-why-data-validation-becomes-a-platform-org-scale-evolution">11.1 Why data validation becomes a platform (org-scale evolution)</h3>

<p>Teams often start with:</p>
<ul>
  <li>ad-hoc SQL queries (“null rate looks high”)</li>
  <li>notebook checks</li>
  <li>one-off scripts</li>
</ul>

<p>At org scale, this fails because:</p>
<ul>
  <li>every team re-implements the same checks differently</li>
  <li>thresholds are undocumented and drift over time</li>
  <li>incidents repeat because postmortems don’t translate into enforceable gates</li>
  <li>no one owns cross-team datasets</li>
</ul>

<p>So validation becomes a platform:</p>
<ul>
  <li>shared schema registry and contracts</li>
  <li>shared expectation suites and policy engine</li>
  <li>shared RCA UI and quarantine flows</li>
  <li>shared governance (ownership, auditability, change logs)</li>
</ul>

<p>This is the same scaling story as anomaly detection: you build a platform because consistency and operability matter more than clever algorithms.</p>

<h3 id="112-training-vs-serving-cost-trade-offs">11.2 Training vs serving cost trade-offs</h3>

<p>Training validation:</p>
<ul>
  <li>can afford heavier scans (batch, offline)</li>
  <li>benefits from deep distribution checks (prevent poisoning)</li>
</ul>

<p>Serving validation:</p>
<ul>
  <li>must be low latency and non-blocking</li>
  <li>focuses on cheap guards + safe fallback behavior</li>
</ul>

<p>If you treat both as “the same pipeline”, you’ll either:</p>
<ul>
  <li>slow down serving (bad)</li>
  <li>or under-validate training (also bad)</li>
</ul>

<h3 id="113-what-to-do-when-you-cant-block-graceful-degradation">11.3 What to do when you can’t block (graceful degradation)</h3>

<p>In many real-time systems, you can’t block data entirely.
Patterns:</p>
<ul>
  <li>default missing features</li>
  <li>clamp out-of-range features</li>
  <li>route to a “safe” model variant that uses fewer features</li>
  <li>route requests to “shadow” logging to capture evidence without impacting users</li>
</ul>

<p>This is effectively “error handling for data”.
It’s an underappreciated part of ML system reliability.</p>

<hr />

<h2 id="12-key-takeaways">12. Key Takeaways</h2>

<ol>
  <li><strong>Define the valid domain</strong>: schema + ranges + constraints.</li>
  <li><strong>Use policy gates</strong>: validation without action is observability, not safety.</li>
  <li><strong>Make it operable</strong>: ownership, change logs, RCA-first reports.</li>
</ol>

<h3 id="121-appendix-a-minimal-expectations-checklist-for-new-datasets">12.1 Appendix: a minimal “expectations checklist” for new datasets</h3>

<p>When onboarding a dataset, start with:</p>

<ul>
  <li><strong>Schema</strong></li>
  <li>required fields present</li>
  <li>types correct</li>
  <li>
    <p>nullability documented</p>
  </li>
  <li><strong>Completeness</strong></li>
  <li>expected partition cadence (hourly/daily)</li>
  <li>record count bounds</li>
  <li>
    <p>freshness/latency bounds</p>
  </li>
  <li><strong>Range and enums</strong></li>
  <li>numeric ranges (guard impossible values)</li>
  <li>
    <p>known enums (country/device/app_version) with a safe “unknown” bucket</p>
  </li>
  <li><strong>Distribution</strong></li>
  <li>basic histograms for critical features</li>
  <li>
    <p>category cardinality limits (avoid explosion)</p>
  </li>
  <li><strong>Policy</strong></li>
  <li>warn vs block definitions</li>
  <li>ownership routing (who gets paged)</li>
  <li>links to runbooks and change logs</li>
</ul>

<p>The point is not perfection; it’s catching high-impact failures early.</p>

<h3 id="122-appendix-howconnects-across-tracks">12.2 Appendix: howconnects across tracks</h3>

<p>The same pattern appears in all four tracks:</p>
<ul>
  <li>DSA: restrict to <code class="language-plaintext highlighter-rouge">[1..n]</code> and detect the missing element</li>
  <li>ML: restrict to valid schema/range domains and detect missing/invalid partitions</li>
  <li>Speech: restrict to valid audio formats and detect corrupt audio</li>
  <li>Agents: restrict to valid tool schemas/policies and prevent unsafe actions</li>
</ul>

<p>It’s the same engineering move: define the domain, encode invariants, and treat edge cases explicitly.</p>

<h3 id="123-appendix-a-minimal-triage-packet-template-for-validators">12.3 Appendix: a minimal “triage packet” template for validators</h3>

<p>If you build a validation UI or Slack alert, include:</p>
<ul>
  <li>dataset name + partition</li>
  <li>failing rule + expected vs observed</li>
  <li>severity + policy decision (warn/block)</li>
  <li>top affected segments (if applicable)</li>
  <li>linked change log events (producer version, schema changes)</li>
  <li>owner and runbook link</li>
</ul>

<p>If you provide this consistently, you will see two outcomes:</p>
<ul>
  <li>faster MTTR</li>
  <li>fewer “mystery regressions” blamed on the model</li>
</ul>

<h3 id="124-appendix-quick-interview-framing">12.4 Appendix: quick interview framing</h3>

<p>If asked “how would you design data validation for ML?”:</p>
<ul>
  <li>Start with Tier 0/1 checks everywhere (schema, counts, nulls, ranges).</li>
  <li>Add Tier 2 drift checks for critical features (histograms/sketches).</li>
  <li>Add a policy engine (warn/block) and quarantine for RCA.</li>
  <li>Make it operable: ownership, audit logs, rollout/shadow mode for new rules.</li>
</ul>

<p>This is a strong answer because it emphasizes operability and governance, not just metrics.</p>

<h3 id="125-appendix-stop-the-line-vs-degrade-gracefully-a-crisp-rule">12.5 Appendix: “stop the line” vs “degrade gracefully” (a crisp rule)</h3>

<p>A simple rule that works well in practice:</p>

<ul>
  <li><strong>Stop the line</strong> (block) when:</li>
  <li>training data integrity is compromised (missing partition, unit mismatch, label leakage)</li>
  <li>
    <p>publishing features would poison many downstream models</p>
  </li>
  <li><strong>Degrade gracefully</strong> (fallback) when:</li>
  <li>serving needs to respond (online inference)</li>
  <li>you can default/clamp safely and alert the owner</li>
</ul>

<p>This prevents the most common confusion:
teams either block everything (causing outages) or allow everything (causing silent correctness failures).</p>

<h3 id="126-appendix-dataset-lineage-and-blast-radius-estimation">12.6 Appendix: dataset lineage and “blast radius” estimation</h3>

<p>When a dataset fails validation, the first question is:</p>
<blockquote>
  <p>“Who is impacted?”</p>
</blockquote>

<p>That requires lineage:</p>
<ul>
  <li>which downstream tables/features are derived from this dataset?</li>
  <li>which models consume those features?</li>
  <li>which products and teams are behind those models?</li>
</ul>

<p>Lineage enables:</p>
<ul>
  <li>targeted paging (don’t page everyone)</li>
  <li>smarter policies (block only the impacted publish path)</li>
  <li>faster mitigation (roll back a single feature, not the whole pipeline)</li>
</ul>

<p>Practical implementation:</p>
<ul>
  <li>maintain a DAG of datasets → features → models</li>
  <li>attach ownership metadata to each node</li>
  <li>include lineage in every validation alert (“blast radius: Models A,B; Products X,Y”)</li>
</ul>

<p>This is also where validation connects to incident management:
lineage is what turns “data is bad” into “here is what to do now”.</p>

<h3 id="127-appendix-validation-maturity-model">12.7 Appendix: validation maturity model</h3>

<p>A useful way to think about progress:</p>

<ul>
  <li><strong>Level 0: ad-hoc</strong></li>
  <li>manual SQL checks, notebooks</li>
  <li>
    <p>failures discovered after model regressions</p>
  </li>
  <li><strong>Level 1: schema checks</strong></li>
  <li>schema registry and required fields</li>
  <li>
    <p>basic null/range checks</p>
  </li>
  <li><strong>Level 2: gated publishing</strong></li>
  <li>warn/block policies</li>
  <li>
    <p>quarantine and RCA packets</p>
  </li>
  <li><strong>Level 3: drift and segment checks</strong></li>
  <li>histograms, sketches, segment baselines</li>
  <li>
    <p>change-log correlation</p>
  </li>
  <li><strong>Level 4: closed-loop reliability</strong></li>
  <li>automated mitigations (rollback, safe fallback models)</li>
  <li>continuous evaluation of validation rules (shadow mode for new rules)</li>
</ul>

<p>Most real orgs are between levels 1 and 3. Level 4 is where validation becomes a competitive advantage.</p>

<h3 id="128-appendix-join-validation-the-silent-correctness-killer">12.8 Appendix: join validation (the silent correctness killer)</h3>

<p>Many ML features come from joins:</p>
<ul>
  <li>user table JOIN events table</li>
  <li>item catalog JOIN impressions</li>
  <li>labels JOIN feature snapshots</li>
</ul>

<p>Joins fail silently in two ways:</p>

<ol>
  <li><strong>Fanout explosions</strong>
    <ul>
      <li>a one-to-one join becomes one-to-many</li>
      <li>record counts inflate</li>
      <li>models learn duplicated labels or skewed weights</li>
    </ul>
  </li>
  <li><strong>Join drop / sparsity</strong>
    <ul>
      <li>key mismatch causes many null joins</li>
      <li>features default unexpectedly</li>
      <li>online/offline skew increases</li>
    </ul>
  </li>
</ol>

<p>Validation checks for joins:</p>
<ul>
  <li>expected row count ratio bounds (post-join / pre-join)</li>
  <li>null join rate bounds (percent missing after join)</li>
  <li>key uniqueness checks (primary keys truly unique)</li>
  <li>“top join keys” diagnostics (are a few keys causing fanout?)</li>
</ul>

<p>This is high leverage because many ML regressions ultimately trace back to “the join changed”.</p>

<h3 id="129-appendix-skew-vs-drift-offline-vs-online-mismatch">12.9 Appendix: skew vs drift (offline vs online mismatch)</h3>

<p>Teams often mix these concepts:</p>
<ul>
  <li><strong>drift</strong>: distribution changes over time (today vs last week)</li>
  <li><strong>skew</strong>: distribution differs between training and serving (offline vs online)</li>
</ul>

<p>You can have:</p>
<ul>
  <li>low drift but high skew (offline pipeline differs from online)</li>
  <li>high drift but low skew (both pipelines drift together)</li>
</ul>

<p>Validation should handle both:</p>
<ul>
  <li>drift checks: time-based comparisons</li>
  <li>skew checks: offline features vs online features on the same entities</li>
</ul>

<p>Skew is especially important for:</p>
<ul>
  <li>feature stores</li>
  <li>real-time personalization</li>
  <li>feedback-loop systems</li>
</ul>

<h3 id="1210-appendix-a-concrete-expectation-suite-example">12.10 Appendix: a concrete expectation suite example</h3>

<p>For a dataset <code class="language-plaintext highlighter-rouge">user_events_daily</code>, a practical expectation suite might include:</p>

<ul>
  <li><strong>Schema</strong></li>
  <li>required: <code class="language-plaintext highlighter-rouge">user_id</code>, <code class="language-plaintext highlighter-rouge">event_time</code>, <code class="language-plaintext highlighter-rouge">event_type</code>, <code class="language-plaintext highlighter-rouge">device</code>, <code class="language-plaintext highlighter-rouge">country</code></li>
  <li>
    <p>types: <code class="language-plaintext highlighter-rouge">user_id</code> string, <code class="language-plaintext highlighter-rouge">event_time</code> timestamp</p>
  </li>
  <li><strong>Completeness</strong></li>
  <li>record_count within [0.8x, 1.2x] of 7-day median</li>
  <li>
    <p>partition latency &lt; 2 hours</p>
  </li>
  <li><strong>Null/range</strong></li>
  <li><code class="language-plaintext highlighter-rouge">country</code> null rate &lt; 0.1%</li>
  <li><code class="language-plaintext highlighter-rouge">event_time</code> within partition date +/- 24h</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">age</code> in [0, 120] if present</p>
  </li>
  <li><strong>Enums</strong></li>
  <li><code class="language-plaintext highlighter-rouge">event_type</code> in allowlist (with unknown bucket)</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">device</code> in known taxonomy buckets</p>
  </li>
  <li><strong>Distribution</strong></li>
  <li><code class="language-plaintext highlighter-rouge">event_type</code> proportion drift L1 distance &lt; threshold</li>
  <li>
    <p>heavy hitter checks on top event types and countries</p>
  </li>
  <li><strong>Join contracts</strong></li>
  <li>join with <code class="language-plaintext highlighter-rouge">user_profile</code> yields &lt; 1% missing profiles</li>
</ul>

<p>The key is not to validate everything; validate what prevents expensive incidents.</p>

<h3 id="1211-appendix-streaming-validation-patterns-fast-gates">12.11 Appendix: streaming validation patterns (fast gates)</h3>

<p>For Kafka-like streams, validation is usually:</p>
<ul>
  <li>schema validation at ingestion (drop/route invalid events)</li>
  <li>rate checks (sudden drop = pipeline outage)</li>
  <li>null-field checks for critical fields</li>
  <li>sampling-based distribution checks (cheap histograms)</li>
</ul>

<p>Streaming validation should be designed to:</p>
<ul>
  <li>be non-blocking (don’t stall producers)</li>
  <li>route bad events to a dead-letter queue</li>
  <li>emit counters and traces for RCA</li>
</ul>

<p>This is exactly how mature infra treats “invalid messages” in distributed systems.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0053-data-validation/">arunbaby.com/ml-system-design/0053-data-validation</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#data-quality" class="page__taxonomy-item p-category" rel="tag">data-quality</a><span class="sep">, </span>
    
      <a href="/tags/#data-validation" class="page__taxonomy-item p-category" rel="tag">data-validation</a><span class="sep">, </span>
    
      <a href="/tags/#drift" class="page__taxonomy-item p-category" rel="tag">drift</a><span class="sep">, </span>
    
      <a href="/tags/#mlops" class="page__taxonomy-item p-category" rel="tag">mlops</a><span class="sep">, </span>
    
      <a href="/tags/#monitoring" class="page__taxonomy-item p-category" rel="tag">monitoring</a><span class="sep">, </span>
    
      <a href="/tags/#schema" class="page__taxonomy-item p-category" rel="tag">schema</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0053-first-missing-positive/" rel="permalink">First Missing Positive
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          20 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The missing number is hiding in plain sight—use the array itself as the hash table.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0053-audio-quality-validation/" rel="permalink">Audio Quality Validation
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          20 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“If you don’t validate audio, you’ll debug ‘model regressions’ that are really microphone bugs.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0053-agent-deployment-patterns/" rel="permalink">Agent Deployment Patterns
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          21 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The hardest part of agents isn’t reasoning—it’s deploying them safely when the world is messy.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Data+Validation%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0053-data-validation%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0053-data-validation%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0053-data-validation/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0052-anomaly-detection/" class="pagination--pager" title="Anomaly Detection">Previous</a>
    
    
      <a href="/ml-system-design/0054-pattern-matching-in-ml/" class="pagination--pager" title="Pattern Matching in ML">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
