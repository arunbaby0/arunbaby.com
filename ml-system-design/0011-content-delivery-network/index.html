<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Content Delivery Networks (CDN) - Arun Baby</title>
<meta name="description" content="Design a global CDN for ML systems: Edge caching reduces latency from 500ms to 50ms. Critical for real-time predictions worldwide.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Content Delivery Networks (CDN)">
<meta property="og:url" content="https://www.arunbaby.com/ml-system-design/0011-content-delivery-network/">


  <meta property="og:description" content="Design a global CDN for ML systems: Edge caching reduces latency from 500ms to 50ms. Critical for real-time predictions worldwide.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Content Delivery Networks (CDN)">
  <meta name="twitter:description" content="Design a global CDN for ML systems: Edge caching reduces latency from 500ms to 50ms. Critical for real-time predictions worldwide.">
  <meta name="twitter:url" content="https://www.arunbaby.com/ml-system-design/0011-content-delivery-network/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T16:05:30+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ml-system-design/0011-content-delivery-network/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Content Delivery Networks (CDN)">
    <meta itemprop="description" content="Design a global CDN for ML systems: Edge caching reduces latency from 500ms to 50ms. Critical for real-time predictions worldwide.">
    <meta itemprop="datePublished" content="2025-12-29T16:05:30+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ml-system-design/0011-content-delivery-network/" itemprop="url">Content Delivery Networks (CDN)
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem-statement">Problem Statement</a><ul><li><a href="#why-do-we-need-a-cdn">Why Do We Need a CDN?</a></li><li><a href="#real-world-impact-on-ml-systems">Real-World Impact on ML Systems</a></li><li><a href="#what-cdn-does-for-you">What CDN Does for You</a></li><li><a href="#requirements">Requirements</a></li></ul></li><li><a href="#high-level-architecture">High-Level Architecture</a></li><li><a href="#core-components">Core Components</a><ul><li><a href="#1-edge-servers">1. Edge Servers</a></li><li><a href="#2-global-load-balancer--geodns">2. Global Load Balancer / GeoDNS</a></li><li><a href="#3-cache-invalidation-system">3. Cache Invalidation System</a></li></ul></li><li><a href="#ml-model-serving-at-edge">ML Model Serving at Edge</a><ul><li><a href="#edge-inference">Edge Inference</a></li><li><a href="#model-distribution-to-edge">Model Distribution to Edge</a></li></ul></li><li><a href="#monitoring--observability">Monitoring &amp; Observability</a><ul><li><a href="#cdn-metrics-dashboard">CDN Metrics Dashboard</a></li></ul></li><li><a href="#cost-optimization">Cost Optimization</a><ul><li><a href="#tiered-caching-strategy">Tiered Caching Strategy</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Design a global CDN for ML systems: Edge caching reduces latency from 500ms to 50ms. Critical for real-time predictions worldwide.</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p>Design a <strong>Content Delivery Network (CDN)</strong> for serving:</p>
<ol>
  <li><strong>ML model inference</strong> (predictions at the edge)</li>
  <li><strong>Static assets</strong> (model weights, configs, embeddings)</li>
  <li><strong>API responses</strong> (cached predictions, feature data)</li>
</ol>

<h3 id="why-do-we-need-a-cdn">Why Do We Need a CDN?</h3>

<p><strong>The Core Problem: Distance Creates Latency</strong></p>

<p>Imagine youâ€™re a user in Tokyo trying to access a website hosted in Virginia, USA:</p>

<p><code class="language-plaintext highlighter-rouge">
User (Tokyo) â”€â”€â”€â”€â”€â”€â”€â”€ 10,000 km â”€â”€â”€â”€â”€â”€â”€â”€ Server (Virginia)
 ~150ms round-trip time
</code></p>

<p><strong>The physics problem:</strong></p>
<ul>
  <li>Light travels at 300,000 km/s</li>
  <li>Signal in fiber: ~200,000 km/s</li>
  <li>Tokyo â†” Virginia: ~10,000 km</li>
  <li><strong>Theoretical minimum:</strong> 50ms</li>
  <li><strong>Reality with routing:</strong> 150-200ms</li>
</ul>

<p><strong>What if we could serve from Tokyo instead?</strong></p>

<p><code class="language-plaintext highlighter-rouge">
User (Tokyo) â”€â”€ 10 km â”€â”€ Edge Server (Tokyo)
 ~1-2ms!
</code></p>

<p>Thatâ€™s a <strong>75-100x improvement</strong> just from being geographically closer!</p>

<h3 id="real-world-impact-on-ml-systems">Real-World Impact on ML Systems</h3>

<p><strong>Scenario: Real-time recommendation system</strong></p>

<table>
  <thead>
    <tr>
      <th>Architecture</th>
      <th>Latency</th>
      <th>User Experience</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Without CDN</strong>: Request â†’ US â†’ Model Inference â†’ Response</td>
      <td>200ms+</td>
      <td>Noticeable delay, users leave</td>
    </tr>
    <tr>
      <td><strong>With CDN</strong>: Request â†’ Local Edge â†’ Cached/Local Inference â†’ Response</td>
      <td>20-50ms</td>
      <td>Feels instant âœ“</td>
    </tr>
  </tbody>
</table>

<p><strong>The business impact:</strong></p>
<ul>
  <li>Every 100ms of latency = 1% drop in sales (Amazon study)</li>
  <li>For ML systems: Users wonâ€™t wait for slow predictions</li>
  <li>CDN makes your ML system feel instant globally</li>
</ul>

<h3 id="what-cdn-does-for-you">What CDN Does for You</h3>

<p><strong>1. Geographic Distribution</strong>
Cache content at multiple locations worldwide (edge servers)</p>

<p><strong>2. Intelligent Caching</strong>
Store frequently accessed content close to users</p>

<p><strong>3. Smart Routing</strong>
Direct users to the best edge server (closest + healthy + low load)</p>

<p><strong>4. Fault Tolerance</strong>
If one edge fails, route to another</p>

<p><strong>5. Bandwidth Savings</strong>
Serve from edge â†’ Less traffic to origin â†’ Lower costs</p>

<h3 id="requirements">Requirements</h3>

<p><strong>Functional:</strong></p>
<ul>
  <li>Serve content from geographically distributed edge locations</li>
  <li>Cache popular content close to users</li>
  <li>Route requests to nearest/best edge server</li>
  <li>Handle cache invalidation and updates</li>
  <li>Support both static and dynamic content</li>
</ul>

<p><strong>Non-Functional:</strong></p>
<ul>
  <li><strong>Latency:</strong> &lt; 50ms p99 for edge hits (vs 200-500ms from origin)</li>
  <li><strong>Availability:</strong> 99.99% uptime (4 minutes downtime/month)</li>
  <li><strong>Scalability:</strong> Handle 1M+ requests/second globally</li>
  <li><strong>Cache hit rate:</strong> &gt; 80% for static content (fewer origin requests)</li>
  <li><strong>Global coverage:</strong> Presence in 50+ regions</li>
</ul>

<hr />

<h2 id="high-level-architecture">High-Level Architecture</h2>

<p><code class="language-plaintext highlighter-rouge">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER REQUESTS â”‚
â”‚ ğŸŒ Asia ğŸŒ Europe ğŸŒ Americas ğŸŒ Africa â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚ â”‚ â”‚ â”‚
 â†“ â†“ â†“ â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DNS / GLOBAL LOAD BALANCER â”‚
â”‚ â€¢ GeoDNS routing â”‚
â”‚ â€¢ Health checks â”‚
â”‚ â€¢ Latency-based routing â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚ â”‚ â”‚ â”‚
 â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
 â”‚ Edge â”‚ â”‚ Edge â”‚ â”‚ Edge â”‚ â”‚ Edge â”‚
 â”‚ Tokyo â”‚ â”‚ London â”‚ â”‚ N.Virginiaâ”‚ â”‚ Mumbai â”‚
 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
 â”‚ L1 Cacheâ”‚ â”‚ L1 Cacheâ”‚ â”‚ L1 Cacheâ”‚ â”‚ L1 Cacheâ”‚
 â”‚ (Redis) â”‚ â”‚ (Redis) â”‚ â”‚ (Redis) â”‚ â”‚ (Redis) â”‚
 â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
 â”‚ ML Modelâ”‚ â”‚ ML Modelâ”‚ â”‚ ML Modelâ”‚ â”‚ ML Modelâ”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
 â”‚ â”‚ â”‚ â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ ORIGIN SERVERS â”‚
 â”‚ â”‚
 â”‚ â€¢ Master modelsâ”‚
 â”‚ â€¢ Databases â”‚
 â”‚ â€¢ Feature storeâ”‚
 â”‚ â€¢ Object store â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></p>

<hr />

<h2 id="core-components">Core Components</h2>

<h3 id="1-edge-servers">1. Edge Servers</h3>

<p><strong>Purpose:</strong> Serve content from locations close to users</p>

<p>Before we dive into code, letâ€™s understand the concept:</p>

<p><strong>What is an Edge Server?</strong></p>

<p>Think of edge servers like local convenience stores:</p>
<ul>
  <li><strong>Origin Server</strong> = Central warehouse (far away, has everything)</li>
  <li><strong>Edge Server</strong> = Local store (nearby, has popular items)</li>
</ul>

<p>When you need milk:</p>
<ul>
  <li>Without edge: Drive to warehouse (30 min)</li>
  <li>With edge: Walk to local store (2 min)</li>
</ul>

<p><strong>Multi-Level Cache Strategy</strong></p>

<p>Edge servers use multiple cache layers:</p>

<p><code class="language-plaintext highlighter-rouge">
Request â†’ L1 Cache (Redis, in-memory) â† Fastest, smallest
 â†“ Miss
 L2 Cache (Disk, local SSD) â† Fast, medium
 â†“ Miss
 Origin Server (Database) â† Slow, largest
</code></p>

<p><strong>Why multiple levels?</strong></p>

<ol>
  <li><strong>L1 (Redis)</strong>: Hot data, 50-100ms access, expensive ($100/GB/month)</li>
  <li><strong>L2 (Disk)</strong>: Warm data, 5-10ms access, cheap ($10/GB/month)</li>
  <li><strong>Origin</strong>: Cold data, 100-500ms access, cheapest ($0.02/GB/month)</li>
</ol>

<p><strong>Trade-off</strong>: Speed vs Cost vs Capacity</p>

<table>
  <thead>
    <tr>
      <th>Cache</th>
      <th>Speed</th>
      <th>Cost</th>
      <th>Capacity</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>L1 (Redis)</td>
      <td>1ms</td>
      <td>High</td>
      <td>Small (10GB)</td>
      <td>Prediction results, hot features</td>
    </tr>
    <tr>
      <td>L2 (Disk)</td>
      <td>10ms</td>
      <td>Medium</td>
      <td>Medium (100GB)</td>
      <td>Model weights, embeddings</td>
    </tr>
    <tr>
      <td>Origin</td>
      <td>200ms</td>
      <td>Low</td>
      <td>Large (TB+)</td>
      <td>Full dataset, historical data</td>
    </tr>
  </tbody>
</table>

<p>``python
class EdgeServer:
 â€œâ€â€
 CDN edge server</p>

<p>Components:</p>
<ul>
  <li>L1 cache (Redis): Hot content</li>
  <li>L2 cache (local disk): Warm content</li>
  <li>ML model: For edge inference</li>
  <li>Origin client: Fetch misses from origin
 â€œâ€â€</li>
</ul>

<p>def <strong>init</strong>(self, region, origin_url):
 self.region = region
 self.origin_url = origin_url</p>

<p># Multi-level cache
 import redis
 import pickle
 self.l1_cache = redis.Redis(host=â€™localhostâ€™, port=6379)</p>

<p># Minimal DiskCache stub for illustration
 class DiskCache:
 def <strong>init</strong>(self, size_gb=100):
 self.store = {}
 def get(self, key):
 return self.store.get(key)
 def set(self, key, value):
 self.store[key] = value
 def delete(self, key):
 self.store.pop(key, None)
 def delete_pattern(self, pattern):
 # naive pattern matcher
 import fnmatch
 keys = [k for k in self.store.keys() if fnmatch.fnmatch(k, pattern)]
 for k in keys:
 self.store.pop(k, None)
 self.l2_cache = DiskCache(size_gb=100)</p>

<p># ML model for edge inference
 def load_model(path):
 return object()
 self.model = load_model(â€˜model.onnxâ€™)</p>

<p># Metrics
 self.metrics = EdgeMetrics()</p>

<p>async def handle_request(self, request):
 â€œâ€â€
 Handle incoming request</p>

<p>Flow:</p>
<ol>
  <li>Check L1 cache (Redis)</li>
  <li>Check L2 cache (disk)</li>
  <li>Fetch from origin</li>
  <li>Update caches
 â€œâ€â€
 import time, json, pickle
 start_time = time.time()</li>
</ol>

<p># Generate cache key
 cache_key = self._generate_cache_key(request)</p>

<p># Try L1 cache
 response = await self._check_l1_cache(cache_key)
 if response:
 self.metrics.record_hit(â€˜l1â€™, time.time() - start_time)
 return response</p>

<p># Try L2 cache
 response = await self._check_l2_cache(cache_key)
 if response:
 # Promote to L1
 await self._store_l1_cache(cache_key, response)
 self.metrics.record_hit(â€˜l2â€™, time.time() - start_time)
 return response</p>

<p># Cache miss: fetch from origin
 response = await self._fetch_from_origin(request)</p>

<p># Update caches
 await self._store_l1_cache(cache_key, response)
 await self._store_l2_cache(cache_key, response)</p>

<p>self.metrics.record_miss(time.time() - start_time)</p>

<p>return response</p>

<p>async def _check_l1_cache(self, key):
 â€œ"â€Check L1 (Redis) cacheâ€â€â€
 try:
 data = self.l1_cache.get(key)
 if data:
 return pickle.loads(data)
 except Exception as e:
 print(fâ€L1 cache error: {e}â€)</p>

<p>return None</p>

<p>async def _store_l1_cache(self, key, value, ttl=300):
 â€œ"â€Store in L1 cache with TTLâ€â€â€
 try:
 self.l1_cache.setex(
 key,
 ttl,
 pickle.dumps(value)
 )
 except Exception as e:
 print(fâ€L1 cache store error: {e}â€)</p>

<p>async def _check_l2_cache(self, key):
 â€œ"â€Check L2 (disk) cacheâ€â€â€
 return self.l2_cache.get(key)</p>

<p>async def _store_l2_cache(self, key, value):
 â€œ"â€Store in L2 cacheâ€â€â€
 self.l2_cache.set(key, value)</p>

<p>async def _fetch_from_origin(self, request):
 â€œ"â€Fetch from origin serverâ€â€â€
 import aiohttp</p>

<p>async with aiohttp.ClientSession() as session:
 async with session.post(
 fâ€{self.origin_url}{request.path}â€,
 json=request.data
 ) as response:
 return await response.json()</p>

<p>def _generate_cache_key(self, request):
 â€œ"â€Generate cache key from requestâ€â€â€
 import hashlib</p>

<p># Include path and normalized data
 key_data = fâ€{request.path}:{json.dumps(request.data, sort_keys=True)}â€
 return hashlib.md5(key_data.encode()).hexdigest()</p>

<p>class EdgeMetrics:
 â€œ"â€Track edge server metricsâ€â€â€</p>

<p>def <strong>init</strong>(self):
 self.l1_hits = 0
 self.l2_hits = 0
 self.misses = 0</p>

<p>self.l1_latencies = []
 self.l2_latencies = []
 self.miss_latencies = []</p>

<p>def record_hit(self, level, latency):
 if level == â€˜l1â€™:
 self.l1_hits += 1
 self.l1_latencies.append(latency)
 elif level == â€˜l2â€™:
 self.l2_hits += 1
 self.l2_latencies.append(latency)</p>

<p>def record_miss(self, latency):
 self.misses += 1
 self.miss_latencies.append(latency)</p>

<p>def get_stats(self):
 total = self.l1_hits + self.l2_hits + self.misses</p>

<p>return {
 â€˜l1_hit_rateâ€™: self.l1_hits / total if total &gt; 0 else 0,
 â€˜l2_hit_rateâ€™: self.l2_hits / total if total &gt; 0 else 0,
 â€˜miss_rateâ€™: self.misses / total if total &gt; 0 else 0,
 â€˜avg_l1_latency_msâ€™: np.mean(self.l1_latencies) * 1000 if self.l1_latencies else 0,
 â€˜avg_l2_latency_msâ€™: np.mean(self.l2_latencies) * 1000 if self.l2_latencies else 0,
 â€˜avg_miss_latency_msâ€™: np.mean(self.miss_latencies) * 1000 if self.miss_latencies else 0,
 }</p>

<h1 id="example-usage">Example usage</h1>
<p>edge = EdgeServer(region=â€™us-east-1â€™, origin_url=â€™https://api.example.comâ€™)</p>

<h1 id="simulate-requests">Simulate requests</h1>
<p>async def simulate_requests():
 for i in range(100):
 request = Request(
 path=â€™/predictâ€™,
 data={â€˜featuresâ€™: [1, 2, 3, 4, 5]}
 )</p>

<p>response = await edge.handle_request(request)
 print(fâ€Request {i}: {response}â€)</p>

<p># Print metrics
 stats = edge.metrics.get_stats()
 print(â€œ\nEdge Server Metrics:â€)
 for key, value in stats.items():
 if â€˜rateâ€™ in key:
 print(fâ€ {key}: {value:.2%}â€)
 else:
 print(fâ€ {key}: {value:.2f}â€)</p>

<h1 id="run">Run</h1>
<p>import asyncio
asyncio.run(simulate_requests())
``</p>

<h3 id="2-global-load-balancer--geodns">2. Global Load Balancer / GeoDNS</h3>

<p><strong>Purpose:</strong> Route requests to optimal edge server</p>

<p>``python
class GlobalLoadBalancer:
 â€œâ€â€
 Route requests to best edge server</p>

<p>Routing strategies:</p>
<ol>
  <li>Geographic proximity</li>
  <li>Server load</li>
  <li>Health status</li>
  <li>Network latency
 â€œâ€â€</li>
</ol>

<p>def <strong>init</strong>(self):
 self.edge_servers = self._discover_edge_servers()
 self.health_checker = HealthChecker(self.edge_servers)</p>

<p># Start health checking
 self.health_checker.start()</p>

<p>def route_request(self, client_ip, request):
 â€œâ€â€
 Route request to best edge server</p>

<p>Args:
 client_ip: Client IP address
 request: Request object</p>

<p>Returns:
 Best edge server
 â€œâ€â€
 # Get client location
 client_location = self._geolocate_ip(client_ip)</p>

<p># Get healthy edge servers
 healthy_servers = self.health_checker.get_healthy_servers()</p>

<p>if not healthy_servers:
 raise Exception(â€œNo healthy edge servers availableâ€)</p>

<p># Score each server
 scores = []</p>

<p>for server in healthy_servers:
 score = self._score_server(
 server,
 client_location,
 request
 )
 scores.append((server, score))</p>

<p># Sort by score (higher is better)
 scores.sort(key=lambda x: x[1], reverse=True)</p>

<p># Return best server
 return scores[0][0]</p>

<p>def _score_server(self, server, client_location, request):
 â€œâ€â€
 Score server for given request</p>

<p>Factors:</p>
<ul>
  <li>Geographic distance (weight: 0.5)</li>
  <li>Server load (weight: 0.3)</li>
  <li>Cache hit rate (weight: 0.2)
 â€œâ€â€
 # Geographic proximity
 distance = self._calculate_distance(
 client_location,
 server.location
 )
 distance_score = 1.0 / (1.0 + distance / 1000) # Normalize</li>
</ul>

<p># Server load
 load = server.get_current_load()
 load_score = 1.0 - min(load, 1.0)</p>

<p># Cache hit rate
 hit_rate = server.metrics.get_stats()[â€˜l1_hit_rateâ€™]</p>

<p># Weighted score
 score = (
 0.5 * distance_score +
 0.3 * load_score +
 0.2 * hit_rate
 )</p>

<p>return score</p>

<p>def _geolocate_ip(self, ip):
 â€œâ€â€
 Get geographic location from IP</p>

<p>Uses MaxMind GeoIP or similar
 â€œâ€â€
 import geoip2.database</p>

<p>reader = geoip2.database.Reader(â€˜GeoLite2-City.mmdbâ€™)
 response = reader.city(ip)</p>

<p>return {
 â€˜latâ€™: response.location.latitude,
 â€˜lonâ€™: response.location.longitude,
 â€˜cityâ€™: response.city.name,
 â€˜countryâ€™: response.country.name
 }</p>

<p>def _calculate_distance(self, loc1, loc2):
 â€œâ€â€
 Calculate distance between two locations (km)</p>

<p>Uses Haversine formula
 â€œâ€â€
 from math import radians, sin, cos, sqrt, atan2</p>

<p>R = 6371 # Earth radius in km</p>

<p>lat1, lon1 = radians(loc1[â€˜latâ€™]), radians(loc1[â€˜lonâ€™])
 lat2, lon2 = radians(loc2[â€˜latâ€™]), radians(loc2[â€˜lonâ€™])</p>

<p>dlat = lat2 - lat1
 dlon = lon2 - lon1</p>

<p>a = sin(dlat/2)<strong>2 + cos(lat1) * cos(lat2) * sin(dlon/2)</strong>2
 c = 2 * atan2(sqrt(a), sqrt(1-a))</p>

<p>distance = R * c</p>

<p>return distance</p>

<p>def _discover_edge_servers(self):
 â€œ"â€Discover available edge serversâ€â€â€
 # In production, this would query service registry
 return [
 EdgeServerInfo(â€˜us-east-1â€™, â€˜https://edge-us-east-1.example.comâ€™, {â€˜latâ€™: 39.0, â€˜lonâ€™: -77.5}),
 EdgeServerInfo(â€˜eu-west-1â€™, â€˜https://edge-eu-west-1.example.comâ€™, {â€˜latâ€™: 53.3, â€˜lonâ€™: -6.3}),
 EdgeServerInfo(â€˜ap-northeast-1â€™, â€˜https://edge-ap-northeast-1.example.comâ€™, {â€˜latâ€™: 35.7, â€˜lonâ€™: 139.7}),
 ]</p>

<p>class HealthChecker:
 â€œâ€â€
 Monitor health of edge servers</p>

<p>Checks:</p>
<ul>
  <li>HTTP health endpoint</li>
  <li>Response time</li>
  <li>Error rate
 â€œâ€â€</li>
</ul>

<p>def <strong>init</strong>(self, servers, check_interval=10):
 self.servers = servers
 self.check_interval = check_interval</p>

<p>self.health_status = {server.region: True for server in servers}
 self.last_check = {server.region: 0 for server in servers}</p>

<p>self.running = False</p>

<p>def start(self):
 â€œ"â€Start health checking in backgroundâ€â€â€
 self.running = True</p>

<p>import threading
 self.thread = threading.Thread(target=self._health_check_loop, daemon=True)
 self.thread.start()</p>

<p>def stop(self):
 â€œ"â€Stop health checkingâ€â€â€
 self.running = False</p>

<p>def _health_check_loop(self):
 â€œ"â€Health check loopâ€â€â€
 while self.running:
 for server in self.servers:
 healthy = self._check_server_health(server)
 self.health_status[server.region] = healthy
 self.last_check[server.region] = time.time()</p>

<p>import time
 time.sleep(self.check_interval)</p>

<p>def _check_server_health(self, server):
 â€œ"â€Check if server is healthyâ€â€â€
 try:
 import requests</p>

<p>response = requests.get(
 fâ€{server.url}/healthâ€,
 timeout=5
 )</p>

<p>if response.status_code == 200:
 # Check response time
 if response.elapsed.total_seconds() &lt; 1.0:
 return True</p>

<p>return False</p>

<p>except Exception as e:
 print(fâ€Health check failed for {server.region}: {e}â€)
 return False</p>

<p>def get_healthy_servers(self):
 â€œ"â€Get list of healthy serversâ€â€â€
 return [
 server for server in self.servers
 if self.health_status[server.region]
 ]</p>

<p>class EdgeServerInfo:
 â€œ"â€Edge server informationâ€â€â€
 def <strong>init</strong>(self, region, url, location):
 self.region = region
 self.url = url
 self.location = location
 self.metrics = EdgeMetrics()</p>

<p>def get_current_load(self):
 â€œ"â€Get current server load (0-1)â€â€â€
 # In production, query server metrics
 return 0.5 # Placeholder</p>

<h1 id="example-usage-1">Example usage</h1>
<p>glb = GlobalLoadBalancer()</p>

<h1 id="route-request">Route request</h1>
<p>client_ip = â€˜8.8.8.8â€™ # Google DNS (US)
request = Request(path=â€™/predictâ€™, data={})</p>

<p>best_server = glb.route_request(client_ip, request)
print(fâ€Routing to: {best_server.region}â€)
``</p>

<h3 id="3-cache-invalidation-system">3. Cache Invalidation System</h3>

<p><strong>Purpose:</strong> Propagate updates across edge servers</p>

<p>``python
class CacheInvalidationSystem:
 â€œâ€â€
 Propagate cache invalidations to edge servers</p>

<p>Methods:</p>
<ol>
  <li>Push-based: Immediate invalidation</li>
  <li>Pull-based: Periodic refresh</li>
  <li>TTL-based: Automatic expiration
 â€œâ€â€</li>
</ol>

<p>def <strong>init</strong>(self, edge_servers):
 self.edge_servers = edge_servers</p>

<p># Message queue for invalidations
 self.invalidation_queue = redis.Redis(host=â€™localhostâ€™, port=6379)</p>

<p># Pub/sub for real-time propagation
 self.pubsub = self.invalidation_queue.pubsub()
 self.pubsub.subscribe(â€˜cache:invalidateâ€™)</p>

<p>def invalidate(self, keys, pattern=False):
 â€œâ€â€
 Invalidate cache keys across all edge servers</p>

<p>Args:
 keys: List of keys to invalidate
 pattern: If True, treat keys as patterns
 â€œâ€â€
 message = {
 â€˜keysâ€™: keys,
 â€˜patternâ€™: pattern,
 â€˜timestampâ€™: time.time()
 }</p>

<p># Publish to all edge servers
 self.invalidation_queue.publish(
 â€˜cache:invalidateâ€™,
 json.dumps(message)
 )</p>

<p>print(fâ€Invalidated {len(keys)} keys across edge networkâ€)</p>

<p>def invalidate_prefix(self, prefix):
 â€œâ€â€
 Invalidate all keys with given prefix</p>

<p>Example: invalidate_prefix(â€˜user:123:*â€™)
 â€œâ€â€
 self.invalidate([prefix], pattern=True)</p>

<p>def invalidate_model_update(self, model_id):
 â€œâ€â€
 Invalidate caches after model update</p>

<p>Invalidates:</p>
<ul>
  <li>Model predictions</li>
  <li>Model metadata</li>
  <li>Related embeddings
 â€œâ€â€
 patterns = [
 fâ€model:{model_id}:<em>â€,
 fâ€prediction:{model_id}:</em>â€,
 fâ€embedding:{model_id}:*â€
 ]</li>
</ul>

<p>self.invalidate(patterns, pattern=True)</p>

<p>print(fâ€Invalidated caches for model {model_id}â€)</p>

<p>class EdgeInvalidationListener:
 â€œâ€â€
 Listen for invalidation messages on edge server
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, edge_server):
 self.edge_server = edge_server</p>

<p># Subscribe to invalidations
 self.pubsub = redis.Redis(host=â€™localhostâ€™, port=6379).pubsub()
 self.pubsub.subscribe(â€˜cache:invalidateâ€™)</p>

<p>self.running = False</p>

<p>def start(self):
 â€œ"â€Start listening for invalidationsâ€â€â€
 self.running = True</p>

<p>import threading
 self.thread = threading.Thread(target=self._listen_loop, daemon=True)
 self.thread.start()</p>

<p>def stop(self):
 â€œ"â€Stop listeningâ€â€â€
 self.running = False</p>

<p>def _listen_loop(self):
 â€œ"â€Listen for invalidation messagesâ€â€â€
 for message in self.pubsub.listen():
 if message[â€˜typeâ€™] == â€˜messageâ€™:
 data = json.loads(message[â€˜dataâ€™])
 self._handle_invalidation(data)</p>

<p>def _handle_invalidation(self, data):
 â€œ"â€Handle invalidation messageâ€â€â€
 keys = data[â€˜keysâ€™]
 pattern = data[â€˜patternâ€™]</p>

<p>if pattern:
 # Invalidate by pattern
 for key_pattern in keys:
 self._invalidate_pattern(key_pattern)
 else:
 # Invalidate specific keys
 for key in keys:
 self._invalidate_key(key)</p>

<p>def _invalidate_key(self, key):
 â€œ"â€Invalidate specific keyâ€â€â€
 # Remove from L1 cache
 self.edge_server.l1_cache.delete(key)</p>

<p># Remove from L2 cache
 self.edge_server.l2_cache.delete(key)</p>

<p>print(fâ€Invalidated key: {key}â€)</p>

<p>def _invalidate_pattern(self, pattern):
 â€œ"â€Invalidate keys matching patternâ€â€â€
 # Scan L1 cache
 for key in self.edge_server.l1_cache.scan_iter(match=pattern):
 self.edge_server.l1_cache.delete(key)</p>

<p># Scan L2 cache
 self.edge_server.l2_cache.delete_pattern(pattern)</p>

<p>print(fâ€Invalidated pattern: {pattern}â€)</p>

<h1 id="example-usage-2">Example usage</h1>
<p>edge_servers = [
 EdgeServer(â€˜us-east-1â€™, â€˜https://origin.example.comâ€™),
 EdgeServer(â€˜eu-west-1â€™, â€˜https://origin.example.comâ€™),
]</p>

<p>invalidation_system = CacheInvalidationSystem(edge_servers)</p>

<h1 id="start-listeners-on-each-edge">Start listeners on each edge</h1>
<p>for edge in edge_servers:
 listener = EdgeInvalidationListener(edge)
 listener.start()</p>

<h1 id="trigger-invalidation">Trigger invalidation</h1>
<p>invalidation_system.invalidate_model_update(â€˜model_v2â€™)
``</p>

<hr />

<h2 id="ml-model-serving-at-edge">ML Model Serving at Edge</h2>

<h3 id="edge-inference">Edge Inference</h3>

<p>``python
class EdgeMLServer:
 â€œâ€â€
 Serve ML models at edge for low-latency inference</p>

<p>Benefits:</p>
<ul>
  <li>Reduced latency (no round trip to origin)</li>
  <li>Reduced bandwidth</li>
  <li>Better privacy (data doesnâ€™t leave region)
 â€œâ€â€</li>
</ul>

<p>def <strong>init</strong>(self, model_path):
 # Load ONNX model for edge inference
 import onnxruntime as ort</p>

<p>self.session = ort.InferenceSession(
 model_path,
 providers=[â€˜CPUExecutionProviderâ€™]
 )</p>

<p>self.input_name = self.session.get_inputs()[0].name
 self.output_name = self.session.get_outputs()[0].name</p>

<p># Cache for predictions
 self.prediction_cache = LRUCache(capacity=10000)</p>

<p>def predict(self, features):
 â€œâ€â€
 Predict with caching</p>

<p>Args:
 features: Input features (must be hashable)</p>

<p>Returns:
 Prediction
 â€œâ€â€
 # Generate cache key
 cache_key = hash(features)</p>

<p># Check cache
 cached_prediction = self.prediction_cache.get(cache_key)
 if cached_prediction != -1:
 return cached_prediction</p>

<p># Run inference
 features_array = np.array([features], dtype=np.float32)</p>

<p>prediction = self.session.run(
 [self.output_name],
 {self.input_name: features_array}
 )[0][0]</p>

<p># Cache result
 self.prediction_cache.put(cache_key, prediction)</p>

<p>return prediction</p>

<p>async def batch_predict(self, features_list):
 â€œâ€â€
 Batch prediction for efficiency</p>

<p>Separates cache hits from misses
 â€œâ€â€
 predictions = {}
 cache_misses = []
 cache_miss_indices = []</p>

<p># Check cache
 for i, features in enumerate(features_list):
 cache_key = hash(features)
 cached = self.prediction_cache.get(cache_key)</p>

<p>if cached != -1:
 predictions[i] = cached
 else:
 cache_misses.append(features)
 cache_miss_indices.append(i)</p>

<p># Batch inference for cache misses
 if cache_misses:
 features_array = np.array(cache_misses, dtype=np.float32)</p>

<p>batch_predictions = self.session.run(
 [self.output_name],
 {self.input_name: features_array}
 )[0]</p>

<p># Store in cache and results
 for i, pred in zip(cache_miss_indices, batch_predictions):
 cache_key = hash(features_list[i])
 self.prediction_cache.put(cache_key, pred)
 predictions[i] = pred</p>

<p># Return in original order
 return [predictions[i] for i in range(len(features_list))]</p>

<h1 id="example-edge-api-server-with-ml-inference">Example: Edge API server with ML inference</h1>
<p>from fastapi import FastAPI
import uvicorn</p>

<p>app = FastAPI()</p>

<h1 id="load-model-at-startup">Load model at startup</h1>
<p>edge_ml_server = EdgeMLServer(â€˜model.onnxâ€™)</p>

<p>@app.post(â€œ/predictâ€)
async def predict(request: dict):
 â€œâ€â€
 Edge prediction endpoint</p>

<p>Returns cached or computed prediction
 â€œâ€â€
 features = tuple(request[â€˜featuresâ€™])</p>

<p>try:
 prediction = edge_ml_server.predict(features)</p>

<p>return {
 â€˜predictionâ€™: float(prediction),
 â€˜cachedâ€™: edge_ml_server.prediction_cache.get(hash(features)) != -1,
 â€˜edge_regionâ€™: â€˜us-east-1â€™
 }
 except Exception as e:
 return {â€˜errorâ€™: str(e)}, 500</p>

<p>@app.post(â€œ/batch_predictâ€)
async def batch_predict(request: dict):
 â€œâ€â€
 Batch prediction endpoint
 â€œâ€â€
 features_list = [tuple(f) for f in request[â€˜featuresâ€™]]</p>

<p>predictions = await edge_ml_server.batch_predict(features_list)</p>

<p>return {
 â€˜predictionsâ€™: [float(p) for p in predictions],
 â€˜countâ€™: len(predictions),
 â€˜edge_regionâ€™: â€˜us-east-1â€™
 }</p>

<h1 id="run-edge-server">Run edge server</h1>
<h1 id="uvicornrunapp-host0000-port8000">uvicorn.run(app, host=â€™0.0.0.0â€™, port=8000)</h1>
<p>``</p>

<h3 id="model-distribution-to-edge">Model Distribution to Edge</h3>

<p>``python
class ModelDistributionSystem:
 â€œâ€â€
 Distribute ML models to edge servers</p>

<p>Challenges:</p>
<ul>
  <li>Large model sizes (GB)</li>
  <li>Many edge locations</li>
  <li>Version management</li>
  <li>Atomic updates
 â€œâ€â€</li>
</ul>

<p>def <strong>init</strong>(self, s3_bucket, edge_servers):
 self.s3_bucket = s3_bucket
 self.edge_servers = edge_servers</p>

<p># Track model versions at each edge
 self.edge_versions = {
 server.region: None
 for server in edge_servers
 }</p>

<p>def distribute_model(self, model_path, version):
 â€œâ€â€
 Distribute model to all edge servers</p>

<p>Steps:</p>
<ol>
  <li>Upload to S3</li>
  <li>Notify edge servers</li>
  <li>Edge servers download</li>
  <li>Edge servers validate</li>
  <li>Edge servers activate
 â€œâ€â€
 print(fâ€Distributing model {version} to {len(self.edge_servers)} edge serversâ€¦â€)</li>
</ol>

<p># Step 1: Upload to S3
 s3_key = fâ€models/{version}/model.onnxâ€
 self._upload_to_s3(model_path, s3_key)</p>

<p># Step 2: Notify edge servers
 results = []</p>

<p>for server in self.edge_servers:
 result = self._distribute_to_edge(server, s3_key, version)
 results.append((server.region, result))</p>

<p># Check results
 successful = [r for r in results if r[1]]
 failed = [r for r in results if not r[1]]</p>

<p>print(fâ€\nDistribution complete:â€)
 print(fâ€ Successful: {len(successful)}/{len(self.edge_servers)}â€)
 print(fâ€ Failed: {len(failed)}â€)</p>

<p>if failed:
 print(fâ€ Failed regions: {[r[0] for r in failed]}â€)</p>

<p>return len(failed) == 0</p>

<p>def _upload_to_s3(self, local_path, s3_key):
 â€œ"â€Upload model to S3â€â€â€
 import boto3</p>

<p>s3 = boto3.client(â€˜s3â€™)</p>

<p>print(fâ€Uploading {local_path} to s3://{self.s3_bucket}/{s3_key}â€)</p>

<p>s3.upload_file(
 local_path,
 self.s3_bucket,
 s3_key,
 ExtraArgs={â€˜ServerSideEncryptionâ€™: â€˜AES256â€™}
 )</p>

<p>def _distribute_to_edge(self, server, s3_key, version):
 â€œâ€â€
 Notify edge server to download model</p>

<p>Edge server will:</p>
<ol>
  <li>Download from S3</li>
  <li>Validate checksum</li>
  <li>Load model</li>
  <li>Run health checks</li>
  <li>Activate (atomic swap)
 â€œâ€â€
 try:
 import requests</li>
</ol>

<p>response = requests.post(
 fâ€{server.url}/admin/update_modelâ€,
 json={
 â€˜s3_bucketâ€™: self.s3_bucket,
 â€˜s3_keyâ€™: s3_key,
 â€˜versionâ€™: version
 },
 timeout=300 # 5 minutes for large models
 )</p>

<p>if response.status_code == 200:
 self.edge_versions[server.region] = version
 print(fâ€ âœ“ {server.region}: Updated to {version}â€)
 return True
 else:
 print(fâ€ âœ— {server.region}: Failed - {response.text}â€)
 return False</p>

<p>except Exception as e:
 print(fâ€ âœ— {server.region}: Error - {e}â€)
 return False</p>

<p>def rollback_model(self, target_version):
 â€œâ€â€
 Rollback to previous model version</p>

<p>Useful if new model has issues
 â€œâ€â€
 print(fâ€Rolling back to version {target_version}â€¦â€)</p>

<p>s3_key = fâ€models/{target_version}/model.onnxâ€</p>

<p>return self.distribute_model(fâ€/tmp/model_{target_version}.onnxâ€, target_version)</p>

<p>def get_version_status(self):
 â€œ"â€Get model versions deployed at each edgeâ€â€â€
 return self.edge_versions</p>

<h1 id="example-usage-3">Example usage</h1>
<p>edge_servers = [
 EdgeServerInfo(â€˜us-east-1â€™, â€˜https://edge-us-east-1.example.comâ€™, {}),
 EdgeServerInfo(â€˜eu-west-1â€™, â€˜https://edge-eu-west-1.example.comâ€™, {}),
 EdgeServerInfo(â€˜ap-northeast-1â€™, â€˜https://edge-ap-northeast-1.example.comâ€™, {}),
]</p>

<p>distributor = ModelDistributionSystem(
 s3_bucket=â€™my-ml-modelsâ€™,
 edge_servers=edge_servers
)</p>

<h1 id="distribute-new-model">Distribute new model</h1>
<p>success = distributor.distribute_model(â€˜model_v3.onnxâ€™, â€˜v3â€™)</p>

<p>if success:
 print(â€œ\nModel distribution successful!â€)
 print(â€œCurrent versions:â€)
 for region, version in distributor.get_version_status().items():
 print(fâ€ {region}: {version}â€)
else:
 print(â€œ\nModel distribution failed, rolling backâ€¦â€)
 distributor.rollback_model(â€˜v2â€™)
``</p>

<hr />

<h2 id="monitoring--observability">Monitoring &amp; Observability</h2>

<h3 id="cdn-metrics-dashboard">CDN Metrics Dashboard</h3>

<p>``python
class CDNMetricsDashboard:
 â€œâ€â€
 Aggregate and visualize CDN metrics</p>

<p>Key metrics:</p>
<ul>
  <li>Cache hit rate</li>
  <li>Latency (p50, p95, p99)</li>
  <li>Bandwidth usage</li>
  <li>Error rate</li>
  <li>Request rate
 â€œâ€â€</li>
</ul>

<p>def <strong>init</strong>(self, edge_servers):
 self.edge_servers = edge_servers</p>

<p># Time series database for metrics
 from prometheus_client import Counter, Histogram, Gauge</p>

<p>self.request_count = Counter(
 â€˜cdn_requests_totalâ€™,
 â€˜Total CDN requestsâ€™,
 [â€˜regionâ€™, â€˜statusâ€™]
 )</p>

<p>self.latency = Histogram(
 â€˜cdn_request_latency_secondsâ€™,
 â€˜CDN request latencyâ€™,
 [â€˜regionâ€™, â€˜cache_levelâ€™]
 )</p>

<p>self.cache_hit_rate = Gauge(
 â€˜cdn_cache_hit_rateâ€™,
 â€˜Cache hit rateâ€™,
 [â€˜regionâ€™, â€˜cache_levelâ€™]
 )</p>

<p>def collect_metrics(self):
 â€œâ€â€
 Collect metrics from all edge servers</p>

<p>Returns aggregated view
 â€œâ€â€
 global_metrics = {
 â€˜total_requestsâ€™: 0,
 â€˜total_cache_hitsâ€™: 0,
 â€˜regionsâ€™: {}
 }</p>

<p>for server in self.edge_servers:
 stats = server.metrics.get_stats()</p>

<p>total_requests = (
 server.metrics.l1_hits +
 server.metrics.l2_hits +
 server.metrics.misses
 )</p>

<p>total_cache_hits = server.metrics.l1_hits + server.metrics.l2_hits</p>

<p>global_metrics[â€˜total_requestsâ€™] += total_requests
 global_metrics[â€˜total_cache_hitsâ€™] += total_cache_hits</p>

<p>global_metrics[â€˜regionsâ€™][server.region] = {
 â€˜requestsâ€™: total_requests,
 â€˜cache_hitsâ€™: total_cache_hits,
 â€˜statsâ€™: stats
 }</p>

<p># Calculate global hit rate
 if global_metrics[â€˜total_requestsâ€™] &gt; 0:
 global_metrics[â€˜cache_hit_rateâ€™] = (
 global_metrics[â€˜total_cache_hitsâ€™] / global_metrics[â€˜total_requestsâ€™]
 )
 else:
 global_metrics[â€˜cache_hit_rateâ€™] = 0</p>

<p>return global_metrics</p>

<p>def print_dashboard(self):
 â€œ"â€Print metrics dashboardâ€â€â€
 metrics = self.collect_metrics()</p>

<p>print(â€œ\nâ€ + â€œ=â€<em>70)
 print(â€œCDN METRICS DASHBOARDâ€)
 print(â€œ=â€</em>70)</p>

<p>print(fâ€\nGlobal Metrics:â€)
 print(fâ€ Total Requests: {metrics[â€˜total_requestsâ€™]:,}â€)
 print(fâ€ Cache Hit Rate: {metrics[â€˜cache_hit_rateâ€™]:.2%}â€)</p>

<p>print(fâ€\nRegional Breakdown:â€)</p>

<p>for region, data in metrics[â€˜regionsâ€™].items():
 print(fâ€\n {region}:â€)
 print(fâ€ Requests: {data[â€˜requestsâ€™]:,}â€)
 print(fâ€ L1 Hit Rate: {data[â€˜statsâ€™][â€˜l1_hit_rateâ€™]:.2%}â€)
 print(fâ€ L2 Hit Rate: {data[â€˜statsâ€™][â€˜l2_hit_rateâ€™]:.2%}â€)
 print(fâ€ Miss Rate: {data[â€˜statsâ€™][â€˜miss_rateâ€™]:.2%}â€)
 print(fâ€ Avg L1 Latency: {data[â€˜statsâ€™][â€˜avg_l1_latency_msâ€™]:.2f}msâ€)
 print(fâ€ Avg Miss Latency: {data[â€˜statsâ€™][â€˜avg_miss_latency_msâ€™]:.2f}msâ€)</p>

<p>print(â€œ=â€*70)</p>

<p>def plot_latency_distribution(self):
 â€œ"â€Plot latency distribution by regionâ€â€â€
 import matplotlib.pyplot as plt</p>

<p>fig, axes = plt.subplots(len(self.edge_servers), 1, figsize=(12, 4 * len(self.edge_servers)))</p>

<p>for i, server in enumerate(self.edge_servers):
 ax = axes[i] if len(self.edge_servers) &gt; 1 else axes</p>

<p># Get latencies
 l1_latencies = np.array(server.metrics.l1_latencies) * 1000 # ms
 l2_latencies = np.array(server.metrics.l2_latencies) * 1000
 miss_latencies = np.array(server.metrics.miss_latencies) * 1000</p>

<p># Plot histograms
 ax.hist(l1_latencies, bins=50, alpha=0.5, label=â€™L1 Cacheâ€™, color=â€™greenâ€™)
 ax.hist(l2_latencies, bins=50, alpha=0.5, label=â€™L2 Cacheâ€™, color=â€™blueâ€™)
 ax.hist(miss_latencies, bins=50, alpha=0.5, label=â€™Originâ€™, color=â€™redâ€™)</p>

<p>ax.set_xlabel(â€˜Latency (ms)â€™)
 ax.set_ylabel(â€˜Frequencyâ€™)
 ax.set_title(fâ€™Latency Distribution - {server.region}â€™)
 ax.legend()
 ax.grid(True, alpha=0.3)</p>

<p>plt.tight_layout()
 plt.savefig(â€˜cdn_latency_distribution.pngâ€™)
 plt.close()</p>

<p>print(â€œLatency distribution plot saved to cdn_latency_distribution.pngâ€)</p>

<h1 id="example-usage-4">Example usage</h1>
<p>edge_servers = [
 # â€¦ initialize edge servers
]</p>

<p>dashboard = CDNMetricsDashboard(edge_servers)</p>

<h1 id="collect-and-display-metrics">Collect and display metrics</h1>
<p>dashboard.print_dashboard()</p>

<h1 id="plot-latency-distribution">Plot latency distribution</h1>
<p>dashboard.plot_latency_distribution()
``</p>

<hr />

<h2 id="cost-optimization">Cost Optimization</h2>

<h3 id="tiered-caching-strategy">Tiered Caching Strategy</h3>

<p>``python
class TieredCachingStrategy:
 â€œâ€â€
 Optimize costs with tiered caching</p>

<p>Tiers:</p>
<ol>
  <li>Hot (L1 - Redis): Most accessed, expensive, fast</li>
  <li>Warm (L2 - Local disk): Frequently accessed, cheap, medium speed</li>
  <li>Cold (S3): Rarely accessed, cheapest, slow</li>
</ol>

<p>Move items between tiers based on access patterns
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.l1_cost_per_gb_per_month = 100 # Redis
 self.l2_cost_per_gb_per_month = 10 # SSD
 self.l3_cost_per_gb_per_month = 0.02 # S3</p>

<p>self.l1_size_gb = 10
 self.l2_size_gb = 100
 self.l3_size_gb = 1000</p>

<p>def calculate_monthly_cost(self):
 â€œ"â€Calculate monthly storage costâ€â€â€
 l1_cost = self.l1_size_gb * self.l1_cost_per_gb_per_month
 l2_cost = self.l2_size_gb * self.l2_cost_per_gb_per_month
 l3_cost = self.l3_size_gb * self.l3_cost_per_gb_per_month</p>

<p>total_cost = l1_cost + l2_cost + l3_cost</p>

<p>return {
 â€˜l1_costâ€™: l1_cost,
 â€˜l2_costâ€™: l2_cost,
 â€˜l3_costâ€™: l3_cost,
 â€˜total_costâ€™: total_cost
 }</p>

<p>def optimize_tier_sizes(self, access_patterns):
 â€œâ€â€
 Optimize tier sizes based on access patterns</p>

<p>Goal: Minimize cost while maintaining hit rate
 â€œâ€â€
 # Analyze access frequency
 access_freq = {}</p>

<p>for item_id, accesses in access_patterns.items():
 access_freq[item_id] = len(accesses)</p>

<p># Sort by frequency
 sorted_items = sorted(
 access_freq.items(),
 key=lambda x: x[1],
 reverse=True
 )</p>

<p># Allocate to tiers
 l1_items = sorted_items[:100] # Top 100
 l2_items = sorted_items[100:1000] # Next 900
 l3_items = sorted_items[1000:] # Rest</p>

<p>print(fâ€Tier allocation:â€)
 print(fâ€ L1 (Hot): {len(l1_items)} itemsâ€)
 print(fâ€ L2 (Warm): {len(l2_items)} itemsâ€)
 print(fâ€ L3 (Cold): {len(l3_items)} itemsâ€)</p>

<p># Calculate expected hit rate
 total_accesses = sum(access_freq.values())
 l1_accesses = sum(freq for _, freq in l1_items)
 l2_accesses = sum(freq for _, freq in l2_items)</p>

<p>l1_hit_rate = l1_accesses / total_accesses
 l2_hit_rate = l2_accesses / total_accesses</p>

<p>print(fâ€\nExpected hit rates:â€)
 print(fâ€ L1: {l1_hit_rate:.2%}â€)
 print(fâ€ L2: {l2_hit_rate:.2%}â€)
 print(fâ€ Combined (L1+L2): {(l1_hit_rate + l2_hit_rate):.2%}â€)</p>

<h1 id="example">Example</h1>
<p>strategy = TieredCachingStrategy()</p>

<h1 id="calculate-costs">Calculate costs</h1>
<p>costs = strategy.calculate_monthly_cost()
print(â€œMonthly CDN storage costs:â€)
for key, value in costs.items():
 print(fâ€ {key}: ${value:.2f}â€)</p>

<h1 id="simulate-access-patterns">Simulate access patterns</h1>
<p>access_patterns = {
 fâ€item_{i}â€: [time.time() - random.random() * 86400 for _ in range(random.randint(1, 100))]
 for i in range(10000)
}</p>

<h1 id="optimize">Optimize</h1>
<p>strategy.optimize_tier_sizes(access_patterns)
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>âœ… <strong>Edge caching</strong> - Serve content close to users for low latency 
âœ… <strong>Multi-level cache</strong> - L1 (Redis), L2 (disk), origin (database) 
âœ… <strong>Smart routing</strong> - GeoDNS + latency-based + load-based 
âœ… <strong>Cache invalidation</strong> - Pub/sub for real-time propagation 
âœ… <strong>Edge ML serving</strong> - Deploy models to edge for fast inference 
âœ… <strong>Cost optimization</strong> - Tiered storage based on access patterns</p>

<p><strong>Key Metrics:</strong></p>
<ul>
  <li>Cache hit rate: &gt; 80%</li>
  <li>P99 latency: &lt; 50ms for cache hits</li>
  <li>Origin latency: 200-500ms</li>
  <li>Bandwidth savings: 70-90%</li>
</ul>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ml-system-design/0011-content-delivery-network/">arunbaby.com/ml-system-design/0011-content-delivery-network</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#caching" class="page__taxonomy-item p-category" rel="tag">caching</a><span class="sep">, </span>
    
      <a href="/tags/#cdn" class="page__taxonomy-item p-category" rel="tag">cdn</a><span class="sep">, </span>
    
      <a href="/tags/#edge-computing" class="page__taxonomy-item p-category" rel="tag">edge-computing</a><span class="sep">, </span>
    
      <a href="/tags/#global-distribution" class="page__taxonomy-item p-category" rel="tag">global-distribution</a><span class="sep">, </span>
    
      <a href="/tags/#latency-optimization" class="page__taxonomy-item p-category" rel="tag">latency-optimization</a><span class="sep">, </span>
    
      <a href="/tags/#load-balancing" class="page__taxonomy-item p-category" rel="tag">load-balancing</a><span class="sep">, </span>
    
      <a href="/tags/#model-serving" class="page__taxonomy-item p-category" rel="tag">model-serving</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ml-system-design" class="page__taxonomy-item p-category" rel="tag">ml-system-design</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0011-lru-cache/" rel="permalink">LRU Cache
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          27 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master LRU cache design: O(1) get/put with hash map + doubly linked list. Critical for interviews and production caching systems.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0011-speech-separation/" rel="permalink">Speech Separation
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          20 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Separate overlapping speakers with 99%+ accuracy: Deep learning solves the cocktail party problem for meeting transcription and voice assistants.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0011-vector-search-for-agents/" rel="permalink">Vector Search for Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">â€œFinding a Needle in a High-Dimensional Haystack: The Mathematics of Recall.â€
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Content+Delivery+Networks+%28CDN%29%20https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0011-content-delivery-network%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fml-system-design%2F0011-content-delivery-network%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ml-system-design/0011-content-delivery-network/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ml-system-design/0010-caching-strategies/" class="pagination--pager" title="Caching Strategies for ML Systems">Previous</a>
    
    
      <a href="/ml-system-design/0012-distributed-systems/" class="pagination--pager" title="Distributed ML Systems">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
