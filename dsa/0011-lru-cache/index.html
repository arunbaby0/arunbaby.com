<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>LRU Cache - Arun Baby</title>
<meta name="description" content="Master LRU cache design: O(1) get/put with hash map + doubly linked list. Critical for interviews and production caching systems.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="LRU Cache">
<meta property="og:url" content="https://www.arunbaby.com/dsa/0011-lru-cache/">


  <meta property="og:description" content="Master LRU cache design: O(1) get/put with hash map + doubly linked list. Critical for interviews and production caching systems.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="LRU Cache">
  <meta name="twitter:description" content="Master LRU cache design: O(1) get/put with hash map + doubly linked list. Critical for interviews and production caching systems.">
  <meta name="twitter:url" content="https://www.arunbaby.com/dsa/0011-lru-cache/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/dsa/0011-lru-cache/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="LRU Cache">
    <meta itemprop="description" content="Master LRU cache design: O(1) get/put with hash map + doubly linked list. Critical for interviews and production caching systems.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/dsa/0011-lru-cache/" itemprop="url">LRU Cache
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          27 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem-statement">Problem Statement</a><ul><li><a href="#examples">Examples</a></li><li><a href="#constraints">Constraints</a></li></ul></li><li><a href="#understanding-lru-cache">Understanding LRU Cache</a><ul><li><a href="#what-is-lru">What is LRU?</a></li><li><a href="#why-lru-the-intuition">Why LRU? The Intuition</a></li><li><a href="#real-world-examples">Real-World Examples</a></li><li><a href="#the-challenge-achieving-o1-operations">The Challenge: Achieving O(1) Operations</a></li><li><a href="#understanding-the-data-structure-choice">Understanding the Data Structure Choice</a></li><li><a href="#the-dummy-node-trick">The Dummy Node Trick</a></li><li><a href="#visualization">Visualization</a></li></ul></li><li><a href="#solution-1-hash-map--doubly-linked-list-optimal">Solution 1: Hash Map + Doubly Linked List (Optimal)</a><ul><li><a href="#intuition">Intuition</a></li><li><a href="#data-structure-design">Data Structure Design</a></li><li><a href="#implementation">Implementation</a></li><li><a href="#complexity-analysis">Complexity Analysis</a></li></ul></li><li><a href="#solution-2-ordereddict-python-built-in">Solution 2: OrderedDict (Python Built-in)</a></li><li><a href="#detailed-walkthrough">Detailed Walkthrough</a></li><li><a href="#common-mistakes--edge-cases">Common Mistakes &amp; Edge Cases</a><ul><li><a href="#mistake-1-forgetting-to-update-on-get">Mistake 1: Forgetting to Update on Get</a></li><li><a href="#mistake-2-not-using-dummy-nodes">Mistake 2: Not Using Dummy Nodes</a></li><li><a href="#mistake-3-incorrect-removal-logic">Mistake 3: Incorrect Removal Logic</a></li><li><a href="#edge-cases-to-test">Edge Cases to Test</a></li></ul></li><li><a href="#production-ready-implementation">Production-Ready Implementation</a><ul><li><a href="#thread-safe-lru-cache">Thread-Safe LRU Cache</a></li><li><a href="#lru-cache-with-ttl-time-to-live">LRU Cache with TTL (Time To Live)</a></li></ul></li><li><a href="#performance-benchmarking">Performance Benchmarking</a></li><li><a href="#connection-to-ml-systems">Connection to ML Systems</a><ul><li><a href="#1-model-prediction-cache">1. Model Prediction Cache</a></li><li><a href="#2-feature-store-cache">2. Feature Store Cache</a></li><li><a href="#3-embedding-cache">3. Embedding Cache</a></li></ul></li><li><a href="#interview-tips">Interview Tips</a><ul><li><a href="#discussion-points">Discussion Points</a></li><li><a href="#follow-up-questions">Follow-up Questions</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Master LRU cache design: O(1) get/put with hash map + doubly linked list. Critical for interviews and production caching systems.</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p>Design a data structure that follows the constraints of a <strong>Least Recently Used (LRU) cache</strong>.</p>

<p>Implement the <code class="language-plaintext highlighter-rouge">LRUCache</code> class:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">LRUCache(int capacity)</code> Initialize the LRU cache with positive size capacity.</li>
  <li><code class="language-plaintext highlighter-rouge">int get(int key)</code> Return the value of the key if the key exists, otherwise return -1.</li>
  <li><code class="language-plaintext highlighter-rouge">put(int key, int value)</code> Update the value of the key if the key exists. Otherwise, add the key-value pair to the cache. If the number of keys exceeds the capacity from this operation, evict the least recently used key.</li>
</ul>

<p>The functions <code class="language-plaintext highlighter-rouge">get</code> and <code class="language-plaintext highlighter-rouge">put</code> must each run in <strong>O(1)</strong> average time complexity.</p>

<h3 id="examples">Examples</h3>

<p>``
Input:
[“LRUCache”, “put”, “put”, “get”, “put”, “get”, “put”, “get”, “get”, “get”]
[[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]</p>

<p>Output:
[null, null, null, 1, null, -1, null, -1, 3, 4]</p>

<p>Explanation:
LRUCache lRUCache = new LRUCache(2);
lRUCache.put(1, 1); // cache is {1=1}
lRUCache.put(2, 2); // cache is {1=1, 2=2}
lRUCache.get(1); // return 1
lRUCache.put(3, 3); // LRU key was 2, evicts key 2, cache is {1=1, 3=3}
lRUCache.get(2); // returns -1 (not found)
lRUCache.put(4, 4); // LRU key was 1, evicts key 1, cache is {4=4, 3=3}
lRUCache.get(1); // return -1 (not found)
lRUCache.get(3); // return 3
lRUCache.get(4); // return 4
``</p>

<h3 id="constraints">Constraints</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">1 &lt;= capacity &lt;= 3000</code></li>
  <li><code class="language-plaintext highlighter-rouge">0 &lt;= key &lt;= 10^4</code></li>
  <li><code class="language-plaintext highlighter-rouge">0 &lt;= value &lt;= 10^5</code></li>
  <li>At most <code class="language-plaintext highlighter-rouge">2 * 10^5</code> calls will be made to <code class="language-plaintext highlighter-rouge">get</code> and <code class="language-plaintext highlighter-rouge">put</code></li>
</ul>

<hr />

<h2 id="understanding-lru-cache">Understanding LRU Cache</h2>

<h3 id="what-is-lru">What is LRU?</h3>

<p><strong>Least Recently Used (LRU)</strong> is a cache eviction policy that removes the least recently accessed item when the cache is full.</p>

<p><strong>Access = Read or Write</strong></p>

<p>When you:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">get(key)</code> - The key becomes most recently used</li>
  <li><code class="language-plaintext highlighter-rouge">put(key, value)</code> - The key becomes most recently used</li>
</ul>

<h3 id="why-lru-the-intuition">Why LRU? The Intuition</h3>

<p>Imagine you’re organizing files on your desk. You have limited space, so you stack recent documents on top. When you need space for a new document, you remove the one at the bottom (least recently used).</p>

<p><strong>The temporal locality principle</strong>: Recently accessed data is likely to be accessed again soon.</p>

<p><strong>Example scenario:</strong>
``
You visit websites: A → B → C → D → A → B</p>

<p>Notice that A and B are accessed multiple times.
LRU keeps these “hot” items in cache, evicting C or D if space is needed.
``</p>

<p><strong>Why not other policies?</strong></p>

<table>
  <thead>
    <tr>
      <th>Policy</th>
      <th>How it works</th>
      <th>Drawback</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>FIFO</strong></td>
      <td>Remove oldest inserted</td>
      <td>Doesn’t consider access patterns</td>
    </tr>
    <tr>
      <td><strong>Random</strong></td>
      <td>Remove random item</td>
      <td>No intelligence, unpredictable</td>
    </tr>
    <tr>
      <td><strong>LFU</strong></td>
      <td>Remove least frequently used</td>
      <td>Complex to implement, doesn’t adapt to changing patterns</td>
    </tr>
    <tr>
      <td><strong>LRU</strong></td>
      <td>Remove least recently used</td>
      <td>✅ Simple, adaptive, good hit rates</td>
    </tr>
  </tbody>
</table>

<h3 id="real-world-examples">Real-World Examples</h3>

<p><strong>1. Browser Cache</strong>
When you visit websites, your browser caches images and assets. If the cache fills up, it removes pages you haven’t visited in a while (LRU).</p>

<p><strong>2. Database Query Cache</strong>
Databases cache query results. Popular queries (accessed recently) stay in cache, while old queries are evicted.</p>

<p><strong>3. CDN Edge Caching</strong>
Content Delivery Networks cache content at edge locations. Popular content (recently accessed) stays cached close to users.</p>

<p><strong>4. Operating System Memory</strong>
When RAM is full, OS moves least recently used pages to disk (swap/page file).</p>

<h3 id="the-challenge-achieving-o1-operations">The Challenge: Achieving O(1) Operations</h3>

<p><strong>The problem</strong>: We need both:</p>
<ul>
  <li><strong>Fast lookup</strong> - O(1) to check if key exists</li>
  <li><strong>Fast reordering</strong> - O(1) to move item to “most recent”</li>
  <li><strong>Fast eviction</strong> - O(1) to remove “least recent”</li>
</ul>

<p><strong>Why is this hard?</strong></p>

<p>If we use only one data structure:</p>
<ul>
  <li><strong>Array</strong>: Lookup O(n), reordering O(n) ❌</li>
  <li><strong>Hash Map</strong>: Lookup O(1), but no ordering ❌</li>
  <li><strong>Linked List</strong>: Reordering O(1), but lookup O(n) ❌</li>
</ul>

<p><strong>The solution</strong>: Combine both!</p>
<ul>
  <li><strong>Hash Map</strong>: For O(1) lookup</li>
  <li><strong>Doubly Linked List</strong>: For O(1) reordering</li>
</ul>

<h3 id="understanding-the-data-structure-choice">Understanding the Data Structure Choice</h3>

<p><strong>Why Doubly Linked List?</strong></p>

<p>Let’s think through what we need:</p>

<ol>
  <li><strong>Add to front (most recent)</strong>: O(1)
    <ul>
      <li>Need to know the front ✓</li>
    </ul>
  </li>
  <li><strong>Remove from back (least recent)</strong>: O(1)
    <ul>
      <li>Need to know the back ✓</li>
      <li>Need <code class="language-plaintext highlighter-rouge">prev</code> pointer to update second-to-last node ✓</li>
    </ul>
  </li>
  <li><strong>Remove from middle</strong> (when accessed): O(1)
    <ul>
      <li>Need <code class="language-plaintext highlighter-rouge">prev</code> pointer to update previous node ✓</li>
      <li>Need <code class="language-plaintext highlighter-rouge">next</code> pointer to update next node ✓</li>
    </ul>
  </li>
</ol>

<p><strong>Singly linked list won’t work</strong> because:</p>
<ul>
  <li>Can’t update <code class="language-plaintext highlighter-rouge">prev.next</code> when removing a node from middle</li>
  <li>Would need to traverse from head to find previous node: O(n) ❌</li>
</ul>

<p><strong>Doubly linked list</strong> gives us:
``
prev ← [Node] → next</p>

<p>Can update both directions without traversal!
``</p>

<p><strong>Why Hash Map?</strong></p>

<p>We need O(1) lookup by key. Hash map provides this:
<code class="language-plaintext highlighter-rouge">python
cache[key] # O(1) average case
</code></p>

<p>Without hash map, we’d need to traverse the list: O(n) ❌</p>

<h3 id="the-dummy-node-trick">The Dummy Node Trick</h3>

<p>One of the most important patterns in linked list problems!</p>

<p><strong>Problem without dummy nodes:</strong>
``python</p>
<h1 id="edge-cases-everywhere">Edge cases everywhere!</h1>
<p>if self.head is None:
 # Special handling for empty list
if node == self.head:
 # Special handling for removing head
if node == self.tail:
 # Special handling for removing tail
``</p>

<p><strong>Solution with dummy nodes:</strong>
``python</p>
<h1 id="dummy-head-and-tail-always-exist">Dummy head and tail always exist</h1>
<p>self.head = Node() # Dummy
self.tail = Node() # Dummy
self.head.next = self.tail
self.tail.prev = self.head</p>

<h1 id="now-we-never-have-to-check-for-none">Now we NEVER have to check for None!</h1>
<h1 id="always-have-prev-and-next-pointers">Always have prev and next pointers</h1>
<p>``</p>

<p><strong>Why this works:</strong></p>

<p>``
Before (without dummy):
head=None OR head → [1] → [2] → None
 ↑ Special case!</p>

<p>After (with dummy):
Dummy Head ↔ [1] ↔ [2] ↔ Dummy Tail
 ↑ Always have structure!</p>

<p>Empty cache:
Dummy Head ↔ Dummy Tail
 ↑ Still valid!
``</p>

<p><strong>Benefits:</strong></p>
<ol>
  <li>No null checks needed</li>
  <li>Same logic for all operations</li>
  <li>Fewer bugs</li>
  <li>Cleaner code</li>
</ol>

<h3 id="visualization">Visualization</h3>

<p>``
Initial: capacity=3, cache is empty</p>

<p>put(1, ‘a’)
Cache: [1=’a’]
 ↑ most recent</p>

<p>put(2, ‘b’)
Cache: [2=’b’] -&gt; [1=’a’]
 ↑ most recent</p>

<p>put(3, ‘c’)
Cache: [3=’c’] -&gt; [2=’b’] -&gt; [1=’a’]
 ↑ most recent ↑ least recent</p>

<p>get(1) # Access 1, move to front
Cache: [1=’a’] -&gt; [3=’c’] -&gt; [2=’b’]
 ↑ most recent ↑ least recent</p>

<p>put(4, ‘d’) # Cache full, evict 2 (LRU)
Cache: [4=’d’] -&gt; [1=’a’] -&gt; [3=’c’]
 ↑ most recent ↑ least recent
``</p>

<hr />

<h2 id="solution-1-hash-map--doubly-linked-list-optimal">Solution 1: Hash Map + Doubly Linked List (Optimal)</h2>

<h3 id="intuition">Intuition</h3>

<p>To achieve <strong>O(1)</strong> for both get and put:</p>

<ol>
  <li><strong>Hash Map</strong> - O(1) lookup by key</li>
  <li><strong>Doubly Linked List</strong> - O(1) insertion/deletion from any position</li>
</ol>

<p><strong>Why doubly linked list?</strong></p>
<ul>
  <li>Move node to front: O(1) (need prev pointer)</li>
  <li>Remove from back: O(1) (need prev pointer)</li>
  <li>Remove from middle: O(1) (need prev pointer)</li>
</ul>

<h3 id="data-structure-design">Data Structure Design</h3>

<p><code class="language-plaintext highlighter-rouge">
┌─────────────────────────────────────────────┐
│ LRU CACHE │
├─────────────────────────────────────────────┤
│ │
│ Hash Map (key -&gt; node) │
│ ┌─────────────────────────────────────┐ │
│ │ key=1 -&gt; Node(1, 'a') │ │
│ │ key=2 -&gt; Node(2, 'b') │ │
│ │ key=3 -&gt; Node(3, 'c') │ │
│ └─────────────────────────────────────┘ │
│ │
│ Doubly Linked List (access order) │
│ ┌──────────────────────────────────────┐ │
│ │ Dummy Head &lt;-&gt; [3] &lt;-&gt; [2] &lt;-&gt; [1] │ │
│ │ &lt;-&gt; Dummy Tail │ │
│ │ ↑ MRU ↑ LRU │ │
│ └──────────────────────────────────────┘ │
│ │
└─────────────────────────────────────────────┘
</code></p>

<h3 id="implementation">Implementation</h3>

<p>Before we dive into code, let’s understand the step-by-step approach:</p>

<p><strong>Step 1: Define the Node</strong>
Each node needs:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">key</code>: To identify the item</li>
  <li><code class="language-plaintext highlighter-rouge">value</code>: The cached data</li>
  <li><code class="language-plaintext highlighter-rouge">prev</code>: Pointer to previous node (for doubly linked list)</li>
  <li><code class="language-plaintext highlighter-rouge">next</code>: Pointer to next node (for doubly linked list)</li>
</ul>

<p><strong>Why store <code class="language-plaintext highlighter-rouge">key</code> in the node?</strong>
When we evict the LRU node, we need its key to delete it from the hash map!</p>

<p>``python
class Node:
 “””
 Doubly linked list node</p>

<p>This is the building block of our cache.
 Each node stores a key-value pair and links to neighbors.</p>

<p>Why doubly linked?</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">prev</code> lets us remove nodes from anywhere in O(1)</li>
  <li><code class="language-plaintext highlighter-rouge">next</code> lets us traverse forward</li>
</ul>

<p>Think of it like a train car:
 [prev car] ← [this car] → [next car]
 “””
 def <strong>init</strong>(self, key=0, value=0):
 self.key = key # Needed to delete from hash map when evicting
 self.value = value # The cached data
 self.prev = None # Link to previous node
 self.next = None # Link to next node</p>

<p>class LRUCache:
 “””
 LRU Cache with O(1) get and put</p>

<p>Data structures:</p>
<ul>
  <li>Hash map: key -&gt; node (for O(1) lookup)</li>
  <li>Doubly linked list: nodes in access order (for O(1) reorder)</li>
</ul>

<p>Layout:
 head &lt;-&gt; [MRU] &lt;-&gt; … &lt;-&gt; [LRU] &lt;-&gt; tail</p>

<p>Most recently used (MRU) is after head
 Least recently used (LRU) is before tail
 “””</p>

<p>def <strong>init</strong>(self, capacity: int):
 “””
 Initialize LRU cache</p>

<p>Time: O(1)
 Space: O(capacity)
 “””
 self.capacity = capacity
 self.cache = {} # key -&gt; node</p>

<p># Dummy head and tail for easier operations
 self.head = Node()
 self.tail = Node()</p>

<p># Connect head and tail
 self.head.next = self.tail
 self.tail.prev = self.head</p>

<p>def get(self, key: int) -&gt; int:
 “””
 Get value by key</p>

<p>If exists, move to front (most recently used)</p>

<p>Time: O(1)
 Space: O(1)</p>

<p>Why move to front on get()?
 Because accessing the key makes it “recently used”!
 LRU means “least RECENTLY USED”, so we need to track when things are accessed.
 “””
 # Step 1: Check if key exists (O(1) hash map lookup)
 if key not in self.cache:
 return -1 # Not found</p>

<p># Step 2: Get the node from hash map (O(1))
 node = self.cache[key]</p>

<p># Step 3: Move to front (O(1))
 # Why? Because we just accessed it, making it “most recently used”
 # 
 # Before: head ↔ [A] ↔ [B] ↔ [C] ↔ tail
 # ↑ We want this
 #
 # After: head ↔ [B] ↔ [A] ↔ [C] ↔ tail
 # ↑ Most recent now!</p>

<p>self._remove(node) # Remove from current position
 self._add_to_front(node) # Add at front (most recent)</p>

<p>return node.value</p>

<p>def put(self, key: int, value: int) -&gt; None:
 “””
 Put key-value pair</p>

<p>If key exists, update value and move to front
 If key doesn’t exist, add to front
 If capacity exceeded, evict LRU (before tail)</p>

<p>Time: O(1)
 Space: O(1)</p>

<p>This is the heart of the LRU cache!
 “””
 # Case 1: Key already exists
 # We need to UPDATE the value and move to front
 if key in self.cache:
 node = self.cache[key]
 node.value = value # Update value</p>

<p># Move to front (most recently used)
 # Why? Because we just wrote to it!
 self._remove(node)
 self._add_to_front(node)</p>

<p># Case 2: Key doesn’t exist - NEW insertion
 else:
 # Create new node
 node = Node(key, value)</p>

<p># Add to hash map (for O(1) lookup)
 self.cache[key] = node</p>

<p># Add to front of list (most recently used)
 self._add_to_front(node)</p>

<p># CRITICAL: Check if we exceeded capacity
 if len(self.cache) &gt; self.capacity:
 # We have ONE TOO MANY items!
 # Must evict the LRU item (the one before tail)</p>

<p># Why tail.prev? 
 # Because dummy tail is at the end, so tail.prev is the actual LRU item
 lru_node = self.tail.prev</p>

<p># Remove from list
 self._remove(lru_node)</p>

<p># IMPORTANT: Also remove from hash map!
 # Many people forget this step in interviews!
 del self.cache[lru_node.key] # lru_node.key tells us which key to delete</p>

<p>def _remove(self, node: Node) -&gt; None:
 “””
 Remove node from doubly linked list</p>

<p>Time: O(1)</p>

<p>This is the magic of doubly linked lists!
 We can remove ANY node in O(1) if we have a reference to it.
 “””
 # Get neighbors
 prev_node = node.prev
 next_node = node.next</p>

<p># Connect neighbors to each other, bypassing node
 # Before: prev ↔ node ↔ next
 # After: prev ↔↔↔↔↔↔ next
 prev_node.next = next_node
 next_node.prev = prev_node</p>

<p># Note: We don’t need to set node.prev or node.next to None
 # because we’ll reuse this node or it will be garbage collected</p>

<p>def _add_to_front(self, node: Node) -&gt; None:
 “””
 Add node to front (after head, before first real node)</p>

<p>Time: O(1)</p>

<p>We always add to the front because that’s the “most recently used” position.</p>

<p>Visual:
 Before: head ↔ [A] ↔ [B] ↔ tail
 After: head ↔ [node] ↔ [A] ↔ [B] ↔ tail
 ↑ Most recent!
 “””
 # Step 1: Set node’s pointers
 node.prev = self.head
 node.next = self.head.next # This is the old first node</p>

<p># Step 2: Update neighbors to point to node
 # Order matters here! Update in the right sequence:
 # First: old first node’s prev should point to new node
 self.head.next.prev = node
 # Second: head’s next should point to new node
 self.head.next = node</p>

<p># Why this order?
 # If we did head.next = node first, we’d lose the reference to old first node!</p>

<p>def <strong>repr</strong>(self):
 “"”String representation for debugging”””
 items = []
 current = self.head.next</p>

<p>while current != self.tail:
 items.append(f”{current.key}={current.value}”)
 current = current.next</p>

<p>return f”LRUCache({self.capacity}): [” + “ -&gt; “.join(items) + “]”</p>

<h1 id="example-usage">Example usage</h1>
<p>cache = LRUCache(2)</p>

<p>cache.put(1, 1)
print(cache) # [1=1]</p>

<p>cache.put(2, 2)
print(cache) # [2=2 -&gt; 1=1]</p>

<p>print(cache.get(1)) # Returns 1
print(cache) # [1=1 -&gt; 2=2] (1 moved to front)</p>

<p>cache.put(3, 3) # Evicts key 2
print(cache) # [3=3 -&gt; 1=1]</p>

<p>print(cache.get(2)) # Returns -1 (not found)</p>

<p>cache.put(4, 4) # Evicts key 1
print(cache) # [4=4 -&gt; 3=3]</p>

<p>print(cache.get(1)) # Returns -1 (not found)
print(cache.get(3)) # Returns 3
print(cache.get(4)) # Returns 4
``</p>

<h3 id="complexity-analysis">Complexity Analysis</h3>

<p><strong>Time Complexity:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">get</code>: <strong>O(1)</strong> - Hash map lookup + linked list reorder</li>
  <li><code class="language-plaintext highlighter-rouge">put</code>: <strong>O(1)</strong> - Hash map insert + linked list operations</li>
</ul>

<p><strong>Space Complexity:</strong></p>
<ul>
  <li><strong>O(capacity)</strong> - Store up to capacity nodes</li>
</ul>

<hr />

<h2 id="solution-2-ordereddict-python-built-in">Solution 2: OrderedDict (Python Built-in)</h2>

<p>Python’s <code class="language-plaintext highlighter-rouge">collections.OrderedDict</code> maintains insertion order and provides <code class="language-plaintext highlighter-rouge">move_to_end()</code> for O(1) reordering.</p>

<p>``python
from collections import OrderedDict</p>

<p>class LRUCache:
 “””
 LRU Cache using OrderedDict</p>

<p>Simpler implementation but same complexity
 “””</p>

<p>def <strong>init</strong>(self, capacity: int):
 self.cache = OrderedDict()
 self.capacity = capacity</p>

<p>def get(self, key: int) -&gt; int:
 “””
 Get value and move to end (most recent)</p>

<p>Time: O(1)
 “””
 if key not in self.cache:
 return -1</p>

<p># Move to end (most recently used)
 self.cache.move_to_end(key)</p>

<p>return self.cache[key]</p>

<p>def put(self, key: int, value: int) -&gt; None:
 “””
 Put key-value and move to end</p>

<p>Time: O(1)
 “””
 if key in self.cache:
 # Update and move to end
 self.cache.move_to_end(key)</p>

<p>self.cache[key] = value</p>

<p># Evict LRU if over capacity
 if len(self.cache) &gt; self.capacity:
 # popitem(last=False) removes first (oldest) item
 self.cache.popitem(last=False)
``</p>

<p><strong>Pros:</strong></p>
<ul>
  <li>Clean and concise</li>
  <li>Good for interviews if allowed</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>Less educational (hides implementation details)</li>
  <li>May not be allowed in interviews</li>
</ul>

<hr />

<h2 id="detailed-walkthrough">Detailed Walkthrough</h2>

<p>Let’s trace through a complete example step by step:</p>

<p>``python
def trace_lru_cache():
 “””
 Trace LRU cache operations with detailed output
 “””
 cache = LRUCache(3)</p>

<p>operations = [
 (‘put’, 1, ‘apple’),
 (‘put’, 2, ‘banana’),
 (‘put’, 3, ‘cherry’),
 (‘get’, 1, None),
 (‘put’, 4, ‘date’),
 (‘get’, 2, None),
 (‘get’, 3, None),
 (‘put’, 5, ‘elderberry’),
 ]</p>

<p>print(“=”<em>60)
 print(“LRU CACHE TRACE (capacity=3)”)
 print(“=”</em>60)</p>

<p>for op in operations:
 if op[0] == ‘put’:
 _, key, value = op
 print(f”\nput({key}, ‘{value}’)”)
 cache.put(key, value)
 else:
 _, key, _ = op
 result = cache.get(key)
 print(f”\nget({key}) -&gt; {result}”)</p>

<p># Print cache state
 print(f” Cache: {cache}”)
 print(f” Size: {len(cache.cache)}/{cache.capacity}”)</p>

<p>trace_lru_cache()
``</p>

<p><strong>Output:</strong>
``
============================================================
LRU CACHE TRACE (capacity=3)
============================================================</p>

<p>put(1, ‘apple’)
 Cache: [1=apple]
 Size: 1/3</p>

<p>put(2, ‘banana’)
 Cache: [2=banana -&gt; 1=apple]
 Size: 2/3</p>

<p>put(3, ‘cherry’)
 Cache: [3=cherry -&gt; 2=banana -&gt; 1=apple]
 Size: 3/3</p>

<p>get(1) -&gt; apple
 Cache: [1=apple -&gt; 3=cherry -&gt; 2=banana]
 Size: 3/3
 (1 moved to front)</p>

<p>put(4, ‘date’)
 Cache: [4=date -&gt; 1=apple -&gt; 3=cherry]
 Size: 3/3
 (2 evicted - was LRU)</p>

<p>get(2) -&gt; -1
 Cache: [4=date -&gt; 1=apple -&gt; 3=cherry]
 Size: 3/3
 (2 not found)</p>

<p>get(3) -&gt; cherry
 Cache: [3=cherry -&gt; 4=date -&gt; 1=apple]
 Size: 3/3
 (3 moved to front)</p>

<p>put(5, ‘elderberry’)
 Cache: [5=elderberry -&gt; 3=cherry -&gt; 4=date]
 Size: 3/3
 (1 evicted - was LRU)
``</p>

<hr />

<h2 id="common-mistakes--edge-cases">Common Mistakes &amp; Edge Cases</h2>

<h3 id="mistake-1-forgetting-to-update-on-get">Mistake 1: Forgetting to Update on Get</h3>

<p>``python</p>
<h1 id="-wrong-dont-update-access-order-on-get">❌ WRONG: Don’t update access order on get</h1>
<p>def get(self, key):
 if key in self.cache:
 return self.cache[key].value # Not moving to front!
 return -1</p>

<h1 id="-correct-move-to-front-on-get">✅ CORRECT: Move to front on get</h1>
<p>def get(self, key):
 if key not in self.cache:
 return -1</p>

<p>node = self.cache[key]
 self._remove(node)
 self._add_to_front(node)
 return node.value
``</p>

<h3 id="mistake-2-not-using-dummy-nodes">Mistake 2: Not Using Dummy Nodes</h3>

<p>``python</p>
<h1 id="-wrong-no-dummy-nodes---many-edge-cases">❌ WRONG: No dummy nodes - many edge cases</h1>
<p>class LRUCache:
 def <strong>init</strong>(self, capacity):
 self.head = None # Can be None!
 self.tail = None # Can be None!</p>

<h1 id="-correct-dummy-nodes-simplify-logic">✅ CORRECT: Dummy nodes simplify logic</h1>
<p>class LRUCache:
 def <strong>init</strong>(self, capacity):
 self.head = Node() # Always exists
 self.tail = Node() # Always exists
 self.head.next = self.tail
 self.tail.prev = self.head
``</p>

<h3 id="mistake-3-incorrect-removal-logic">Mistake 3: Incorrect Removal Logic</h3>

<p>``python</p>
<h1 id="-wrong-forgetting-to-update-both-prev-and-next">❌ WRONG: Forgetting to update both prev and next</h1>
<p>def _remove(self, node):
 node.prev.next = node.next # Only updates next!</p>

<h1 id="-correct-update-both-connections">✅ CORRECT: Update both connections</h1>
<p>def _remove(self, node):
 node.prev.next = node.next
 node.next.prev = node.prev
``</p>

<h3 id="edge-cases-to-test">Edge Cases to Test</h3>

<p>``python
def test_edge_cases():
 “"”Test important edge cases”””</p>

<p># Edge case 1: Capacity of 1
 print(“Test 1: Capacity 1”)
 cache = LRUCache(1)
 cache.put(1, 1)
 cache.put(2, 2) # Should evict 1
 assert cache.get(1) == -1
 assert cache.get(2) == 2
 print(“✓ Passed”)</p>

<p># Edge case 2: Update existing key
 print(“\nTest 2: Update existing key”)
 cache = LRUCache(2)
 cache.put(1, 1)
 cache.put(2, 2)
 cache.put(1, 10) # Update 1
 assert cache.get(1) == 10
 print(“✓ Passed”)</p>

<p># Edge case 3: Get non-existent key
 print(“\nTest 3: Get non-existent key”)
 cache = LRUCache(2)
 assert cache.get(999) == -1
 print(“✓ Passed”)</p>

<p># Edge case 4: Fill to capacity
 print(“\nTest 4: Fill to capacity”)
 cache = LRUCache(3)
 cache.put(1, 1)
 cache.put(2, 2)
 cache.put(3, 3)
 cache.put(4, 4) # Should evict 1
 assert cache.get(1) == -1
 print(“✓ Passed”)</p>

<p># Edge case 5: Access pattern
 print(“\nTest 5: Complex access pattern”)
 cache = LRUCache(2)
 cache.put(2, 1)
 cache.put(1, 1)
 cache.put(2, 3)
 cache.put(4, 1)
 assert cache.get(1) == -1
 assert cache.get(2) == 3
 print(“✓ Passed”)</p>

<p>print(“\n” + “=”<em>40)
 print(“All edge case tests passed!”)
 print(“=”</em>40)</p>

<p>test_edge_cases()
``</p>

<hr />

<h2 id="production-ready-implementation">Production-Ready Implementation</h2>

<h3 id="thread-safe-lru-cache">Thread-Safe LRU Cache</h3>

<p>``python
import threading
from collections import OrderedDict</p>

<p>class ThreadSafeLRUCache:
 “””
 Thread-safe LRU cache for concurrent access</p>

<p>Uses RLock (reentrant lock) to protect cache operations
 “””</p>

<p>def <strong>init</strong>(self, capacity: int):
 self.cache = OrderedDict()
 self.capacity = capacity
 self.lock = threading.RLock()</p>

<p># Metrics
 self.hits = 0
 self.misses = 0</p>

<p>def get(self, key: int) -&gt; int:
 “"”Thread-safe get”””
 with self.lock:
 if key not in self.cache:
 self.misses += 1
 return -1</p>

<p>self.hits += 1
 self.cache.move_to_end(key)
 return self.cache[key]</p>

<p>def put(self, key: int, value: int) -&gt; None:
 “"”Thread-safe put”””
 with self.lock:
 if key in self.cache:
 self.cache.move_to_end(key)</p>

<p>self.cache[key] = value</p>

<p>if len(self.cache) &gt; self.capacity:
 self.cache.popitem(last=False)</p>

<p>def get_stats(self):
 “"”Get cache statistics”””
 with self.lock:
 total = self.hits + self.misses
 hit_rate = self.hits / total if total &gt; 0 else 0</p>

<p>return {
 ‘hits’: self.hits,
 ‘misses’: self.misses,
 ‘total’: total,
 ‘hit_rate’: hit_rate,
 ‘size’: len(self.cache),
 ‘capacity’: self.capacity
 }</p>

<h1 id="test-thread-safety">Test thread safety</h1>
<p>import time
import random</p>

<p>def worker(cache, worker_id, operations=1000):
 “"”Worker thread that performs cache operations”””
 for _ in range(operations):
 key = random.randint(1, 20)</p>

<p>if random.random() &lt; 0.7:
 # 70% reads
 cache.get(key)
 else:
 # 30% writes
 cache.put(key, worker_id)</p>

<h1 id="create-cache-and-threads">Create cache and threads</h1>
<p>cache = ThreadSafeLRUCache(capacity=10)
threads = []</p>

<p>print(“Testing thread safety with 10 concurrent threads…”)
start = time.time()</p>

<p>for i in range(10):
 t = threading.Thread(target=worker, args=(cache, i, 1000))
 threads.append(t)
 t.start()</p>

<p>for t in threads:
 t.join()</p>

<p>elapsed = time.time() - start</p>

<p>print(f”\nCompleted in {elapsed:.2f}s”)
print(“\nCache Statistics:”)
stats = cache.get_stats()
for key, value in stats.items():
 if key == ‘hit_rate’:
 print(f” {key}: {value:.2%}”)
 else:
 print(f” {key}: {value}”)
``</p>

<h3 id="lru-cache-with-ttl-time-to-live">LRU Cache with TTL (Time To Live)</h3>

<p>``python
import time</p>

<p>class LRUCacheWithTTL:
 “””
 LRU cache with time-based expiration</p>

<p>Items expire after TTL seconds
 “””</p>

<p>def <strong>init</strong>(self, capacity: int, ttl: int = 300):
 “””
 Args:
 capacity: Max number of items
 ttl: Time to live in seconds
 “””
 self.capacity = capacity
 self.ttl = ttl
 self.cache = OrderedDict()
 self.timestamps = {} # key -&gt; insertion time</p>

<p>def get(self, key: int) -&gt; int:
 “"”Get with expiration check”””
 if key not in self.cache:
 return -1</p>

<p># Check if expired
 if time.time() - self.timestamps[key] &gt; self.ttl:
 # Expired, remove
 del self.cache[key]
 del self.timestamps[key]
 return -1</p>

<p># Not expired, move to end
 self.cache.move_to_end(key)
 return self.cache[key]</p>

<p>def put(self, key: int, value: int) -&gt; None:
 “"”Put with timestamp”””
 if key in self.cache:
 self.cache.move_to_end(key)</p>

<p>self.cache[key] = value
 self.timestamps[key] = time.time()</p>

<p># Evict LRU if over capacity
 if len(self.cache) &gt; self.capacity:
 lru_key = next(iter(self.cache))
 del self.cache[lru_key]
 del self.timestamps[lru_key]</p>

<p>def cleanup_expired(self):
 “"”Remove all expired items”””
 current_time = time.time()
 expired_keys = [
 key for key, timestamp in self.timestamps.items()
 if current_time - timestamp &gt; self.ttl
 ]</p>

<p>for key in expired_keys:
 del self.cache[key]
 del self.timestamps[key]</p>

<p>return len(expired_keys)</p>

<h1 id="example">Example</h1>
<p>cache = LRUCacheWithTTL(capacity=3, ttl=5)</p>

<p>cache.put(1, ‘apple’)
cache.put(2, ‘banana’)</p>

<p>print(“Immediately after insert:”)
print(f”get(1) = {cache.get(1)}”) # ‘apple’</p>

<p>print(“\nWait 6 seconds…”)
time.sleep(6)</p>

<p>print(“After TTL expired:”)
print(f”get(1) = {cache.get(1)}”) # -1 (expired)</p>

<p>print(f”\nCleaned up {cache.cleanup_expired()} expired items”)
``</p>

<hr />

<h2 id="performance-benchmarking">Performance Benchmarking</h2>

<p>``python
import time
import random
import matplotlib.pyplot as plt</p>

<p>def benchmark_lru_implementations():
 “””
 Compare performance of different LRU implementations
 “””
 capacities = [10, 100, 1000, 10000]
 implementations = {
 ‘Custom (Doubly Linked List)’: LRUCache,
 ‘OrderedDict’: lambda cap: LRUCacheOrderedDict(cap),
 }</p>

<p>results = {name: [] for name in implementations}</p>

<p>print(“Benchmarking LRU Cache Implementations”)
 print(“=”*60)</p>

<p>for capacity in capacities:
 print(f”\nCapacity: {capacity}”)</p>

<p>for name, impl_class in implementations.items():
 cache = impl_class(capacity)</p>

<p># Generate workload
 num_operations = 10000
 operations = []</p>

<p>for _ in range(num_operations):
 if random.random() &lt; 0.7:
 # 70% reads
 key = random.randint(0, capacity * 2)
 operations.append((‘get’, key))
 else:
 # 30% writes
 key = random.randint(0, capacity * 2)
 value = random.randint(0, 1000)
 operations.append((‘put’, key, value))</p>

<p># Benchmark
 start = time.perf_counter()</p>

<p>for op in operations:
 if op[0] == ‘get’:
 cache.get(op[1])
 else:
 cache.put(op[1], op[2])</p>

<p>elapsed = (time.perf_counter() - start) * 1000 # ms</p>

<p>results[name].append(elapsed)
 print(f” {name:30s}: {elapsed:6.2f}ms”)</p>

<p># Plot results
 plt.figure(figsize=(10, 6))</p>

<p>for name, times in results.items():
 plt.plot(capacities, times, marker=’o’, label=name)</p>

<p>plt.xlabel(‘Capacity’)
 plt.ylabel(‘Time (ms) for 10,000 operations’)
 plt.title(‘LRU Cache Performance Comparison’)
 plt.legend()
 plt.grid(True)
 plt.xscale(‘log’)
 plt.savefig(‘lru_benchmark.png’)
 plt.close()</p>

<p>print(“\n” + “=”*60)
 print(“Benchmark complete! Plot saved to lru_benchmark.png”)</p>

<p>class LRUCacheOrderedDict:
 “"”OrderedDict implementation for comparison”””
 def <strong>init</strong>(self, capacity):
 self.cache = OrderedDict()
 self.capacity = capacity</p>

<p>def get(self, key):
 if key not in self.cache:
 return -1
 self.cache.move_to_end(key)
 return self.cache[key]</p>

<p>def put(self, key, value):
 if key in self.cache:
 self.cache.move_to_end(key)
 self.cache[key] = value
 if len(self.cache) &gt; self.capacity:
 self.cache.popitem(last=False)</p>

<p>benchmark_lru_implementations()
``</p>

<hr />

<h2 id="connection-to-ml-systems">Connection to ML Systems</h2>

<h3 id="1-model-prediction-cache">1. Model Prediction Cache</h3>

<p>``python
class ModelPredictionCache:
 “””
 Cache model predictions to avoid recomputation</p>

<p>Use case: Frequently requested predictions
 “””</p>

<p>def <strong>init</strong>(self, model, capacity=1000):
 self.model = model
 self.cache = LRUCache(capacity)</p>

<p>def predict(self, features):
 “””
 Predict with caching</p>

<p>Args:
 features: tuple of feature values (must be hashable)</p>

<p>Returns:
 prediction
 “””
 # Create cache key from features
 cache_key = hash(features)</p>

<p># Try cache
 cached_prediction = self.cache.get(cache_key)
 if cached_prediction != -1:
 return cached_prediction</p>

<p># Cache miss: compute prediction
 prediction = self.model.predict([features])[0]</p>

<p># Store in cache
 self.cache.put(cache_key, prediction)</p>

<p>return prediction</p>

<h1 id="example-1">Example</h1>
<p>from sklearn.ensemble import RandomForestClassifier
import numpy as np</p>

<h1 id="train-model">Train model</h1>
<p>X_train = np.random.randn(100, 5)
y_train = (X_train.sum(axis=1) &gt; 0).astype(int)
model = RandomForestClassifier(n_estimators=10)
model.fit(X_train, y_train)</p>

<h1 id="create-cached-predictor">Create cached predictor</h1>
<p>cached_model = ModelPredictionCache(model, capacity=100)</p>

<h1 id="make-predictions">Make predictions</h1>
<p>for _ in range(1000):
 features = tuple(np.random.randint(0, 10, size=5))
 prediction = cached_model.predict(features)</p>

<p>print(“Cache statistics:”)
print(f” Capacity: {cached_model.cache.capacity}”)
print(f” Current size: {len(cached_model.cache.cache)}”)
``</p>

<h3 id="2-feature-store-cache">2. Feature Store Cache</h3>

<p>``python
class FeatureStoreCache:
 “””
 Cache feature store lookups</p>

<p>Feature stores can be slow (database/API calls)
 LRU cache reduces latency
 “””</p>

<p>def <strong>init</strong>(self, feature_store, capacity=10000):
 self.feature_store = feature_store
 self.cache = ThreadSafeLRUCache(capacity)</p>

<p>def get_features(self, entity_id):
 “””
 Get features for entity with caching</p>

<p>Args:
 entity_id: Unique entity identifier</p>

<p>Returns:
 Feature dictionary
 “””
 # Try cache
 cached_features = self.cache.get(entity_id)
 if cached_features != -1:
 return cached_features</p>

<p># Cache miss: query feature store
 features = self.feature_store.query(entity_id)</p>

<p># Store in cache
 self.cache.put(entity_id, features)</p>

<p>return features</p>

<p>def get_cache_stats(self):
 “"”Get cache performance statistics”””
 return self.cache.get_stats()</p>

<h1 id="example-usage-1">Example usage</h1>
<p>class MockFeatureStore:
 “"”Mock feature store with latency”””
 def query(self, entity_id):
 time.sleep(0.001) # Simulate 1ms latency
 return {‘feature1’: entity_id * 2, ‘feature2’: entity_id ** 2}</p>

<p>feature_store = MockFeatureStore()
cached_store = FeatureStoreCache(feature_store, capacity=1000)</p>

<h1 id="simulate-requests">Simulate requests</h1>
<p>start = time.time()
for _ in range(10000):
 entity_id = random.randint(1, 500)
 features = cached_store.get_features(entity_id)</p>

<p>elapsed = time.time() - start</p>

<p>print(f”Total time: {elapsed:.2f}s”)
print(“\nCache stats:”)
stats = cached_store.get_cache_stats()
for key, value in stats.items():
 if key == ‘hit_rate’:
 print(f” {key}: {value:.2%}”)
 else:
 print(f” {key}: {value}”)
``</p>

<h3 id="3-embedding-cache">3. Embedding Cache</h3>

<p>``python
class EmbeddingCache:
 “””
 Cache embeddings for frequently queried items</p>

<p>Useful for recommendation systems, search, etc.
 “””</p>

<p>def <strong>init</strong>(self, embedding_model, capacity=10000):
 self.model = embedding_model
 self.cache = LRUCache(capacity)</p>

<p>def get_embedding(self, item_id):
 “””
 Get embedding with caching</p>

<p>Computing embeddings can be expensive (neural network inference)
 “””
 # Try cache
 cached_embedding = self.cache.get(item_id)
 if cached_embedding != -1:
 return cached_embedding</p>

<p># Cache miss: compute embedding
 embedding = self.model.encode(item_id)</p>

<p># Store in cache
 self.cache.put(item_id, embedding)</p>

<p>return embedding</p>

<p>def batch_get_embeddings(self, item_ids):
 “””
 Get embeddings for multiple items</p>

<p>Separate cache hits from misses for efficient batch processing
 “””
 embeddings = {}
 cache_misses = []</p>

<p># Check cache
 for item_id in item_ids:
 cached = self.cache.get(item_id)
 if cached != -1:
 embeddings[item_id] = cached
 else:
 cache_misses.append(item_id)</p>

<p># Batch compute misses
 if cache_misses:
 batch_embeddings = self.model.batch_encode(cache_misses)</p>

<p>for item_id, embedding in zip(cache_misses, batch_embeddings):
 self.cache.put(item_id, embedding)
 embeddings[item_id] = embedding</p>

<p>return [embeddings[item_id] for item_id in item_ids]
``</p>

<hr />

<h2 id="interview-tips">Interview Tips</h2>

<h3 id="discussion-points">Discussion Points</h3>

<ol>
  <li><strong>Why not just use a hash map?</strong>
    <ul>
      <li>Hash map gives O(1) lookup but doesn’t track access order</li>
      <li>Need additional data structure for ordering</li>
    </ul>
  </li>
  <li><strong>Why doubly linked list instead of array?</strong>
    <ul>
      <li>Array: O(n) to remove from middle</li>
      <li>Doubly linked list: O(1) to remove from any position</li>
    </ul>
  </li>
  <li><strong>Why dummy head/tail?</strong>
    <ul>
      <li>Simplifies edge cases</li>
      <li>No null checks needed</li>
      <li>Consistent operations</li>
    </ul>
  </li>
  <li><strong>Can you use a single data structure?</strong>
    <ul>
      <li>No, you need both:</li>
      <li>Fast lookup: Hash map</li>
      <li>Fast reordering: Linked list</li>
    </ul>
  </li>
</ol>

<h3 id="follow-up-questions">Follow-up Questions</h3>

<p><strong>Q: How would you implement LFU (Least Frequently Used)?</strong></p>

<p>``python
class LFUCache:
 “””
 Least Frequently Used cache</p>

<p>Evicts item with lowest access frequency
 “””</p>

<p>def <strong>init</strong>(self, capacity):
 self.capacity = capacity
 self.cache = {} # key -&gt; (value, freq)
 self.freq_map = {} # freq -&gt; OrderedDict of keys
 self.min_freq = 0</p>

<p>def get(self, key):
 if key not in self.cache:
 return -1</p>

<p>value, freq = self.cache[key]</p>

<p># Increment frequency
 self._increment_freq(key, value, freq)</p>

<p>return value</p>

<p>def put(self, key, value):
 if self.capacity == 0:
 return</p>

<p>if key in self.cache:
 # Update existing
 _, freq = self.cache[key]
 self._increment_freq(key, value, freq)
 else:
 # Add new
 if len(self.cache) &gt;= self.capacity:
 # Evict LFU
 self._evict()</p>

<p>self.cache[key] = (value, 1)</p>

<p>if 1 not in self.freq_map:
 self.freq_map[1] = OrderedDict()</p>

<p>self.freq_map[1][key] = None
 self.min_freq = 1</p>

<p>def _increment_freq(self, key, value, freq):
 “"”Move key to higher frequency bucket”””
 # Remove from current frequency
 del self.freq_map[freq][key]</p>

<p>if not self.freq_map[freq] and freq == self.min_freq:
 self.min_freq += 1</p>

<p># Add to next frequency
 new_freq = freq + 1
 if new_freq not in self.freq_map:
 self.freq_map[new_freq] = OrderedDict()</p>

<p>self.freq_map[new_freq][key] = None
 self.cache[key] = (value, new_freq)</p>

<p>def _evict(self):
 “"”Evict least frequently used (and least recently used within that frequency)”””
 # Get first key from min frequency bucket
 key_to_evict = next(iter(self.freq_map[self.min_freq]))</p>

<p>del self.freq_map[self.min_freq][key_to_evict]
 del self.cache[key_to_evict]
``</p>

<p><strong>Q: How would you handle cache invalidation?</strong></p>

<p>``python
class LRUCacheWithInvalidation(LRUCache):
 “””
 LRU cache with manual invalidation</p>

<p>Useful when data changes externally
 “””</p>

<p>def invalidate(self, key):
 “"”Remove key from cache”””
 if key not in self.cache:
 return False</p>

<p>node = self.cache[key]
 self._remove(node)
 del self.cache[key]</p>

<p>return True</p>

<p>def invalidate_pattern(self, pattern):
 “””
 Invalidate keys matching pattern</p>

<p>Example: invalidate_pattern(‘user_*’)
 “””
 import fnmatch</p>

<p>keys_to_remove = [
 key for key in self.cache.keys()
 if fnmatch.fnmatch(str(key), pattern)
 ]</p>

<p>for key in keys_to_remove:
 self.invalidate(key)</p>

<p>return len(keys_to_remove)
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>O(1) operations</strong> - Hash map + doubly linked list 
✅ <strong>Dummy nodes</strong> - Simplify edge case handling 
✅ <strong>Update on access</strong> - Both get() and put() update recency 
✅ <strong>Thread safety</strong> - Use locks for concurrent access 
✅ <strong>Real-world use</strong> - Prediction cache, feature store, embeddings</p>

<p><strong>Core Pattern:</strong></p>
<ul>
  <li>Hash map for O(1) lookup</li>
  <li>Doubly linked list for O(1) reordering</li>
  <li>MRU at head, LRU at tail</li>
</ul>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/dsa/0011-lru-cache/">arunbaby.com/dsa/0011-lru-cache</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#cache" class="page__taxonomy-item p-category" rel="tag">cache</a><span class="sep">, </span>
    
      <a href="/tags/#design" class="page__taxonomy-item p-category" rel="tag">design</a><span class="sep">, </span>
    
      <a href="/tags/#hash-map" class="page__taxonomy-item p-category" rel="tag">hash-map</a><span class="sep">, </span>
    
      <a href="/tags/#linked-list" class="page__taxonomy-item p-category" rel="tag">linked-list</a><span class="sep">, </span>
    
      <a href="/tags/#lru" class="page__taxonomy-item p-category" rel="tag">lru</a><span class="sep">, </span>
    
      <a href="/tags/#system-design" class="page__taxonomy-item p-category" rel="tag">system-design</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#dsa" class="page__taxonomy-item p-category" rel="tag">dsa</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0011-content-delivery-network/" rel="permalink">Content Delivery Networks (CDN)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          14 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Design a global CDN for ML systems: Edge caching reduces latency from 500ms to 50ms. Critical for real-time predictions worldwide.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0011-speech-separation/" rel="permalink">Speech Separation
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          20 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Separate overlapping speakers with 99%+ accuracy: Deep learning solves the cocktail party problem for meeting transcription and voice assistants.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0011-vector-search-for-agents/" rel="permalink">Vector Search for Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Finding a Needle in a High-Dimensional Haystack: The Mathematics of Recall.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=LRU+Cache%20https%3A%2F%2Fwww.arunbaby.com%2Fdsa%2F0011-lru-cache%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fdsa%2F0011-lru-cache%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/dsa/0011-lru-cache/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/dsa/0010-reverse-linked-list/" class="pagination--pager" title="Reverse Linked List">Previous</a>
    
    
      <a href="/dsa/0012-add-two-numbers/" class="pagination--pager" title="Add Two Numbers">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
