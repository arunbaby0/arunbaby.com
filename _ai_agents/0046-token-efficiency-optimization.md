---
title: "Token Efficiency Optimization"
day: 46
collection: ai_agents
categories:
  - ai-agents
tags:
  - token-optimization
  - cost-reduction
  - prompt-engineering
  - context-management
  - llm-efficiency
difficulty: Hard
subdomain: "Agent Operations"
tech_stack: Python, OpenAI, Anthropic
scale: "Millions of requests, significant cost savings"
companies: OpenAI, Anthropic, Google, Stripe, Intercom
related_dsa_day: 46
related_ml_day: 46
related_speech_day: 46
---

**"Every token costs money. Every wasted token is wasted money."**

## 1. Introduction: Why Token Efficiency Matters

When you're building a prototype AI agent, token efficiency might seem like a minor concern. You're focused on making it work, not making it cheap. But as you scale to production—hundreds of thousands or millions of requests—tokens become the dominant cost.

Let's do some quick math. Say your agent:
- Uses GPT-4 Turbo ($0.01 per 1K input tokens, $0.03 per 1K output tokens)
- Processes 100,000 requests per day
- Uses 3,000 tokens per request on average

**Monthly cost: 100,000 × 30 × 3 × $0.02 = $180,000 per month**

Now imagine you can reduce token usage by 50% through smart optimization:

**Optimized monthly cost: $90,000 per month**

That's $1 million in annual savings from efficiency alone. Token optimization isn't just technical housekeeping—it's a strategic business decision.

---

## 2. Understanding Where Tokens Go

Before optimizing, you need to understand where your tokens are actually being spent. Most developers are surprised by the breakdown.

### 2.1 The Anatomy of an Agent Request

A typical AI agent request includes several components:

**System Prompt (15-40% of tokens)**

The system prompt tells the model who it is, how to behave, and what tools are available. This is sent with *every single request*, making it the most impactful area to optimize.

Example breakdown:
- Base instructions: 200-500 tokens
- Tool definitions: 500-2,000 tokens (if you have many tools)
- Examples (few-shot): 300-1,000 tokens
- Guardrails and safety instructions: 100-300 tokens

**Conversation History (20-50% of tokens)**

For multi-turn conversations, you include previous messages. This grows with every turn:
- Turn 1: 500 tokens
- Turn 5: 2,500 tokens
- Turn 10: 5,000 tokens

Without management, conversation history can dominate your token usage.

**User Query (5-15% of tokens)**

The actual question or request from the user. Often the smallest component!

**Retrieved Context (10-30% of tokens)**

If you're using RAG (Retrieval-Augmented Generation), you inject relevant documents. Easy to over-retrieve.

**Model Response (10-30% of tokens)**

The tokens generated by the model. You pay even more for these (typically 2-3x the input token rate).

### 2.2 Measuring Your Token Distribution

The first step in optimization is measurement. Log the token counts for each component:

```
Request ID: abc123
├── System prompt:        800 tokens (27%)
├── Tool definitions:     600 tokens (20%)
├── Conversation history: 900 tokens (30%)
├── User query:          150 tokens (5%)
├── Retrieved context:    400 tokens (13%)
└── Response:            150 tokens (5%)
Total: 3,000 tokens
```

Once you see where tokens are going, you can prioritize optimization efforts.

---

## 3. Optimizing the System Prompt

Since the system prompt is sent with every request, even small reductions compound enormously.

### 3.1 The Conciseness Principle

Many system prompts are written like novels. They don't need to be. LLMs are good at understanding concise instructions.

**Before (verbose):**
```
You are a helpful customer service assistant for Acme Corporation. Your primary 
responsibility is to assist customers with their inquiries in a professional, 
friendly, and helpful manner. You should always strive to provide accurate 
information and resolve customer issues to the best of your ability. If you 
don't know something, it's perfectly acceptable to say so rather than guessing.
```

**After (concise):**
```
You're Acme's support AI. Be helpful, accurate, and friendly. If unsure, say so.
```

The meaning is identical; the token count dropped from ~70 to ~20.

### 3.2 Remove Redundancy

LLMs don't need things repeated. If you say "be professional" once, you don't need to say it again in different words.

**Common redundancies to remove:**
- Restating the same rule in different words
- Explaining obvious things ("You can use the tools available to you")
- Excessive politeness ("Please always remember to...")

### 3.3 Use Structure Over Prose

Bullet points and structured formats are often more token-efficient than paragraphs:

**Before:**
```
When helping with refunds, you should first verify the customer's identity by 
asking for their email address. Then, look up their order history to find the 
relevant order. Check if the order is within the refund window. If it is, 
process the refund. If it isn't, explain the policy kindly.
```

**After:**
```
Refund process:
1. Verify identity (email)
2. Look up order
3. Check refund window
4. If eligible: process
5. If not: explain policy
```

Clearer AND shorter.

### 3.4 Dynamic System Prompts

Not every request needs every instruction. Build your system prompt based on context:

**Detected intent: billing question**
→ Include: billing guidelines, refund policy
→ Exclude: technical troubleshooting steps, account security protocols

**Detected intent: password reset**
→ Include: security verification, reset procedures
→ Exclude: billing information, product features

This requires more engineering but can reduce system prompt tokens by 40-60%.

---

## 4. Optimizing Tool Definitions

If your agent has tools, their definitions can consume massive token budgets.

### 4.1 The Hidden Cost of Tools

Each tool you provide to the model includes:
- Tool name and description
- Parameters (names, types, descriptions)
- Examples of usage

A single tool might be 50-200 tokens. If you have 15 tools, that's 750-3,000 tokens *per request*.

### 4.2 Dynamic Tool Selection

The solution: don't send all tools every time. Analyze the user's query and send only relevant tools.

**User asks about billing:** Send payment tools, refund tools, invoice tools (3 tools)
**User asks about their account:** Send profile tools, settings tools (2 tools)
**User asks technical question:** Send documentation search, ticket creation tools (2 tools)

This requires a lightweight classification step, but the token savings are substantial.

### 4.3 Compress Tool Descriptions

Tool descriptions tend to be over-written:

**Before:**
```
This tool allows you to search through the company's internal knowledge base 
to find relevant articles, documentation, and FAQs that might help answer 
the customer's question. Use this when you need to look up information.
```

**After:**
```
Search knowledge base for articles/docs. Use for factual lookups.
```

The model understands both equally well.

### 4.4 Parameter Description Optimization

Parameter descriptions are often excessive:

**Before:**
```
query (string, required): The search query to look for in the knowledge base. 
This should be a clear, concise description of what information you're looking 
for. Try to include relevant keywords.
```

**After:**
```
query (string, required): Search terms
```

---

## 5. Managing Conversation History

For multi-turn conversations, history management is critical.

### 5.1 The Explosion Problem

Without management, history grows linearly (or worse) with each turn:

```
Turn 1:  500 tokens (initial query + response)
Turn 5:  2,500 tokens 
Turn 10: 5,000 tokens
Turn 20: 10,000 tokens  ← This might exceed context limits!
```

Not only does cost increase, but you may hit context window limits.

### 5.2 Strategy 1: Sliding Window

Keep only the last N turns. Simple but effective.

**Trade-off:** The model loses context about earlier parts of the conversation. Works well when recent context is most relevant.

### 5.3 Strategy 2: Summarization

Periodically summarize older messages into a compact form:

**Original history (500 tokens):**
```
User: I'm having trouble with my order #12345
Agent: I see your order. What's the issue?
User: It arrived damaged
Agent: I'm sorry about that. Can you describe the damage?
User: The box was crushed and items inside broken
Agent: I'll arrange a replacement. Do you want the same items?
User: Yes please
```

**Summarized (50 tokens):**
```
[Earlier: Customer reported order #12345 arrived with crushed box and broken 
items. Agent arranged replacement of same items.]
```

90% token reduction while preserving essential context.

### 5.4 Strategy 3: Importance-Based Retention

Not all messages are equally important. Assign importance scores:

- **High importance:** Key decisions, confirmed information, user preferences
- **Medium importance:** Questions, clarifications
- **Low importance:** Acknowledgments, pleasantries

When pruning, remove low-importance messages first.

### 5.5 Strategy 4: Hybrid Approach

Combine strategies for best results:

1. Keep last 3 turns in full (recency)
2. Summarize turns 4-10 (recent context)
3. Highly condense or drop turns 11+ (distant history)

This preserves recent context while managing overall size.

---

## 6. Optimizing Retrieved Context (RAG)

If you're retrieving documents to augment the prompt, over-retrieval is a common problem.

### 6.1 The Over-Retrieval Trap

It's tempting to retrieve many chunks "just in case":
- 10 chunks × 200 tokens = 2,000 tokens of context

But often, only 1-2 chunks are actually relevant. The rest is noise that:
- Costs tokens
- May confuse the model
- Dilutes the signal from relevant content

### 6.2 Smarter Retrieval Strategies

**Retrieve less, but better:**
- Improve embedding quality
- Use re-ranking to filter initial results
- Retrieve 10, re-rank, keep top 3

**Contextual retrieval:**
- Include conversation history in the retrieval query
- Retrieve based on inferred intent, not just keywords

**Chunk summarization:**
- Instead of full chunks, retrieve chunk summaries
- Expand to full content only if needed

### 6.3 Query the Model for What It Needs

Instead of always retrieving, ask the model first:

```
User: What's your return policy?

Model: "I need to look up the return policy to answer accurately."

[Only now do you retrieve the return policy document]
```

This avoids unnecessary retrieval for questions the model can answer from its training.

---

## 7. Optimizing Model Responses

You also pay for output tokens—often at a higher rate than input tokens.

### 7.1 Set Appropriate max_tokens

The `max_tokens` parameter limits response length. Set it based on expected response type:

| Query Type | Appropriate max_tokens |
|-----------|----------------------|
| Yes/No question | 20-50 |
| Factual lookup | 100-200 |
| Explanation | 300-500 |
| Code generation | 500-1000 |

Don't set max_tokens to 4096 "just in case."

### 7.2 Request Concise Responses

Explicitly ask for concise answers in your system prompt:

```
Respond concisely. Avoid unnecessary preamble ("I'd be happy to help...") 
and verbose explanations. Use lists over paragraphs when appropriate.
```

### 7.3 Structured Output

Request JSON or structured output when appropriate:

```
Respond with a JSON object containing:
- answer: The direct answer
- confidence: high/medium/low
- sources: List of source IDs used

No additional text.
```

Structured output is often more compact and easier to parse.

---

## 8. Caching: Avoiding Redundant Computation

The greenest token is one you never use. Caching eliminates redundant LLM calls entirely.

### 8.1 Exact Match Caching

Cache responses for identical queries:
- User asks "What are your hours?" → Cached response
- Same question again → Return cached answer, no LLM call

**Limitation:** Only works for exact matches. "What are your hours?" and "When are you open?" are different strings but the same question.

### 8.2 Semantic Caching

Cache based on semantic similarity:

1. Compute embedding of new query
2. Compare to cached query embeddings
3. If similarity > threshold, return cached response

**Trade-off:** Requires embedding computation per query, but much cheaper than a full LLM call.

### 8.3 What to Cache

Not everything should be cached:

**Good for caching:**
- Static information (business hours, policies)
- Common questions with stable answers
- Computationally expensive retrievals

**Bad for caching:**
- Personalized information (account status)
- Time-sensitive data (stock prices)
- Questions dependent on conversation context

---

## 9. Model Selection and Routing

Different queries need different models. Using GPT-4 for every request is overkill.

### 9.1 The Model Spectrum

| Model | Relative Cost | Best For |
|-------|---------------|----------|
| GPT-3.5 Turbo | $0.002/1K | Simple queries, classification |
| GPT-4 Turbo | $0.02/1K | Complex reasoning, nuanced tasks |
| Claude 3 Haiku | $0.0025/1K | Simple, fast responses |
| Claude 3 Sonnet | $0.015/1K | Balanced capability |
| Claude 3 Opus | $0.075/1K | Highest capability tasks |

10x cost difference between simplest and most capable!

### 9.2 Query Routing

Build a classifier that routes queries to appropriate models:

**Simple queries (70% of traffic):**
- "What are your hours?"
- "How do I reset my password?"
- "Is my order shipped?"
→ Route to cheap/fast model

**Complex queries (30% of traffic):**
- "Compare these three plans and recommend one for my situation..."
- "Debug this code and explain the issue..."
- "Draft a formal complaint letter..."
→ Route to capable model

If 70% of queries go to a 10x cheaper model, you reduce costs by ~60%.

### 9.3 Cascading

Start with a cheap model; escalate if needed:

1. Send query to GPT-3.5
2. Check response quality (confidence, coherence)
3. If quality is low, resend to GPT-4

Most queries are handled cheaply; only difficult ones escalate.

---

## 10. Measuring and Monitoring

Optimization requires ongoing measurement.

### 10.1 Key Metrics to Track

**Token metrics:**
- Average tokens per request (total, by component)
- Token distribution (system, history, context, response)
- Tokens per dollar spent

**Efficiency metrics:**
- Cache hit rate
- Model routing distribution
- Summarization compression ratio

**Business metrics:**
- Cost per conversation
- Cost per resolved ticket
- Cost per successful task

### 10.2 Alerting on Anomalies

Set alerts for:
- Sudden increase in average tokens per request
- Cache hit rate dropping
- Unusual routing patterns (too many queries escalating to expensive models)

---

## 11. Connection to Transfer Learning

Token efficiency shares a pattern with today's transfer learning topic:

| Transfer Learning | Token Efficiency |
|------------------|------------------|
| Reuse learned knowledge | Cache computed responses |
| Don't train from scratch | Don't regenerate static content |
| Fine-tune only what's needed | Send only needed context |
| Parameter efficiency (adapters) | Token efficiency (compression) |

Both are fundamentally about **avoiding redundant work**—whether that's redundant learning or redundant token usage.

---

## 12. Key Takeaways

1. **Measure before optimizing**: Know where your tokens are going. System prompt and tool definitions often dominate.

2. **System prompt optimization compounds**: Every token saved is saved on every request. A 200-token reduction across 1M requests is 200M tokens saved.

3. **Manage conversation history aggressively**: Summarize, prune, and use sliding windows. History is the fastest-growing component.

4. **Use dynamic context**: Not every request needs every tool, every instruction, or every retrieved document.

5. **Cache semantically similar queries**: The cheapest token is one you don't use at all.

6. **Route to appropriate models**: Use expensive models only when necessary. 70% of queries might be handleable by cheaper models.

7. **Monitor continuously**: Token usage patterns change as your product evolves. Keep measuring.

Token efficiency isn't just about cost savings—it's about building sustainable, scalable AI systems. The most successful production agents are those that deliver value without burning money on every request.

---

**Originally published at:** [arunbaby.com/ai-agents/0046-token-efficiency-optimization](https://www.arunbaby.com/ai-agents/0046-token-efficiency-optimization/)

*If you found this helpful, consider sharing it with others who might benefit.*
