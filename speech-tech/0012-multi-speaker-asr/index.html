<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Multi-Speaker ASR - Arun Baby</title>
<meta name="description" content="Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Multi-Speaker ASR">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/">


  <meta property="og:description" content="Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Multi-Speaker ASR">
  <meta name="twitter:description" content="Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-01T10:50:00+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Multi-Speaker ASR">
    <meta itemprop="description" content="Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.">
    <meta itemprop="datePublished" content="2025-12-01T10:50:00+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/" itemprop="url">Multi-Speaker ASR
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          25 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem-statement">Problem Statement</a><ul><li><a href="#why-is-this-hard">Why Is This Hard?</a></li><li><a href="#real-world-use-cases">Real-World Use Cases</a></li></ul></li><li><a href="#understanding-multi-speaker-asr">Understanding Multi-Speaker ASR</a><ul><li><a href="#the-full-system-pipeline">The Full System Pipeline</a></li><li><a href="#why-this-pipeline">Why This Pipeline?</a></li><li><a href="#the-mathematics-behind-speaker-embeddings">The Mathematics Behind Speaker Embeddings</a></li><li><a href="#comparison-different-diarization-approaches">Comparison: Different Diarization Approaches</a></li></ul></li><li><a href="#component-1-voice-activity-detection-vad">Component 1: Voice Activity Detection (VAD)</a></li><li><a href="#component-2-speaker-diarization">Component 2: Speaker Diarization</a><ul><li><a href="#speaker-embeddings">Speaker Embeddings</a></li><li><a href="#clustering-embeddings">Clustering Embeddings</a></li></ul></li><li><a href="#component-3-asr-per-speaker">Component 3: ASR Per Speaker</a></li><li><a href="#handling-overlapping-speech">Handling Overlapping Speech</a><ul><li><a href="#challenge">Challenge</a></li><li><a href="#approach-1-overlap-detection--best-effort">Approach 1: Overlap Detection + Best Effort</a></li><li><a href="#approach-2-multi-channel-source-separation">Approach 2: Multi-Channel Source Separation</a></li></ul></li><li><a href="#real-time-streaming">Real-Time Streaming</a><ul><li><a href="#streaming-architecture">Streaming Architecture</a></li></ul></li><li><a href="#common-failure-modes--debugging">Common Failure Modes &amp; Debugging</a><ul><li><a href="#failure-mode-1-speaker-confusion">Failure Mode 1: Speaker Confusion</a></li><li><a href="#failure-mode-2-overlap-mis-attribution">Failure Mode 2: Overlap Mis-attribution</a></li><li><a href="#failure-mode-3-far-field-audio-degradation">Failure Mode 3: Far-Field Audio Degradation</a></li><li><a href="#debugging-tools">Debugging Tools</a></li></ul></li><li><a href="#production-considerations">Production Considerations</a><ul><li><a href="#1-latency-optimization">1. Latency Optimization</a></li><li><a href="#2-accuracy-vs-speed-trade-off">2. Accuracy vs Speed Trade-off</a></li><li><a href="#3-speaker-persistence">3. Speaker Persistence</a></li><li><a href="#4-monitoring--debugging">4. Monitoring &amp; Debugging</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p>Design a <strong>multi-speaker ASR system</strong> that can:</p>

<ol>
  <li><strong>Recognize speech</strong> from multiple speakers in a conversation</li>
  <li><strong>Identify who spoke</strong> each word/sentence (speaker diarization)</li>
  <li><strong>Handle overlapping speech</strong> when multiple people speak simultaneously</li>
  <li><strong>Work in real-time</strong> with &lt; 300ms latency for live transcription</li>
  <li><strong>Scale</strong> to meetings with 2-10 speakers</li>
</ol>

<h3 id="why-is-this-hard">Why Is This Hard?</h3>

<p><strong>Single-speaker ASR</strong> (covered in Day 1) assumes:</p>
<ul>
  <li>✅ One speaker at a time</li>
  <li>✅ No speaker changes mid-sentence</li>
  <li>✅ No overlapping speech</li>
</ul>

<p><strong>Real-world conversations</strong> break all these assumptions:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Time:     0s         1s         2s         3s         4s
Speaker A: "So I think we should..."
Speaker B:             "Wait, can I..."
Speaker C:                        "Actually..."
          ↑ Overlap! ↑            ↑ Overlap! ↑

Single-speaker ASR would produce: "So I wait can think actually should we..."
Multi-speaker ASR must produce:
  [A, 0.0-1.5s]: "So I think we should"
  [B, 1.2-2.3s]: "Wait, can I"
  [C, 2.8-4.0s]: "Actually"
</code></pre></div></div>

<p><strong>The core challenges:</strong></p>

<table>
  <thead>
    <tr>
      <th>Challenge</th>
      <th>Why It’s Hard</th>
      <th>Impact</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Speaker changes</strong></td>
      <td>Voice characteristics change suddenly</td>
      <td>Acoustic model confused</td>
    </tr>
    <tr>
      <td><strong>Overlapping speech</strong></td>
      <td>Multiple audio sources mixed</td>
      <td>Can’t separate cleanly</td>
    </tr>
    <tr>
      <td><strong>Speaker identification</strong></td>
      <td>Need to know <em>who</em> said <em>what</em></td>
      <td>Requires speaker embeddings</td>
    </tr>
    <tr>
      <td><strong>Real-time processing</strong></td>
      <td>Must process while speakers still talking</td>
      <td>Latency constraints</td>
    </tr>
    <tr>
      <td><strong>Unknown # of speakers</strong></td>
      <td>Don’t know speaker count in advance</td>
      <td>Can’t pre-allocate resources</td>
    </tr>
  </tbody>
</table>

<h3 id="real-world-use-cases">Real-World Use Cases</h3>

<table>
  <thead>
    <tr>
      <th>Application</th>
      <th>Requirements</th>
      <th>Challenges</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Meeting transcription</strong> (Zoom, Teams)</td>
      <td>2-10 speakers, real-time</td>
      <td>Overlaps, background noise</td>
    </tr>
    <tr>
      <td><strong>Call center analytics</strong></td>
      <td>2 speakers (agent + customer)</td>
      <td>Quality monitoring, compliance</td>
    </tr>
    <tr>
      <td><strong>Podcast transcription</strong></td>
      <td>2-5 hosts + guests</td>
      <td>High accuracy needed</td>
    </tr>
    <tr>
      <td><strong>Courtroom transcription</strong></td>
      <td>Multiple speakers, legal record</td>
      <td>99%+ accuracy, speaker IDs</td>
    </tr>
    <tr>
      <td><strong>Medical consultations</strong></td>
      <td>Doctor + patient(s)</td>
      <td>HIPAA compliance, accuracy</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="understanding-multi-speaker-asr">Understanding Multi-Speaker ASR</h2>

<h3 id="the-full-system-pipeline">The Full System Pipeline</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────────────────────────────┐
│                    MULTI-SPEAKER ASR PIPELINE                │
└─────────────────────────────────────────────────────────────┘

Step 1: AUDIO INPUT
┌────────────────────────────────────────────┐
│  Mixed audio (all speakers combined)       │
│  [Speaker A + Speaker B + Speaker C + ...]│
└────────────────┬───────────────────────────┘
                 ▼
Step 2: VOICE ACTIVITY DETECTION (VAD)
┌────────────────────────────────────────────┐
│  Find speech regions (vs silence)          │
│  Output: [(0.0s, 3.2s), (3.5s, 7.1s), ...] │
└────────────────┬───────────────────────────┘
                 ▼
Step 3: SPEAKER DIARIZATION
┌────────────────────────────────────────────┐
│  Cluster speech by speaker                 │
│  Output: [(A, 0-1.5s), (B, 1.2-2.3s), ...]│
└────────────────┬───────────────────────────┘
                 ▼
Step 4: ASR (per speaker segment)
┌────────────────────────────────────────────┐
│  Transcribe each speaker segment           │
│  Output: [(A, "Hello"), (B, "Hi"), ...]   │
└────────────────┬───────────────────────────┘
                 ▼
Step 5: POST-PROCESSING
┌────────────────────────────────────────────┐
│  • Merge overlaps                          │
│  • Add punctuation                         │
│  • Format output                           │
└────────────────────────────────────────────┘
</code></pre></div></div>

<p>Each step has challenges! Let’s dig into each.</p>

<h3 id="why-this-pipeline">Why This Pipeline?</h3>

<p><strong>Why not just run ASR on everything?</strong></p>

<p>Imagine you have a 1-hour meeting:</p>
<ul>
  <li>Raw audio: 1 hour</li>
  <li>Actual speech: ~30 minutes (50% silence/pauses)</li>
  <li>Running ASR on silence: <strong>Waste of 30 minutes compute!</strong></li>
</ul>

<p><strong>Why not run ASR first, then diarize?</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Option A: ASR → Diarization (BAD)
Problem: ASR produces one continuous text blob
"Hello there hi how are you fine thanks"
↑ Can't tell where speakers change!

Option B: Diarization → ASR (GOOD)
Step 1: Find speaker segments
  [A: 0-2s], [B: 2-4s], [A: 4-6s]
Step 2: Transcribe each segment separately
  A: "Hello there"
  B: "Hi, how are you?"
  A: "Fine, thanks"
↑ Clean separation!
</code></pre></div></div>

<p><strong>Why separate VAD from diarization?</strong></p>

<ul>
  <li><strong>VAD</strong> is fast (simple energy-based or small model)</li>
  <li><strong>Diarization</strong> is slow (needs embeddings + clustering)</li>
  <li>Don’t waste diarization compute on silence!</li>
</ul>

<p><strong>Pipeline efficiency:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1 hour audio
  ↓ VAD (fast, eliminates silence)
30 min speech segments
  ↓ Diarization (slow, but only on speech)
30 min speaker-labeled segments
  ↓ ASR (slowest, but parallelizable)
Transcriptions
</code></pre></div></div>

<h3 id="the-mathematics-behind-speaker-embeddings">The Mathematics Behind Speaker Embeddings</h3>

<p><strong>Key question</strong>: How do we represent a voice mathematically?</p>

<p><strong>Answer</strong>: Deep learning learns to compress voice characteristics into a fixed-size vector.</p>

<p><strong>Training process</strong> (simplified):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Step 1: Collect data
  Speaker 1: 100 utterances
  Speaker 2: 100 utterances
  ...
  Speaker 10,000: 100 utterances

Step 2: Train neural network
  Input: Audio waveform or spectrogram
  Output: 512-dimensional embedding
  
  Goal: Minimize distance between embeddings of same speaker,
        maximize distance between different speakers

Step 3: Loss function (Triplet Loss)
  Anchor: Speaker A, utterance 1
  Positive: Speaker A, utterance 2 (same speaker)
  Negative: Speaker B, utterance 1 (different speaker)
  
  Loss = max(0, distance(anchor, positive) - distance(anchor, negative) + margin)
  
  This forces:
  - distance(A_utt1, A_utt2) &lt; distance(A_utt1, B_utt1)
</code></pre></div></div>

<p><strong>Visual intuition:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Before training (random embeddings):
Speaker A utterances: scattered everywhere
Speaker B utterances: scattered everywhere
No clustering!

After training:
Speaker A utterances: tight cluster in embedding space
Speaker B utterances: different tight cluster, far from A
Clear separation!
</code></pre></div></div>

<p><strong>Why 512 dimensions?</strong></p>

<ul>
  <li><strong>Lower (e.g., 64)</strong>: Not enough capacity to capture all voice variations</li>
  <li><strong>Higher (e.g., 2048)</strong>: Overfitting, slow, unnecessary</li>
  <li><strong>512</strong>: Sweet spot (empirically found by researchers)</li>
</ul>

<p><strong>What does the embedding capture?</strong></p>

<ul>
  <li>Pitch/fundamental frequency</li>
  <li>Formant structure (vocal tract resonances)</li>
  <li>Speaking rate</li>
  <li>Accent/dialect</li>
  <li>Voice quality (breathy, creaky, etc.)</li>
</ul>

<p><strong>What it should NOT capture</strong> (ideally):</p>

<ul>
  <li>Spoken words (content)</li>
  <li>Emotions (though it does somewhat)</li>
  <li>Background noise</li>
</ul>

<h3 id="comparison-different-diarization-approaches">Comparison: Different Diarization Approaches</h3>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>How It Works</th>
      <th>Pros</th>
      <th>Cons</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Clustering-based</strong></td>
      <td>Extract embeddings → Cluster</td>
      <td>Simple, interpretable</td>
      <td>Needs good embeddings</td>
      <td>General purpose</td>
    </tr>
    <tr>
      <td><strong>End-to-end neural</strong></td>
      <td>Single model: audio → labels</td>
      <td>Best accuracy</td>
      <td>Slow, black-box</td>
      <td>High-accuracy needs</td>
    </tr>
    <tr>
      <td><strong>Online diarization</strong></td>
      <td>Process stream incrementally</td>
      <td>Real-time capable</td>
      <td>Lower accuracy</td>
      <td>Live captions</td>
    </tr>
    <tr>
      <td><strong>Supervised (known speakers)</strong></td>
      <td>Match to registered voices</td>
      <td>Very accurate for known speakers</td>
      <td>Requires enrollment</td>
      <td>Authentication, personalization</td>
    </tr>
  </tbody>
</table>

<p><strong>Example scenario: Meeting with known participants</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">KnownSpeakerDiarization</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    When you know who</span><span class="sh">'</span><span class="s">s in the meeting
    
    Much more accurate than unsupervised clustering
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">speaker_profiles</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># speaker_name → mean embedding
</span>    
    <span class="k">def</span> <span class="nf">enroll_speaker</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">speaker_name</span><span class="p">,</span> <span class="n">audio_samples</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Register a speaker
        
        Args:
            speaker_name: </span><span class="sh">"</span><span class="s">Alice</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">Bob</span><span class="sh">"</span><span class="s">, etc.
            audio_samples: List of audio clips of this speaker
        </span><span class="sh">"""</span>
        <span class="c1"># Extract embeddings from all samples
</span>        <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">extract_embedding</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">audio</span> <span class="ow">in</span> <span class="n">audio_samples</span>
        <span class="p">]</span>
        
        <span class="c1"># Compute mean embedding (speaker profile)
</span>        <span class="n">mean_embedding</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Store
</span>        <span class="n">self</span><span class="p">.</span><span class="n">speaker_profiles</span><span class="p">[</span><span class="n">speaker_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_embedding</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">✓ Enrolled </span><span class="si">{</span><span class="n">speaker_name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">identify_speaker</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio_segment</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Identify which registered speaker this is
        
        Much more accurate than unsupervised clustering!
        </span><span class="sh">"""</span>
        <span class="c1"># Extract embedding
</span>        <span class="n">test_embedding</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">extract_embedding</span><span class="p">(</span><span class="n">audio_segment</span><span class="p">)</span>
        
        <span class="c1"># Compare with all registered speakers
</span>        <span class="n">best_match</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">best_similarity</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">profile_embedding</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">speaker_profiles</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">similarity</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_cosine_similarity</span><span class="p">(</span><span class="n">test_embedding</span><span class="p">,</span> <span class="n">profile_embedding</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">similarity</span> <span class="o">&gt;</span> <span class="n">best_similarity</span><span class="p">:</span>
                <span class="n">best_similarity</span> <span class="o">=</span> <span class="n">similarity</span>
                <span class="n">best_match</span> <span class="o">=</span> <span class="n">name</span>
        
        <span class="c1"># Threshold
</span>        <span class="k">if</span> <span class="n">best_similarity</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">best_match</span><span class="p">,</span> <span class="n">best_similarity</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="sh">"</span><span class="s">UNKNOWN</span><span class="sh">"</span><span class="p">,</span> <span class="n">best_similarity</span>

    <span class="k">def</span> <span class="nf">extract_embedding</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Placeholder: replace with real embedding extractor</span><span class="sh">"""</span>
        <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
        <span class="n">audio</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">audio</span><span class="p">[:</span><span class="mi">512</span><span class="p">]</span> <span class="k">if</span> <span class="n">audio</span><span class="p">.</span><span class="n">size</span> <span class="o">&gt;=</span> <span class="mi">512</span> <span class="k">else</span> <span class="n">np</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">512</span> <span class="o">-</span> <span class="n">audio</span><span class="p">.</span><span class="n">size</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_cosine_similarity</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">a</span><span class="p">);</span> <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
        <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">b</span><span class="p">))</span> <span class="o">+</span> <span class="mf">1e-10</span>
        <span class="k">return</span> <span class="nf">float</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="n">denom</span><span class="p">)</span>

<span class="c1"># Usage
</span><span class="n">diarizer</span> <span class="o">=</span> <span class="nc">KnownSpeakerDiarization</span><span class="p">()</span>

<span class="c1"># Enroll meeting participants
</span><span class="n">diarizer</span><span class="p">.</span><span class="nf">enroll_speaker</span><span class="p">(</span><span class="sh">"</span><span class="s">Alice</span><span class="sh">"</span><span class="p">,</span> <span class="n">alice_audio_samples</span><span class="p">)</span>
<span class="n">diarizer</span><span class="p">.</span><span class="nf">enroll_speaker</span><span class="p">(</span><span class="sh">"</span><span class="s">Bob</span><span class="sh">"</span><span class="p">,</span> <span class="n">bob_audio_samples</span><span class="p">)</span>
<span class="n">diarizer</span><span class="p">.</span><span class="nf">enroll_speaker</span><span class="p">(</span><span class="sh">"</span><span class="s">Carol</span><span class="sh">"</span><span class="p">,</span> <span class="n">carol_audio_samples</span><span class="p">)</span>

<span class="c1"># Now identify speakers in meeting
</span><span class="k">for</span> <span class="n">segment</span> <span class="ow">in</span> <span class="n">meeting_segments</span><span class="p">:</span>
    <span class="n">speaker</span><span class="p">,</span> <span class="n">confidence</span> <span class="o">=</span> <span class="n">diarizer</span><span class="p">.</span><span class="nf">identify_speaker</span><span class="p">(</span><span class="n">segment</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">speaker</span><span class="si">}</span><span class="s"> (</span><span class="si">{</span><span class="n">confidence</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">): </span><span class="si">{</span><span class="nf">transcribe</span><span class="p">(</span><span class="n">segment</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>This is how Zoom/Teams could improve</strong>:</p>
<ul>
  <li>Ask users to speak their name when joining</li>
  <li>Build speaker profile</li>
  <li>Use it for accurate diarization</li>
</ul>

<hr />

<h2 id="component-1-voice-activity-detection-vad">Component 1: Voice Activity Detection (VAD)</h2>

<p><strong>Goal</strong>: Find when <em>any</em> speaker is talking</p>

<p><strong>Why needed</strong>: Don’t waste compute on silence</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VoiceActivityDetector</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Detect speech vs non-speech
    
    Uses energy + spectral features
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sample_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span> <span class="n">frame_ms</span><span class="o">=</span><span class="mi">30</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sample_rate</span> <span class="o">=</span> <span class="n">sample_rate</span>
        <span class="n">self</span><span class="p">.</span><span class="n">frame_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">sample_rate</span> <span class="o">*</span> <span class="n">frame_ms</span> <span class="o">/</span> <span class="mi">1000</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">detect</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Detect speech regions
        
        Args:
            audio: numpy array, shape (samples,)
        
        Returns:
            List of (start_time, end_time) tuples
        </span><span class="sh">"""</span>
        <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
        
        <span class="c1"># Split into frames
</span>        <span class="n">frames</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_split_frames</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
        
        <span class="c1"># Compute features for each frame
</span>        <span class="n">is_speech</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">frames</span><span class="p">:</span>
            <span class="c1"># Energy-based detection
</span>            <span class="n">energy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">frame</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            
            <span class="c1"># Spectral flatness (voice has low flatness)
</span>            <span class="n">flatness</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_spectral_flatness</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
            
            <span class="c1"># Simple threshold
</span>            <span class="n">speech</span> <span class="o">=</span> <span class="p">(</span><span class="n">energy</span> <span class="o">&gt;</span> <span class="mf">0.01</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">flatness</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">)</span>
            <span class="n">is_speech</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">speech</span><span class="p">)</span>
        
        <span class="c1"># Convert frame-level to time segments
</span>        <span class="n">segments</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_merge_segments</span><span class="p">(</span><span class="n">is_speech</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">segments</span>
    
    <span class="k">def</span> <span class="nf">_split_frames</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Split audio into overlapping frames</span><span class="sh">"""</span>
        <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
        
        <span class="n">frames</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">hop_size</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">frame_size</span> <span class="o">//</span> <span class="mi">2</span>  <span class="c1"># 50% overlap
</span>        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">frame_size</span><span class="p">,</span> <span class="n">hop_size</span><span class="p">):</span>
            <span class="n">frame</span> <span class="o">=</span> <span class="n">audio</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">frame_size</span><span class="p">]</span>
            <span class="n">frames</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">frame</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">frames</span>
    
    <span class="k">def</span> <span class="nf">_spectral_flatness</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Compute spectral flatness
        
        Low for voice (harmonic), high for noise (flat spectrum)
        </span><span class="sh">"""</span>
        <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
        <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">signal</span>
        
        <span class="c1"># FFT
</span>        <span class="n">fft</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">fft</span><span class="p">.</span><span class="nf">rfft</span><span class="p">(</span><span class="n">frame</span><span class="p">))</span>
        
        <span class="c1"># Geometric mean / arithmetic mean
</span>        <span class="n">geometric_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">fft</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)))</span>
        <span class="n">arithmetic_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">fft</span><span class="p">)</span>
        
        <span class="n">flatness</span> <span class="o">=</span> <span class="n">geometric_mean</span> <span class="o">/</span> <span class="p">(</span><span class="n">arithmetic_mean</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">flatness</span>
    
    <span class="k">def</span> <span class="nf">_merge_segments</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">is_speech</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Merge consecutive speech frames into segments
        
        Args:
            is_speech: List of bools per frame
        
        Returns:
            List of (start_time, end_time)
        </span><span class="sh">"""</span>
        <span class="n">segments</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">in_segment</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="n">frame_duration</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">frame_size</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">sample_rate</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">speech</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">is_speech</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">speech</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">in_segment</span><span class="p">:</span>
                <span class="c1"># Start new segment
</span>                <span class="n">start</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">frame_duration</span>
                <span class="n">in_segment</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">speech</span> <span class="ow">and</span> <span class="n">in_segment</span><span class="p">:</span>
                <span class="c1"># End segment
</span>                <span class="n">end</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">frame_duration</span>
                <span class="n">segments</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">))</span>
                <span class="n">in_segment</span> <span class="o">=</span> <span class="bp">False</span>
        
        <span class="c1"># Handle case where last frame is speech
</span>        <span class="k">if</span> <span class="n">in_segment</span><span class="p">:</span>
            <span class="n">segments</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">start</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">is_speech</span><span class="p">)</span> <span class="o">*</span> <span class="n">frame_duration</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">segments</span>
</code></pre></div></div>

<p><strong>Modern approach</strong>: Use pre-trained VAD models (more accurate)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">vad_pretrained</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">sample_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Use pre-trained VAD model (Silero VAD)
    
    More accurate than energy-based
    </span><span class="sh">"""</span>
    <span class="kn">import</span> <span class="n">torch</span>
    
    <span class="c1"># Load pre-trained model
</span>    <span class="n">model</span><span class="p">,</span> <span class="n">utils</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">hub</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span>
        <span class="n">repo_or_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">snakers4/silero-vad</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="sh">'</span><span class="s">silero_vad</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">force_reload</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>
    
    <span class="p">(</span><span class="n">get_speech_timestamps</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="n">utils</span>
    
    <span class="c1"># Detect speech
</span>    <span class="n">speech_timestamps</span> <span class="o">=</span> <span class="nf">get_speech_timestamps</span><span class="p">(</span>
        <span class="n">audio</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">sampling_rate</span><span class="o">=</span><span class="n">sample_rate</span><span class="p">,</span>
        <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span>
    <span class="p">)</span>
    
    <span class="c1"># Convert to seconds
</span>    <span class="n">segments</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">ts</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> <span class="n">sample_rate</span><span class="p">,</span> <span class="n">ts</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> <span class="n">sample_rate</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">speech_timestamps</span>
    <span class="p">]</span>
    
    <span class="k">return</span> <span class="n">segments</span>
</code></pre></div></div>

<hr />

<h2 id="component-2-speaker-diarization">Component 2: Speaker Diarization</h2>

<p><strong>Goal</strong>: Cluster speech segments by speaker</p>

<p><strong>Key idea</strong>: Speakers have unique voice characteristics (embeddings)</p>

<h3 id="speaker-embeddings">Speaker Embeddings</h3>

<p><strong>Concept</strong>: Convert speech to fixed-size vector that captures speaker identity</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Speaker A: "Hello" → [0.2, 0.8, -0.3, ...] (512-dim)
Speaker A: "How are you" → [0.21, 0.79, -0.31, ...] (similar!)
Speaker B: "Hi there" → [-0.5, 0.1, 0.9, ...] (different!)
</code></pre></div></div>

<p><strong>Models</strong>: x-vector, d-vector, ECAPA-TDNN</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SpeakerEmbeddingExtractor</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Extract speaker embeddings using ECAPA-TDNN
    
    Embeddings capture speaker identity
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="kn">from</span> <span class="n">speechbrain.pretrained</span> <span class="kn">import</span> <span class="n">EncoderClassifier</span>
        
        <span class="c1"># Load pre-trained model
</span>        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">EncoderClassifier</span><span class="p">.</span><span class="nf">from_hparams</span><span class="p">(</span>
            <span class="n">source</span><span class="o">=</span><span class="sh">"</span><span class="s">speechbrain/spkrec-ecapa-voxceleb</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">savedir</span><span class="o">=</span><span class="sh">"</span><span class="s">pretrained_models/spkrec-ecapa</span><span class="sh">"</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">extract</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">,</span> <span class="n">sample_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Extract speaker embedding
        
        Args:
            audio: numpy array
        
        Returns:
            embedding: numpy array, shape (512,)
        </span><span class="sh">"""</span>
        <span class="kn">import</span> <span class="n">torch</span>
        
        <span class="c1"># Convert to tensor
</span>        <span class="n">audio_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nc">FloatTensor</span><span class="p">(</span><span class="n">audio</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Extract embedding
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">embedding</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">encode_batch</span><span class="p">(</span><span class="n">audio_tensor</span><span class="p">)</span>
        
        <span class="c1"># Convert to numpy
</span>        <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">embedding</span>

<span class="c1"># Usage
</span><span class="n">extractor</span> <span class="o">=</span> <span class="nc">SpeakerEmbeddingExtractor</span><span class="p">()</span>

<span class="c1"># Extract embeddings for each speech segment
</span><span class="n">segments</span> <span class="o">=</span> <span class="p">[(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">),</span> <span class="p">(</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)]</span>  <span class="c1"># From VAD
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">segments</span><span class="p">:</span>
    <span class="n">segment_audio</span> <span class="o">=</span> <span class="n">audio</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">start</span><span class="o">*</span><span class="n">sr</span><span class="p">):</span><span class="nf">int</span><span class="p">(</span><span class="n">end</span><span class="o">*</span><span class="n">sr</span><span class="p">)]</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">extractor</span><span class="p">.</span><span class="nf">extract</span><span class="p">(</span><span class="n">segment_audio</span><span class="p">)</span>
    <span class="n">embeddings</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>

<span class="c1"># Now cluster embeddings to identify speakers
</span></code></pre></div></div>

<h3 id="clustering-embeddings">Clustering Embeddings</h3>

<p><strong>Goal</strong>: Group similar embeddings (same speaker)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SpeakerClustering</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Cluster speaker embeddings
    
    Same speaker → similar embeddings → same cluster
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">spectral</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
    
    <span class="k">def</span> <span class="nf">cluster</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">num_speakers</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Cluster embeddings into speakers
        
        Args:
            embeddings: List of numpy arrays
            num_speakers: If known, specify; else auto-detect
        
        Returns:
            labels: Array of speaker IDs per segment
        </span><span class="sh">"""</span>
        <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
        <span class="kn">from</span> <span class="n">sklearn.cluster</span> <span class="kn">import</span> <span class="n">SpectralClustering</span><span class="p">,</span> <span class="n">AgglomerativeClustering</span>
        
        <span class="c1"># Convert to matrix
</span>        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">num_speakers</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># Auto-detect number of speakers
</span>            <span class="n">num_speakers</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_estimate_num_speakers</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="c1"># Cluster
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">method</span> <span class="o">==</span> <span class="sh">'</span><span class="s">spectral</span><span class="sh">'</span><span class="p">:</span>
            <span class="c1"># Use precomputed affinity for better control
</span>            <span class="kn">from</span> <span class="n">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
            <span class="n">affinity</span> <span class="o">=</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">clusterer</span> <span class="o">=</span> <span class="nc">SpectralClustering</span><span class="p">(</span>
                <span class="n">n_clusters</span><span class="o">=</span><span class="n">num_speakers</span><span class="p">,</span>
                <span class="n">affinity</span><span class="o">=</span><span class="sh">'</span><span class="s">precomputed</span><span class="sh">'</span>
            <span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">clusterer</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">affinity</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">labels</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">clusterer</span> <span class="o">=</span> <span class="nc">AgglomerativeClustering</span><span class="p">(</span>
                <span class="n">n_clusters</span><span class="o">=</span><span class="n">num_speakers</span><span class="p">,</span>
                <span class="n">linkage</span><span class="o">=</span><span class="sh">'</span><span class="s">average</span><span class="sh">'</span><span class="p">,</span>
                <span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">cosine</span><span class="sh">'</span>
            <span class="p">)</span>
        
        <span class="n">labels</span> <span class="o">=</span> <span class="n">clusterer</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">labels</span>
    
    <span class="k">def</span> <span class="nf">_estimate_num_speakers</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Estimate number of speakers
        
        Use eigengap heuristic or elbow method
        </span><span class="sh">"""</span>
        <span class="kn">from</span> <span class="n">sklearn.cluster</span> <span class="kn">import</span> <span class="n">SpectralClustering</span>
        <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
        
        <span class="c1"># Try different numbers of clusters
</span>        <span class="n">max_speakers</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span>
        
        <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_speakers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="kn">from</span> <span class="n">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
            <span class="n">aff</span> <span class="o">=</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
            <span class="n">clusterer</span> <span class="o">=</span> <span class="nc">SpectralClustering</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">affinity</span><span class="o">=</span><span class="sh">'</span><span class="s">precomputed</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">clusterer</span><span class="p">.</span><span class="nf">fit_predict</span><span class="p">(</span><span class="n">aff</span><span class="p">)</span>
            
            <span class="c1"># Compute silhouette score
</span>            <span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span>
            <span class="n">score</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">cosine</span><span class="sh">'</span><span class="p">)</span>
            <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        
        <span class="c1"># Pick k with highest score
</span>        <span class="n">best_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
        
        <span class="k">return</span> <span class="n">best_k</span>
</code></pre></div></div>

<p><strong>Production library</strong>: Use <code class="language-plaintext highlighter-rouge">pyannote.audio</code> (state-of-the-art)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">diarize_with_pyannote</span><span class="p">(</span><span class="n">audio_path</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Speaker diarization using pyannote.audio
    
    Production-ready, state-of-the-art
    </span><span class="sh">"""</span>
    <span class="kn">from</span> <span class="n">pyannote.audio</span> <span class="kn">import</span> <span class="n">Pipeline</span>
    
    <span class="c1"># Load pre-trained pipeline
</span>    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">pyannote/speaker-diarization</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">use_auth_token</span><span class="o">=</span><span class="sh">"</span><span class="s">YOUR_HF_TOKEN</span><span class="sh">"</span>
    <span class="p">)</span>
    
    <span class="c1"># Run diarization
</span>    <span class="n">diarization</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="n">audio_path</span><span class="p">)</span>
    
    <span class="c1"># Extract speaker segments
</span>    <span class="n">segments</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">turn</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">speaker</span> <span class="ow">in</span> <span class="n">diarization</span><span class="p">.</span><span class="nf">itertracks</span><span class="p">(</span><span class="n">yield_label</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">segments</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
            <span class="sh">'</span><span class="s">speaker</span><span class="sh">'</span><span class="p">:</span> <span class="n">speaker</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">:</span> <span class="n">turn</span><span class="p">.</span><span class="n">start</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">:</span> <span class="n">turn</span><span class="p">.</span><span class="n">end</span>
        <span class="p">})</span>
    
    <span class="k">return</span> <span class="n">segments</span>

<span class="c1"># Example output:
# [
#   {'speaker': 'SPEAKER_00', 'start': 0.0, 'end': 1.5},
#   {'speaker': 'SPEAKER_01', 'start': 1.2, 'end': 2.3},
#   {'speaker': 'SPEAKER_00', 'start': 2.8, 'end': 4.0},
# ]
</span></code></pre></div></div>

<hr />

<h2 id="component-3-asr-per-speaker">Component 3: ASR Per Speaker</h2>

<p><strong>Goal</strong>: Transcribe each speaker segment</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiSpeakerASR</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Complete multi-speaker ASR system
    
    Combines VAD + Diarization + ASR
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Load models
</span>        <span class="n">self</span><span class="p">.</span><span class="n">vad</span> <span class="o">=</span> <span class="nc">VoiceActivityDetector</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_extractor</span> <span class="o">=</span> <span class="nc">SpeakerEmbeddingExtractor</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">clustering</span> <span class="o">=</span> <span class="nc">SpeakerClustering</span><span class="p">()</span>
        
        <span class="c1"># ASR model (Whisper)
</span>        <span class="kn">import</span> <span class="n">whisper</span>
        <span class="n">self</span><span class="p">.</span><span class="n">asr_model</span> <span class="o">=</span> <span class="n">whisper</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span><span class="sh">"</span><span class="s">base</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">transcribe</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">,</span> <span class="n">sample_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Multi-speaker transcription
        
        Returns:
            List of {speaker, start, end, text}
        </span><span class="sh">"""</span>
        <span class="c1"># Step 1: VAD
</span>        <span class="n">speech_segments</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vad</span><span class="p">.</span><span class="nf">detect</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Found </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">speech_segments</span><span class="p">)</span><span class="si">}</span><span class="s"> speech segments</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># Step 2: Extract embeddings
</span>        <span class="n">embeddings</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="ow">in</span> <span class="n">speech_segments</span><span class="p">:</span>
            <span class="n">segment</span> <span class="o">=</span> <span class="n">audio</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">start</span><span class="o">*</span><span class="n">sample_rate</span><span class="p">):</span><span class="nf">int</span><span class="p">(</span><span class="n">end</span><span class="o">*</span><span class="n">sample_rate</span><span class="p">)]</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding_extractor</span><span class="p">.</span><span class="nf">extract</span><span class="p">(</span><span class="n">segment</span><span class="p">)</span>
            <span class="n">embeddings</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
        
        <span class="c1"># Step 3: Cluster by speaker
</span>        <span class="n">speaker_labels</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">clustering</span><span class="p">.</span><span class="nf">cluster</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Detected </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">speaker_labels</span><span class="p">))</span><span class="si">}</span><span class="s"> speakers</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># Step 4: Transcribe each segment
</span>        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">speech_segments</span><span class="p">):</span>
            <span class="n">segment</span> <span class="o">=</span> <span class="n">audio</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">start</span><span class="o">*</span><span class="n">sample_rate</span><span class="p">):</span><span class="nf">int</span><span class="p">(</span><span class="n">end</span><span class="o">*</span><span class="n">sample_rate</span><span class="p">)]</span>
            
            <span class="c1"># Transcribe (Whisper expects float32 numpy audio @16k)
</span>            <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">asr_model</span><span class="p">.</span><span class="nf">transcribe</span><span class="p">(</span><span class="n">segment</span><span class="p">,</span> <span class="n">fp16</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">]</span>
            
            <span class="c1"># Add speaker label
</span>            <span class="n">speaker</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">SPEAKER_</span><span class="si">{</span><span class="n">speaker_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span>
            
            <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
                <span class="sh">'</span><span class="s">speaker</span><span class="sh">'</span><span class="p">:</span> <span class="n">speaker</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">:</span> <span class="n">start</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">:</span> <span class="n">end</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="n">text</span>
            <span class="p">})</span>
        
        <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Usage
</span><span class="n">asr</span> <span class="o">=</span> <span class="nc">MultiSpeakerASR</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">asr</span><span class="p">.</span><span class="nf">transcribe</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>

<span class="c1"># Output:
# [
#   {'speaker': 'SPEAKER_0', 'start': 0.0, 'end': 1.5, 'text': 'Hello everyone'},
#   {'speaker': 'SPEAKER_1', 'start': 1.2, 'end': 2.3, 'text': 'Hi there'},
#   {'speaker': 'SPEAKER_0', 'start': 2.8, 'end': 4.0, 'text': 'How are you'},
# ]
</span></code></pre></div></div>

<hr />

<h2 id="handling-overlapping-speech">Handling Overlapping Speech</h2>

<p><strong>The hardest problem</strong>: Multiple speakers at once</p>

<h3 id="challenge">Challenge</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Time:     0s         1s         2s
Speaker A: "Hello there..."
Speaker B:         "Hi..."
Audio:     [A]    [A+B]     [A]
                    ↑
              Overlapped!
</code></pre></div></div>

<p><strong>Problem</strong>: Single-channel audio can’t separate perfectly</p>

<h3 id="approach-1-overlap-detection--best-effort">Approach 1: Overlap Detection + Best Effort</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">OverlapHandler</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Detect overlapping speech and handle gracefully
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">detect_overlaps</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">segments</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Find overlapping segments
        
        Args:
            segments: List of {speaker, start, end}
        
        Returns:
            List of overlap regions
        </span><span class="sh">"""</span>
        <span class="n">overlaps</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seg1</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">segments</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">seg2</span> <span class="ow">in</span> <span class="n">segments</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:]:</span>
                <span class="c1"># Check if overlapping
</span>                <span class="k">if</span> <span class="n">seg1</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">seg2</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">]</span> <span class="ow">and</span> <span class="n">seg1</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">seg2</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">]:</span>
                    <span class="c1"># Compute overlap region
</span>                    <span class="n">overlap_start</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">seg1</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">],</span> <span class="n">seg2</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">])</span>
                    <span class="n">overlap_end</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">seg1</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">],</span> <span class="n">seg2</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">])</span>
                    
                    <span class="n">overlaps</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
                        <span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">:</span> <span class="n">overlap_start</span><span class="p">,</span>
                        <span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">:</span> <span class="n">overlap_end</span><span class="p">,</span>
                        <span class="sh">'</span><span class="s">speakers</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="n">seg1</span><span class="p">[</span><span class="sh">'</span><span class="s">speaker</span><span class="sh">'</span><span class="p">],</span> <span class="n">seg2</span><span class="p">[</span><span class="sh">'</span><span class="s">speaker</span><span class="sh">'</span><span class="p">]]</span>
                    <span class="p">})</span>
        
        <span class="k">return</span> <span class="n">overlaps</span>
    
    <span class="k">def</span> <span class="nf">handle_overlap</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">,</span> <span class="n">overlap</span><span class="p">,</span> <span class="n">speakers</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Handle overlapped region
        
        Options:
        1. Transcribe mixed audio (less accurate)
        2. Mark as [OVERLAP] in transcript
        3. Use source separation (advanced)
        </span><span class="sh">"""</span>
        <span class="c1"># Option 1: Transcribe mixed audio
</span>        <span class="n">segment</span> <span class="o">=</span> <span class="n">audio</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="n">overlap</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="n">sr</span><span class="p">):</span><span class="nf">int</span><span class="p">(</span><span class="n">overlap</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="n">sr</span><span class="p">)]</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">asr_model</span><span class="p">.</span><span class="nf">transcribe</span><span class="p">(</span><span class="n">segment</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">overlap</span><span class="sh">'</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">speakers</span><span class="sh">'</span><span class="p">:</span> <span class="n">overlap</span><span class="p">[</span><span class="sh">'</span><span class="s">speakers</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">:</span> <span class="n">overlap</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">:</span> <span class="n">overlap</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">confidence</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">low</span><span class="sh">'</span>  <span class="c1"># Mark as uncertain
</span>        <span class="p">}</span>
</code></pre></div></div>

<h3 id="approach-2-multi-channel-source-separation">Approach 2: Multi-Channel Source Separation</h3>

<p><strong>If you have multiple microphones</strong>, you can separate speakers!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiChannelSeparation</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Use multiple microphones to separate speakers
    
    Requires: Multiple audio channels (e.g., mic array)
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Use beamforming or deep learning separation
</span>        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">separate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">multi_channel_audio</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Separate speakers using spatial information
        
        Args:
            multi_channel_audio: (channels, samples)
        
        Returns:
            separated_sources: List of (speaker_audio, speaker_id)
        </span><span class="sh">"""</span>
        <span class="c1"># Advanced: Use Conv-TasNet or similar (from Day 11)
</span>        <span class="c1"># Here we'll use simple beamforming
</span>        
        <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">signal</span>
        
        <span class="c1"># Beamforming toward each speaker
</span>        <span class="c1"># (Simplified - real implementation is complex)
</span>        
        <span class="c1"># For now, just return multi-channel as-is
</span>        <span class="c1"># In production, use libraries like:
</span>        <span class="c1"># - pyroomacoustics (beamforming)
</span>        <span class="c1"># - asteroid (deep learning separation)
</span>        
        <span class="k">return</span> <span class="n">multi_channel_audio</span>
</code></pre></div></div>

<hr />

<h2 id="real-time-streaming">Real-Time Streaming</h2>

<p><strong>Challenge</strong>: Process live audio with low latency</p>

<h3 id="streaming-architecture">Streaming Architecture</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>User's mic
    ↓
[Capture] → [Buffer] → [VAD] → [Diarization] → [ASR] → [Display]
   20ms      500ms     50ms       100ms         100ms      10ms
                                                ↑
                                          Total: ~270ms latency
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">StreamingMultiSpeakerASR</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Real-time multi-speaker ASR
    
    Processes audio chunks as they arrive
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">chunk_duration</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">chunk_duration</span> <span class="o">=</span> <span class="n">chunk_duration</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">speaker_history</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># Track speaker embeddings over time
</span>        
        <span class="c1"># Models
</span>        <span class="kn">import</span> <span class="n">whisper</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vad</span> <span class="o">=</span> <span class="nc">VoiceActivityDetector</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_extractor</span> <span class="o">=</span> <span class="nc">SpeakerEmbeddingExtractor</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">asr_model</span> <span class="o">=</span> <span class="n">whisper</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span><span class="sh">"</span><span class="s">tiny</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># Faster for real-time
</span>    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">process_stream</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio_stream</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Process audio stream in real-time
        
        Args:
            audio_stream: Async iterator yielding audio chunks
        </span><span class="sh">"""</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">audio_stream</span><span class="p">:</span>
            <span class="c1"># Add to buffer
</span>            <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
            
            <span class="c1"># Process if buffer large enough
</span>            <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nf">int</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">chunk_duration</span> <span class="o">*</span> <span class="mi">16000</span><span class="p">):</span>
                <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">_process_chunk</span><span class="p">()</span>
                
                <span class="k">if</span> <span class="n">result</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="n">result</span>
    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">_process_chunk</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Process buffered audio chunk</span><span class="sh">"""</span>
        <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
        
        <span class="c1"># Get chunk
</span>        <span class="n">chunk</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">[:</span><span class="nf">int</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">chunk_duration</span> <span class="o">*</span> <span class="mi">16000</span><span class="p">)])</span>
        
        <span class="c1"># Remove from buffer (with overlap for continuity)
</span>        <span class="n">overlap_samples</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mf">0.1</span> <span class="o">*</span> <span class="mi">16000</span><span class="p">)</span>  <span class="c1"># 100ms overlap
</span>        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span> <span class="o">-</span> <span class="n">overlap_samples</span><span class="p">:]</span>
        
        <span class="c1"># VAD
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="nf">_is_speech</span><span class="p">(</span><span class="n">chunk</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">None</span>
        
        <span class="c1"># Extract embedding
</span>        <span class="n">embedding</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding_extractor</span><span class="p">.</span><span class="nf">extract</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>
        
        <span class="c1"># Identify speaker (match with history)
</span>        <span class="n">speaker_id</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_identify_speaker</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
        
        <span class="c1"># Transcribe (async to not block)
</span>        <span class="kn">import</span> <span class="n">asyncio</span>
        <span class="n">text</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">to_thread</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">asr_model</span><span class="p">.</span><span class="n">transcribe</span><span class="p">,</span> <span class="n">chunk</span><span class="p">,</span> <span class="n">fp16</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">speaker</span><span class="sh">'</span><span class="p">:</span> <span class="n">speaker_id</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="n">text</span><span class="p">[</span><span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">timestamp</span><span class="sh">'</span><span class="p">:</span> <span class="nf">__import__</span><span class="p">(</span><span class="sh">'</span><span class="s">time</span><span class="sh">'</span><span class="p">).</span><span class="nf">time</span><span class="p">()</span>
        <span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">_is_speech</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">chunk</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Quick speech check</span><span class="sh">"""</span>
        <span class="n">energy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">chunk</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">energy</span> <span class="o">&gt;</span> <span class="mf">0.01</span>
    
    <span class="k">def</span> <span class="nf">_identify_speaker</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embedding</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Match embedding to known speakers
        
        If new speaker, assign new ID
        </span><span class="sh">"""</span>
        <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
        
        <span class="c1"># Compare with known speakers
</span>        <span class="n">best_match</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">best_similarity</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        
        <span class="k">for</span> <span class="n">speaker_id</span><span class="p">,</span> <span class="n">known_embedding</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">speaker_history</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="c1"># Cosine similarity
</span>            <span class="n">similarity</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">known_embedding</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
                <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">known_embedding</span><span class="p">)</span>
            <span class="p">)</span>
            
            <span class="k">if</span> <span class="n">similarity</span> <span class="o">&gt;</span> <span class="n">best_similarity</span><span class="p">:</span>
                <span class="n">best_similarity</span> <span class="o">=</span> <span class="n">similarity</span>
                <span class="n">best_match</span> <span class="o">=</span> <span class="n">speaker_id</span>
        
        <span class="c1"># Threshold for same speaker
</span>        <span class="k">if</span> <span class="n">best_similarity</span> <span class="o">&gt;</span> <span class="mf">0.75</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">best_match</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># New speaker
</span>            <span class="n">new_id</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">SPEAKER_</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">speaker_history</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span>
            <span class="n">self</span><span class="p">.</span><span class="n">speaker_history</span><span class="p">[</span><span class="n">new_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>
            <span class="k">return</span> <span class="n">new_id</span>

<span class="c1"># Usage with WebSocket
</span><span class="kn">import</span> <span class="n">asyncio</span>
<span class="kn">import</span> <span class="n">websockets</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">handle_client</span><span class="p">(</span><span class="n">websocket</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle incoming audio stream from client</span><span class="sh">"""</span>
    <span class="n">asr</span> <span class="o">=</span> <span class="nc">StreamingMultiSpeakerASR</span><span class="p">()</span>
    
    <span class="k">async</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">asr</span><span class="p">.</span><span class="nf">process_stream</span><span class="p">(</span><span class="n">websocket</span><span class="p">):</span>
        <span class="c1"># Send transcription back to client
</span>        <span class="k">await</span> <span class="n">websocket</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>

<span class="c1"># Start server
</span><span class="n">start_server</span> <span class="o">=</span> <span class="n">websockets</span><span class="p">.</span><span class="nf">serve</span><span class="p">(</span><span class="n">handle_client</span><span class="p">,</span> <span class="sh">"</span><span class="s">localhost</span><span class="sh">"</span><span class="p">,</span> <span class="mi">8765</span><span class="p">)</span>
<span class="n">asyncio</span><span class="p">.</span><span class="nf">get_event_loop</span><span class="p">().</span><span class="nf">run_until_complete</span><span class="p">(</span><span class="n">start_server</span><span class="p">)</span>
<span class="n">asyncio</span><span class="p">.</span><span class="nf">get_event_loop</span><span class="p">().</span><span class="nf">run_forever</span><span class="p">()</span>
</code></pre></div></div>

<hr />

<h2 id="common-failure-modes--debugging">Common Failure Modes &amp; Debugging</h2>

<h3 id="failure-mode-1-speaker-confusion">Failure Mode 1: Speaker Confusion</h3>

<p><strong>Symptom</strong>: System assigns same utterance to multiple speakers or switches mid-sentence</p>

<p><strong>Example:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Ground truth:
  [Alice, 0-5s]: "Hello, how are you today?"

System output (WRONG):
  [Alice, 0-2s]: "Hello, how"
  [Bob, 2-5s]: "are you today?"
</code></pre></div></div>

<p><strong>Root causes:</strong></p>

<ol>
  <li><strong>Insufficient speech for embedding</strong>
    <ul>
      <li>Embeddings need 2-3 seconds minimum</li>
      <li>Short utterances (&lt;1s) have unreliable embeddings</li>
    </ul>
  </li>
  <li><strong>Similar voices</strong>
    <ul>
      <li>Two speakers with similar pitch/timbre</li>
      <li>System can’t distinguish</li>
    </ul>
  </li>
  <li><strong>Poor audio quality</strong>
    <ul>
      <li>Background noise corrupts embeddings</li>
      <li>Low SNR (&lt;10dB) confuses system</li>
    </ul>
  </li>
</ol>

<p><strong>Solutions:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RobustSpeakerIdentification</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Handle edge cases in speaker identification
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">min_segment_duration</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">min_segment_duration</span> <span class="o">=</span> <span class="n">min_segment_duration</span>
        <span class="n">self</span><span class="p">.</span><span class="n">speaker_history</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Track recent speakers
</span>    
    <span class="k">def</span> <span class="nf">identify_with_context</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio_segment</span><span class="p">,</span> <span class="n">duration</span><span class="p">,</span> <span class="n">prev_speaker</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Identify speaker with contextual hints
        
        Args:
            audio_segment: Audio to identify
            duration: Segment duration in seconds
            prev_speaker: Who spoke last (context)
        
        Returns:
            speaker_id, confidence
        </span><span class="sh">"""</span>
        <span class="c1"># Check 1: Is segment long enough?
</span>        <span class="k">if</span> <span class="n">duration</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">min_segment_duration</span><span class="p">:</span>
            <span class="c1"># Too short for reliable embedding
</span>            <span class="c1"># Use speaker from previous segment (continuity assumption)
</span>            <span class="k">if</span> <span class="n">prev_speaker</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">prev_speaker</span><span class="p">,</span> <span class="mf">0.5</span>  <span class="c1"># Low confidence
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="sh">"</span><span class="s">UNKNOWN</span><span class="sh">"</span><span class="p">,</span> <span class="mf">0.0</span>
        
        <span class="c1"># Check 2: Extract embedding
</span>        <span class="n">embedding</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">extract_embedding</span><span class="p">(</span><span class="n">audio_segment</span><span class="p">)</span>
        
        <span class="c1"># Check 3: Identify with threshold
</span>        <span class="n">speaker</span><span class="p">,</span> <span class="n">similarity</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">identify_speaker</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
        
        <span class="c1"># Check 4: Apply contextual prior
</span>        <span class="k">if</span> <span class="n">prev_speaker</span> <span class="ow">and</span> <span class="n">similarity</span> <span class="o">&lt;</span> <span class="mf">0.75</span><span class="p">:</span>
            <span class="c1"># Ambiguous - bias toward previous speaker (people usually finish sentences)
</span>            <span class="k">return</span> <span class="n">prev_speaker</span><span class="p">,</span> <span class="mf">0.6</span>
        
        <span class="k">return</span> <span class="n">speaker</span><span class="p">,</span> <span class="n">similarity</span>
</code></pre></div></div>

<h3 id="failure-mode-2-overlap-mis-attribution">Failure Mode 2: Overlap Mis-attribution</h3>

<p><strong>Symptom</strong>: During overlaps, words from Speaker A attributed to Speaker B</p>

<p><strong>Example:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Ground truth:
  [Alice, 0-3s]: "I think we should consider this option"
  [Bob, 2-4s]: "Wait, what about the other approach?"

System output (WRONG):
  [Alice, 0-2s]: "I think we should"
  [Bob, 2-4s]: "consider this option Wait, what about the other approach?"
                ↑ These words are Alice's, not Bob's!
</code></pre></div></div>

<p><strong>Root cause</strong>: Diarization boundaries don’t align with actual speaker turns</p>

<p><strong>Solution</strong>: Post-processing refinement</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">OverlapRefiner</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Refine transcriptions in overlap regions
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">refine_overlaps</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">segments</span><span class="p">,</span> <span class="n">asr_results</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Use ASR confidence to refine overlap boundaries
        
        Idea: Low-confidence words might be from the other speaker
        </span><span class="sh">"""</span>
        <span class="n">refined</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">seg</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">segments</span><span class="p">,</span> <span class="n">asr_results</span><span class="p">)):</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="sh">'</span><span class="s">words</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># Word-level timestamps + confidence
</span>            
            <span class="c1"># Check if next segment overlaps
</span>            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">segments</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">next_seg</span> <span class="o">=</span> <span class="n">segments</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
                
                <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="nf">_is_overlapping</span><span class="p">(</span><span class="n">seg</span><span class="p">,</span> <span class="n">next_seg</span><span class="p">):</span>
                    <span class="c1"># Refine boundary based on word confidence
</span>                    <span class="n">words</span><span class="p">,</span> <span class="n">next_words</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_split_by_confidence</span><span class="p">(</span>
                        <span class="n">words</span><span class="p">,</span> <span class="n">seg</span><span class="p">,</span> <span class="n">next_seg</span>
                    <span class="p">)</span>
            
            <span class="n">refined</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
                <span class="sh">'</span><span class="s">speaker</span><span class="sh">'</span><span class="p">:</span> <span class="n">seg</span><span class="p">[</span><span class="sh">'</span><span class="s">speaker</span><span class="sh">'</span><span class="p">],</span>
                <span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">:</span> <span class="n">seg</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">],</span>
                <span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">:</span> <span class="n">seg</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">],</span>
                <span class="sh">'</span><span class="s">words</span><span class="sh">'</span><span class="p">:</span> <span class="n">words</span>
            <span class="p">})</span>
        
        <span class="k">return</span> <span class="n">refined</span>
    
    <span class="k">def</span> <span class="nf">_split_by_confidence</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span> <span class="n">seg1</span><span class="p">,</span> <span class="n">seg2</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Split words between two overlapping segments
        
        High-confidence words stay, low-confidence might belong to other speaker
        </span><span class="sh">"""</span>
        <span class="n">overlap_start</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">seg1</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">],</span> <span class="n">seg2</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">overlap_end</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">seg1</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">],</span> <span class="n">seg2</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">])</span>
        
        <span class="n">seg1_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">seg2_words</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="c1"># Check if word is in overlap region
</span>            <span class="k">if</span> <span class="n">overlap_start</span> <span class="o">&lt;=</span> <span class="n">word</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">overlap_end</span><span class="p">:</span>
                <span class="c1"># In overlap - check confidence
</span>                <span class="k">if</span> <span class="n">word</span><span class="p">[</span><span class="sh">'</span><span class="s">confidence</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.8</span><span class="p">:</span>
                    <span class="n">seg1_words</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>  <span class="c1"># Keep in current segment
</span>                <span class="k">else</span><span class="p">:</span>
                    <span class="n">seg2_words</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>  <span class="c1"># Might belong to other speaker
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="n">seg1_words</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">seg1_words</span><span class="p">,</span> <span class="n">seg2_words</span>
</code></pre></div></div>

<h3 id="failure-mode-3-far-field-audio-degradation">Failure Mode 3: Far-Field Audio Degradation</h3>

<p><strong>Symptom</strong>: Accuracy drops significantly when speaker is far from microphone</p>

<p><strong>Example metrics:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Near-field (&lt; 1m from mic):
  WER: 5%
  Diarization accuracy: 95%

Far-field (&gt; 3m from mic):
  WER: 25% ← 5x worse!
  Diarization accuracy: 70%
</code></pre></div></div>

<p><strong>Root cause</strong>:</p>
<ul>
  <li>Lower SNR (signal-to-noise ratio)</li>
  <li>More reverberation</li>
  <li>Acoustic reflections</li>
</ul>

<p><strong>Solutions:</strong></p>

<ol>
  <li><strong>Beamforming</strong> (if mic array available)</li>
  <li><strong>Speech enhancement</strong> pre-processing</li>
  <li><strong>Specialized far-field models</strong></li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FarFieldPreprocessor</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Enhance far-field audio before ASR
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">enhance</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">,</span> <span class="n">sample_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Apply far-field enhancements
        
        1. Dereverb (reduce echo)
        2. Denoise
        3. Equalize (boost high frequencies)
        </span><span class="sh">"""</span>
        <span class="c1"># Step 1: Dereverberation (WPE algorithm)
</span>        <span class="n">enhanced</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_dereverb_wpe</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">sample_rate</span><span class="p">)</span>
        
        <span class="c1"># Step 2: Noise reduction (spectral subtraction)
</span>        <span class="n">enhanced</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_denoise</span><span class="p">(</span><span class="n">enhanced</span><span class="p">,</span> <span class="n">sample_rate</span><span class="p">)</span>
        
        <span class="c1"># Step 3: Equalization (boost consonants)
</span>        <span class="n">enhanced</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_equalize</span><span class="p">(</span><span class="n">enhanced</span><span class="p">,</span> <span class="n">sample_rate</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">enhanced</span>
    
    <span class="k">def</span> <span class="nf">_dereverb_wpe</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">,</span> <span class="n">sr</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Weighted Prediction Error (WPE) dereverberation
        
        Removes room echo/reverberation
        </span><span class="sh">"""</span>
        <span class="c1"># Simplified - use library like `nara_wpe` in production
</span>        <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">signal</span>
        
        <span class="c1"># High-pass filter to remove low-freq rumble
</span>        <span class="n">sos</span> <span class="o">=</span> <span class="n">signal</span><span class="p">.</span><span class="nf">butter</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="sh">'</span><span class="s">highpass</span><span class="sh">'</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="sh">'</span><span class="s">sos</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">filtered</span> <span class="o">=</span> <span class="n">signal</span><span class="p">.</span><span class="nf">sosfilt</span><span class="p">(</span><span class="n">sos</span><span class="p">,</span> <span class="n">audio</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">filtered</span>
    
    <span class="k">def</span> <span class="nf">_denoise</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">,</span> <span class="n">sr</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Spectral subtraction noise reduction
        </span><span class="sh">"""</span>
        <span class="kn">import</span> <span class="n">noisereduce</span> <span class="k">as</span> <span class="n">nr</span>
        
        <span class="c1"># Estimate noise from first 0.5s (assuming silence/noise)
</span>        <span class="n">reduced</span> <span class="o">=</span> <span class="n">nr</span><span class="p">.</span><span class="nf">reduce_noise</span><span class="p">(</span>
            <span class="n">y</span><span class="o">=</span><span class="n">audio</span><span class="p">,</span>
            <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span>
            <span class="n">stationary</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">prop_decrease</span><span class="o">=</span><span class="mf">0.8</span>
        <span class="p">)</span>
        
        <span class="k">return</span> <span class="n">reduced</span>
    
    <span class="k">def</span> <span class="nf">_equalize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">,</span> <span class="n">sr</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Boost high frequencies (consonants)
        
        Far-field audio loses high-freq content
        </span><span class="sh">"""</span>
        <span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">signal</span>
        
        <span class="c1"># Boost 2-8kHz (consonant region)
</span>        <span class="n">sos</span> <span class="o">=</span> <span class="n">signal</span><span class="p">.</span><span class="nf">butter</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">8000</span><span class="p">],</span> <span class="sh">'</span><span class="s">bandpass</span><span class="sh">'</span><span class="p">,</span> <span class="n">fs</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="sh">'</span><span class="s">sos</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">boosted</span> <span class="o">=</span> <span class="n">signal</span><span class="p">.</span><span class="nf">sosfilt</span><span class="p">(</span><span class="n">sos</span><span class="p">,</span> <span class="n">audio</span><span class="p">)</span>
        
        <span class="c1"># Mix with original (50-50)
</span>        <span class="n">enhanced</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">audio</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">boosted</span>
        
        <span class="k">return</span> <span class="n">enhanced</span>
</code></pre></div></div>

<h3 id="debugging-tools">Debugging Tools</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiSpeakerASRDebugger</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Tools for debugging multi-speaker ASR issues
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">visualize_diarization</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">segments</span><span class="p">,</span> <span class="n">audio_duration</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Visual timeline of speakers
        
        Helps spot issues like:
        - Too many speaker switches
        - Missing speakers
        - Wrong boundaries
        </span><span class="sh">"""</span>
        <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
        <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
        
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        
        <span class="c1"># Plot each segment
</span>        <span class="k">for</span> <span class="n">seg</span> <span class="ow">in</span> <span class="n">segments</span><span class="p">:</span>
            <span class="n">speaker_id</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">seg</span><span class="p">[</span><span class="sh">'</span><span class="s">speaker</span><span class="sh">'</span><span class="p">].</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">_</span><span class="sh">'</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">color</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="nf">tab10</span><span class="p">(</span><span class="n">speaker_id</span><span class="p">)</span>
            
            <span class="n">ax</span><span class="p">.</span><span class="nf">barh</span><span class="p">(</span>
                <span class="n">y</span><span class="o">=</span><span class="n">speaker_id</span><span class="p">,</span>
                <span class="n">width</span><span class="o">=</span><span class="n">seg</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">]</span> <span class="o">-</span> <span class="n">seg</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">],</span>
                <span class="n">left</span><span class="o">=</span><span class="n">seg</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">],</span>
                <span class="n">height</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
                <span class="n">label</span><span class="o">=</span><span class="n">seg</span><span class="p">[</span><span class="sh">'</span><span class="s">speaker</span><span class="sh">'</span><span class="p">]</span>
            <span class="p">)</span>
        
        <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Time (seconds)</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Speaker</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Speaker Diarization Timeline</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">audio_duration</span><span class="p">)</span>
        
        <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">diarization_debug.png</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">✓ Saved visualization to diarization_debug.png</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">predicted_segments</span><span class="p">,</span> <span class="n">ground_truth_segments</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Compute diarization metrics
        
        DER (Diarization Error Rate) = 
          (False Alarm + Miss + Speaker Confusion) / Total
        </span><span class="sh">"""</span>
        <span class="kn">from</span> <span class="n">pyannote.metrics.diarization</span> <span class="kn">import</span> <span class="n">DiarizationErrorRate</span>
        
        <span class="n">der</span> <span class="o">=</span> <span class="nc">DiarizationErrorRate</span><span class="p">()</span>
        
        <span class="c1"># Convert to pyannote format
</span>        <span class="n">pred_annotation</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_to_annotation</span><span class="p">(</span><span class="n">predicted_segments</span><span class="p">)</span>
        <span class="n">gt_annotation</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_to_annotation</span><span class="p">(</span><span class="n">ground_truth_segments</span><span class="p">)</span>
        
        <span class="c1"># Compute DER
</span>        <span class="n">error_rate</span> <span class="o">=</span> <span class="nf">der</span><span class="p">(</span><span class="n">gt_annotation</span><span class="p">,</span> <span class="n">pred_annotation</span><span class="p">)</span>
        
        <span class="c1"># Detailed breakdown
</span>        <span class="n">details</span> <span class="o">=</span> <span class="n">der</span><span class="p">.</span><span class="nf">components</span><span class="p">(</span><span class="n">gt_annotation</span><span class="p">,</span> <span class="n">pred_annotation</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">DER</span><span class="sh">'</span><span class="p">:</span> <span class="n">error_rate</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">false_alarm</span><span class="sh">'</span><span class="p">:</span> <span class="n">details</span><span class="p">[</span><span class="sh">'</span><span class="s">false alarm</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">missed_detection</span><span class="sh">'</span><span class="p">:</span> <span class="n">details</span><span class="p">[</span><span class="sh">'</span><span class="s">missed detection</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">speaker_confusion</span><span class="sh">'</span><span class="p">:</span> <span class="n">details</span><span class="p">[</span><span class="sh">'</span><span class="s">confusion</span><span class="sh">'</span><span class="p">]</span>
        <span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">_to_annotation</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">segments</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Convert segments to pyannote Annotation format</span><span class="sh">"""</span>
        <span class="kn">from</span> <span class="n">pyannote.core</span> <span class="kn">import</span> <span class="n">Annotation</span><span class="p">,</span> <span class="n">Segment</span>
        
        <span class="n">annotation</span> <span class="o">=</span> <span class="nc">Annotation</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">seg</span> <span class="ow">in</span> <span class="n">segments</span><span class="p">:</span>
            <span class="n">annotation</span><span class="p">[</span><span class="nc">Segment</span><span class="p">(</span><span class="n">seg</span><span class="p">[</span><span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">],</span> <span class="n">seg</span><span class="p">[</span><span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">])]</span> <span class="o">=</span> <span class="n">seg</span><span class="p">[</span><span class="sh">'</span><span class="s">speaker</span><span class="sh">'</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">annotation</span>
</code></pre></div></div>

<hr />

<h2 id="production-considerations">Production Considerations</h2>

<h3 id="1-latency-optimization">1. Latency Optimization</h3>

<p><strong>Target</strong>: &lt; 300ms end-to-end for real-time feel</p>

<p><strong>Breakdown:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Audio capture: 20ms
Buffering: 100ms
VAD: 10ms
Embedding: 50ms
ASR: 100ms
Network: 20ms
Total: 300ms
</code></pre></div></div>

<p><strong>Optimizations:</strong></p>
<ul>
  <li>Use smaller ASR models (Whisper tiny/base)</li>
  <li>Batch embedding extraction</li>
  <li>Pre-compute speaker profiles</li>
  <li>GPU acceleration</li>
  <li>Reduce network round-trips</li>
</ul>

<h3 id="2-accuracy-vs-speed-trade-off">2. Accuracy vs Speed Trade-off</h3>

<table>
  <thead>
    <tr>
      <th>Model Size</th>
      <th>Latency</th>
      <th>WER</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Whisper tiny</td>
      <td>50ms</td>
      <td>10%</td>
      <td>Live captions</td>
    </tr>
    <tr>
      <td>Whisper base</td>
      <td>100ms</td>
      <td>7%</td>
      <td>Meetings</td>
    </tr>
    <tr>
      <td>Whisper medium</td>
      <td>300ms</td>
      <td>5%</td>
      <td>Post-processing</td>
    </tr>
    <tr>
      <td>Whisper large</td>
      <td>1000ms</td>
      <td>3%</td>
      <td>Archival transcription</td>
    </tr>
  </tbody>
</table>

<h3 id="3-speaker-persistence">3. Speaker Persistence</h3>

<p><strong>Challenge</strong>: Same speaker should have consistent ID across session</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SpeakerRegistry</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Maintain consistent speaker IDs
    
    Matches new embeddings to registered speakers
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">similarity_threshold</span><span class="o">=</span><span class="mf">0.75</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">speakers</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># id -&gt; mean embedding
</span>        <span class="n">self</span><span class="p">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">similarity_threshold</span>
    
    <span class="k">def</span> <span class="nf">register_or_identify</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">embedding</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Register new speaker or identify existing
        </span><span class="sh">"""</span>
        <span class="c1"># Check against known speakers
</span>        <span class="k">for</span> <span class="n">speaker_id</span><span class="p">,</span> <span class="n">known_emb</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">speakers</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">similarity</span> <span class="o">=</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">known_emb</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">similarity</span> <span class="o">&gt;</span> <span class="n">self</span><span class="p">.</span><span class="n">threshold</span><span class="p">:</span>
                <span class="c1"># Update running average
</span>                <span class="n">self</span><span class="p">.</span><span class="n">speakers</span><span class="p">[</span><span class="n">speaker_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="mf">0.9</span> <span class="o">*</span> <span class="n">known_emb</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">embedding</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="n">speaker_id</span>
        
        <span class="c1"># New speaker
</span>        <span class="n">new_id</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">SPEAKER_</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">speakers</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="sh">"</span>
        <span class="n">self</span><span class="p">.</span><span class="n">speakers</span><span class="p">[</span><span class="n">new_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding</span>
        <span class="k">return</span> <span class="n">new_id</span>
</code></pre></div></div>

<h3 id="4-monitoring--debugging">4. Monitoring &amp; Debugging</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MultiSpeakerASRMetrics</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Track system performance
    </span><span class="sh">"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">latency_ms</span><span class="sh">'</span><span class="p">:</span> <span class="p">[],</span>
            <span class="sh">'</span><span class="s">overlap_ratio</span><span class="sh">'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">speaker_switches_per_minute</span><span class="sh">'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">wer_per_speaker</span><span class="sh">'</span><span class="p">:</span> <span class="p">{}</span>
        <span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">log_latency</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">latency_ms</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">metrics</span><span class="p">[</span><span class="sh">'</span><span class="s">latency_ms</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">latency_ms</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">report</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
        
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">p50_latency_ms</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">median</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">metrics</span><span class="p">[</span><span class="sh">'</span><span class="s">latency_ms</span><span class="sh">'</span><span class="p">]),</span>
            <span class="sh">'</span><span class="s">p95_latency_ms</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">metrics</span><span class="p">[</span><span class="sh">'</span><span class="s">latency_ms</span><span class="sh">'</span><span class="p">],</span> <span class="mi">95</span><span class="p">),</span>
            <span class="sh">'</span><span class="s">overlap_ratio</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">metrics</span><span class="p">[</span><span class="sh">'</span><span class="s">overlap_ratio</span><span class="sh">'</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">speaker_switches_per_minute</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">metrics</span><span class="p">[</span><span class="sh">'</span><span class="s">speaker_switches_per_minute</span><span class="sh">'</span><span class="p">]</span>
        <span class="p">}</span>
</code></pre></div></div>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Multi-speaker ASR</strong> = VAD + Diarization + ASR<br />
✅ <strong>Speaker embeddings</strong> capture voice identity<br />
✅ <strong>Clustering</strong> groups segments by speaker<br />
✅ <strong>Overlaps</strong> are hard - detect and handle gracefully<br />
✅ <strong>Real-time</strong> requires careful latency optimization<br />
✅ <strong>State-of-the-art</strong>: Use <code class="language-plaintext highlighter-rouge">pyannote.audio</code> + Whisper</p>

<p><strong>Production tips:</strong></p>
<ul>
  <li>Start with <code class="language-plaintext highlighter-rouge">pyannote</code> + <code class="language-plaintext highlighter-rouge">whisper</code> (best quality)</li>
  <li>Optimize latency with smaller models if needed</li>
  <li>Handle overlaps explicitly (mark in transcript)</li>
  <li>Maintain speaker consistency across session</li>
  <li>Monitor latency and accuracy per speaker</li>
</ul>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/">arunbaby.com/speech-tech/0012-multi-speaker-asr</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#multi-speaker" class="page__taxonomy-item p-category" rel="tag">multi-speaker</a><span class="sep">, </span>
    
      <a href="/tags/#overlapping-speech" class="page__taxonomy-item p-category" rel="tag">overlapping-speech</a><span class="sep">, </span>
    
      <a href="/tags/#real-time" class="page__taxonomy-item p-category" rel="tag">real-time</a><span class="sep">, </span>
    
      <a href="/tags/#speaker-diarization" class="page__taxonomy-item p-category" rel="tag">speaker-diarization</a><span class="sep">, </span>
    
      <a href="/tags/#streaming" class="page__taxonomy-item p-category" rel="tag">streaming</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0012-add-two-numbers/" rel="permalink">Add Two Numbers
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master digit-by-digit addition with linked lists: Handle carry propagation elegantly. Classic problem teaching pointer manipulation and edge cases.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0012-distributed-systems/" rel="permalink">Distributed ML Systems
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          25 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Design distributed ML systems that scale to billions of predictions: Master replication, sharding, consensus, and fault tolerance for production ML.
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Multi-Speaker+ASR%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0012-multi-speaker-asr%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0012-multi-speaker-asr%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0011-speech-separation/" class="pagination--pager" title="Speech Separation">Previous</a>
    
    
      <a href="/speech-tech/0013-compute-allocation-for-speech-models/" class="pagination--pager" title="Compute Allocation for Speech Models">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
