<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Multi-Speaker ASR - Arun Baby</title>
<meta name="description" content="Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Multi-Speaker ASR">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/">


  <meta property="og:description" content="Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Multi-Speaker ASR">
  <meta name="twitter:description" content="Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Multi-Speaker ASR">
    <meta itemprop="description" content="Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/" itemprop="url">Multi-Speaker ASR
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          25 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem-statement">Problem Statement</a><ul><li><a href="#why-is-this-hard">Why Is This Hard?</a></li><li><a href="#real-world-use-cases">Real-World Use Cases</a></li></ul></li><li><a href="#understanding-multi-speaker-asr">Understanding Multi-Speaker ASR</a><ul><li><a href="#the-full-system-pipeline">The Full System Pipeline</a></li><li><a href="#why-this-pipeline">Why This Pipeline?</a></li><li><a href="#the-mathematics-behind-speaker-embeddings">The Mathematics Behind Speaker Embeddings</a></li><li><a href="#comparison-different-diarization-approaches">Comparison: Different Diarization Approaches</a></li></ul></li><li><a href="#component-1-voice-activity-detection-vad">Component 1: Voice Activity Detection (VAD)</a></li><li><a href="#component-2-speaker-diarization">Component 2: Speaker Diarization</a><ul><li><a href="#speaker-embeddings">Speaker Embeddings</a></li><li><a href="#clustering-embeddings">Clustering Embeddings</a></li></ul></li><li><a href="#component-3-asr-per-speaker">Component 3: ASR Per Speaker</a></li><li><a href="#handling-overlapping-speech">Handling Overlapping Speech</a><ul><li><a href="#challenge">Challenge</a></li><li><a href="#approach-1-overlap-detection--best-effort">Approach 1: Overlap Detection + Best Effort</a></li><li><a href="#approach-2-multi-channel-source-separation">Approach 2: Multi-Channel Source Separation</a></li></ul></li><li><a href="#real-time-streaming">Real-Time Streaming</a><ul><li><a href="#streaming-architecture">Streaming Architecture</a></li></ul></li><li><a href="#common-failure-modes--debugging">Common Failure Modes &amp; Debugging</a><ul><li><a href="#failure-mode-1-speaker-confusion">Failure Mode 1: Speaker Confusion</a></li><li><a href="#failure-mode-2-overlap-mis-attribution">Failure Mode 2: Overlap Mis-attribution</a></li><li><a href="#failure-mode-3-far-field-audio-degradation">Failure Mode 3: Far-Field Audio Degradation</a></li><li><a href="#debugging-tools">Debugging Tools</a></li></ul></li><li><a href="#production-considerations">Production Considerations</a><ul><li><a href="#1-latency-optimization">1. Latency Optimization</a></li><li><a href="#2-accuracy-vs-speed-trade-off">2. Accuracy vs Speed Trade-off</a></li><li><a href="#3-speaker-persistence">3. Speaker Persistence</a></li><li><a href="#4-monitoring--debugging">4. Monitoring &amp; Debugging</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p>Design a <strong>multi-speaker ASR system</strong> that can:</p>

<ol>
  <li><strong>Recognize speech</strong> from multiple speakers in a conversation</li>
  <li><strong>Identify who spoke</strong> each word/sentence (speaker diarization)</li>
  <li><strong>Handle overlapping speech</strong> when multiple people speak simultaneously</li>
  <li><strong>Work in real-time</strong> with &lt; 300ms latency for live transcription</li>
  <li><strong>Scale</strong> to meetings with 2-10 speakers</li>
</ol>

<h3 id="why-is-this-hard">Why Is This Hard?</h3>

<p><strong>Single-speaker ASR</strong> (covered in ) assumes:</p>
<ul>
  <li>✅ One speaker at a time</li>
  <li>✅ No speaker changes mid-sentence</li>
  <li>✅ No overlapping speech</li>
</ul>

<p><strong>Real-world conversations</strong> break all these assumptions:</p>

<p>``
Time: 0s 1s 2s 3s 4s
Speaker A: “So I think we should…”
Speaker B: “Wait, can I…”
Speaker C: “Actually…”
 ↑ Overlap! ↑ ↑ Overlap! ↑</p>

<p>Single-speaker ASR would produce: “So I wait can think actually should we…”
Multi-speaker ASR must produce:
 [A, 0.0-1.5s]: “So I think we should”
 [B, 1.2-2.3s]: “Wait, can I”
 [C, 2.8-4.0s]: “Actually”
``</p>

<p><strong>The core challenges:</strong></p>

<table>
  <thead>
    <tr>
      <th>Challenge</th>
      <th>Why It’s Hard</th>
      <th>Impact</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Speaker changes</strong></td>
      <td>Voice characteristics change suddenly</td>
      <td>Acoustic model confused</td>
    </tr>
    <tr>
      <td><strong>Overlapping speech</strong></td>
      <td>Multiple audio sources mixed</td>
      <td>Can’t separate cleanly</td>
    </tr>
    <tr>
      <td><strong>Speaker identification</strong></td>
      <td>Need to know <em>who</em> said <em>what</em></td>
      <td>Requires speaker embeddings</td>
    </tr>
    <tr>
      <td><strong>Real-time processing</strong></td>
      <td>Must process while speakers still talking</td>
      <td>Latency constraints</td>
    </tr>
    <tr>
      <td><strong>Unknown # of speakers</strong></td>
      <td>Don’t know speaker count in advance</td>
      <td>Can’t pre-allocate resources</td>
    </tr>
  </tbody>
</table>

<h3 id="real-world-use-cases">Real-World Use Cases</h3>

<table>
  <thead>
    <tr>
      <th>Application</th>
      <th>Requirements</th>
      <th>Challenges</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Meeting transcription</strong> (Zoom, Teams)</td>
      <td>2-10 speakers, real-time</td>
      <td>Overlaps, background noise</td>
    </tr>
    <tr>
      <td><strong>Call center analytics</strong></td>
      <td>2 speakers (agent + customer)</td>
      <td>Quality monitoring, compliance</td>
    </tr>
    <tr>
      <td><strong>Podcast transcription</strong></td>
      <td>2-5 hosts + guests</td>
      <td>High accuracy needed</td>
    </tr>
    <tr>
      <td><strong>Courtroom transcription</strong></td>
      <td>Multiple speakers, legal record</td>
      <td>99%+ accuracy, speaker IDs</td>
    </tr>
    <tr>
      <td><strong>Medical consultations</strong></td>
      <td>Doctor + patient(s)</td>
      <td>HIPAA compliance, accuracy</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="understanding-multi-speaker-asr">Understanding Multi-Speaker ASR</h2>

<h3 id="the-full-system-pipeline">The Full System Pipeline</h3>

<p>``
┌─────────────────────────────────────────────────────────────┐
│ MULTI-SPEAKER ASR PIPELINE │
└─────────────────────────────────────────────────────────────┘</p>

<p>Step 1: AUDIO INPUT
┌────────────────────────────────────────────┐
│ Mixed audio (all speakers combined) │
│ [Speaker A + Speaker B + Speaker C + …]│
└────────────────┬───────────────────────────┘
 ▼
Step 2: VOICE ACTIVITY DETECTION (VAD)
┌────────────────────────────────────────────┐
│ Find speech regions (vs silence) │
│ Output: [(0.0s, 3.2s), (3.5s, 7.1s), …] │
└────────────────┬───────────────────────────┘
 ▼
Step 3: SPEAKER DIARIZATION
┌────────────────────────────────────────────┐
│ Cluster speech by speaker │
│ Output: [(A, 0-1.5s), (B, 1.2-2.3s), …]│
└────────────────┬───────────────────────────┘
 ▼
Step 4: ASR (per speaker segment)
┌────────────────────────────────────────────┐
│ Transcribe each speaker segment │
│ Output: [(A, “Hello”), (B, “Hi”), …] │
└────────────────┬───────────────────────────┘
 ▼
Step 5: POST-PROCESSING
┌────────────────────────────────────────────┐
│ • Merge overlaps │
│ • Add punctuation │
│ • Format output │
└────────────────────────────────────────────┘
``</p>

<p>Each step has challenges! Let’s dig into each.</p>

<h3 id="why-this-pipeline">Why This Pipeline?</h3>

<p><strong>Why not just run ASR on everything?</strong></p>

<p>Imagine you have a 1-hour meeting:</p>
<ul>
  <li>Raw audio: 1 hour</li>
  <li>Actual speech: ~30 minutes (50% silence/pauses)</li>
  <li>Running ASR on silence: <strong>Waste of 30 minutes compute!</strong></li>
</ul>

<p><strong>Why not run ASR first, then diarize?</strong></p>

<p>``
Option A: ASR → Diarization (BAD)
Problem: ASR produces one continuous text blob
“Hello there hi how are you fine thanks”
↑ Can’t tell where speakers change!</p>

<p>Option B: Diarization → ASR (GOOD)
Step 1: Find speaker segments
 [A: 0-2s], [B: 2-4s], [A: 4-6s]
Step 2: Transcribe each segment separately
 A: “Hello there”
 B: “Hi, how are you?”
 A: “Fine, thanks”
↑ Clean separation!
``</p>

<p><strong>Why separate VAD from diarization?</strong></p>

<ul>
  <li><strong>VAD</strong> is fast (simple energy-based or small model)</li>
  <li><strong>Diarization</strong> is slow (needs embeddings + clustering)</li>
  <li>Don’t waste diarization compute on silence!</li>
</ul>

<p><strong>Pipeline efficiency:</strong></p>

<p><code class="language-plaintext highlighter-rouge">
1 hour audio
 ↓ VAD (fast, eliminates silence)
30 min speech segments
 ↓ Diarization (slow, but only on speech)
30 min speaker-labeled segments
 ↓ ASR (slowest, but parallelizable)
Transcriptions
</code></p>

<h3 id="the-mathematics-behind-speaker-embeddings">The Mathematics Behind Speaker Embeddings</h3>

<p><strong>Key question</strong>: How do we represent a voice mathematically?</p>

<p><strong>Answer</strong>: Deep learning learns to compress voice characteristics into a fixed-size vector.</p>

<p><strong>Training process</strong> (simplified):</p>

<p>``
Step 1: Collect data
 Speaker 1: 100 utterances
 Speaker 2: 100 utterances
 …
 Speaker 10,000: 100 utterances</p>

<p>Step 2: Train neural network
 Input: Audio waveform or spectrogram
 Output: 512-dimensional embedding</p>

<p>Goal: Minimize distance between embeddings of same speaker,
 maximize distance between different speakers</p>

<p>Step 3: Loss function (Triplet Loss)
 Anchor: Speaker A, utterance 1
 Positive: Speaker A, utterance 2 (same speaker)
 Negative: Speaker B, utterance 1 (different speaker)</p>

<p>Loss = max(0, distance(anchor, positive) - distance(anchor, negative) + margin)</p>

<p>This forces:</p>
<ul>
  <li>distance(A_utt1, A_utt2) &lt; distance(A_utt1, B_utt1)
``</li>
</ul>

<p><strong>Visual intuition:</strong></p>

<p>``
Before training (random embeddings):
Speaker A utterances: scattered everywhere
Speaker B utterances: scattered everywhere
No clustering!</p>

<p>After training:
Speaker A utterances: tight cluster in embedding space
Speaker B utterances: different tight cluster, far from A
Clear separation!
``</p>

<p><strong>Why 512 dimensions?</strong></p>

<ul>
  <li><strong>Lower (e.g., 64)</strong>: Not enough capacity to capture all voice variations</li>
  <li><strong>Higher (e.g., 2048)</strong>: Overfitting, slow, unnecessary</li>
  <li><strong>512</strong>: Sweet spot (empirically found by researchers)</li>
</ul>

<p><strong>What does the embedding capture?</strong></p>

<ul>
  <li>Pitch/fundamental frequency</li>
  <li>Formant structure (vocal tract resonances)</li>
  <li>Speaking rate</li>
  <li>Accent/dialect</li>
  <li>Voice quality (breathy, creaky, etc.)</li>
</ul>

<p><strong>What it should NOT capture</strong> (ideally):</p>

<ul>
  <li>Spoken words (content)</li>
  <li>Emotions (though it does somewhat)</li>
  <li>Background noise</li>
</ul>

<h3 id="comparison-different-diarization-approaches">Comparison: Different Diarization Approaches</h3>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>How It Works</th>
      <th>Pros</th>
      <th>Cons</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Clustering-based</strong></td>
      <td>Extract embeddings → Cluster</td>
      <td>Simple, interpretable</td>
      <td>Needs good embeddings</td>
      <td>General purpose</td>
    </tr>
    <tr>
      <td><strong>End-to-end neural</strong></td>
      <td>Single model: audio → labels</td>
      <td>Best accuracy</td>
      <td>Slow, black-box</td>
      <td>High-accuracy needs</td>
    </tr>
    <tr>
      <td><strong>Online diarization</strong></td>
      <td>Process stream incrementally</td>
      <td>Real-time capable</td>
      <td>Lower accuracy</td>
      <td>Live captions</td>
    </tr>
    <tr>
      <td><strong>Supervised (known speakers)</strong></td>
      <td>Match to registered voices</td>
      <td>Very accurate for known speakers</td>
      <td>Requires enrollment</td>
      <td>Authentication, personalization</td>
    </tr>
  </tbody>
</table>

<p><strong>Example scenario: Meeting with known participants</strong></p>

<p>``python
class KnownSpeakerDiarization:
 “””
 When you know who’s in the meeting</p>

<p>Much more accurate than unsupervised clustering
 “””</p>

<p>def <strong>init</strong>(self):
 self.speaker_profiles = {} # speaker_name → mean embedding</p>

<p>def enroll_speaker(self, speaker_name, audio_samples):
 “””
 Register a speaker</p>

<p>Args:
 speaker_name: “Alice”, “Bob”, etc.
 audio_samples: List of audio clips of this speaker
 “””
 # Extract embeddings from all samples
 embeddings = [
 self.extract_embedding(audio)
 for audio in audio_samples
 ]</p>

<p># Compute mean embedding (speaker profile)
 mean_embedding = np.mean(embeddings, axis=0)</p>

<p># Store
 self.speaker_profiles[speaker_name] = mean_embedding
 print(f”✓ Enrolled {speaker_name}”)</p>

<p>def identify_speaker(self, audio_segment):
 “””
 Identify which registered speaker this is</p>

<p>Much more accurate than unsupervised clustering!
 “””
 # Extract embedding
 test_embedding = self.extract_embedding(audio_segment)</p>

<p># Compare with all registered speakers
 best_match = None
 best_similarity = -1</p>

<p>for name, profile_embedding in self.speaker_profiles.items():
 similarity = self._cosine_similarity(test_embedding, profile_embedding)</p>

<p>if similarity &gt; best_similarity:
 best_similarity = similarity
 best_match = name</p>

<p># Threshold
 if best_similarity &gt; 0.7:
 return best_match, best_similarity
 else:
 return “UNKNOWN”, best_similarity</p>

<p>def extract_embedding(self, audio):
 “"”Placeholder: replace with real embedding extractor”””
 import numpy as np
 audio = np.asarray(audio)
 return audio[:512] if audio.size &gt;= 512 else np.pad(audio, (0, max(0, 512 - audio.size)))</p>

<p>def _cosine_similarity(self, a, b):
 import numpy as np
 a = np.asarray(a); b = np.asarray(b)
 denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-10
 return float(np.dot(a, b) / denom)</p>

<h1 id="usage">Usage</h1>
<p>diarizer = KnownSpeakerDiarization()</p>

<h1 id="enroll-meeting-participants">Enroll meeting participants</h1>
<p>diarizer.enroll_speaker(“Alice”, alice_audio_samples)
diarizer.enroll_speaker(“Bob”, bob_audio_samples)
diarizer.enroll_speaker(“Carol”, carol_audio_samples)</p>

<h1 id="now-identify-speakers-in-meeting">Now identify speakers in meeting</h1>
<p>for segment in meeting_segments:
 speaker, confidence = diarizer.identify_speaker(segment)
 print(f”{speaker} ({confidence:.2f}): {transcribe(segment)}”)
``</p>

<p><strong>This is how Zoom/Teams could improve</strong>:</p>
<ul>
  <li>Ask users to speak their name when joining</li>
  <li>Build speaker profile</li>
  <li>Use it for accurate diarization</li>
</ul>

<hr />

<h2 id="component-1-voice-activity-detection-vad">Component 1: Voice Activity Detection (VAD)</h2>

<p><strong>Goal</strong>: Find when <em>any</em> speaker is talking</p>

<p><strong>Why needed</strong>: Don’t waste compute on silence</p>

<p>``python
class VoiceActivityDetector:
 “””
 Detect speech vs non-speech</p>

<p>Uses energy + spectral features
 “””</p>

<p>def <strong>init</strong>(self, sample_rate=16000, frame_ms=30):
 self.sample_rate = sample_rate
 self.frame_size = int(sample_rate * frame_ms / 1000)</p>

<p>def detect(self, audio):
 “””
 Detect speech regions</p>

<p>Args:
 audio: numpy array, shape (samples,)</p>

<p>Returns:
 List of (start_time, end_time) tuples
 “””
 import numpy as np</p>

<p># Split into frames
 frames = self._split_frames(audio)</p>

<p># Compute features for each frame
 is_speech = []
 for frame in frames:
 # Energy-based detection
 energy = np.mean(frame ** 2)</p>

<p># Spectral flatness (voice has low flatness)
 flatness = self._spectral_flatness(frame)</p>

<p># Simple threshold
 speech = (energy &gt; 0.01) and (flatness &lt; 0.5)
 is_speech.append(speech)</p>

<p># Convert frame-level to time segments
 segments = self._merge_segments(is_speech)</p>

<p>return segments</p>

<p>def _split_frames(self, audio):
 “"”Split audio into overlapping frames”””
 import numpy as np</p>

<p>frames = []
 hop_size = self.frame_size // 2 # 50% overlap</p>

<p>for i in range(0, len(audio) - self.frame_size, hop_size):
 frame = audio[i:i + self.frame_size]
 frames.append(frame)</p>

<p>return frames</p>

<p>def _spectral_flatness(self, frame):
 “””
 Compute spectral flatness</p>

<p>Low for voice (harmonic), high for noise (flat spectrum)
 “””
 import numpy as np
 from scipy import signal</p>

<p># FFT
 fft = np.abs(np.fft.rfft(frame))</p>

<p># Geometric mean / arithmetic mean
 geometric_mean = np.exp(np.mean(np.log(fft + 1e-10)))
 arithmetic_mean = np.mean(fft)</p>

<p>flatness = geometric_mean / (arithmetic_mean + 1e-10)</p>

<p>return flatness</p>

<p>def _merge_segments(self, is_speech):
 “””
 Merge consecutive speech frames into segments</p>

<p>Args:
 is_speech: List of bools per frame</p>

<p>Returns:
 List of (start_time, end_time)
 “””
 segments = []
 in_segment = False
 start = 0</p>

<p>frame_duration = self.frame_size / self.sample_rate</p>

<p>for i, speech in enumerate(is_speech):
 if speech and not in_segment:
 # Start new segment
 start = i * frame_duration
 in_segment = True
 elif not speech and in_segment:
 # End segment
 end = i * frame_duration
 segments.append((start, end))
 in_segment = False</p>

<p># Handle case where last frame is speech
 if in_segment:
 segments.append((start, len(is_speech) * frame_duration))</p>

<p>return segments
``</p>

<p><strong>Modern approach</strong>: Use pre-trained VAD models (more accurate)</p>

<p>``python
def vad_pretrained(audio, sample_rate=16000):
 “””
 Use pre-trained VAD model (Silero VAD)</p>

<p>More accurate than energy-based
 “””
 import torch</p>

<p># Load pre-trained model
 model, utils = torch.hub.load(
 repo_or_dir=’snakers4/silero-vad’,
 model=’silero_vad’,
 force_reload=False
 )</p>

<p>(get_speech_timestamps, _, _, _, _) = utils</p>

<p># Detect speech
 speech_timestamps = get_speech_timestamps(
 audio,
 model,
 sampling_rate=sample_rate,
 threshold=0.5
 )</p>

<p># Convert to seconds
 segments = [
 (ts[‘start’] / sample_rate, ts[‘end’] / sample_rate)
 for ts in speech_timestamps
 ]</p>

<p>return segments
``</p>

<hr />

<h2 id="component-2-speaker-diarization">Component 2: Speaker Diarization</h2>

<p><strong>Goal</strong>: Cluster speech segments by speaker</p>

<p><strong>Key idea</strong>: Speakers have unique voice characteristics (embeddings)</p>

<h3 id="speaker-embeddings">Speaker Embeddings</h3>

<p><strong>Concept</strong>: Convert speech to fixed-size vector that captures speaker identity</p>

<p><code class="language-plaintext highlighter-rouge">
Speaker A: "Hello" → [0.2, 0.8, -0.3, ...] (512-dim)
Speaker A: "How are you" → [0.21, 0.79, -0.31, ...] (similar!)
Speaker B: "Hi there" → [-0.5, 0.1, 0.9, ...] (different!)
</code></p>

<p><strong>Models</strong>: x-vector, d-vector, ECAPA-TDNN</p>

<p>``python
class SpeakerEmbeddingExtractor:
 “””
 Extract speaker embeddings using ECAPA-TDNN</p>

<p>Embeddings capture speaker identity
 “””</p>

<p>def <strong>init</strong>(self):
 from speechbrain.pretrained import EncoderClassifier</p>

<p># Load pre-trained model
 self.model = EncoderClassifier.from_hparams(
 source=”speechbrain/spkrec-ecapa-voxceleb”,
 savedir=”pretrained_models/spkrec-ecapa”
 )</p>

<p>def extract(self, audio, sample_rate=16000):
 “””
 Extract speaker embedding</p>

<p>Args:
 audio: numpy array</p>

<p>Returns:
 embedding: numpy array, shape (512,)
 “””
 import torch</p>

<p># Convert to tensor
 audio_tensor = torch.FloatTensor(audio).unsqueeze(0)</p>

<p># Extract embedding
 with torch.no_grad():
 embedding = self.model.encode_batch(audio_tensor)</p>

<p># Convert to numpy
 embedding = embedding.squeeze().cpu().numpy()</p>

<p>return embedding</p>

<h1 id="usage-1">Usage</h1>
<p>extractor = SpeakerEmbeddingExtractor()</p>

<h1 id="extract-embeddings-for-each-speech-segment">Extract embeddings for each speech segment</h1>
<p>segments = [(0.0, 1.5), (1.2, 2.3), (2.8, 4.0)] # From VAD
embeddings = []</p>

<p>for start, end in segments:
 segment_audio = audio[int(start<em>sr):int(end</em>sr)]
 embedding = extractor.extract(segment_audio)
 embeddings.append(embedding)</p>

<h1 id="now-cluster-embeddings-to-identify-speakers">Now cluster embeddings to identify speakers</h1>
<p>``</p>

<h3 id="clustering-embeddings">Clustering Embeddings</h3>

<p><strong>Goal</strong>: Group similar embeddings (same speaker)</p>

<p>``python
class SpeakerClustering:
 “””
 Cluster speaker embeddings</p>

<p>Same speaker → similar embeddings → same cluster
 “””</p>

<p>def <strong>init</strong>(self, method=’spectral’):
 self.method = method</p>

<p>def cluster(self, embeddings, num_speakers=None):
 “””
 Cluster embeddings into speakers</p>

<p>Args:
 embeddings: List of numpy arrays
 num_speakers: If known, specify; else auto-detect</p>

<p>Returns:
 labels: Array of speaker IDs per segment
 “””
 import numpy as np
 from sklearn.cluster import SpectralClustering, AgglomerativeClustering</p>

<p># Convert to matrix
 X = np.array(embeddings)</p>

<p>if num_speakers is None:
 # Auto-detect number of speakers
 num_speakers = self._estimate_num_speakers(X)</p>

<p># Cluster
 if self.method == ‘spectral’:
 # Use precomputed affinity for better control
 from sklearn.metrics.pairwise import cosine_similarity
 affinity = cosine_similarity(X)
 clusterer = SpectralClustering(
 n_clusters=num_speakers,
 affinity=’precomputed’
 )
 labels = clusterer.fit_predict(affinity)
 return labels
 else:
 clusterer = AgglomerativeClustering(
 n_clusters=num_speakers,
 linkage=’average’,
 metric=’cosine’
 )</p>

<p>labels = clusterer.fit_predict(X)
 return labels</p>

<p>def _estimate_num_speakers(self, embeddings):
 “””
 Estimate number of speakers</p>

<p>Use eigengap heuristic or elbow method
 “””
 from sklearn.cluster import SpectralClustering
 import numpy as np</p>

<p># Try different numbers of clusters
 max_speakers = min(10, len(embeddings))</p>

<p>scores = []
 for k in range(2, max_speakers + 1):
 from sklearn.metrics.pairwise import cosine_similarity
 aff = cosine_similarity(embeddings)
 clusterer = SpectralClustering(n_clusters=k, affinity=’precomputed’)
 labels = clusterer.fit_predict(aff)</p>

<p># Compute silhouette score
 from sklearn.metrics import silhouette_score
 score = silhouette_score(embeddings, labels, metric=’cosine’)
 scores.append(score)</p>

<p># Pick k with highest score
 best_k = np.argmax(scores) + 2</p>

<p>return best_k
``</p>

<p><strong>Production library</strong>: Use <code class="language-plaintext highlighter-rouge">pyannote.audio</code> (state-of-the-art)</p>

<p>``python
def diarize_with_pyannote(audio_path):
 “””
 Speaker diarization using pyannote.audio</p>

<p>Production-ready, state-of-the-art
 “””
 from pyannote.audio import Pipeline</p>

<p># Load pre-trained pipeline
 pipeline = Pipeline.from_pretrained(
 “pyannote/speaker-diarization”,
 use_auth_token=”YOUR_HF_TOKEN”
 )</p>

<p># Run diarization
 diarization = pipeline(audio_path)</p>

<p># Extract speaker segments
 segments = []
 for turn, _, speaker in diarization.itertracks(yield_label=True):
 segments.append({
 ‘speaker’: speaker,
 ‘start’: turn.start,
 ‘end’: turn.end
 })</p>

<p>return segments</p>

<h1 id="example-output">Example output:</h1>
<h1>[</h1>
<h1 id="speaker-speaker_00-start-00-end-15">{‘speaker’: ‘SPEAKER_00’, ‘start’: 0.0, ‘end’: 1.5},</h1>
<h1 id="speaker-speaker_01-start-12-end-23">{‘speaker’: ‘SPEAKER_01’, ‘start’: 1.2, ‘end’: 2.3},</h1>
<h1 id="speaker-speaker_00-start-28-end-40">{‘speaker’: ‘SPEAKER_00’, ‘start’: 2.8, ‘end’: 4.0},</h1>
<h1 id="-1">]</h1>
<p>``</p>

<hr />

<h2 id="component-3-asr-per-speaker">Component 3: ASR Per Speaker</h2>

<p><strong>Goal</strong>: Transcribe each speaker segment</p>

<p>``python
class MultiSpeakerASR:
 “””
 Complete multi-speaker ASR system</p>

<p>Combines VAD + Diarization + ASR
 “””</p>

<p>def <strong>init</strong>(self):
 # Load models
 self.vad = VoiceActivityDetector()
 self.embedding_extractor = SpeakerEmbeddingExtractor()
 self.clustering = SpeakerClustering()</p>

<p># ASR model (Whisper)
 import whisper
 self.asr_model = whisper.load_model(“base”)</p>

<p>def transcribe(self, audio, sample_rate=16000):
 “””
 Multi-speaker transcription</p>

<p>Returns:
 List of {speaker, start, end, text}
 “””
 # Step 1: VAD
 speech_segments = self.vad.detect(audio)
 print(f”Found {len(speech_segments)} speech segments”)</p>

<p># Step 2: Extract embeddings
 embeddings = []
 for start, end in speech_segments:
 segment = audio[int(start<em>sample_rate):int(end</em>sample_rate)]
 emb = self.embedding_extractor.extract(segment)
 embeddings.append(emb)</p>

<p># Step 3: Cluster by speaker
 speaker_labels = self.clustering.cluster(embeddings)
 print(f”Detected {len(set(speaker_labels))} speakers”)</p>

<p># Step 4: Transcribe each segment
 results = []
 for i, (start, end) in enumerate(speech_segments):
 segment = audio[int(start<em>sample_rate):int(end</em>sample_rate)]</p>

<p># Transcribe (Whisper expects float32 numpy audio @16k)
 result = self.asr_model.transcribe(segment, fp16=False)
 text = result[‘text’]</p>

<p># Add speaker label
 speaker = f”SPEAKER_{speaker_labels[i]}”</p>

<p>results.append({
 ‘speaker’: speaker,
 ‘start’: start,
 ‘end’: end,
 ‘text’: text
 })</p>

<p>return results</p>

<h1 id="usage-2">Usage</h1>
<p>asr = MultiSpeakerASR()
results = asr.transcribe(audio)</p>

<h1 id="output">Output:</h1>
<h1 id="-2">[</h1>
<h1 id="speaker-speaker_0-start-00-end-15-text-hello-everyone">{‘speaker’: ‘SPEAKER_0’, ‘start’: 0.0, ‘end’: 1.5, ‘text’: ‘Hello everyone’},</h1>
<h1 id="speaker-speaker_1-start-12-end-23-text-hi-there">{‘speaker’: ‘SPEAKER_1’, ‘start’: 1.2, ‘end’: 2.3, ‘text’: ‘Hi there’},</h1>
<h1 id="speaker-speaker_0-start-28-end-40-text-how-are-you">{‘speaker’: ‘SPEAKER_0’, ‘start’: 2.8, ‘end’: 4.0, ‘text’: ‘How are you’},</h1>
<h1 id="-3">]</h1>
<p>``</p>

<hr />

<h2 id="handling-overlapping-speech">Handling Overlapping Speech</h2>

<p><strong>The hardest problem</strong>: Multiple speakers at once</p>

<h3 id="challenge">Challenge</h3>

<p><code class="language-plaintext highlighter-rouge">
Time: 0s 1s 2s
Speaker A: "Hello there..."
Speaker B: "Hi..."
Audio: [A] [A+B] [A]
 ↑
 Overlapped!
</code></p>

<p><strong>Problem</strong>: Single-channel audio can’t separate perfectly</p>

<h3 id="approach-1-overlap-detection--best-effort">Approach 1: Overlap Detection + Best Effort</h3>

<p>``python
class OverlapHandler:
 “””
 Detect overlapping speech and handle gracefully
 “””</p>

<p>def detect_overlaps(self, segments):
 “””
 Find overlapping segments</p>

<p>Args:
 segments: List of {speaker, start, end}</p>

<p>Returns:
 List of overlap regions
 “””
 overlaps = []</p>

<p>for i, seg1 in enumerate(segments):
 for seg2 in segments[i+1:]:
 # Check if overlapping
 if seg1[‘end’] &gt; seg2[‘start’] and seg1[‘start’] &lt; seg2[‘end’]:
 # Compute overlap region
 overlap_start = max(seg1[‘start’], seg2[‘start’])
 overlap_end = min(seg1[‘end’], seg2[‘end’])</p>

<p>overlaps.append({
 ‘start’: overlap_start,
 ‘end’: overlap_end,
 ‘speakers’: [seg1[‘speaker’], seg2[‘speaker’]]
 })</p>

<p>return overlaps</p>

<p>def handle_overlap(self, audio, overlap, speakers):
 “””
 Handle overlapped region</p>

<p>Options:</p>
<ol>
  <li>Transcribe mixed audio (less accurate)</li>
  <li>Mark as [OVERLAP] in transcript</li>
  <li>Use source separation (advanced)
 “””
 # Option 1: Transcribe mixed audio
 segment = audio[int(overlap[‘start’]<em>sr):int(overlap[‘end’]</em>sr)]
 result = asr_model.transcribe(segment)</li>
</ol>

<p>return {
 ‘type’: ‘overlap’,
 ‘speakers’: overlap[‘speakers’],
 ‘start’: overlap[‘start’],
 ‘end’: overlap[‘end’],
 ‘text’: result[‘text’],
 ‘confidence’: ‘low’ # Mark as uncertain
 }
``</p>

<h3 id="approach-2-multi-channel-source-separation">Approach 2: Multi-Channel Source Separation</h3>

<p><strong>If you have multiple microphones</strong>, you can separate speakers!</p>

<p>``python
class MultiChannelSeparation:
 “””
 Use multiple microphones to separate speakers</p>

<p>Requires: Multiple audio channels (e.g., mic array)
 “””</p>

<p>def <strong>init</strong>(self):
 # Use beamforming or deep learning separation
 pass</p>

<p>def separate(self, multi_channel_audio):
 “””
 Separate speakers using spatial information</p>

<p>Args:
 multi_channel_audio: (channels, samples)</p>

<p>Returns:
 separated_sources: List of (speaker_audio, speaker_id)
 “””
 # Advanced: Use Conv-TasNet or similar (from )
 # Here we’ll use simple beamforming</p>

<p>from scipy import signal</p>

<p># Beamforming toward each speaker
 # (Simplified - real implementation is complex)</p>

<p># For now, just return multi-channel as-is
 # In production, use libraries like:
 # - pyroomacoustics (beamforming)
 # - asteroid (deep learning separation)</p>

<p>return multi_channel_audio
``</p>

<hr />

<h2 id="real-time-streaming">Real-Time Streaming</h2>

<p><strong>Challenge</strong>: Process live audio with low latency</p>

<h3 id="streaming-architecture">Streaming Architecture</h3>

<p><code class="language-plaintext highlighter-rouge">
User's mic
 ↓
[Capture] → [Buffer] → [VAD] → [Diarization] → [ASR] → [Display]
 20ms 500ms 50ms 100ms 100ms 10ms
 ↑
 Total: ~270ms latency
</code></p>

<p>``python
class StreamingMultiSpeakerASR:
 “””
 Real-time multi-speaker ASR</p>

<p>Processes audio chunks as they arrive
 “””</p>

<p>def <strong>init</strong>(self, chunk_duration=0.5):
 self.chunk_duration = chunk_duration
 self.buffer = []
 self.speaker_history = {} # Track speaker embeddings over time</p>

<p># Models
 import whisper
 self.vad = VoiceActivityDetector()
 self.embedding_extractor = SpeakerEmbeddingExtractor()
 self.asr_model = whisper.load_model(“tiny”) # Faster for real-time</p>

<p>async def process_stream(self, audio_stream):
 “””
 Process audio stream in real-time</p>

<p>Args:
 audio_stream: Async iterator yielding audio chunks
 “””
 async for chunk in audio_stream:
 # Add to buffer
 self.buffer.extend(chunk)</p>

<p># Process if buffer large enough
 if len(self.buffer) &gt;= int(self.chunk_duration * 16000):
 result = await self._process_chunk()</p>

<p>if result:
 yield result</p>

<p>async def _process_chunk(self):
 “"”Process buffered audio chunk”””
 import numpy as np</p>

<p># Get chunk
 chunk = np.array(self.buffer[:int(self.chunk_duration * 16000)])</p>

<p># Remove from buffer (with overlap for continuity)
 overlap_samples = int(0.1 * 16000) # 100ms overlap
 self.buffer = self.buffer[len(chunk) - overlap_samples:]</p>

<p># VAD
 if not self._is_speech(chunk):
 return None</p>

<p># Extract embedding
 embedding = self.embedding_extractor.extract(chunk)</p>

<p># Identify speaker (match with history)
 speaker_id = self._identify_speaker(embedding)</p>

<p># Transcribe (async to not block)
 import asyncio
 text = await asyncio.to_thread(self.asr_model.transcribe, chunk, fp16=False)</p>

<p>return {
 ‘speaker’: speaker_id,
 ‘text’: text[‘text’],
 ‘timestamp’: <strong>import</strong>(‘time’).time()
 }</p>

<p>def _is_speech(self, chunk):
 “"”Quick speech check”””
 energy = np.mean(chunk ** 2)
 return energy &gt; 0.01</p>

<p>def _identify_speaker(self, embedding):
 “””
 Match embedding to known speakers</p>

<p>If new speaker, assign new ID
 “””
 import numpy as np</p>

<p># Compare with known speakers
 best_match = None
 best_similarity = -1</p>

<p>for speaker_id, known_embedding in self.speaker_history.items():
 # Cosine similarity
 similarity = np.dot(embedding, known_embedding) / (
 np.linalg.norm(embedding) * np.linalg.norm(known_embedding)
 )</p>

<p>if similarity &gt; best_similarity:
 best_similarity = similarity
 best_match = speaker_id</p>

<p># Threshold for same speaker
 if best_similarity &gt; 0.75:
 return best_match
 else:
 # New speaker
 new_id = f”SPEAKER_{len(self.speaker_history)}”
 self.speaker_history[new_id] = embedding
 return new_id</p>

<h1 id="usage-with-websocket">Usage with WebSocket</h1>
<p>import asyncio
import websockets</p>

<p>async def handle_client(websocket, path):
 “"”Handle incoming audio stream from client”””
 asr = StreamingMultiSpeakerASR()</p>

<p>async for result in asr.process_stream(websocket):
 # Send transcription back to client
 await websocket.send(json.dumps(result))</p>

<h1 id="start-server">Start server</h1>
<p>start_server = websockets.serve(handle_client, “localhost”, 8765)
asyncio.get_event_loop().run_until_complete(start_server)
asyncio.get_event_loop().run_forever()
``</p>

<hr />

<h2 id="common-failure-modes--debugging">Common Failure Modes &amp; Debugging</h2>

<h3 id="failure-mode-1-speaker-confusion">Failure Mode 1: Speaker Confusion</h3>

<p><strong>Symptom</strong>: System assigns same utterance to multiple speakers or switches mid-sentence</p>

<p><strong>Example:</strong>
``
Ground truth:
 [Alice, 0-5s]: “Hello, how are you today?”</p>

<p>System output (WRONG):
 [Alice, 0-2s]: “Hello, how”
 [Bob, 2-5s]: “are you today?”
``</p>

<p><strong>Root causes:</strong></p>

<ol>
  <li><strong>Insufficient speech for embedding</strong>
    <ul>
      <li>Embeddings need 2-3 seconds minimum</li>
      <li>Short utterances (&lt;1s) have unreliable embeddings</li>
    </ul>
  </li>
  <li><strong>Similar voices</strong>
    <ul>
      <li>Two speakers with similar pitch/timbre</li>
      <li>System can’t distinguish</li>
    </ul>
  </li>
  <li><strong>Poor audio quality</strong>
    <ul>
      <li>Background noise corrupts embeddings</li>
      <li>Low SNR (&lt;10dB) confuses system</li>
    </ul>
  </li>
</ol>

<p><strong>Solutions:</strong></p>

<p>``python
class RobustSpeakerIdentification:
 “””
 Handle edge cases in speaker identification
 “””</p>

<p>def <strong>init</strong>(self, min_segment_duration=2.0):
 self.min_segment_duration = min_segment_duration
 self.speaker_history = [] # Track recent speakers</p>

<p>def identify_with_context(self, audio_segment, duration, prev_speaker=None):
 “””
 Identify speaker with contextual hints</p>

<p>Args:
 audio_segment: Audio to identify
 duration: Segment duration in seconds
 prev_speaker: Who spoke last (context)</p>

<p>Returns:
 speaker_id, confidence
 “””
 # Check 1: Is segment long enough?
 if duration &lt; self.min_segment_duration:
 # Too short for reliable embedding
 # Use speaker from previous segment (continuity assumption)
 if prev_speaker:
 return prev_speaker, 0.5 # Low confidence
 else:
 return “UNKNOWN”, 0.0</p>

<p># Check 2: Extract embedding
 embedding = self.extract_embedding(audio_segment)</p>

<p># Check 3: Identify with threshold
 speaker, similarity = self.identify_speaker(embedding)</p>

<p># Check 4: Apply contextual prior
 if prev_speaker and similarity &lt; 0.75:
 # Ambiguous - bias toward previous speaker (people usually finish sentences)
 return prev_speaker, 0.6</p>

<p>return speaker, similarity
``</p>

<h3 id="failure-mode-2-overlap-mis-attribution">Failure Mode 2: Overlap Mis-attribution</h3>

<p><strong>Symptom</strong>: During overlaps, words from Speaker A attributed to Speaker B</p>

<p><strong>Example:</strong>
``
Ground truth:
 [Alice, 0-3s]: “I think we should consider this option”
 [Bob, 2-4s]: “Wait, what about the other approach?”</p>

<p>System output (WRONG):
 [Alice, 0-2s]: “I think we should”
 [Bob, 2-4s]: “consider this option Wait, what about the other approach?”
 ↑ These words are Alice’s, not Bob’s!
``</p>

<p><strong>Root cause</strong>: Diarization boundaries don’t align with actual speaker turns</p>

<p><strong>Solution</strong>: Post-processing refinement</p>

<p>``python
class OverlapRefiner:
 “””
 Refine transcriptions in overlap regions
 “””</p>

<p>def refine_overlaps(self, segments, asr_results):
 “””
 Use ASR confidence to refine overlap boundaries</p>

<p>Idea: Low-confidence words might be from the other speaker
 “””
 refined = []</p>

<p>for i, (seg, result) in enumerate(zip(segments, asr_results)):
 words = result[‘words’] # Word-level timestamps + confidence</p>

<p># Check if next segment overlaps
 if i &lt; len(segments) - 1:
 next_seg = segments[i+1]</p>

<p>if self._is_overlapping(seg, next_seg):
 # Refine boundary based on word confidence
 words, next_words = self._split_by_confidence(
 words, seg, next_seg
 )</p>

<p>refined.append({
 ‘speaker’: seg[‘speaker’],
 ‘start’: seg[‘start’],
 ‘end’: seg[‘end’],
 ‘words’: words
 })</p>

<p>return refined</p>

<p>def _split_by_confidence(self, words, seg1, seg2):
 “””
 Split words between two overlapping segments</p>

<p>High-confidence words stay, low-confidence might belong to other speaker
 “””
 overlap_start = max(seg1[‘start’], seg2[‘start’])
 overlap_end = min(seg1[‘end’], seg2[‘end’])</p>

<p>seg1_words = []
 seg2_words = []</p>

<p>for word in words:
 # Check if word is in overlap region
 if overlap_start &lt;= word[‘start’] &lt;= overlap_end:
 # In overlap - check confidence
 if word[‘confidence’] &gt; 0.8:
 seg1_words.append(word) # Keep in current segment
 else:
 seg2_words.append(word) # Might belong to other speaker
 else:
 seg1_words.append(word)</p>

<p>return seg1_words, seg2_words
``</p>

<h3 id="failure-mode-3-far-field-audio-degradation">Failure Mode 3: Far-Field Audio Degradation</h3>

<p><strong>Symptom</strong>: Accuracy drops significantly when speaker is far from microphone</p>

<p><strong>Example metrics:</strong>
``
Near-field (&lt; 1m from mic):
 WER: 5%
 Diarization accuracy: 95%</p>

<p>Far-field (&gt; 3m from mic):
 WER: 25% ← 5x worse!
 Diarization accuracy: 70%
``</p>

<p><strong>Root cause</strong>:</p>
<ul>
  <li>Lower SNR (signal-to-noise ratio)</li>
  <li>More reverberation</li>
  <li>Acoustic reflections</li>
</ul>

<p><strong>Solutions:</strong></p>

<ol>
  <li><strong>Beamforming</strong> (if mic array available)</li>
  <li><strong>Speech enhancement</strong> pre-processing</li>
  <li><strong>Specialized far-field models</strong></li>
</ol>

<p>``python
class FarFieldPreprocessor:
 “””
 Enhance far-field audio before ASR
 “””</p>

<p>def enhance(self, audio, sample_rate=16000):
 “””
 Apply far-field enhancements</p>

<ol>
  <li>Dereverb (reduce echo)</li>
  <li>Denoise</li>
  <li>Equalize (boost high frequencies)
 “””
 # Step 1: Dereverberation (WPE algorithm)
 enhanced = self._dereverb_wpe(audio, sample_rate)</li>
</ol>

<p># Step 2: Noise reduction (spectral subtraction)
 enhanced = self._denoise(enhanced, sample_rate)</p>

<p># Step 3: Equalization (boost consonants)
 enhanced = self._equalize(enhanced, sample_rate)</p>

<p>return enhanced</p>

<p>def _dereverb_wpe(self, audio, sr):
 “””
 Weighted Prediction Error (WPE) dereverberation</p>

<p>Removes room echo/reverberation
 “””
 # Simplified - use library like <code class="language-plaintext highlighter-rouge">nara_wpe</code> in production
 from scipy import signal</p>

<p># High-pass filter to remove low-freq rumble
 sos = signal.butter(5, 100, ‘highpass’, fs=sr, output=’sos’)
 filtered = signal.sosfilt(sos, audio)</p>

<p>return filtered</p>

<p>def _denoise(self, audio, sr):
 “””
 Spectral subtraction noise reduction
 “””
 import noisereduce as nr</p>

<p># Estimate noise from first 0.5s (assuming silence/noise)
 reduced = nr.reduce_noise(
 y=audio,
 sr=sr,
 stationary=True,
 prop_decrease=0.8
 )</p>

<p>return reduced</p>

<p>def _equalize(self, audio, sr):
 “””
 Boost high frequencies (consonants)</p>

<p>Far-field audio loses high-freq content
 “””
 from scipy import signal</p>

<p># Boost 2-8kHz (consonant region)
 sos = signal.butter(3, [2000, 8000], ‘bandpass’, fs=sr, output=’sos’)
 boosted = signal.sosfilt(sos, audio)</p>

<p># Mix with original (50-50)
 enhanced = 0.5 * audio + 0.5 * boosted</p>

<p>return enhanced
``</p>

<h3 id="debugging-tools">Debugging Tools</h3>

<p>``python
class MultiSpeakerASRDebugger:
 “””
 Tools for debugging multi-speaker ASR issues
 “””</p>

<p>def visualize_diarization(self, segments, audio_duration):
 “””
 Visual timeline of speakers</p>

<p>Helps spot issues like:</p>
<ul>
  <li>Too many speaker switches</li>
  <li>Missing speakers</li>
  <li>Wrong boundaries
 “””
 import matplotlib.pyplot as plt
 import numpy as np</li>
</ul>

<p>fig, ax = plt.subplots(figsize=(15, 3))</p>

<p># Plot each segment
 for seg in segments:
 speaker_id = int(seg[‘speaker’].split(‘_’)[1])
 color = plt.cm.tab10(speaker_id)</p>

<p>ax.barh(
 y=speaker_id,
 width=seg[‘end’] - seg[‘start’],
 left=seg[‘start’],
 height=0.8,
 color=color,
 label=seg[‘speaker’]
 )</p>

<p>ax.set_xlabel(‘Time (seconds)’)
 ax.set_ylabel(‘Speaker’)
 ax.set_title(‘Speaker Diarization Timeline’)
 ax.set_xlim(0, audio_duration)</p>

<p>plt.tight_layout()
 plt.savefig(‘diarization_debug.png’)
 print(“✓ Saved visualization to diarization_debug.png”)</p>

<p>def compute_metrics(self, predicted_segments, ground_truth_segments):
 “””
 Compute diarization metrics</p>

<p>DER (Diarization Error Rate) = 
 (False Alarm + Miss + Speaker Confusion) / Total
 “””
 from pyannote.metrics.diarization import DiarizationErrorRate</p>

<p>der = DiarizationErrorRate()</p>

<p># Convert to pyannote format
 pred_annotation = self._to_annotation(predicted_segments)
 gt_annotation = self._to_annotation(ground_truth_segments)</p>

<p># Compute DER
 error_rate = der(gt_annotation, pred_annotation)</p>

<p># Detailed breakdown
 details = der.components(gt_annotation, pred_annotation)</p>

<p>return {
 ‘DER’: error_rate,
 ‘false_alarm’: details[‘false alarm’],
 ‘missed_detection’: details[‘missed detection’],
 ‘speaker_confusion’: details[‘confusion’]
 }</p>

<p>def _to_annotation(self, segments):
 “"”Convert segments to pyannote Annotation format”””
 from pyannote.core import Annotation, Segment</p>

<p>annotation = Annotation()</p>

<p>for seg in segments:
 annotation[Segment(seg[‘start’], seg[‘end’])] = seg[‘speaker’]</p>

<p>return annotation
``</p>

<hr />

<h2 id="production-considerations">Production Considerations</h2>

<h3 id="1-latency-optimization">1. Latency Optimization</h3>

<p><strong>Target</strong>: &lt; 300ms end-to-end for real-time feel</p>

<p><strong>Breakdown:</strong>
<code class="language-plaintext highlighter-rouge">
Audio capture: 20ms
Buffering: 100ms
VAD: 10ms
Embedding: 50ms
ASR: 100ms
Network: 20ms
Total: 300ms
</code></p>

<p><strong>Optimizations:</strong></p>
<ul>
  <li>Use smaller ASR models (Whisper tiny/base)</li>
  <li>Batch embedding extraction</li>
  <li>Pre-compute speaker profiles</li>
  <li>GPU acceleration</li>
  <li>Reduce network round-trips</li>
</ul>

<h3 id="2-accuracy-vs-speed-trade-off">2. Accuracy vs Speed Trade-off</h3>

<table>
  <thead>
    <tr>
      <th>Model Size</th>
      <th>Latency</th>
      <th>WER</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Whisper tiny</td>
      <td>50ms</td>
      <td>10%</td>
      <td>Live captions</td>
    </tr>
    <tr>
      <td>Whisper base</td>
      <td>100ms</td>
      <td>7%</td>
      <td>Meetings</td>
    </tr>
    <tr>
      <td>Whisper medium</td>
      <td>300ms</td>
      <td>5%</td>
      <td>Post-processing</td>
    </tr>
    <tr>
      <td>Whisper large</td>
      <td>1000ms</td>
      <td>3%</td>
      <td>Archival transcription</td>
    </tr>
  </tbody>
</table>

<h3 id="3-speaker-persistence">3. Speaker Persistence</h3>

<p><strong>Challenge</strong>: Same speaker should have consistent ID across session</p>

<p>``python
class SpeakerRegistry:
 “””
 Maintain consistent speaker IDs</p>

<p>Matches new embeddings to registered speakers
 “””</p>

<p>def <strong>init</strong>(self, similarity_threshold=0.75):
 self.speakers = {} # id -&gt; mean embedding
 self.threshold = similarity_threshold</p>

<p>def register_or_identify(self, embedding):
 “””
 Register new speaker or identify existing
 “””
 # Check against known speakers
 for speaker_id, known_emb in self.speakers.items():
 similarity = cosine_similarity(embedding, known_emb)</p>

<p>if similarity &gt; self.threshold:
 # Update running average
 self.speakers[speaker_id] = (
 0.9 * known_emb + 0.1 * embedding
 )
 return speaker_id</p>

<p># New speaker
 new_id = f”SPEAKER_{len(self.speakers) + 1}”
 self.speakers[new_id] = embedding
 return new_id
``</p>

<h3 id="4-monitoring--debugging">4. Monitoring &amp; Debugging</h3>

<p>``python
class MultiSpeakerASRMetrics:
 “””
 Track system performance
 “””</p>

<p>def <strong>init</strong>(self):
 self.metrics = {
 ‘latency_ms’: [],
 ‘overlap_ratio’: 0,
 ‘speaker_switches_per_minute’: 0,
 ‘wer_per_speaker’: {}
 }</p>

<p>def log_latency(self, latency_ms):
 self.metrics[‘latency_ms’].append(latency_ms)</p>

<p>def report(self):
 import numpy as np</p>

<p>return {
 ‘p50_latency_ms’: np.median(self.metrics[‘latency_ms’]),
 ‘p95_latency_ms’: np.percentile(self.metrics[‘latency_ms’], 95),
 ‘overlap_ratio’: self.metrics[‘overlap_ratio’],
 ‘speaker_switches_per_minute’: self.metrics[‘speaker_switches_per_minute’]
 }
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Multi-speaker ASR</strong> = VAD + Diarization + ASR 
✅ <strong>Speaker embeddings</strong> capture voice identity 
✅ <strong>Clustering</strong> groups segments by speaker 
✅ <strong>Overlaps</strong> are hard - detect and handle gracefully 
✅ <strong>Real-time</strong> requires careful latency optimization 
✅ <strong>State-of-the-art</strong>: Use <code class="language-plaintext highlighter-rouge">pyannote.audio</code> + Whisper</p>

<p><strong>Production tips:</strong></p>
<ul>
  <li>Start with <code class="language-plaintext highlighter-rouge">pyannote</code> + <code class="language-plaintext highlighter-rouge">whisper</code> (best quality)</li>
  <li>Optimize latency with smaller models if needed</li>
  <li>Handle overlaps explicitly (mark in transcript)</li>
  <li>Maintain speaker consistency across session</li>
  <li>Monitor latency and accuracy per speaker</li>
</ul>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/">arunbaby.com/speech-tech/0012-multi-speaker-asr</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#multi-speaker" class="page__taxonomy-item p-category" rel="tag">multi-speaker</a><span class="sep">, </span>
    
      <a href="/tags/#overlapping-speech" class="page__taxonomy-item p-category" rel="tag">overlapping-speech</a><span class="sep">, </span>
    
      <a href="/tags/#real-time" class="page__taxonomy-item p-category" rel="tag">real-time</a><span class="sep">, </span>
    
      <a href="/tags/#speaker-diarization" class="page__taxonomy-item p-category" rel="tag">speaker-diarization</a><span class="sep">, </span>
    
      <a href="/tags/#streaming" class="page__taxonomy-item p-category" rel="tag">streaming</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0012-add-two-numbers/" rel="permalink">Add Two Numbers
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master digit-by-digit addition with linked lists: Handle carry propagation elegantly. Classic problem teaching pointer manipulation and edge cases.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0012-distributed-systems/" rel="permalink">Distributed ML Systems
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          25 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Design distributed ML systems that scale to billions of predictions: Master replication, sharding, consensus, and fault tolerance for production ML.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0012-context-window-management/" rel="permalink">Context Window Management
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The Finite Canvas of Intelligence: Managing the Agent’s RAM.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Multi-Speaker+ASR%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0012-multi-speaker-asr%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0012-multi-speaker-asr%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0012-multi-speaker-asr/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0011-speech-separation/" class="pagination--pager" title="Speech Separation">Previous</a>
    
    
      <a href="/speech-tech/0013-compute-allocation-for-speech-models/" class="pagination--pager" title="Compute Allocation for Speech Models">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
