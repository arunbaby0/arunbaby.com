<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Compute Allocation for Speech Models - Arun Baby</title>
<meta name="description" content="Optimize speech pipeline throughput by allocating compute to bottleneck stages using greedy resource management.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Compute Allocation for Speech Models">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0013-compute-allocation-for-speech-models/">


  <meta property="og:description" content="Optimize speech pipeline throughput by allocating compute to bottleneck stages using greedy resource management.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Compute Allocation for Speech Models">
  <meta name="twitter:description" content="Optimize speech pipeline throughput by allocating compute to bottleneck stages using greedy resource management.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0013-compute-allocation-for-speech-models/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T16:05:30+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0013-compute-allocation-for-speech-models/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Compute Allocation for Speech Models">
    <meta itemprop="description" content="Optimize speech pipeline throughput by allocating compute to bottleneck stages using greedy resource management.">
    <meta itemprop="datePublished" content="2025-12-29T16:05:30+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0013-compute-allocation-for-speech-models/" itemprop="url">Compute Allocation for Speech Models
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem-statement">Problem Statement</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li></ul></li><li><a href="#understanding-the-problem">Understanding the Problem</a><ul><li><a href="#typical-speech-pipeline">Typical Speech Pipeline</a></li><li><a href="#the-greedy-optimization-connection">The Greedy Optimization Connection</a></li></ul></li><li><a href="#high-level-architecture">High-Level Architecture</a><ul><li><a href="#key-components">Key Components</a></li></ul></li><li><a href="#component-deep-dives">Component Deep-Dives</a><ul><li><a href="#1-pipeline-profiler---bottleneck-detection">1. Pipeline Profiler - Bottleneck Detection</a></li><li><a href="#2-compute-optimizer---greedy-allocation-strategy">2. Compute Optimizer - Greedy Allocation Strategy</a></li><li><a href="#3-dynamic-batch-scheduler---gpu-optimization">3. Dynamic Batch Scheduler - GPU Optimization</a></li><li><a href="#4-resource-manager---execute-allocation">4. Resource Manager - Execute Allocation</a></li></ul></li><li><a href="#data-flow">Data Flow</a><ul><li><a href="#request-processing-flow">Request Processing Flow</a></li><li><a href="#monitoring-loop">Monitoring Loop</a></li></ul></li><li><a href="#production-deployment">Production Deployment</a><ul><li><a href="#multi-region-architecture">Multi-Region Architecture</a></li><li><a href="#kubernetes-deployment">Kubernetes Deployment</a></li><li><a href="#model-optimization-techniques">Model Optimization Techniques</a></li></ul></li><li><a href="#scaling-strategies">Scaling Strategies</a><ul><li><a href="#vertical-scaling---gpu-selection">Vertical Scaling - GPU Selection</a></li><li><a href="#horizontal-scaling---auto-scaling-rules">Horizontal Scaling - Auto-scaling Rules</a></li></ul></li><li><a href="#real-world-case-study-google-assistant">Real-World Case Study: Google Assistant</a><ul><li><a href="#googles-speech-pipeline">Google’s Speech Pipeline</a></li><li><a href="#key-lessons">Key Lessons</a></li></ul></li><li><a href="#cost-analysis">Cost Analysis</a><ul><li><a href="#cost-breakdown-10k-rps-speech-pipeline">Cost Breakdown (10K rps speech pipeline)</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a><ul><li><a href="#connection-to-thematic-link-greedy-optimization-and-resource-management">Connection to Thematic Link: Greedy Optimization and Resource Management</a></li><li><a href="#universal-principle">Universal Principle</a></li></ul></li></ul>
            </nav>
          </aside>
        
        <p><strong>Optimize speech pipeline throughput by allocating compute to bottleneck stages using greedy resource management.</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p>Design a <strong>compute allocation system for speech processing pipelines</strong> that efficiently distributes CPU/GPU resources across multiple stages (feature extraction, acoustic model, language model, post-processing) to maximize throughput while meeting strict latency SLAs.</p>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Multi-stage pipeline:</strong> Allocate resources across 4-6 pipeline stages</li>
  <li><strong>Real-time processing:</strong> Meet &lt;100ms latency for streaming ASR</li>
  <li><strong>Dynamic scaling:</strong> Adjust allocation based on load and bottlenecks</li>
  <li><strong>Multi-model support:</strong> Handle ASR, TTS, speaker recognition, etc.</li>
  <li><strong>Heterogeneous compute:</strong> Mix of CPU (feature extraction) and GPU (neural models)</li>
  <li><strong>Batch optimization:</strong> Dynamic batching for GPU efficiency</li>
  <li><strong>Quality-aware:</strong> Maintain accuracy while optimizing for speed</li>
  <li><strong>Cost-efficient:</strong> Minimize cloud spending per request</li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Latency:</strong> p95 &lt; 100ms for ASR, &lt;200ms for TTS</li>
  <li><strong>Throughput:</strong> 10,000+ concurrent requests</li>
  <li><strong>Accuracy:</strong> WER &lt; 5% (ASR), MOS &gt; 4.0 (TTS)</li>
  <li><strong>Availability:</strong> 99.95% uptime</li>
  <li><strong>Cost:</strong> &lt;$0.001 per request</li>
  <li><strong>GPU utilization:</strong> &gt;80%</li>
  <li><strong>Scalability:</strong> Handle 10x traffic spikes</li>
</ol>

<h2 id="understanding-the-problem">Understanding the Problem</h2>

<p>Speech processing pipelines are <strong>compute-intensive</strong> and <strong>latency-sensitive</strong>. Poor compute allocation leads to:</p>

<ul>
  <li><strong>Bottlenecks:</strong> One slow stage limits entire pipeline throughput</li>
  <li><strong>Wasted resources:</strong> Over-provisioning fast stages wastes money</li>
  <li><strong>Latency violations:</strong> Under-provisioning causes SLA breaches</li>
  <li><strong>Poor GPU utilization:</strong> Inefficient batching leaves GPUs idle</li>
</ul>

<h3 id="typical-speech-pipeline">Typical Speech Pipeline</h3>

<p>``
Audio Input (16kHz PCM)
 ↓
┌─────────────────────────────────────────────────────────────┐
│ Speech Pipeline │
├─────────────────────────────────────────────────────────────┤
│ │
│ Stage 1: Feature Extraction (CPU) │
│ - Convert audio to mel spectrograms │
│ - Time: ~5ms per 100ms audio │
│ - Memory: 1MB per request │
│ ↓ │
│ Stage 2: Acoustic Model (GPU) │
│ - Neural network (Conformer/Wav2Vec2) │
│ - Time: ~20ms per 100ms audio (batched) │
│ - Memory: 500MB model + 10MB per request │
│ ↓ │
│ Stage 3: Language Model (GPU/CPU) │
│ - Beam search with n-gram or neural LM │
│ - Time: ~15ms per 100ms audio │
│ - Memory: 2GB model + 5MB per request │
│ ↓ │
│ Stage 4: Post-processing (CPU) │
│ - Punctuation, capitalization, formatting │
│ - Time: ~2ms per request │
│ - Memory: 100KB per request │
│ ↓ │
│ Text Output │
└─────────────────────────────────────────────────────────────┘</p>

<p>Total latency: ~42ms (with perfect pipelining)
Bottleneck: Acoustic Model (47% of time)
``</p>

<h3 id="the-greedy-optimization-connection">The Greedy Optimization Connection</h3>

<p>Just like the <strong>Container With Most Water</strong> problem and <strong>Resource Allocation for ML</strong> systems:</p>

<table>
  <thead>
    <tr>
      <th>Container Problem</th>
      <th>Speech Compute Allocation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Two lines (heights)</td>
      <td>Multiple pipeline stages</td>
    </tr>
    <tr>
      <td>Bottleneck (shorter line)</td>
      <td>Slowest stage limits throughput</td>
    </tr>
    <tr>
      <td>Maximize area</td>
      <td>Maximize throughput</td>
    </tr>
    <tr>
      <td>Greedy: move shorter pointer</td>
      <td>Greedy: allocate to bottleneck</td>
    </tr>
    <tr>
      <td>Width vs height tradeoff</td>
      <td>Latency vs throughput tradeoff</td>
    </tr>
  </tbody>
</table>

<p><strong>Core insight:</strong> Identify the bottleneck stage and allocate resources greedily to maximize end-to-end throughput.</p>

<h2 id="high-level-architecture">High-Level Architecture</h2>

<p><code class="language-plaintext highlighter-rouge">
┌─────────────────────────────────────────────────────────────────┐
│ Compute Allocation Controller │
│ │
│ ┌──────────────────────┐ ┌──────────────────────┐ │
│ │ Profiler │ │ Optimizer │ │
│ │ - Measure latency │─────▶│ - Identify │ │
│ │ - Track utilization │ │ bottleneck │ │
│ │ - Detect bottleneck │ │ - Reallocation │ │
│ └──────────────────────┘ │ strategy │ │
│ └──────────┬───────────┘ │
│ │ │
│ ▼ │
│ ┌──────────────────────┐ │
│ │ Resource Manager │ │
│ │ - CPU pool │ │
│ │ - GPU pool │ │
│ │ - Batch scheduler │ │
│ └──────────┬───────────┘ │
└────────────────────────────────────────────┼────────────────────┘
 │
 ▼
┌─────────────────────────────────────────────────────────────────┐
│ Speech Pipeline Workers │
│ │
│ ┌────────────┐ ┌────────────┐ ┌────────────┐ ┌──────────┐ │
│ │ Feature │ │ Acoustic │ │ Language │ │ Post- │ │
│ │ Extract │─▶│ Model │─▶│ Model │─▶│ Process │ │
│ │ │ │ │ │ │ │ │ │
│ │ CPU × N │ │ GPU × M │ │ GPU × K │ │ CPU × P │ │
│ └────────────┘ └────────────┘ └────────────┘ └──────────┘ │
│ │
│ Compute: 4 CPUs → 2 GPUs → 1 GPU → 2 CPUs (example) │
└─────────────────────────────────────────────────────────────────┘
</code></p>

<h3 id="key-components">Key Components</h3>

<ol>
  <li><strong>Profiler:</strong> Continuously measures stage latencies and resource utilization</li>
  <li><strong>Optimizer:</strong> Identifies bottlenecks and computes optimal allocation</li>
  <li><strong>Resource Manager:</strong> Executes allocation decisions (spawn/kill workers)</li>
  <li><strong>Pipeline Workers:</strong> Actual compute resources running each stage</li>
</ol>

<h2 id="component-deep-dives">Component Deep-Dives</h2>

<h3 id="1-pipeline-profiler---bottleneck-detection">1. Pipeline Profiler - Bottleneck Detection</h3>

<p>The profiler tracks per-stage metrics to identify bottlenecks.</p>

<p>``python
from dataclasses import dataclass
from typing import Dict, List, Optional
from collections import deque
from datetime import datetime
import numpy as np</p>

<p>@dataclass
class StageMetrics:
 “"”Metrics for a single pipeline stage.”””
 stage_name: str
 latency_ms: deque # Rolling window of latencies
 utilization: float # 0.0 to 1.0
 throughput_rps: float # Requests per second
 queue_size: int
 num_workers: int
 worker_type: str # “CPU” or “GPU”</p>

<p>def <strong>post_init</strong>(self):
 if not isinstance(self.latency_ms, deque):
 self.latency_ms = deque(maxlen=1000) # Last 1000 requests</p>

<p>@property
 def avg_latency_ms(self) -&gt; float:
 “"”Average latency over window.”””
 return np.mean(self.latency_ms) if self.latency_ms else 0.0</p>

<p>@property
 def p95_latency_ms(self) -&gt; float:
 “"”P95 latency over window.”””
 return np.percentile(self.latency_ms, 95) if self.latency_ms else 0.0</p>

<p>@property
 def p99_latency_ms(self) -&gt; float:
 “"”P99 latency over window.”””
 return np.percentile(self.latency_ms, 99) if self.latency_ms else 0.0</p>

<p>@property
 def is_bottleneck(self) -&gt; bool:
 “””
 Heuristic: stage is bottleneck if:</p>
<ol>
  <li>High utilization (&gt;80%)</li>
  <li>Growing queue</li>
  <li>High latency variance
 “””
 high_utilization = self.utilization &gt; 0.80
 has_queue = self.queue_size &gt; 10
 high_variance = (
 self.p99_latency_ms &gt; 1.5 * self.avg_latency_ms
 if self.latency_ms else False
 )</li>
</ol>

<p>return high_utilization and (has_queue or high_variance)</p>

<p>class PipelineProfiler:
 “””
 Profiles speech pipeline to identify bottlenecks.</p>

<p>Similar to Container With Most Water:</p>
<ul>
  <li>Each stage is a “line” with capacity (height)</li>
  <li>Bottleneck stage (shortest line) limits throughput (area)
 “””</li>
</ul>

<p>def <strong>init</strong>(self, stages: List[str]):
 self.stages = stages
 self.metrics: Dict[str, StageMetrics] = {
 stage: StageMetrics(
 stage_name=stage,
 latency_ms=deque(maxlen=1000),
 utilization=0.0,
 throughput_rps=0.0,
 queue_size=0,
 num_workers=1,
 worker_type=”CPU” if stage in [“feature_extraction”, “post_process”] else “GPU”
 )
 for stage in stages
 }
 self.request_count = 0
 self.start_time = datetime.now()</p>

<p>def record_latency(self, stage: str, latency_ms: float):
 “"”Record latency measurement for a stage.”””
 if stage in self.metrics:
 self.metrics[stage].latency_ms.append(latency_ms)
 self.request_count += 1</p>

<p>def update_utilization(self, stage: str, utilization: float):
 “"”Update utilization measurement.”””
 if stage in self.metrics:
 self.metrics[stage].utilization = utilization</p>

<p>def update_queue_size(self, stage: str, queue_size: int):
 “"”Update queue size.”””
 if stage in self.metrics:
 self.metrics[stage].queue_size = queue_size</p>

<p>def identify_bottleneck(self) -&gt; Optional[str]:
 “””
 Identify bottleneck stage using greedy heuristic.</p>

<p>Greedy choice: stage with highest “pressure” score.
 Pressure = weighted combination of:</p>
<ul>
  <li>Latency (40%)</li>
  <li>Utilization (30%)</li>
  <li>Queue size (30%)</li>
</ul>

<p>Returns:
 Bottleneck stage name or None
 “””
 if not self.metrics:
 return None</p>

<p>max_pressure = 0.0
 bottleneck = None</p>

<p># Normalize metrics for comparison
 max_latency = max(m.avg_latency_ms for m in self.metrics.values())
 max_queue = max(m.queue_size for m in self.metrics.values())</p>

<p>for stage, metrics in self.metrics.items():
 # Calculate pressure score
 latency_score = (
 metrics.avg_latency_ms / max_latency if max_latency &gt; 0 else 0
 )
 util_score = metrics.utilization
 queue_score = (
 metrics.queue_size / max_queue if max_queue &gt; 0 else 0
 )</p>

<p># Weighted pressure
 pressure = (
 0.40 * latency_score +
 0.30 * util_score +
 0.30 * queue_score
 )</p>

<p>if pressure &gt; max_pressure:
 max_pressure = pressure
 bottleneck = stage</p>

<p>return bottleneck if max_pressure &gt; 0.5 else None</p>

<p>def get_pipeline_summary(self) -&gt; Dict:
 “"”Get overall pipeline statistics.”””
 total_latency = sum(m.avg_latency_ms for m in self.metrics.values())</p>

<p># Find bottleneck
 bottleneck = self.identify_bottleneck()
 bottleneck_metrics = self.metrics.get(bottleneck) if bottleneck else None</p>

<p># Calculate end-to-end throughput
 # Limited by bottleneck stage
 if bottleneck_metrics:
 e2e_throughput = (
 bottleneck_metrics.num_workers * 
 (1000.0 / bottleneck_metrics.avg_latency_ms)
 if bottleneck_metrics.avg_latency_ms &gt; 0 else 0
 )
 else:
 e2e_throughput = 0</p>

<p>return {
 “total_requests”: self.request_count,
 “avg_latency_ms”: total_latency,
 “bottleneck_stage”: bottleneck,
 “bottleneck_latency_ms”: (
 bottleneck_metrics.avg_latency_ms if bottleneck_metrics else 0
 ),
 “estimated_throughput_rps”: e2e_throughput,
 “stage_breakdown”: {
 stage: {
 “avg_latency_ms”: m.avg_latency_ms,
 “p95_latency_ms”: m.p95_latency_ms,
 “utilization”: m.utilization,
 “queue_size”: m.queue_size,
 “is_bottleneck”: m.is_bottleneck,
 }
 for stage, m in self.metrics.items()
 }
 }
``</p>

<h3 id="2-compute-optimizer---greedy-allocation-strategy">2. Compute Optimizer - Greedy Allocation Strategy</h3>

<p>The optimizer decides how to allocate compute resources to maximize throughput.</p>

<p>``python
from typing import Tuple, List
import math</p>

<p>@dataclass
class ComputeResource:
 “"”A compute resource (CPU core or GPU).”””
 resource_id: str
 resource_type: str # “CPU” or “GPU”
 cost_per_hour: float
 max_batch_size: int = 1 # For GPUs</p>

<p>@dataclass
class AllocationPlan:
 “"”Compute allocation plan for pipeline.”””
 stage_allocations: Dict[str, int] # stage -&gt; num_workers
 expected_throughput_rps: float
 expected_latency_ms: float
 estimated_cost_per_hour: float</p>

<p>class ComputeOptimizer:
 “””
 Greedy optimizer for compute allocation.</p>

<p>Strategy (like Container With Most Water):</p>
<ol>
  <li>Identify bottleneck stage (shortest line)</li>
  <li>Allocate more resources to bottleneck (greedy choice)</li>
  <li>Repeat until:
    <ul>
      <li>Throughput target met</li>
      <li>Budget exhausted</li>
      <li>Bottleneck shifts to different stage
 “””</li>
    </ul>
  </li>
</ol>

<p>def <strong>init</strong>(
 self,
 profiler: PipelineProfiler,
 target_throughput_rps: float,
 max_latency_ms: float,
 budget_per_hour: float
 ):
 self.profiler = profiler
 self.target_throughput = target_throughput_rps
 self.max_latency = max_latency_ms
 self.budget = budget_per_hour</p>

<p># Resource costs (example AWS pricing)
 self.cpu_cost = 0.10 # per core per hour
 self.gpu_cost = 3.00 # per GPU per hour (T4)</p>

<p>def compute_optimal_allocation(self) -&gt; AllocationPlan:
 “””
 Compute optimal resource allocation using greedy algorithm.</p>

<p>Greedy approach:</p>
<ol>
  <li>Start with minimal allocation (1 worker per stage)</li>
  <li>Iteratively add resources to bottleneck</li>
  <li>Stop when target met or budget exhausted</li>
</ol>

<p>Time: O(N × M) where N=stages, M=max_workers
 Similar to two-pointer approach in container problem
 “””
 # Start with baseline allocation
 allocation = {
 stage: 1
 for stage in self.profiler.stages
 }</p>

<p># Iteratively improve
 max_iterations = 100
 for iteration in range(max_iterations):
 # Simulate current allocation
 throughput, latency, cost = self._simulate_allocation(allocation)</p>

<p># Check if targets met
 if (throughput &gt;= self.target_throughput and
 latency &lt;= self.max_latency and
 cost &lt;= self.budget):
 # Success!
 return AllocationPlan(
 stage_allocations=allocation,
 expected_throughput_rps=throughput,
 expected_latency_ms=latency,
 estimated_cost_per_hour=cost
 )</p>

<p># Greedy: add resource to bottleneck
 bottleneck = self._find_bottleneck_stage(allocation)
 if not bottleneck:
 break</p>

<p># Check if adding resource exceeds budget
 new_cost = self._calculate_incremental_cost(bottleneck, allocation)
 if cost + new_cost &gt; self.budget:
 break # Budget constraint</p>

<p># Add resource to bottleneck (greedy choice)
 allocation[bottleneck] += 1</p>

<p># Return best effort allocation
 throughput, latency, cost = self._simulate_allocation(allocation)
 return AllocationPlan(
 stage_allocations=allocation,
 expected_throughput_rps=throughput,
 expected_latency_ms=latency,
 estimated_cost_per_hour=cost
 )</p>

<p>def _simulate_allocation(
 self,
 allocation: Dict[str, int]
 ) -&gt; Tuple[float, float, float]:
 “””
 Simulate pipeline performance with given allocation.</p>

<p>Returns:
 (throughput_rps, latency_ms, cost_per_hour)
 “””
 # Get baseline metrics from profiler
 summary = self.profiler.get_pipeline_summary()</p>

<p># Calculate per-stage throughput
 stage_throughputs = {}
 for stage, num_workers in allocation.items():
 metrics = self.profiler.metrics[stage]</p>

<p>if metrics.avg_latency_ms &gt; 0:
 # Throughput = workers / latency
 # With batching for GPU stages
 batch_factor = 1.0
 if metrics.worker_type == “GPU”:
 batch_factor = min(8, num_workers * 2) # Assume batch size ~8-16</p>

<p>throughput = (
 num_workers * batch_factor * 1000.0 / metrics.avg_latency_ms
 )
 stage_throughputs[stage] = throughput
 else:
 stage_throughputs[stage] = float(‘inf’)</p>

<p># End-to-end throughput limited by slowest stage
 min_throughput = min(stage_throughputs.values())</p>

<p># End-to-end latency is sum of stage latencies
 # (assuming perfect pipelining, otherwise add queuing delays)
 total_latency = sum(
 self.profiler.metrics[stage].avg_latency_ms
 for stage in self.profiler.stages
 )</p>

<p># Calculate cost
 cost = 0.0
 for stage, num_workers in allocation.items():
 worker_type = self.profiler.metrics[stage].worker_type
 if worker_type == “GPU”:
 cost += num_workers * self.gpu_cost
 else:
 cost += num_workers * self.cpu_cost</p>

<p>return min_throughput, total_latency, cost</p>

<p>def _find_bottleneck_stage(self, allocation: Dict[str, int]) -&gt; Optional[str]:
 “””
 Find bottleneck stage given current allocation.</p>

<p>Bottleneck = stage with lowest throughput capacity.
 (Like finding shorter line in container problem)
 “””
 min_throughput = float(‘inf’)
 bottleneck = None</p>

<p>for stage in self.profiler.stages:
 metrics = self.profiler.metrics[stage]
 num_workers = allocation[stage]</p>

<p>if metrics.avg_latency_ms &gt; 0:
 # Calculate stage throughput
 batch_factor = 1.0
 if metrics.worker_type == “GPU”:
 batch_factor = min(8, num_workers * 2)</p>

<p>throughput = (
 num_workers * batch_factor * 1000.0 / metrics.avg_latency_ms
 )</p>

<p>if throughput &lt; min_throughput:
 min_throughput = throughput
 bottleneck = stage</p>

<p>return bottleneck</p>

<p>def _calculate_incremental_cost(
 self,
 stage: str,
 current_allocation: Dict[str, int]
 ) -&gt; float:
 “"”Calculate cost of adding one more worker to stage.”””
 worker_type = self.profiler.metrics[stage].worker_type
 return self.gpu_cost if worker_type == “GPU” else self.cpu_cost
``</p>

<h3 id="3-dynamic-batch-scheduler---gpu-optimization">3. Dynamic Batch Scheduler - GPU Optimization</h3>

<p>For GPU stages (acoustic model, language model), batching is critical for efficiency.</p>

<p>``python
import asyncio
from asyncio import Queue
from typing import List
import time</p>

<p>@dataclass
class SpeechRequest:
 “"”A speech processing request.”””
 request_id: str
 audio_data: bytes
 duration_ms: float
 timestamp: float</p>

<p>class DynamicBatchScheduler:
 “””
 Dynamic batching for GPU inference.</p>

<p>Trade-off:</p>
<ul>
  <li>Large batches: Higher throughput, higher latency</li>
  <li>Small batches: Lower latency, lower throughput</li>
</ul>

<p>Greedy strategy:</p>
<ul>
  <li>Wait for batch to fill up to <code class="language-plaintext highlighter-rouge">target_batch_size</code></li>
  <li>But timeout after <code class="language-plaintext highlighter-rouge">max_wait_ms</code> to maintain latency SLA
 “””</li>
</ul>

<p>def <strong>init</strong>(
 self,
 target_batch_size: int = 16,
 max_wait_ms: float = 10.0,
 max_queue_size: int = 1000
 ):
 self.target_batch_size = target_batch_size
 self.max_wait_ms = max_wait_ms / 1000.0 # Convert to seconds
 self.queue: Queue[SpeechRequest] = Queue(maxsize=max_queue_size)
 self.batch_count = 0</p>

<p>async def add_request(self, request: SpeechRequest):
 “"”Add request to batch queue.”””
 await self.queue.put(request)</p>

<p>async def get_batch(self) -&gt; List[SpeechRequest]:
 “””
 Get next batch using greedy strategy.</p>

<p>Greedy decision:</p>
<ol>
  <li>If batch_size reached: return immediately (maximize throughput)</li>
  <li>If timeout: return partial batch (maintain latency SLA)</li>
  <li>Else: keep waiting</li>
</ol>

<p>Returns:
 List of requests (1 to target_batch_size)
 “””
 batch = []
 start_time = time.time()</p>

<p>while len(batch) &lt; self.target_batch_size:
 remaining_time = self.max_wait_ms - (time.time() - start_time)</p>

<p># Timeout check (latency SLA)
 if remaining_time &lt;= 0 and batch:
 break # Return partial batch</p>

<p>try:
 # Wait for next request (with timeout)
 request = await asyncio.wait_for(
 self.queue.get(),
 timeout=max(remaining_time, 0.001)
 )
 batch.append(request)</p>

<p># Greedy: if we have enough, return immediately
 if len(batch) &gt;= self.target_batch_size:
 break</p>

<p>except asyncio.TimeoutError:
 # Timeout - return what we have
 if batch:
 break
 else:
 continue # Keep waiting if empty</p>

<p>self.batch_count += 1
 return batch</p>

<p>def get_stats(self) -&gt; Dict:
 “"”Get batching statistics.”””
 return {
 “queue_size”: self.queue.qsize(),
 “batch_count”: self.batch_count,
 “avg_batch_size”: “N/A”, # Would track in production
 }</p>

<h1 id="example-usage-in-acoustic-model-inference">Example usage in acoustic model inference</h1>
<p>class AcousticModelWorker:
 “"”GPU worker for acoustic model inference with batching.”””</p>

<p>def <strong>init</strong>(self, model, device=”cuda”):
 self.model = model
 self.device = device
 self.scheduler = DynamicBatchScheduler(
 target_batch_size=16,
 max_wait_ms=10.0
 )</p>

<p>async def process_loop(self):
 “"”Main processing loop.”””
 while True:
 # Get batch (greedy batching)
 batch = await self.scheduler.get_batch()</p>

<p>if not batch:
 await asyncio.sleep(0.001)
 continue</p>

<p># Process batch on GPU
 results = await self._inference_batch(batch)</p>

<p># Return results to each request
 # … send results back …</p>

<p>async def _inference_batch(self, batch: List[SpeechRequest]):
 “"”Run batched inference on GPU.”””
 # Prepare batch
 # Run model
 # Return results
 pass
``</p>

<h3 id="4-resource-manager---execute-allocation">4. Resource Manager - Execute Allocation</h3>

<p>``python
import subprocess
from typing import Dict, List</p>

<p>class ResourceManager:
 “””
 Manages compute resources (spawn/kill workers).</p>

<p>Executes allocation decisions from optimizer.
 “””</p>

<p>def <strong>init</strong>(self):
 self.workers: Dict[str, List[subprocess.Popen]] = {}
 for stage in [“feature_extraction”, “acoustic_model”, “language_model”, “post_process”]:
 self.workers[stage] = []</p>

<p>def apply_allocation(self, plan: AllocationPlan):
 “””
 Apply allocation plan by spawning/killing workers.</p>

<p>Greedy approach:</p>
<ol>
  <li>Calculate delta (target - current)</li>
  <li>Spawn new workers if delta &gt; 0</li>
  <li>Kill excess workers if delta &lt; 0
 “””
 for stage, target_count in plan.stage_allocations.items():
 current_count = len(self.workers[stage])
 delta = target_count - current_count</li>
</ol>

<p>if delta &gt; 0:
 # Spawn new workers
 self._spawn_workers(stage, delta)
 elif delta &lt; 0:
 # Kill excess workers
 self._kill_workers(stage, abs(delta))</p>

<p>def _spawn_workers(self, stage: str, count: int):
 “"”Spawn worker processes.”””
 for i in range(count):
 # In production: spawn Kubernetes pod or start process
 # Example: subprocess.Popen([“python”, f”{stage}_worker.py”])
 pass</p>

<p>def _kill_workers(self, stage: str, count: int):
 “"”Gracefully terminate workers.”””
 for i in range(count):
 if self.workers[stage]:
 worker = self.workers[stage].pop()
 # worker.terminate()
 # worker.wait(timeout=30)
``</p>

<h2 id="data-flow">Data Flow</h2>

<h3 id="request-processing-flow">Request Processing Flow</h3>

<p>``</p>
<ol>
  <li>
    <p>Request arrives
 └─&gt; Load balancer routes to available feature extraction worker</p>
  </li>
  <li>
    <p>Feature Extraction (CPU)
 └─&gt; Extract mel spectrogram (5ms)
 └─&gt; Send to batch scheduler for acoustic model</p>
  </li>
  <li>
    <p>Acoustic Model (GPU) - Batching
 └─&gt; Wait for batch (up to 10ms)
 └─&gt; Process batch of 16 requests (20ms)
 └─&gt; Amortized: ~1.25ms per request (batched)
 └─&gt; Send to language model</p>
  </li>
  <li>
    <p>Language Model (GPU)
 └─&gt; Beam search decoding (15ms)
 └─&gt; Send to post-processing</p>
  </li>
  <li>
    <p>Post-processing (CPU)
 └─&gt; Punctuation, capitalization (2ms)
 └─&gt; Return result</p>
  </li>
</ol>

<p>Total: 5ms + 10ms + 1.25ms + 15ms + 2ms ≈ 33ms (with batching)
Without batching: 5ms + 20ms + 15ms + 2ms = 42ms
``</p>

<h3 id="monitoring-loop">Monitoring Loop</h3>

<p>``python
async def monitoring_loop(
 profiler: PipelineProfiler,
 optimizer: ComputeOptimizer,
 resource_manager: ResourceManager
):
 “””
 Continuous monitoring and reallocation loop.</p>

<p>Every 60 seconds:</p>
<ol>
  <li>Check for bottlenecks</li>
  <li>Compute optimal allocation</li>
  <li>Apply if significantly different
 “””
 while True:
 # Get current state
 summary = profiler.get_pipeline_summary()</li>
</ol>

<p># Log metrics
 print(f”Bottleneck: {summary[‘bottleneck_stage’]}”)
 print(f”Throughput: {summary[‘estimated_throughput_rps’]:.1f} rps”)
 print(f”Latency: {summary[‘avg_latency_ms’]:.1f}ms”)</p>

<p># Recompute optimal allocation
 new_plan = optimizer.compute_optimal_allocation()</p>

<p># Apply if significant change (&gt;20% difference)
 if should_reallocate(new_plan, resource_manager):
 print(f”Reallocating: {new_plan.stage_allocations}”)
 resource_manager.apply_allocation(new_plan)</p>

<p># Wait before next check
 await asyncio.sleep(60)</p>

<p>def should_reallocate(
 new_plan: AllocationPlan,
 resource_manager: ResourceManager
) -&gt; bool:
 “"”Check if reallocation is worthwhile.”””
 # Avoid thrashing - only reallocate if significant change
 for stage, target in new_plan.stage_allocations.items():
 current = len(resource_manager.workers[stage])
 if abs(target - current) &gt;= 2: # At least 2 worker difference
 return True
 return False
``</p>

<h2 id="production-deployment">Production Deployment</h2>

<h3 id="multi-region-architecture">Multi-Region Architecture</h3>

<p><code class="language-plaintext highlighter-rouge">
 ┌─────────────────┐
 │ Global LB │
 │ (Route53) │
 └────────┬────────┘
 │
 ┌────────────────────┼────────────────────┐
 │ │ │
 ┌────▼────┐ ┌────▼────┐ ┌────▼────┐
 │ us-west │ │ us-east │ │ eu-west │
 │ Region │ │ Region │ │ Region │
 └────┬────┘ └────┬────┘ └────┬────┘
 │ │ │
 ┌────▼─────────┐ ┌───▼──────────┐ ┌───▼──────────┐
 │ Pipeline │ │ Pipeline │ │ Pipeline │
 │ Cluster │ │ Cluster │ │ Cluster │
 │ │ │ │ │ │
 │ • 4 Feature │ │ • 4 Feature │ │ • 4 Feature │
 │ • 2 Acoustic │ │ • 2 Acoustic │ │ • 2 Acoustic │
 │ • 1 LM │ │ • 1 LM │ │ • 1 LM │
 │ • 2 Post │ │ • 2 Post │ │ • 2 Post │
 └──────────────┘ └──────────────┘ └──────────────┘
</code></p>

<h3 id="kubernetes-deployment">Kubernetes Deployment</h3>

<p>``yaml</p>
<h1 id="acoustic-model-deploymentyaml">acoustic-model-deployment.yaml</h1>
<p>apiVersion: apps/v1
kind: Deployment
metadata:
 name: acoustic-model
spec:
 replicas: 2 # Managed by HPA + custom controller
 selector:
 matchLabels:
 app: acoustic-model
 template:
 metadata:
 labels:
 app: acoustic-model
 spec:
 containers:</p>
<ul>
  <li>name: model-server
 image: speech-pipeline/acoustic-model:v1.2.3
 resources:
 requests:
 nvidia.com/gpu: 1
 cpu: “4”
 memory: “16Gi”
 limits:
 nvidia.com/gpu: 1
 cpu: “8”
 memory: “32Gi”
 env:</li>
  <li>name: BATCH_SIZE
 value: “16”</li>
  <li>name: MAX_WAIT_MS
 value: “10”
—
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
 name: acoustic-model-hpa
spec:
 scaleTargetRef:
 apiVersion: apps/v1
 kind: Deployment
 name: acoustic-model
 minReplicas: 1
 maxReplicas: 10
 metrics:</li>
  <li>type: Pods
 pods:
 metric:
 name: gpu_utilization
 target:
 type: AverageValue
 averageValue: “80”</li>
  <li>type: Pods
 pods:
 metric:
 name: queue_size
 target:
 type: AverageValue
 averageValue: “50”
``</li>
</ul>

<h3 id="model-optimization-techniques">Model Optimization Techniques</h3>

<p>``python
import torch
import tensorrt as trt
from onnx import onnx
import onnxruntime as ort</p>

<p>class ModelOptimizer:
 “"”Optimize models for production inference.”””</p>

<p>@staticmethod
 def quantize_model(model: torch.nn.Module, calibration_data):
 “””
 Quantize model to INT8 for faster inference.</p>

<p>Benefits:</p>
<ul>
  <li>4x smaller model size</li>
  <li>2-4x faster inference</li>
  <li>Cost: ~1-2% accuracy drop
 “””
 model.eval()</li>
</ul>

<p># Dynamic quantization (weights only)
 quantized_model = torch.quantization.quantize_dynamic(
 model,
 {torch.nn.Linear, torch.nn.Conv1d},
 dtype=torch.qint8
 )</p>

<p>return quantized_model</p>

<p>@staticmethod
 def export_to_onnx(model: torch.nn.Module, dummy_input: torch.Tensor, path: str):
 “””
 Export to ONNX for deployment.</p>

<p>Benefits:</p>
<ul>
  <li>Framework agnostic</li>
  <li>Optimized runtime (ONNX Runtime)</li>
  <li>TensorRT compilation
 “””
 model.eval()
 torch.onnx.export(
 model,
 dummy_input,
 path,
 input_names=[“audio_features”],
 output_names=[“logits”],
 dynamic_axes={
 “audio_features”: {0: “batch_size”, 1: “time”},
 “logits”: {0: “batch_size”, 1: “time”}
 },
 opset_version=14
 )</li>
</ul>

<p>@staticmethod
 def compile_tensorrt(onnx_path: str, engine_path: str):
 “””
 Compile ONNX model to TensorRT engine.</p>

<p>Benefits:</p>
<ul>
  <li>2-6x faster on NVIDIA GPUs</li>
  <li>Automatic kernel fusion</li>
  <li>Mixed precision (FP16)
 “””
 # Build TensorRT engine
 logger = trt.Logger(trt.Logger.WARNING)
 builder = trt.Builder(logger)
 network = builder.create_network(1 « int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
 parser = trt.OnnxParser(network, logger)</li>
</ul>

<p># Parse ONNX
 with open(onnx_path, ‘rb’) as model_file:
 parser.parse(model_file.read())</p>

<p># Build engine
 config = builder.create_builder_config()
 config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 « 30) # 1GB
 config.set_flag(trt.BuilderFlag.FP16) # Enable FP16</p>

<p>engine = builder.build_serialized_network(network, config)</p>

<p># Save engine
 with open(engine_path, ‘wb’) as f:
 f.write(engine)</p>

<p>return engine_path</p>

<h1 id="example-usage">Example usage</h1>
<p>def optimize_acoustic_model():
 “"”Full optimization pipeline.”””
 # 1. Load PyTorch model
 model = torch.load(“acoustic_model.pt”)</p>

<p># 2. Quantize (optional - for CPU deployment)
 quantized = ModelOptimizer.quantize_model(model, calibration_data=None)</p>

<p># 3. Export to ONNX
 dummy_input = torch.randn(1, 100, 80) # batch=1, time=100, features=80
 ModelOptimizer.export_to_onnx(model, dummy_input, “acoustic_model.onnx”)</p>

<p># 4. Compile to TensorRT (for GPU deployment)
 ModelOptimizer.compile_tensorrt(“acoustic_model.onnx”, “acoustic_model.trt”)</p>

<p>print(“Optimization complete!”)
 print(“- Original: ~500MB, ~20ms latency”)
 print(“- Quantized: ~125MB, ~15ms latency”)
 print(“- TensorRT: ~125MB, ~5ms latency (batched)”)
``</p>

<h2 id="scaling-strategies">Scaling Strategies</h2>

<h3 id="vertical-scaling---gpu-selection">Vertical Scaling - GPU Selection</h3>

<table>
  <thead>
    <tr>
      <th>GPU</th>
      <th>Memory</th>
      <th>FP16 TFLOPS</th>
      <th>Cost/hr</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>T4</td>
      <td>16GB</td>
      <td>65</td>
      <td>$0.35</td>
      <td>Small models, inference</td>
    </tr>
    <tr>
      <td>V100</td>
      <td>16GB</td>
      <td>125</td>
      <td>$2.50</td>
      <td>Medium models</td>
    </tr>
    <tr>
      <td>A10</td>
      <td>24GB</td>
      <td>125</td>
      <td>$0.75</td>
      <td>Cost-efficient inference</td>
    </tr>
    <tr>
      <td>A100</td>
      <td>40GB</td>
      <td>312</td>
      <td>$3.00</td>
      <td>Large models, training</td>
    </tr>
  </tbody>
</table>

<p><strong>Greedy choice:</strong> Match GPU to model size and throughput requirements.</p>

<h3 id="horizontal-scaling---auto-scaling-rules">Horizontal Scaling - Auto-scaling Rules</h3>

<p>``python
@dataclass
class ScalingRule:
 “"”Auto-scaling rule for speech pipeline.”””
 metric: str
 threshold: float
 scale_up_by: int
 cooldown_seconds: int</p>

<p>scaling_rules = [
 ScalingRule(
 metric=”gpu_utilization”,
 threshold=85.0,
 scale_up_by=1,
 cooldown_seconds=120
 ),
 ScalingRule(
 metric=”queue_size”,
 threshold=100,
 scale_up_by=2,
 cooldown_seconds=60
 ),
 ScalingRule(
 metric=”p95_latency_ms”,
 threshold=150.0,
 scale_up_by=1,
 cooldown_seconds=90
 ),
]
``</p>

<h2 id="real-world-case-study-google-assistant">Real-World Case Study: Google Assistant</h2>

<h3 id="googles-speech-pipeline">Google’s Speech Pipeline</h3>

<p>Google Assistant processes billions of speech requests daily with &lt;100ms latency.</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Multi-tiered inference:</strong>
    <ul>
      <li>On-device: Lightweight model for simple queries</li>
      <li>Edge: Medium model at regional data centers</li>
      <li>Cloud: Large model for complex queries</li>
    </ul>
  </li>
  <li><strong>Dynamic model selection:</strong>
    <ul>
      <li>Greedy choice: use smallest model that meets confidence threshold</li>
      <li>Fallback to larger model if confidence &lt; 0.9</li>
    </ul>
  </li>
  <li><strong>Batching strategy:</strong>
    <ul>
      <li>Dynamic batch sizes: 1-32 based on queue</li>
      <li>Adaptive timeout: 5-20ms based on SLA</li>
    </ul>
  </li>
  <li><strong>Resource allocation:</strong>
    <ul>
      <li>Per-region optimization</li>
      <li>TPU v4 pods for large models</li>
      <li>GPU for medium models</li>
      <li>CPU for feature extraction</li>
    </ul>
  </li>
</ol>

<p><strong>Results:</strong></p>
<ul>
  <li><strong>p95 latency:</strong> 85ms</li>
  <li><strong>Throughput:</strong> 100K+ rps per region</li>
  <li><strong>GPU utilization:</strong> 88%</li>
  <li><strong>Cost:</strong> &lt;$0.0005 per request</li>
</ul>

<h3 id="key-lessons">Key Lessons</h3>

<ol>
  <li><strong>Multi-tiered models:</strong> Use appropriate model size for each query</li>
  <li><strong>Aggressive batching:</strong> Critical for GPU efficiency</li>
  <li><strong>Edge deployment:</strong> Reduces latency and cost</li>
  <li><strong>Continuous profiling:</strong> Identify bottlenecks in real-time</li>
  <li><strong>Greedy allocation works:</strong> Simple strategy scales to billions of requests</li>
</ol>

<h2 id="cost-analysis">Cost Analysis</h2>

<h3 id="cost-breakdown-10k-rps-speech-pipeline">Cost Breakdown (10K rps speech pipeline)</h3>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Resources</th>
      <th>Cost/hr</th>
      <th>Cost/request</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Feature extraction</td>
      <td>40 CPUs</td>
      <td><code class="language-plaintext highlighter-rouge">4 | </code>0.00010</td>
      <td> </td>
    </tr>
    <tr>
      <td>Acoustic model</td>
      <td>10 T4 GPUs</td>
      <td><code class="language-plaintext highlighter-rouge">3.50 | </code>0.00009</td>
      <td> </td>
    </tr>
    <tr>
      <td>Language model</td>
      <td>5 T4 GPUs</td>
      <td><code class="language-plaintext highlighter-rouge">1.75 | </code>0.00004</td>
      <td> </td>
    </tr>
    <tr>
      <td>Post-processing</td>
      <td>20 CPUs</td>
      <td><code class="language-plaintext highlighter-rouge">2 | </code>0.00005</td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>Total</strong></td>
      <td> </td>
      <td><strong><code class="language-plaintext highlighter-rouge">11.25/hr** | **</code>0.00028</strong></td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p><strong>Optimization strategies:</strong></p>

<ol>
  <li><strong>Batching:</strong> Reduces GPU count by 50%
    <ul>
      <li>Before: 20 GPUs @ <code class="language-plaintext highlighter-rouge">0.35/hr = </code>7/hr</li>
      <li>After: 10 GPUs @ <code class="language-plaintext highlighter-rouge">0.35/hr = </code>3.50/hr</li>
      <li>Savings: <strong>50%</strong></li>
    </ul>
  </li>
  <li><strong>Model quantization:</strong> Reduces GPU count by 30%
    <ul>
      <li>INT8 models are 2-3x faster</li>
      <li>Need fewer GPUs for same throughput</li>
      <li>Savings: <strong>30%</strong></li>
    </ul>
  </li>
  <li><strong>Right-sizing instances:</strong>
    <ul>
      <li>Use T4 (<code class="language-plaintext highlighter-rouge">0.35/hr) instead of V100 (</code>2.50/hr)</li>
      <li>Savings: <strong>86%</strong></li>
    </ul>
  </li>
  <li><strong>Spot instances:</strong>
    <ul>
      <li>70% discount on interruptible workloads</li>
      <li>Use for batch processing, not real-time</li>
      <li>Savings: <strong>70%</strong> (for applicable workloads)</li>
    </ul>
  </li>
</ol>

<p><strong>Total optimized cost:</strong> $0.00012 per request (57% reduction)</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Speech pipelines have bottlenecks</strong> - identify and optimize the slowest stage first (greedy)</p>

<p>✅ <strong>Dynamic batching is critical</strong> for GPU efficiency - trade off latency vs throughput</p>

<p>✅ <strong>Continuous profiling</strong> identifies bottlenecks in real-time</p>

<p>✅ <strong>Greedy allocation strategy</strong> - add resources to bottleneck stage iteratively</p>

<p>✅ <strong>Model optimization</strong> (quantization, TensorRT) reduces compute requirements by 50%+</p>

<p>✅ <strong>Multi-region deployment</strong> reduces latency and improves availability</p>

<p>✅ <strong>Right-sizing GPU types</strong> saves 80%+ on costs</p>

<p>✅ <strong>Kubernetes + auto-scaling</strong> enables dynamic resource allocation</p>

<p>✅ <strong>Same principles as DSA</strong> - bottleneck (shorter line) limits throughput (area)</p>

<p>✅ <strong>Same principles as ML systems</strong> - greedy optimization for resource allocation</p>

<h3 id="connection-to-thematic-link-greedy-optimization-and-resource-management">Connection to Thematic Link: Greedy Optimization and Resource Management</h3>

<p>All three topics converge on the same fundamental insight:</p>

<p><strong>DSA (Container With Most Water):</strong></p>
<ul>
  <li>Two lines with heights h₁, h₂</li>
  <li>Container area = min(h₁, h₂) × width</li>
  <li>Bottleneck: shorter line limits capacity</li>
  <li><strong>Greedy:</strong> Move pointer at shorter line</li>
</ul>

<p><strong>ML System Design (Resource Allocation):</strong></p>
<ul>
  <li>Multiple ML jobs competing for GPUs</li>
  <li>System throughput limited by resource bottleneck</li>
  <li><strong>Greedy:</strong> Allocate to highest-priority job that fits</li>
</ul>

<p><strong>Speech Tech (Compute Allocation):</strong></p>
<ul>
  <li>Multi-stage pipeline with different latencies</li>
  <li>End-to-end throughput limited by slowest stage</li>
  <li><strong>Greedy:</strong> Allocate compute to bottleneck stage</li>
</ul>

<h3 id="universal-principle">Universal Principle</h3>

<p><strong>The Bottleneck Principle:</strong></p>
<blockquote>
  <p>In any multi-component system, the component with the lowest capacity determines the overall system throughput.</p>
</blockquote>

<p><strong>Greedy Optimization:</strong></p>
<blockquote>
  <p>Iteratively improve the bottleneck until:</p>
  <ol>
    <li>Target performance achieved</li>
    <li>Budget exhausted</li>
    <li>Bottleneck shifts to different component</li>
  </ol>
</blockquote>

<p>This principle applies to:</p>
<ul>
  <li>Algorithm design (two-pointer technique)</li>
  <li>Infrastructure (resource allocation)</li>
  <li>Production systems (pipeline optimization)</li>
  <li>Real-time processing (compute allocation)</li>
</ul>

<p><strong>Why it works:</strong></p>
<ul>
  <li><strong>Simple:</strong> Easy to implement and reason about</li>
  <li><strong>Fast:</strong> O(N) time complexity</li>
  <li><strong>Effective:</strong> Proven to work at scale (Google, Meta, etc.)</li>
  <li><strong>Robust:</strong> Handles dynamic workloads and changing bottlenecks</li>
</ul>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0013-compute-allocation-for-speech-models/">arunbaby.com/speech-tech/0013-compute-allocation-for-speech-models</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#compute-allocation" class="page__taxonomy-item p-category" rel="tag">compute-allocation</a><span class="sep">, </span>
    
      <a href="/tags/#inference-optimization" class="page__taxonomy-item p-category" rel="tag">inference-optimization</a><span class="sep">, </span>
    
      <a href="/tags/#optimization" class="page__taxonomy-item p-category" rel="tag">optimization</a><span class="sep">, </span>
    
      <a href="/tags/#real-time-processing" class="page__taxonomy-item p-category" rel="tag">real-time-processing</a><span class="sep">, </span>
    
      <a href="/tags/#speech-pipeline" class="page__taxonomy-item p-category" rel="tag">speech-pipeline</a><span class="sep">, </span>
    
      <a href="/tags/#tts" class="page__taxonomy-item p-category" rel="tag">tts</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0013-container-with-most-water/" rel="permalink">Container With Most Water
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master the two-pointer greedy technique that powers resource optimization in production ML systems.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0013-resource-allocation-for-ml/" rel="permalink">Resource Allocation for ML
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          28 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Build production ML infrastructure that dynamically allocates resources using greedy optimization to maximize throughput and minimize costs.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0013-multi-step-reasoning/" rel="permalink">Multi-Step Reasoning
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Thinking Fast and Slow: How to make LLMs stop guessing and start solving.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Compute+Allocation+for+Speech+Models%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0013-compute-allocation-for-speech-models%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0013-compute-allocation-for-speech-models%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0013-compute-allocation-for-speech-models/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0012-multi-speaker-asr/" class="pagination--pager" title="Multi-Speaker ASR">Previous</a>
    
    
      <a href="/speech-tech/0014-multi-model-speech-ensemble/" class="pagination--pager" title="Multi-model Speech Ensemble">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
