<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>End-to-End Speech Model Design - Arun Baby</title>
<meta name="description" content="Goodbye HMMs. Goodbye Phonemes. Goodbye Lexicons. We are teaching the machine to Listen, Attend, and Spell.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="End-to-End Speech Model Design">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0027-end-to-end-speech-model-design/">


  <meta property="og:description" content="Goodbye HMMs. Goodbye Phonemes. Goodbye Lexicons. We are teaching the machine to Listen, Attend, and Spell.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="End-to-End Speech Model Design">
  <meta name="twitter:description" content="Goodbye HMMs. Goodbye Phonemes. Goodbye Lexicons. We are teaching the machine to Listen, Attend, and Spell.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0027-end-to-end-speech-model-design/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0027-end-to-end-speech-model-design/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="End-to-End Speech Model Design">
    <meta itemprop="description" content="Goodbye HMMs. Goodbye Phonemes. Goodbye Lexicons. We are teaching the machine to Listen, Attend, and Spell.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0027-end-to-end-speech-model-design/" itemprop="url">End-to-End Speech Model Design
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem-statement">Problem Statement</a></li><li><a href="#fundamentals-the-three-pillars-of-e2e">Fundamentals: The Three Pillars of E2E</a></li><li><a href="#architecture-1-listen-attend-and-spell-las">Architecture 1: Listen, Attend and Spell (LAS)</a><ul><li><a href="#the-listener-encoder">The Listener (Encoder)</a></li><li><a href="#the-speller-decoder">The Speller (Decoder)</a></li></ul></li><li><a href="#architecture-2-rnn-transducer-rnn-t">Architecture 2: RNN-Transducer (RNN-T)</a><ul><li><a href="#the-components">The Components</a></li><li><a href="#the-mathematics-of-the-blank-token">The Mathematics of the Blank Token</a></li></ul></li><li><a href="#deep-dive-end-of-sentence-eos-detection">Deep Dive: End-of-Sentence (EOS) Detection</a><ul><li><a href="#1-voice-activity-detection-vad">1. Voice Activity Detection (VAD)</a></li><li><a href="#2-decoder-based-eos">2. Decoder-based EOS</a></li><li><a href="#3-semantic-endpointing">3. Semantic Endpointing</a></li></ul></li><li><a href="#deep-dive-shallow-fusion-math">Deep Dive: Shallow Fusion Math</a></li><li><a href="#deep-dive-the-cocktail-party-problem-multi-speaker">Deep Dive: The Cocktail Party Problem (Multi-Speaker)</a></li><li><a href="#deep-dive-the-alignment-problem">Deep Dive: The Alignment Problem</a></li><li><a href="#deep-dive-connectionist-temporal-classification-ctc">Deep Dive: Connectionist Temporal Classification (CTC)</a><ul><li><a href="#the-logic-of-ctc">The Logic of CTC</a></li><li><a href="#the-ctc-loss-function">The CTC Loss Function</a></li><li><a href="#limitations-of-ctc">Limitations of CTC</a></li></ul></li><li><a href="#deep-dive-rnn-transducer-rnn-t">Deep Dive: RNN-Transducer (RNN-T)</a><ul><li><a href="#the-architecture">The Architecture</a></li><li><a href="#the-decoding-grid">The Decoding Grid</a></li><li><a href="#training-rnn-t">Training RNN-T</a></li></ul></li><li><a href="#architecture-3-conformer-cnn--transformer">Architecture 3: Conformer (CNN + Transformer)</a></li><li><a href="#implementation-a-conformer-block-in-pytorch">Implementation: A Conformer Block in PyTorch</a></li><li><a href="#deep-dive-streaming-constraints-the-lookahead">Deep Dive: Streaming Constraints (The Lookahead)</a></li><li><a href="#deep-dive-on-device-asr-tinyml">Deep Dive: On-Device ASR (TinyML)</a><ul><li><a href="#1-quantization">1. Quantization</a></li><li><a href="#2-svd-singular-value-decomposition">2. SVD (Singular Value Decomposition)</a></li></ul></li><li><a href="#deep-dive-integrating-language-models">Deep Dive: Integrating Language Models</a><ul><li><a href="#1-shallow-fusion">1. Shallow Fusion</a></li><li><a href="#2-deep-fusion">2. Deep Fusion</a></li><li><a href="#3-cold-fusion">3. Cold Fusion</a></li></ul></li><li><a href="#deep-dive-beam-search-decoding">Deep Dive: Beam Search Decoding</a><ul><li><a href="#1-greedy-decoding">1. Greedy Decoding</a></li><li><a href="#2-beam-search">2. Beam Search</a></li><li><a href="#3-prefix-beam-search-for-ctc">3. Prefix Beam Search (for CTC)</a></li></ul></li><li><a href="#deep-dive-specaugment-details">Deep Dive: SpecAugment Details</a><ul><li><a href="#1-time-masking">1. Time Masking</a></li><li><a href="#2-frequency-masking">2. Frequency Masking</a></li><li><a href="#3-time-warping">3. Time Warping</a></li></ul></li><li><a href="#training-considerations">Training Considerations</a><ul><li><a href="#1-the-ctc-loss">1. The CTC Loss</a></li><li><a href="#2-specaugment">2. SpecAugment</a></li><li><a href="#3-curriculum-learning">3. Curriculum Learning</a></li><li><a href="#4-self-supervised-pre-training-wav2vec-20">4. Self-Supervised Pre-training (Wav2Vec 2.0)</a></li></ul></li><li><a href="#common-failure-modes">Common Failure Modes</a></li><li><a href="#state-of-the-art-whisper-weakly-supervised">State-of-the-Art: Whisper (Weakly Supervised)</a></li><li><a href="#deep-dive-training-loop-implementation">Deep Dive: Training Loop Implementation</a></li><li><a href="#top-interview-questions">Top Interview Questions</a></li><li><a href="#deep-dive-whisper-architecture-details">Deep Dive: Whisper Architecture Details</a><ul><li><a href="#1-the-data">1. The Data</a></li><li><a href="#2-the-model">2. The Model</a></li><li><a href="#3-the-multitask-format">3. The Multitask Format</a></li></ul></li><li><a href="#deep-dive-word-error-rate-wer">Deep Dive: Word Error Rate (WER)</a></li><li><a href="#case-study-the-evolution-of-nvidias-asr-models">Case Study: The Evolution of NVIDIA’s ASR Models</a><ul><li><a href="#1-jasper-just-another-speech-recognizer">1. Jasper (Just Another Speech Recognizer)</a></li><li><a href="#2-quartznet">2. QuartzNet</a></li><li><a href="#3-citrinet">3. Citrinet</a></li></ul></li><li><a href="#deep-dive-hardware-acceleration-tpu-vs-gpu">Deep Dive: Hardware Acceleration (TPU vs GPU)</a><ul><li><a href="#1-tpus-google">1. TPUs (Google)</a></li><li><a href="#2-gpus-nvidia">2. GPUs (NVIDIA)</a></li></ul></li><li><a href="#further-reading">Further Reading</a></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Goodbye HMMs. Goodbye Phonemes. Goodbye Lexicons. We are teaching the machine to Listen, Attend, and Spell.</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p>Traditional ASR systems (Hybrid HMM-DNN) are a “Frankenstein” of separate components:</p>
<ol>
  <li><strong>Acoustic Model:</strong> Maps audio to phonemes.</li>
  <li><strong>Lexicon:</strong> Maps phonemes to words.</li>
  <li><strong>Language Model:</strong> Maps words to sentences.</li>
  <li><strong>G2P (Grapheme-to-Phoneme):</strong> Handles unknown words.</li>
</ol>

<p><strong>The Problem:</strong> Errors cascade. If the G2P fails, the Lexicon fails. If the Lexicon fails, the ASR fails. Optimizing one component doesn’t necessarily improve the whole system (WER).</p>

<p><strong>The Solution:</strong> <strong>End-to-End (E2E) ASR</strong>.
One Neural Network. Input: Audio. Output: Text.
Optimize a single loss function.</p>

<h2 id="fundamentals-the-three-pillars-of-e2e">Fundamentals: The Three Pillars of E2E</h2>

<p>There are three main architectures for E2E ASR. Each solves the alignment problem (Audio Length (T) » Text Length (L)) differently.</p>

<ol>
  <li><strong>CTC (Connectionist Temporal Classification):</strong>
    <ul>
      <li><strong>Mechanism:</strong> Predicts a token (or “blank”) for every frame. Merges repeats.</li>
      <li><strong>Assumption:</strong> Frames are conditionally independent.</li>
      <li><strong>Pros:</strong> Fast, non-autoregressive.</li>
      <li><strong>Cons:</strong> Weak Language Modeling capabilities.</li>
    </ul>
  </li>
  <li><strong>AED (Attention-based Encoder-Decoder) / LAS:</strong>
    <ul>
      <li><strong>Mechanism:</strong> “Listen, Attend and Spell”. Encoder processes audio. Decoder attends to encoder outputs to generate text.</li>
      <li><strong>Pros:</strong> Best accuracy (Global context).</li>
      <li><strong>Cons:</strong> Not streaming friendly (needs full audio). (O(T \cdot L)) complexity.</li>
    </ul>
  </li>
  <li><strong>RNN-T (Transducer):</strong>
    <ul>
      <li><strong>Mechanism:</strong> Combines an Acoustic Encoder and a Label Encoder (LM) via a Joint Network.</li>
      <li><strong>Pros:</strong> Streaming friendly. Strong LM integration.</li>
      <li><strong>Cons:</strong> Memory intensive training.</li>
    </ul>
  </li>
</ol>

<h2 id="architecture-1-listen-attend-and-spell-las">Architecture 1: Listen, Attend and Spell (LAS)</h2>

<p>LAS (Google, 2015) was the breakthrough that proved E2E could match Hybrid systems.</p>

<p><code class="language-plaintext highlighter-rouge">ascii
 [ "C", "A", "T" ] &lt;-- Output
 ^
 |
 +-------------+
 | Speller | (Decoder / RNN)
 | (Attention) |
 +-------------+
 ^
 | Context Vector
 +-------------+
 | Listener | (Encoder / Pyramidal RNN)
 +-------------+
 ^
 |
 [ Spectrogram ] &lt;-- Input
</code></p>

<h3 id="the-listener-encoder">The Listener (Encoder)</h3>
<p>A deep LSTM (or Conformer) that converts low-level features (Filterbanks) into high-level acoustic features.
<strong>Pyramidal Structure:</strong> We must reduce the time resolution. Audio is 100 frames/sec. Text is ~3 chars/sec.
The Listener performs <code class="language-plaintext highlighter-rouge">subsampling</code> (stride 2 pooling) to reduce (T) by 4x or 8x.</p>

<h3 id="the-speller-decoder">The Speller (Decoder)</h3>
<p>An RNN that generates one character at a time.
At step (i), it computes an <strong>Attention</strong> score over all encoder states (h).
[ c_i = \sum \alpha_{i,j} h_j ]
It uses (c_i) and the previous character (y_{i-1}) to predict (y_i).</p>

<h2 id="architecture-2-rnn-transducer-rnn-t">Architecture 2: RNN-Transducer (RNN-T)</h2>

<p>RNN-T is the industry standard for <strong>Streaming ASR</strong> (Siri, Assistant).</p>

<p><code class="language-plaintext highlighter-rouge">ascii
 [ Softmax ]
 ^
 |
 +-------------+
 | Joint Net | (Feed Forward)
 +-------------+
 ^ ^
 | |
 +-------------+ +-------------+
 | Encoder | | Prediction |
 | (Audio) | | Network |
 +-------------+ | (Text) |
 ^ +-------------+
 | ^
 [ Spectrogram ] |
 [ Previous Token ]
</code></p>

<h3 id="the-components">The Components</h3>
<ol>
  <li><strong>Encoder (Transcription Network):</strong> Analogous to the Acoustic Model. Processes audio.</li>
  <li><strong>Prediction Network:</strong> Analogous to the Language Model. Processes the history of non-blank tokens.</li>
  <li><strong>Joint Network:</strong> Combines them.
 [ J(t, u) = \text{ReLU}(W_e h_t + W_p g_u) ]
 [ P(y | t, u) = \text{Softmax}(W_o J(t, u)) ]</li>
</ol>

<p><strong>Why it wins:</strong> It is <strong>monotonic</strong>. It can only move forward in time. This makes it perfect for streaming.</p>

<h3 id="the-mathematics-of-the-blank-token">The Mathematics of the Blank Token</h3>

<p>Why do we need a blank?
Consider the word “too”.
Phonetically: <code class="language-plaintext highlighter-rouge">t</code> -&gt; <code class="language-plaintext highlighter-rouge">u</code> -&gt; <code class="language-plaintext highlighter-rouge">u</code>.
Acoustically (10ms frames): <code class="language-plaintext highlighter-rouge">t t t u u u u u u</code>.
If we just collapse repeats: <code class="language-plaintext highlighter-rouge">t u</code>. We lost the second ‘o’.</p>

<p><strong>With Blank (<code class="language-plaintext highlighter-rouge">-</code>):</strong>
<code class="language-plaintext highlighter-rouge">t t t - u u u - u u u</code> -&gt; <code class="language-plaintext highlighter-rouge">t - u - u</code> -&gt; <code class="language-plaintext highlighter-rouge">tuu</code> (“too”).
The blank is a <strong>mandatory separator</strong> for repeated characters.</p>

<p><strong>Probability Distribution:</strong>
Usually, the Blank probability dominates.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">P(-) &gt; 0.9</code> for most frames (silence or steady state).</li>
  <li><code class="language-plaintext highlighter-rouge">P(char)</code> spikes only at the transition boundaries.</li>
  <li>This “Spiky” behavior is characteristic of CTC.</li>
</ul>

<h2 id="deep-dive-end-of-sentence-eos-detection">Deep Dive: End-of-Sentence (EOS) Detection</h2>

<p>In streaming ASR, the user never presses “Stop”. The model must decide when to stop listening.</p>

<h3 id="1-voice-activity-detection-vad">1. Voice Activity Detection (VAD)</h3>
<ul>
  <li><strong>Energy-based:</strong> If volume &lt; threshold for 500ms.</li>
  <li><strong>Model-based:</strong> A small NN (Silero VAD) classifies frames as <code class="language-plaintext highlighter-rouge">Speech</code> or <code class="language-plaintext highlighter-rouge">Silence</code>.</li>
  <li><strong>Logic:</strong> <code class="language-plaintext highlighter-rouge">if silence_duration &gt; 700ms: send_eos()</code>.</li>
</ul>

<h3 id="2-decoder-based-eos">2. Decoder-based EOS</h3>
<ul>
  <li>The ASR model itself can predict a special <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token.</li>
  <li><strong>Problem:</strong> E2E models are trained on trimmed audio. They rarely see long silences. They tend to hallucinate during silence.</li>
  <li><strong>Fix:</strong> Train with “Endpointing” data (audio with trailing silence).</li>
</ul>

<h3 id="3-semantic-endpointing">3. Semantic Endpointing</h3>
<ul>
  <li>Wait for the NLU to confirm the command is complete.</li>
  <li>“Turn off the…” (Wait)</li>
  <li>“…lights” (Execute).</li>
  <li>If the user pauses after “lights”, the NLU says “Complete Intent”, so we close the mic.</li>
</ul>

<h2 id="deep-dive-shallow-fusion-math">Deep Dive: Shallow Fusion Math</h2>

<p>Shallow Fusion is the most common way to boost ASR with an external Language Model (trained on text).</p>

<p><strong>The Equation:</strong>
[ \hat{y} = \text{argmax}<em>y \left( \log P</em>{ASR}(y|x) + \lambda \log P_{LM}(y) + \beta \cdot \text{len}(y) \right) ]</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>**(P_{ASR}(y</td>
          <td>x)):** The probability from the E2E model (AM).</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>(P_{LM}(y)):</strong> The probability from the external LM (e.g., GPT-2).</li>
  <li><strong>(\lambda) (Lambda):</strong> The weight of the LM (usually 0.1 - 0.5).</li>
  <li><strong>(\beta) (Beta):</strong> Length reward. E2E models tend to prefer short sentences. This forces them to generate longer output.</li>
</ul>

<p><strong>Why it works:</strong>
The ASR model is good at acoustics (“It sounds like ‘red’”).
The LM is good at grammar (“‘The read apple’ is wrong, ‘The red apple’ is right”).
By combining them, we fix homophone errors.</p>

<h2 id="deep-dive-the-cocktail-party-problem-multi-speaker">Deep Dive: The Cocktail Party Problem (Multi-Speaker)</h2>

<p>Standard ASR fails when two people talk at once.
<strong>Solution:</strong> Permutation Invariant Training (PIT).</p>

<ol>
  <li><strong>Output:</strong> The model outputs <strong>two</strong> streams of text: (y_1) and (y_2).</li>
  <li><strong>Loss:</strong> We calculate the loss for both permutations:
    <ul>
      <li>Loss A: (L(y_1, \text{Ref}_1) + L(y_2, \text{Ref}_2))</li>
      <li>Loss B: (L(y_1, \text{Ref}_2) + L(y_2, \text{Ref}_1))</li>
    </ul>
  </li>
  <li><strong>Update:</strong> We backpropagate the <strong>minimum</strong> of Loss A and Loss B.
 [ L = \min(\text{Loss A}, \text{Loss B}) ]</li>
</ol>

<p>This teaches the model to separate the speakers without forcing it to assign “Speaker 1” to a specific output channel.</p>

<h2 id="deep-dive-the-alignment-problem">Deep Dive: The Alignment Problem</h2>

<p>The core difficulty in ASR is that we don’t know which audio frame corresponds to which character.</p>
<ul>
  <li><strong>HMM-GMM (Old):</strong> Used <strong>Viterbi Alignment</strong> (Hard alignment). We explicitly assigned <code class="language-plaintext highlighter-rouge">frame_5</code> to phoneme <code class="language-plaintext highlighter-rouge">/k/</code>.</li>
  <li><strong>E2E (New):</strong> Uses <strong>Soft Alignment</strong> (Attention/CTC). The model learns a probability distribution over alignments.</li>
  <li><strong>CTC:</strong> Sums over all valid monotonic alignments.</li>
  <li><strong>Attention:</strong> Computes a “Soft” weight vector for every output step.</li>
</ul>

<h2 id="deep-dive-connectionist-temporal-classification-ctc">Deep Dive: Connectionist Temporal Classification (CTC)</h2>

<p>CTC is the “Hello World” of E2E ASR. It solves the problem: “I have 1000 audio frames but only 50 characters. How do I align them?”</p>

<h3 id="the-logic-of-ctc">The Logic of CTC</h3>
<p>CTC introduces a special <strong>Blank Token</strong> (<code class="language-plaintext highlighter-rouge">&lt;eps&gt;</code> or <code class="language-plaintext highlighter-rouge">-</code>).
It predicts a probability distribution over <code class="language-plaintext highlighter-rouge">Vocabulary + {Blank}</code> for every frame.</p>

<p><strong>Decoding Rules:</strong></p>
<ol>
  <li><strong>Collapse Repeats:</strong> <code class="language-plaintext highlighter-rouge">aa</code> -&gt; <code class="language-plaintext highlighter-rouge">a</code>.</li>
  <li><strong>Remove Blanks:</strong> <code class="language-plaintext highlighter-rouge">-</code> -&gt; `.</li>
</ol>

<p><strong>Example:</strong></p>
<ul>
  <li>Audio: <code class="language-plaintext highlighter-rouge">[frame1, frame2, frame3, frame4, frame5]</code></li>
  <li>Model Output: <code class="language-plaintext highlighter-rouge">c</code>, <code class="language-plaintext highlighter-rouge">c</code>, <code class="language-plaintext highlighter-rouge">-</code>, <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">t</code></li>
  <li>Collapse: <code class="language-plaintext highlighter-rouge">c</code>, <code class="language-plaintext highlighter-rouge">-</code>, <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">t</code></li>
  <li>Remove Blanks: <code class="language-plaintext highlighter-rouge">cat</code></li>
</ul>

<h3 id="the-ctc-loss-function">The CTC Loss Function</h3>
<p>We don’t know the <em>exact</em> alignment (e.g., did “c” start at frame 1 or 2?).
CTC sums the probability of <strong>all valid alignments</strong>.</p>

<p>[ P(Y|X) = \sum_{A \in \mathcal{B}^{-1}(Y)} P(A|X) ]
Where (\mathcal{B}) is the collapse function.</p>

<p><strong>Forward-Backward Algorithm:</strong>
Calculating the sum of exponentially many paths is hard. We use Dynamic Programming.</p>
<ul>
  <li>(\alpha_t(s)): Probability of generating the first (s) tokens of the target by time (t).</li>
  <li>Similar to HMM training.</li>
  <li><strong>Complexity:</strong> (O(T \cdot L)).</li>
</ul>

<h3 id="limitations-of-ctc">Limitations of CTC</h3>
<ol>
  <li><strong>Conditional Independence:</strong> It assumes the prediction at time (t) depends <em>only</em> on the audio at time (t). It doesn’t know that “q” is usually followed by “u”.</li>
  <li><strong>Spiky Output:</strong> CTC tends to wait until it is 100% sure, emits a spike, and then predicts Blanks. This makes it bad for timestamp estimation.</li>
</ol>

<h2 id="deep-dive-rnn-transducer-rnn-t">Deep Dive: RNN-Transducer (RNN-T)</h2>

<p>RNN-T fixes the “Conditional Independence” problem of CTC.</p>

<h3 id="the-architecture">The Architecture</h3>
<p>It has two encoders:</p>
<ol>
  <li><strong>Audio Encoder:</strong> (f^{enc} = \text{Encoder}(x_t))</li>
  <li><strong>Label Encoder (Prediction Network):</strong> (g^{pred} = \text{PredNet}(y_{u-1}))</li>
</ol>

<p>The <strong>Joint Network</strong> combines them:
[ z_{t,u} = \text{Joint}(f^{enc}_t, g^{pred}_u) ]</p>

<h3 id="the-decoding-grid">The Decoding Grid</h3>
<p>Imagine a grid where:</p>
<ul>
  <li><strong>X-axis:</strong> Time frames ((T)).</li>
  <li><strong>Y-axis:</strong> Output tokens ((U)).</li>
</ul>

<p>We start at ((0,0)). At each step, we can:</p>
<ol>
  <li><strong>Emit a Token:</strong> Move Up ((t, u+1)). (We output a character, stay at the same audio frame).</li>
  <li><strong>Emit Blank:</strong> Move Right ((t+1, u)). (We consume an audio frame, output nothing).</li>
</ol>

<p><strong>Why is this better?</strong>
The Prediction Network acts as an <strong>Internal Language Model</strong>. It knows that after “q”, “u” is likely, regardless of the audio.
This allows RNN-T to model language structure much better than CTC.</p>

<h3 id="training-rnn-t">Training RNN-T</h3>
<p>The loss function is the negative log-likelihood of the target sequence.
Like CTC, we sum over all valid paths through the grid.
<strong>Memory Issue:</strong> The Joint Network computes a tensor of size ((B, T, U, V)).</p>
<ul>
  <li>(B): Batch Size (32)</li>
  <li>(T): Time Frames (1000)</li>
  <li>(U): Text Length (100)</li>
  <li>(V): Vocabulary (1000)</li>
  <li>Total: (3.2 \times 10^9) floats = ~12GB memory!
<strong>Fix:</strong> Use <strong>Pruned RNN-T</strong> (k2/icefall) or optimized CUDA kernels (warp-rnnt) that only compute the diagonal of the grid.</li>
</ul>

<h2 id="architecture-3-conformer-cnn--transformer">Architecture 3: Conformer (CNN + Transformer)</h2>

<p>Whether you use LAS or RNN-T, you need a powerful <strong>Encoder</strong>.
Google introduced the <strong>Conformer</strong>, combining the best of both worlds:</p>
<ul>
  <li><strong>Transformers:</strong> Good at capturing global context (Long-range dependencies).</li>
  <li><strong>CNNs:</strong> Good at capturing local context (Edges, Formants).</li>
</ul>

<p><strong>The Conformer Block:</strong></p>
<ol>
  <li>Feed Forward Module.</li>
  <li>Multi-Head Self Attention.</li>
  <li>Convolution Module.</li>
  <li>Feed Forward Module.</li>
  <li>Layer Norm.</li>
</ol>

<p>This “Macaron” style (FFN at start and end) proved superior to standard Transformers.</p>

<h2 id="implementation-a-conformer-block-in-pytorch">Implementation: A Conformer Block in PyTorch</h2>

<p>``python
import torch
import torch.nn as nn</p>

<p>class ConformerBlock(nn.Module):
 def <strong>init</strong>(self, d_model, n_head, kernel_size, dropout=0.1):
 super().<strong>init</strong>()</p>

<p># 1. Feed Forward (Half Step)
 self.ff1 = nn.Sequential(
 nn.LayerNorm(d_model),
 nn.Linear(d_model, d_model * 4),
 nn.SiLU(), # Swish
 nn.Dropout(dropout),
 nn.Linear(d_model * 4, d_model),
 nn.Dropout(dropout)
 )</p>

<p># 2. Self-Attention
 self.attn_norm = nn.LayerNorm(d_model)
 self.attn = nn.MultiheadAttention(d_model, n_head, dropout=dropout)</p>

<p># 3. Convolution Module
 self.conv_module = nn.Sequential(
 nn.LayerNorm(d_model),
 # Pointwise
 nn.Conv1d(d_model, d_model * 2, 1), 
 nn.GLU(dim=1),
 # Depthwise
 nn.Conv1d(d_model, d_model, kernel_size, groups=d_model, padding=kernel_size//2),
 nn.BatchNorm1d(d_model),
 nn.SiLU(),
 # Pointwise
 nn.Conv1d(d_model, d_model, 1),
 nn.Dropout(dropout)
 )</p>

<p># 4. Feed Forward (Half Step)
 self.ff2 = nn.Sequential(
 nn.LayerNorm(d_model),
 nn.Linear(d_model, d_model * 4),
 nn.SiLU(),
 nn.Dropout(dropout),
 nn.Linear(d_model * 4, d_model),
 nn.Dropout(dropout)
 )</p>

<p>self.final_norm = nn.LayerNorm(d_model)</p>

<p>def forward(self, x):
 # x: [Time, Batch, Dim]</p>

<p># Macaron Style: 0.5 * FF1
 x = x + 0.5 * self.ff1(x)</p>

<p># Attention
 residual = x
 x = self.attn_norm(x)
 x, _ = self.attn(x, x, x)
 x = residual + x</p>

<p># Convolution (Requires [Batch, Dim, Time])
 residual = x
 x = x.permute(1, 2, 0) # T, B, D -&gt; B, D, T
 x = self.conv_module(x)
 x = x.permute(2, 0, 1) # B, D, T -&gt; T, B, D
 x = residual + x</p>

<p># Macaron Style: 0.5 * FF2
 x = x + 0.5 * self.ff2(x)</p>

<p>return self.final_norm(x)</p>

<h1 id="test">Test</h1>
<p>block = ConformerBlock(d_model=256, n_head=4, kernel_size=31)
x = torch.randn(100, 8, 256) # Time=100, Batch=8, Dim=256
y = block(x)
print(y.shape) # torch.Size([100, 8, 256])
``</p>

<h2 id="deep-dive-streaming-constraints-the-lookahead">Deep Dive: Streaming Constraints (The Lookahead)</h2>

<p>In a bidirectional LSTM or Transformer, the model sees the future.
In Streaming, we can’t see the future.
<strong>Compromise:</strong> Limited Lookahead.</p>
<ul>
  <li><strong>Latency:</strong> If we look ahead 300ms, we add 300ms latency.</li>
  <li><strong>Accuracy:</strong> If we look ahead 0ms, accuracy drops (we can’t distinguish “The” vs “A” without context).</li>
  <li><strong>Sweet Spot:</strong> 100ms - 300ms lookahead.</li>
</ul>

<p><strong>Streaming Conformer:</strong>
Uses <strong>Block Processing</strong>.</p>
<ul>
  <li>It processes a “Central Block” (current audio).</li>
  <li>It attends to a “Left Context” (past, cached).</li>
  <li>It attends to a “Right Context” (future, lookahead).</li>
</ul>

<h2 id="deep-dive-on-device-asr-tinyml">Deep Dive: On-Device ASR (TinyML)</h2>

<p>Running ASR on a Pixel phone (without cloud) requires extreme optimization.</p>

<h3 id="1-quantization">1. Quantization</h3>
<ul>
  <li>Convert weights from <code class="language-plaintext highlighter-rouge">float32</code> (4 bytes) to <code class="language-plaintext highlighter-rouge">int8</code> (1 byte).</li>
  <li><strong>Size:</strong> 4x smaller.</li>
  <li><strong>Speed:</strong> 2-3x faster (using NEON/DSP instructions).</li>
  <li><strong>Accuracy:</strong> &lt; 1% WER degradation if done correctly (Quantization Aware Training).</li>
</ul>

<h3 id="2-svd-singular-value-decomposition">2. SVD (Singular Value Decomposition)</h3>
<ul>
  <li>Factorize large weight matrices into two smaller matrices.</li>
  <li>(W (1024 \times 1024) \approx U (1024 \times 128) \times V (128 \times 1024)).</li>
  <li>Reduces parameters by 4x.</li>
</ul>

<h2 id="deep-dive-integrating-language-models">Deep Dive: Integrating Language Models</h2>

<p>E2E models learn “Audio -&gt; Text” directly. But text data is much more abundant than audio-text pairs. How do we use a text-only LM (like GPT) to improve ASR?</p>

<h3 id="1-shallow-fusion">1. Shallow Fusion</h3>
<ul>
  <li><strong>Inference Time Only.</strong></li>
  <li>We linearly interpolate the scores during Beam Search.
 [ \text{Score}(y) = \log P_{ASR}(y|x) + \lambda \log P_{LM}(y) ]</li>
  <li><strong>Pros:</strong> Simple. No retraining of ASR.</li>
  <li><strong>Cons:</strong> The ASR model doesn’t know about the LM.</li>
</ul>

<h3 id="2-deep-fusion">2. Deep Fusion</h3>
<ul>
  <li><strong>Training Time Integration.</strong></li>
  <li>We fuse the hidden states of the LM into the ASR decoder.</li>
  <li><strong>Mechanism:</strong> Concatenate <code class="language-plaintext highlighter-rouge">hidden_ASR</code> and <code class="language-plaintext highlighter-rouge">hidden_LM</code>, then pass through a Gating mechanism.</li>
  <li><strong>Pros:</strong> Better integration.</li>
  <li><strong>Cons:</strong> Requires retraining.</li>
</ul>

<h3 id="3-cold-fusion">3. Cold Fusion</h3>
<ul>
  <li><strong>Idea:</strong> Train the ASR decoder <em>conditional</em> on the LM state.</li>
  <li>The ASR decoder learns to “correct” the LM or rely on it when the audio is noisy.</li>
</ul>

<h2 id="deep-dive-beam-search-decoding">Deep Dive: Beam Search Decoding</h2>

<p>How do we turn probabilities into text?
<code class="language-plaintext highlighter-rouge">P(c | audio)</code> gives us a matrix of probabilities.</p>

<h3 id="1-greedy-decoding">1. Greedy Decoding</h3>
<ul>
  <li><strong>Algorithm:</strong> At each step, pick the token with the highest probability.</li>
  <li><strong>Problem:</strong> It makes local decisions. It can’t backtrack.</li>
  <li><strong>Example:</strong> Audio sounds like “The red…”.</li>
  <li>Greedy: “The read” (because ‘read’ is more common).</li>
  <li>Next word is “apple”.</li>
  <li>Greedy is stuck with “The read apple”.</li>
</ul>

<h3 id="2-beam-search">2. Beam Search</h3>
<ul>
  <li><strong>Algorithm:</strong> Keep the top (K) (Beam Width) most likely hypotheses at each step.</li>
  <li><strong>Example (K=2):</strong></li>
  <li>Step 1: [“The”, “A”]</li>
  <li>Step 2: [“The red”, “The read”, “A red”, “A read”] -&gt; Prune to top 2 -&gt; [“The red”, “The read”]</li>
  <li>Step 3: [“The red apple”, …]</li>
  <li><strong>Result:</strong> Finds the global optimum (mostly).</li>
</ul>

<h3 id="3-prefix-beam-search-for-ctc">3. Prefix Beam Search (for CTC)</h3>
<p>CTC is tricky because multiple paths map to the same string (<code class="language-plaintext highlighter-rouge">aa</code> -&gt; <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">a</code> -&gt; <code class="language-plaintext highlighter-rouge">a</code>).</p>
<ul>
  <li>We merge paths that result in the same prefix.</li>
  <li>We track two probabilities for each prefix:
    <ol>
      <li><code class="language-plaintext highlighter-rouge">P_b</code>: Probability ending in Blank.</li>
      <li><code class="language-plaintext highlighter-rouge">P_nb</code>: Probability ending in Non-Blank.</li>
    </ol>
  </li>
</ul>

<h2 id="deep-dive-specaugment-details">Deep Dive: SpecAugment Details</h2>

<p>SpecAugment is the “Dropout” of Speech.</p>

<h3 id="1-time-masking">1. Time Masking</h3>
<ul>
  <li><strong>Operation:</strong> Select a time interval ([t, t+\tau)) and set all frequency channels to mean/zero.</li>
  <li><strong>Effect:</strong> Forces the model to rely on context. If “banana” is masked, it infers it from “I ate a …”.</li>
</ul>

<h3 id="2-frequency-masking">2. Frequency Masking</h3>
<ul>
  <li><strong>Operation:</strong> Select a frequency band ([f, f+\nu)) and set all time steps to mean/zero.</li>
  <li><strong>Effect:</strong> Makes the model robust to microphone variations (e.g., loss of high frequencies).</li>
</ul>

<h3 id="3-time-warping">3. Time Warping</h3>
<ul>
  <li><strong>Operation:</strong> Select a point in time and warp the spectrogram to the left or right.</li>
  <li><strong>Effect:</strong> Makes the model robust to speaking rate variations (fast/slow speech).</li>
</ul>

<h2 id="training-considerations">Training Considerations</h2>

<h3 id="1-the-ctc-loss">1. The CTC Loss</h3>
<p>Even in Encoder-Decoder models, we often add an auxiliary CTC loss to the Encoder.
[ L = \lambda L_{att} + (1-\lambda) L_{ctc} ]
<strong>Why?</strong></p>
<ul>
  <li>CTC helps convergence (monotonic alignment).</li>
  <li>CTC enforces left-to-right constraints.</li>
</ul>

<h3 id="2-specaugment">2. SpecAugment</h3>
<p>The most important data augmentation for E2E models.
Instead of augmenting the waveform (speed, noise), we augment the <strong>Spectrogram</strong>.</p>
<ol>
  <li><strong>Time Masking:</strong> Mask out (t) consecutive time steps. (Simulates dropped packets).</li>
  <li><strong>Frequency Masking:</strong> Mask out (f) consecutive frequency channels. (Simulates microphone EQ issues).</li>
  <li><strong>Time Warping:</strong> Stretch/squeeze the image.</li>
</ol>

<h3 id="3-curriculum-learning">3. Curriculum Learning</h3>
<p>Start by training on short utterances (2-3 seconds). Gradually increase to long utterances (15-20 seconds). This stabilizes the Attention mechanism.</p>

<h3 id="4-self-supervised-pre-training-wav2vec-20">4. Self-Supervised Pre-training (Wav2Vec 2.0)</h3>

<p>Before training the E2E model on labeled text, we can pre-train the Encoder on <strong>unlabeled audio</strong> (which is cheap and abundant).</p>

<p><strong>Wav2Vec 2.0 Mechanism:</strong></p>
<ol>
  <li><strong>Masking:</strong> Mask parts of the latent speech representation.</li>
  <li><strong>Contrastive Loss:</strong> The model tries to predict the true quantized representation of the masked segment among a set of distractors.</li>
  <li><strong>Result:</strong> The Encoder learns a rich representation of phonemes and speech structure without ever seeing a transcript.</li>
  <li><strong>Fine-tuning:</strong> Add a linear layer on top and train on labeled data with CTC loss. This achieves SOTA with 100x less labeled data.</li>
</ol>

<h2 id="common-failure-modes">Common Failure Modes</h2>

<ol>
  <li><strong>Attention Failure (Looping):</strong>
    <ul>
      <li><em>Symptom:</em> “The cat cat cat cat…”</li>
      <li><em>Cause:</em> The attention mechanism gets stuck on a specific frame.</li>
      <li><em>Fix:</em> Add “Location-Aware” attention (let the model know where it attended previously). Use Windowed Attention.</li>
    </ul>
  </li>
  <li><strong>The “Long-Tail” Problem:</strong>
    <ul>
      <li><em>Symptom:</em> Fails on proper nouns (“Arun”, “PyTorch”).</li>
      <li><em>Cause:</em> E2E models rely on sub-word units (BPE). If a word is rare, its BPE sequence is rare.</li>
      <li><em>Fix:</em> <strong>Contextual Biasing</strong>. Inject a list of expected phrases (Contact names) into the Beam Search decoding graph.</li>
    </ul>
  </li>
</ol>

<h2 id="state-of-the-art-whisper-weakly-supervised">State-of-the-Art: Whisper (Weakly Supervised)</h2>

<p>OpenAI’s Whisper (2022) is an E2E Transformer trained on 680,000 hours of <strong>weakly labeled</strong> web data.</p>
<ul>
  <li><strong>Architecture:</strong> Standard Encoder-Decoder Transformer.</li>
  <li><strong>Innovation:</strong> It’s not the architecture; it’s the <strong>Data</strong>.</li>
  <li><strong>Multitasking:</strong> It predicts special tokens: <code class="language-plaintext highlighter-rouge">&lt;|transcribe|&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;|translate|&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;|timestamps|&gt;</code>.</li>
  <li><strong>Robustness:</strong> Because it saw noisy, messy web data, it is incredibly robust to accents and background noise compared to models trained on clean LibriSpeech.</li>
</ul>

<h2 id="deep-dive-training-loop-implementation">Deep Dive: Training Loop Implementation</h2>

<p>Training E2E models requires handling variable length sequences. We use <code class="language-plaintext highlighter-rouge">pad_sequence</code> and <code class="language-plaintext highlighter-rouge">pack_padded_sequence</code>.</p>

<p>``python
import torchaudio</p>

<p>def train_ctc(model, train_loader, optimizer, epoch):
 model.train()
 ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)</p>

<p>for batch_idx, (waveform, valid_lengths, transcripts, transcript_lengths) in enumerate(train_loader):
 # waveform: [Batch, Time, Channels]
 # transcripts: [Batch, Max_Len]</p>

<p>optimizer.zero_grad()</p>

<p># 1. Forward Pass
 # output: [Time, Batch, Vocab] (Required by PyTorch CTCLoss)
 output = model(waveform) 
 output = output.log_softmax(2)</p>

<p># 2. Calculate Loss
 # input_lengths must be the length of the output after subsampling
 input_lengths = valid_lengths // 4 # Assuming 4x subsampling</p>

<p>loss = ctc_loss(output, transcripts, input_lengths, transcript_lengths)</p>

<p># 3. Backward
 loss.backward()</p>

<p># 4. Gradient Clipping (Crucial for RNNs/Transformers)
 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)</p>

<p>optimizer.step()</p>

<p>if batch_idx % 100 == 0:
 print(f”Epoch {epoch} | Batch {batch_idx} | Loss {loss.item():.4f}”)</p>

<p>``</p>

<h2 id="top-interview-questions">Top Interview Questions</h2>

<p><strong>Q1: Explain the difference between CTC and RNN-T.</strong>
<em>Answer:</em></p>
<ul>
  <li><strong>CTC:</strong> Assumes conditional independence. Output length &lt;= Input length. Non-autoregressive (fast). Weak at language modeling.</li>
  <li><strong>RNN-T:</strong> Removes independence assumption. Output length can be &gt; Input length (technically). Autoregressive (slower). Strong language modeling via Prediction Network.</li>
</ul>

<p><strong>Q2: Why do we need “Subsampling” in the Encoder?</strong>
<em>Answer:</em>
Audio has a high frame rate (100 Hz for 10ms shift). Speech is slow (~3-4 syllables/sec).
Without subsampling (e.g., Stride 2 Conv layers), the sequence length (T) is too long for the Attention mechanism ((O(T^2))) or LSTM ((O(T))). Subsampling by 4x or 8x matches the acoustic rate to the linguistic rate.</p>

<p><strong>Q3: How does Beam Search work for CTC?</strong>
<em>Answer:</em>
Standard Beam Search keeps the top K paths. In CTC, multiple paths map to the same string (<code class="language-plaintext highlighter-rouge">aa</code> -&gt; <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">a</code> -&gt; <code class="language-plaintext highlighter-rouge">a</code>).
CTC Beam Search merges these paths. It maintains two probabilities for each prefix: (P_{blank}) (ending in blank) and (P_{non_blank}) (ending in symbol).</p>

<p><strong>Q4: What is the “Exposure Bias” problem in Autoregressive models (LAS/RNN-T)?</strong>
<em>Answer:</em>
During training, we use <strong>Teacher Forcing</strong> (feed the ground truth previous token).
During inference, we feed the <em>predicted</em> previous token.
If the model makes a mistake during inference, it enters a state it never saw during training, leading to cascading errors.
<em>Fix:</em> Scheduled Sampling (occasionally feed predicted tokens during training).</p>

<p><strong>Q5: Why is Conformer better than Transformer for Speech?</strong>
<em>Answer:</em>
Speech has both local structure (formants, phoneme transitions) and global structure (sentence meaning).</p>
<ul>
  <li><strong>CNNs</strong> capture local structure efficiently.</li>
  <li><strong>Transformers</strong> capture global structure.
Conformer combines both. A pure Transformer needs many layers to learn local patterns that a single Conv layer can capture instantly.</li>
</ul>

<h2 id="deep-dive-whisper-architecture-details">Deep Dive: Whisper Architecture Details</h2>

<p>OpenAI’s Whisper is a masterclass in <strong>Weak Supervision</strong>.</p>

<h3 id="1-the-data">1. The Data</h3>
<ul>
  <li>680,000 hours of audio from the internet.</li>
  <li>Includes non-English audio, background noise, and “hallucinations” (bad transcripts).</li>
  <li><strong>Filtering:</strong> They used a heuristic to remove machine-generated transcripts (which are too clean).</li>
</ul>

<h3 id="2-the-model">2. The Model</h3>
<ul>
  <li>Standard Encoder-Decoder Transformer.</li>
  <li><strong>Input:</strong> Log-Mel Spectrogram (80 channels).</li>
  <li><strong>Positional Encoding:</strong> Sinusoidal.</li>
</ul>

<h3 id="3-the-multitask-format">3. The Multitask Format</h3>
<p>The decoder is prompted with special tokens to control behavior:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">&lt;|startoftranscript|&gt;</code></li>
  <li><code class="language-plaintext highlighter-rouge">&lt;|en|&gt;</code> (Language ID)</li>
  <li><code class="language-plaintext highlighter-rouge">&lt;|transcribe|&gt;</code> (Task: ASR) or <code class="language-plaintext highlighter-rouge">&lt;|translate|&gt;</code> (Task: S2T Translation)</li>
  <li><code class="language-plaintext highlighter-rouge">&lt;|timestamps|&gt;</code> (Predict start/end times)</li>
</ul>

<p>This allows one model to replace a pipeline of (LID -&gt; ASR -&gt; Translation -&gt; Alignment).</p>

<h2 id="deep-dive-word-error-rate-wer">Deep Dive: Word Error Rate (WER)</h2>

<p>WER is the standard metric for ASR. It is the Levenshtein Distance normalized by sequence length.</p>

<p>[ \text{WER} = \frac{S + D + I}{N} ]</p>

<ul>
  <li><strong>S (Substitutions):</strong> “cat” -&gt; “bat”</li>
  <li><strong>D (Deletions):</strong> “the cat” -&gt; “cat”</li>
  <li><strong>I (Insertions):</strong> “cat” -&gt; “the cat”</li>
  <li><strong>N:</strong> Total words in reference.</li>
</ul>

<p><strong>Python Implementation:</strong></p>

<p>``python
def calculate_wer(reference, hypothesis):
 r = reference.split()
 h = hypothesis.split()
 d = np.zeros((len(r) + 1, len(h) + 1))</p>

<p>for i in range(len(r) + 1): d[i][0] = i
 for j in range(len(h) + 1): d[0][j] = j</p>

<p>for i in range(1, len(r) + 1):
 for j in range(1, len(h) + 1):
 if r[i-1] == h[j-1]:
 d[i][j] = d[i-1][j-1]
 else:
 sub = d[i-1][j-1] + 1
 ins = d[i][j-1] + 1
 dele = d[i-1][j] + 1
 d[i][j] = min(sub, ins, dele)</p>

<p>return d[len(r)][len(h)] / len(r)
``</p>

<p><strong>Note:</strong> WER can be &gt; 100% if the model inserts many hallucinations.</p>

<h2 id="case-study-the-evolution-of-nvidias-asr-models">Case Study: The Evolution of NVIDIA’s ASR Models</h2>

<p>NVIDIA has pushed the boundaries of CNN-based ASR (unlike Google’s Transformer push).</p>

<h3 id="1-jasper-just-another-speech-recognizer">1. Jasper (Just Another Speech Recognizer)</h3>
<ul>
  <li><strong>Architecture:</strong> Deep stack of 1D Convolutions + Residual connections.</li>
  <li><strong>Key:</strong> Uses <code class="language-plaintext highlighter-rouge">ReLU</code> and <code class="language-plaintext highlighter-rouge">Dropout</code> heavily.</li>
  <li><strong>Result:</strong> Matched state-of-the-art with simple Conv blocks.</li>
</ul>

<h3 id="2-quartznet">2. QuartzNet</h3>
<ul>
  <li><strong>Architecture:</strong> Like Jasper, but uses <strong>Time-Channel Separable Convolutions</strong> (Depthwise Separable).</li>
  <li><strong>Result:</strong> 96% fewer parameters than Jasper for the same accuracy. Runs on edge devices.</li>
</ul>

<h3 id="3-citrinet">3. Citrinet</h3>
<ul>
  <li><strong>Architecture:</strong> QuartzNet + Squeeze-and-Excitation (SE) blocks.</li>
  <li><strong>Result:</strong> Even better accuracy/parameter ratio.</li>
</ul>

<p>This shows that <strong>Efficiency</strong> (Separable Convs) and <strong>Attention</strong> (SE Blocks) are universal principles, applicable to both Vision and Speech.</p>

<h2 id="deep-dive-hardware-acceleration-tpu-vs-gpu">Deep Dive: Hardware Acceleration (TPU vs GPU)</h2>

<p>Speech models are often trained on TPUs (Tensor Processing Units).</p>

<h3 id="1-tpus-google">1. TPUs (Google)</h3>
<ul>
  <li><strong>Architecture:</strong> Systolic Array. Optimized for massive Matrix Multiplications (MXU).</li>
  <li><strong>Pros:</strong> Extremely fast for large batch sizes. High bandwidth interconnect (ICI).</li>
  <li><strong>Cons:</strong> Hard to debug (XLA compilation).</li>
</ul>

<h3 id="2-gpus-nvidia">2. GPUs (NVIDIA)</h3>
<ul>
  <li><strong>Architecture:</strong> SIMT (Single Instruction Multiple Threads).</li>
  <li><strong>Pros:</strong> Flexible. Great ecosystem (PyTorch/CUDA).</li>
  <li><strong>Cons:</strong> Memory bandwidth can be a bottleneck for RNNs.</li>
</ul>

<p><strong>Warp-RNNT:</strong> A CUDA kernel optimization that maps the RNN-T loss calculation to GPU warps, achieving 30x speedup over naive PyTorch implementation.</p>

<h2 id="further-reading">Further Reading</h2>

<ol>
  <li><strong>CTC:</strong> <a href="https://www.cs.toronto.edu/~graves/icml_2006.pdf">Connectionist Temporal Classification (Graves et al., 2006)</a></li>
  <li><strong>LAS:</strong> <a href="https://arxiv.org/abs/1508.01211">Listen, Attend and Spell (Chan et al., 2015)</a></li>
  <li><strong>RNN-T:</strong> <a href="https://arxiv.org/abs/1211.3711">Sequence Transduction with Recurrent Neural Networks (Graves, 2012)</a></li>
  <li><strong>SpecAugment:</strong> <a href="https://arxiv.org/abs/1904.08779">A Simple Data Augmentation Method for ASR (Park et al., 2019)</a></li>
  <li><strong>Conformer:</strong> <a href="https://arxiv.org/abs/2005.08100">Convolution-augmented Transformer for Speech Recognition (Gulati et al., 2020)</a></li>
</ol>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li><strong>E2E simplifies the stack:</strong> One model, one loss, direct optimization of WER.</li>
  <li><strong>RNN-T for Streaming:</strong> If you need low latency, use Transducers.</li>
  <li><strong>Conformer for Encoding:</strong> The combination of CNN (local) and Transformer (global) is the current gold standard for acoustic encoding.</li>
  <li><strong>SpecAugment is mandatory:</strong> It prevents overfitting and forces the model to learn robust features.</li>
  <li><strong>Hybrid isn’t dead:</strong> For domains with very little data or massive vocabulary constraints (e.g., Medical Dictation), Hybrid systems with explicit Lexicons can still outperform E2E.</li>
</ol>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0027-end-to-end-speech-model-design/">arunbaby.com/speech-tech/0027-end-to-end-speech-model-design</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#conformer" class="page__taxonomy-item p-category" rel="tag">conformer</a><span class="sep">, </span>
    
      <a href="/tags/#ctc" class="page__taxonomy-item p-category" rel="tag">ctc</a><span class="sep">, </span>
    
      <a href="/tags/#end-to-end" class="page__taxonomy-item p-category" rel="tag">end-to-end</a><span class="sep">, </span>
    
      <a href="/tags/#las" class="page__taxonomy-item p-category" rel="tag">las</a><span class="sep">, </span>
    
      <a href="/tags/#rnn-t" class="page__taxonomy-item p-category" rel="tag">rnn-t</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/categories/#deep-learning" class="page__taxonomy-item p-category" rel="tag">deep-learning</a><span class="sep">, </span>
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0027-construct-binary-tree/" rel="permalink">Construct Binary Tree from Preorder and Inorder Traversal
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Given two arrays, can you rebuild the original tree? It’s like solving a jigsaw puzzle where the pieces are numbers.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0027-model-architecture-design/" rel="permalink">Model Architecture Design
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Architecture is destiny. The difference between 50% accuracy and 90% accuracy is often just a skip connection.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0027-api-integration-patterns/" rel="permalink">API Integration Patterns for AI Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Connecting the brain to the world’s nervous system.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=End-to-End+Speech+Model+Design%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0027-end-to-end-speech-model-design%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0027-end-to-end-speech-model-design%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0027-end-to-end-speech-model-design/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0026-batch-speech-processing/" class="pagination--pager" title="Batch Speech Processing">Previous</a>
    
    
      <a href="/speech-tech/0028-voice-search-ranking/" class="pagination--pager" title="Voice Search Ranking">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
