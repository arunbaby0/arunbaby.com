<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Speech Boundary Detection - Arun Baby</title>
<meta name="description" content="“Knowing when to listen and when to stop.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Speech Boundary Detection">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0035-speech-boundary-detection/">


  <meta property="og:description" content="“Knowing when to listen and when to stop.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Speech Boundary Detection">
  <meta name="twitter:description" content="“Knowing when to listen and when to stop.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0035-speech-boundary-detection/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-02T22:27:09+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0035-speech-boundary-detection/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Speech Boundary Detection">
    <meta itemprop="description" content="“Knowing when to listen and when to stop.”">
    <meta itemprop="datePublished" content="2025-12-02T22:27:09+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0035-speech-boundary-detection/" itemprop="url">Speech Boundary Detection
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-the-problem-segmentation">1. The Problem: Segmentation</a></li><li><a href="#2-voice-activity-detection-vad">2. Voice Activity Detection (VAD)</a><ul><li><a href="#1-energy-based-classical">1. Energy-Based (Classical)</a></li><li><a href="#2-gaussian-mixture-models-gmm">2. Gaussian Mixture Models (GMM)</a></li><li><a href="#3-deep-learning-vad-silero-webrtc">3. Deep Learning VAD (Silero, WebRTC)</a></li></ul></li><li><a href="#3-forced-alignment-wordphone-boundaries">3. Forced Alignment (Word/Phone Boundaries)</a></li><li><a href="#4-ctc-segmentation">4. CTC Segmentation</a></li><li><a href="#5-system-design-smart-speaker-wake-word">5. System Design: Smart Speaker Wake Word</a></li><li><a href="#6-deep-dive-pyannote-speaker-segmentation">6. Deep Dive: Pyannote (Speaker Segmentation)</a></li><li><a href="#7-real-world-case-studies">7. Real-World Case Studies</a><ul><li><a href="#case-study-1-spotify-podcast-ad-insertion">Case Study 1: Spotify Podcast Ad Insertion</a></li><li><a href="#case-study-2-descript-audio-editing">Case Study 2: Descript (Audio Editing)</a></li></ul></li><li><a href="#8-summary">8. Summary</a></li><li><a href="#9-deep-dive-implementing-a-production-vad">9. Deep Dive: Implementing a Production VAD</a></li><li><a href="#10-deep-dive-webrtc-vad-industry-standard">10. Deep Dive: WebRTC VAD (Industry Standard)</a></li><li><a href="#11-deep-dive-forced-alignment-with-montreal-forced-aligner">11. Deep Dive: Forced Alignment with Montreal Forced Aligner</a></li><li><a href="#12-deep-dive-ctc-based-segmentation">12. Deep Dive: CTC-Based Segmentation</a></li><li><a href="#13-deep-dive-endpointing-end-of-query-detection">13. Deep Dive: Endpointing (End-of-Query Detection)</a></li><li><a href="#14-deep-dive-speaker-diarization-boundaries">14. Deep Dive: Speaker Diarization Boundaries</a></li><li><a href="#15-system-design-real-time-podcast-transcription">15. System Design: Real-Time Podcast Transcription</a></li><li><a href="#16-production-considerations">16. Production Considerations</a></li><li><a href="#17-summary">17. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Knowing when to listen and when to stop.”</strong></p>

<h2 id="1-the-problem-segmentation">1. The Problem: Segmentation</h2>

<p>Speech is a continuous stream. Computers need discrete units.</p>
<ul>
  <li><strong>VAD (Voice Activity Detection):</strong> Speech vs. Silence.</li>
  <li><strong>Speaker Diarization:</strong> Speaker A vs. Speaker B.</li>
  <li><strong>Word Boundary:</strong> “Ice cream” vs. “I scream”.</li>
  <li><strong>Phoneme Boundary:</strong> Start/End of /s/, /p/, /t/.</li>
</ul>

<p><strong>Applications:</strong></p>
<ul>
  <li><strong>Smart Speakers:</strong> Only wake up on speech (save battery).</li>
  <li><strong>Transcription:</strong> Chop long audio into 30s chunks for ASR.</li>
  <li><strong>Editing:</strong> “Remove silences” feature in podcast editors.</li>
</ul>

<h2 id="2-voice-activity-detection-vad">2. Voice Activity Detection (VAD)</h2>

<p>The most fundamental boundary.</p>

<h3 id="1-energy-based-classical">1. Energy-Based (Classical)</h3>
<ul>
  <li>Calculate Short-Time Energy (STE).</li>
  <li>If $Energy &gt; Threshold$, assume speech.</li>
  <li><strong>Pros:</strong> Ultra-fast, $O(1)$.</li>
  <li><strong>Cons:</strong> Fails with background noise (AC, traffic).</li>
</ul>

<h3 id="2-gaussian-mixture-models-gmm">2. Gaussian Mixture Models (GMM)</h3>
<ul>
  <li>Train two GMMs: one for Speech, one for Noise.</li>
  <li>Calculate Log-Likelihood Ratio (LLR).</li>
  <li><strong>Pros:</strong> Robust to stationary noise.</li>
</ul>

<h3 id="3-deep-learning-vad-silero-webrtc">3. Deep Learning VAD (Silero, WebRTC)</h3>
<ul>
  <li><strong>Model:</strong> LSTM or small CNN.</li>
  <li><strong>Input:</strong> Mel-spectrogram frames.</li>
  <li><strong>Output:</strong> Probability of speech $P(speech)$.</li>
  <li><strong>Latency:</strong> &lt; 10ms.</li>
</ul>

<h2 id="3-forced-alignment-wordphone-boundaries">3. Forced Alignment (Word/Phone Boundaries)</h2>

<p>Given Audio + Transcript, find the exact timestamp of every word.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Lexicon:</strong> Convert text to phonemes. “Hello” -&gt; <code class="language-plaintext highlighter-rouge">HH AH L OW</code>.</li>
  <li><strong>HMM (Hidden Markov Model):</strong>
    <ul>
      <li>States: Phonemes.</li>
      <li>Transitions: <code class="language-plaintext highlighter-rouge">HH -&gt; AH -&gt; L -&gt; OW</code>.</li>
    </ul>
  </li>
  <li><strong>Viterbi Alignment:</strong> Find the most likely path through the HMM states that aligns with the acoustic features (MFCCs).</li>
</ol>

<p><strong>Tool:</strong> <strong>Montreal Forced Aligner (MFA)</strong> is the industry standard.</p>

<h2 id="4-ctc-segmentation">4. CTC Segmentation</h2>

<p>Modern End-to-End ASR uses <strong>CTC (Connectionist Temporal Classification)</strong>.</p>

<ul>
  <li><strong>CTC Output:</strong> A spike probability for each character/phoneme.</li>
  <li><strong>Boundary:</strong> The peak of the spike is the center of the unit. The “blank” token represents the boundary.</li>
  <li><strong>Advantage:</strong> No HMMs needed. Works directly with neural networks.</li>
</ul>

<h2 id="5-system-design-smart-speaker-wake-word">5. System Design: Smart Speaker Wake Word</h2>

<p><strong>Scenario:</strong> “Alexa, play music.”</p>

<p><strong>Pipeline:</strong></p>
<ol>
  <li><strong>Hardware VAD:</strong> Low-power DSP checks for energy. (0.1 mW).</li>
  <li><strong>Streaming Wake Word:</strong> Small CNN runs on-device. Checks for “Alexa”.</li>
  <li><strong>Boundary Detection:</strong>
    <ul>
      <li>Start of Command: Immediately after “Alexa”.</li>
      <li>End of Command: Detect &gt; 700ms of silence.</li>
    </ul>
  </li>
  <li><strong>Cloud ASR:</strong> Send only the command audio to the cloud.</li>
</ol>

<p><strong>The “End-of-Query” Problem:</strong></p>
<ul>
  <li>User: “Play…” (pause) “…music.”</li>
  <li>If timeout is too short, we cut them off.</li>
  <li>If timeout is too long, the system feels sluggish.</li>
  <li><strong>Solution:</strong> <strong>Endpointing</strong>. Use a model to predict “Is the user done?” based on prosody (pitch drop) and semantics (complete sentence?).</li>
</ul>

<h2 id="6-deep-dive-pyannote-speaker-segmentation">6. Deep Dive: Pyannote (Speaker Segmentation)</h2>

<p><strong>Pyannote</strong> is a popular library for diarization.</p>

<p><strong>Pipeline:</strong></p>
<ol>
  <li><strong>Segmentation:</strong> A sliding window model (SincNet) predicts <code class="language-plaintext highlighter-rouge">[Speaker_1, Speaker_2, ...]</code> activity for every frame.</li>
  <li><strong>Embedding:</strong> Extract x-vectors for each segment.</li>
  <li><strong>Clustering:</strong> Group segments by speaker similarity.</li>
</ol>

<h2 id="7-real-world-case-studies">7. Real-World Case Studies</h2>

<h3 id="case-study-1-spotify-podcast-ad-insertion">Case Study 1: Spotify Podcast Ad Insertion</h3>
<ul>
  <li><strong>Problem:</strong> Insert ads at natural breaks.</li>
  <li><strong>Solution:</strong> Detect “Topic Boundaries”.</li>
  <li><strong>Features:</strong> Long pauses, change in speaker distribution, semantic shift (BERT on transcript).</li>
</ul>

<h3 id="case-study-2-descript-audio-editing">Case Study 2: Descript (Audio Editing)</h3>
<ul>
  <li><strong>Feature:</strong> “Shorten Word Gaps”.</li>
  <li><strong>Tech:</strong> Forced Alignment to find precise start/end of every word.</li>
  <li><strong>Action:</strong> If <code class="language-plaintext highlighter-rouge">gap &gt; 1.0s</code>, cut audio to <code class="language-plaintext highlighter-rouge">0.5s</code> and cross-fade.</li>
</ul>

<h2 id="8-summary">8. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Level</th>
      <th style="text-align: left">Task</th>
      <th style="text-align: left">Technology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Signal</strong></td>
      <td style="text-align: left">Speech vs. Silence</td>
      <td style="text-align: left">WebRTC VAD, Silero</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Speaker</strong></td>
      <td style="text-align: left">Speaker A vs. B</td>
      <td style="text-align: left">Pyannote, x-vectors</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Linguistic</strong></td>
      <td style="text-align: left">Word Timestamps</td>
      <td style="text-align: left">Montreal Forced Aligner, CTC</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Semantic</strong></td>
      <td style="text-align: left">Turn-taking</td>
      <td style="text-align: left">Endpointing Models</td>
    </tr>
  </tbody>
</table>

<h2 id="9-deep-dive-implementing-a-production-vad">9. Deep Dive: Implementing a Production VAD</h2>

<p>Let’s build a robust VAD using energy + spectral features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">librosa</span>

<span class="k">class</span> <span class="nc">ProductionVAD</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span> <span class="n">frame_length</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">frame_shift</span><span class="o">=</span><span class="mf">0.010</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sr</span> <span class="o">=</span> <span class="n">sr</span>
        <span class="n">self</span><span class="p">.</span><span class="n">frame_length</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">sr</span> <span class="o">*</span> <span class="n">frame_length</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">frame_shift</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">sr</span> <span class="o">*</span> <span class="n">frame_shift</span><span class="p">)</span>
        
        <span class="c1"># Thresholds (tuned on dev set)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">energy_threshold</span> <span class="o">=</span> <span class="mf">0.02</span>
        <span class="n">self</span><span class="p">.</span><span class="n">zcr_threshold</span> <span class="o">=</span> <span class="mf">0.3</span>
        <span class="n">self</span><span class="p">.</span><span class="n">spectral_centroid_threshold</span> <span class="o">=</span> <span class="mi">2000</span>
        
    <span class="k">def</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">):</span>
        <span class="c1"># Short-Time Energy
</span>        <span class="n">energy</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">feature</span><span class="p">.</span><span class="nf">rms</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">audio</span><span class="p">,</span> <span class="n">frame_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">frame_length</span><span class="p">,</span> 
                                      <span class="n">hop_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">frame_shift</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Zero Crossing Rate
</span>        <span class="n">zcr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">feature</span><span class="p">.</span><span class="nf">zero_crossing_rate</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">frame_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">frame_length</span><span class="p">,</span>
                                                   <span class="n">hop_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">frame_shift</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Spectral Centroid
</span>        <span class="n">spectral_centroid</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">feature</span><span class="p">.</span><span class="nf">spectral_centroid</span><span class="p">(</span>
            <span class="n">y</span><span class="o">=</span><span class="n">audio</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">sr</span><span class="p">,</span> <span class="n">n_fft</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">frame_length</span><span class="p">,</span> 
            <span class="n">hop_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">frame_shift</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">energy</span><span class="p">,</span> <span class="n">zcr</span><span class="p">,</span> <span class="n">spectral_centroid</span>
    
    <span class="k">def</span> <span class="nf">detect</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">):</span>
        <span class="n">energy</span><span class="p">,</span> <span class="n">zcr</span><span class="p">,</span> <span class="n">spectral_centroid</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">extract_features</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
        
        <span class="c1"># Decision logic
</span>        <span class="n">speech_frames</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">energy</span> <span class="o">&gt;</span> <span class="n">self</span><span class="p">.</span><span class="n">energy_threshold</span><span class="p">)</span> <span class="o">&amp;</span>
            <span class="p">(</span><span class="n">zcr</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">zcr_threshold</span><span class="p">)</span> <span class="o">&amp;</span>
            <span class="p">(</span><span class="n">spectral_centroid</span> <span class="o">&gt;</span> <span class="n">self</span><span class="p">.</span><span class="n">spectral_centroid_threshold</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="k">return</span> <span class="n">speech_frames</span>
    
    <span class="k">def</span> <span class="nf">get_speech_segments</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">):</span>
        <span class="n">speech_frames</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">detect</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
        
        <span class="c1"># Convert frame indices to time
</span>        <span class="n">segments</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">in_speech</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">is_speech</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">speech_frames</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">is_speech</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">in_speech</span><span class="p">:</span>
                <span class="n">start_idx</span> <span class="o">=</span> <span class="n">i</span>
                <span class="n">in_speech</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">elif</span> <span class="ow">not</span> <span class="n">is_speech</span> <span class="ow">and</span> <span class="n">in_speech</span><span class="p">:</span>
                <span class="n">start_time</span> <span class="o">=</span> <span class="n">start_idx</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">frame_shift</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">sr</span>
                <span class="n">end_time</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">frame_shift</span> <span class="o">/</span> <span class="n">self</span><span class="p">.</span><span class="n">sr</span>
                <span class="n">segments</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">start_time</span><span class="p">,</span> <span class="n">end_time</span><span class="p">))</span>
                <span class="n">in_speech</span> <span class="o">=</span> <span class="bp">False</span>
        
        <span class="k">return</span> <span class="n">segments</span>

<span class="c1"># Usage
</span><span class="n">vad</span> <span class="o">=</span> <span class="nc">ProductionVAD</span><span class="p">()</span>
<span class="n">audio</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">audio.wav</span><span class="sh">"</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16000</span><span class="p">)</span>
<span class="n">segments</span> <span class="o">=</span> <span class="n">vad</span><span class="p">.</span><span class="nf">get_speech_segments</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Speech segments: </span><span class="si">{</span><span class="n">segments</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="10-deep-dive-webrtc-vad-industry-standard">10. Deep Dive: WebRTC VAD (Industry Standard)</h2>

<p>WebRTC VAD is used in Zoom, Google Meet, etc.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Gaussian Mixture Model (GMM):</strong> Two GMMs (Speech vs. Noise).</li>
  <li><strong>Features:</strong> 6 spectral features per frame.</li>
  <li><strong>Modes:</strong> Aggressive (0), Normal (1), Conservative (2), Very Conservative (3).</li>
</ol>

<p><strong>Python Wrapper:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">webrtcvad</span>
<span class="kn">import</span> <span class="n">struct</span>

<span class="k">def</span> <span class="nf">read_wave</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">wave</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="sh">'</span><span class="s">rb</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">wf</span><span class="p">:</span>
        <span class="n">sample_rate</span> <span class="o">=</span> <span class="n">wf</span><span class="p">.</span><span class="nf">getframerate</span><span class="p">()</span>
        <span class="n">pcm_data</span> <span class="o">=</span> <span class="n">wf</span><span class="p">.</span><span class="nf">readframes</span><span class="p">(</span><span class="n">wf</span><span class="p">.</span><span class="nf">getnframes</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">pcm_data</span><span class="p">,</span> <span class="n">sample_rate</span>

<span class="k">def</span> <span class="nf">frame_generator</span><span class="p">(</span><span class="n">frame_duration_ms</span><span class="p">,</span> <span class="n">audio</span><span class="p">,</span> <span class="n">sample_rate</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">sample_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">frame_duration_ms</span> <span class="o">/</span> <span class="mf">1000.0</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">n</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">audio</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">audio</span><span class="p">[</span><span class="n">offset</span><span class="p">:</span><span class="n">offset</span> <span class="o">+</span> <span class="n">n</span><span class="p">]</span>
        <span class="n">offset</span> <span class="o">+=</span> <span class="n">n</span>

<span class="n">vad</span> <span class="o">=</span> <span class="n">webrtcvad</span><span class="p">.</span><span class="nc">Vad</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Mode 2 (Normal)
</span><span class="n">audio</span><span class="p">,</span> <span class="n">sample_rate</span> <span class="o">=</span> <span class="nf">read_wave</span><span class="p">(</span><span class="sh">'</span><span class="s">audio.wav</span><span class="sh">'</span><span class="p">)</span>

<span class="n">frames</span> <span class="o">=</span> <span class="nf">frame_generator</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">audio</span><span class="p">,</span> <span class="n">sample_rate</span><span class="p">)</span>  <span class="c1"># 30ms frames
</span><span class="k">for</span> <span class="n">frame</span> <span class="ow">in</span> <span class="n">frames</span><span class="p">:</span>
    <span class="n">is_speech</span> <span class="o">=</span> <span class="n">vad</span><span class="p">.</span><span class="nf">is_speech</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">sample_rate</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Speech: </span><span class="si">{</span><span class="n">is_speech</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="11-deep-dive-forced-alignment-with-montreal-forced-aligner">11. Deep Dive: Forced Alignment with Montreal Forced Aligner</h2>

<p><strong>Installation:</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install</span> <span class="nt">-c</span> conda-forge montreal-forced-aligner
</code></pre></div></div>

<p><strong>Usage:</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 1. Prepare data</span>
<span class="c"># Directory structure:</span>
<span class="c"># corpus/</span>
<span class="c">#   audio1.wav</span>
<span class="c">#   audio1.txt  # Transcript</span>
<span class="c">#   audio2.wav</span>
<span class="c">#   audio2.txt</span>

<span class="c"># 2. Download acoustic model and dictionary</span>
mfa model download acoustic english_us_arpa
mfa model download dictionary english_us_arpa

<span class="c"># 3. Align</span>
mfa align corpus/ english_us_arpa english_us_arpa output/

<span class="c"># 4. Output: TextGrid files with word/phone timestamps</span>
</code></pre></div></div>

<p><strong>Parsing TextGrid:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">textgrid</span>

<span class="n">tg</span> <span class="o">=</span> <span class="n">textgrid</span><span class="p">.</span><span class="n">TextGrid</span><span class="p">.</span><span class="nf">fromFile</span><span class="p">(</span><span class="sh">"</span><span class="s">output/audio1.TextGrid</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Extract word boundaries
</span><span class="n">words_tier</span> <span class="o">=</span> <span class="n">tg</span><span class="p">.</span><span class="nf">getFirst</span><span class="p">(</span><span class="sh">'</span><span class="s">words</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">interval</span> <span class="ow">in</span> <span class="n">words_tier</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">interval</span><span class="p">.</span><span class="n">mark</span><span class="p">:</span>  <span class="c1"># Non-empty
</span>        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">interval</span><span class="p">.</span><span class="n">mark</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">interval</span><span class="p">.</span><span class="n">minTime</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">s - </span><span class="si">{</span><span class="n">interval</span><span class="p">.</span><span class="n">maxTime</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">s</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Extract phone boundaries
</span><span class="n">phones_tier</span> <span class="o">=</span> <span class="n">tg</span><span class="p">.</span><span class="nf">getFirst</span><span class="p">(</span><span class="sh">'</span><span class="s">phones</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">interval</span> <span class="ow">in</span> <span class="n">phones_tier</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">interval</span><span class="p">.</span><span class="n">mark</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">interval</span><span class="p">.</span><span class="n">mark</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">interval</span><span class="p">.</span><span class="n">minTime</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">s - </span><span class="si">{</span><span class="n">interval</span><span class="p">.</span><span class="n">maxTime</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">s</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="12-deep-dive-ctc-based-segmentation">12. Deep Dive: CTC-Based Segmentation</h2>

<p>Modern End-to-End ASR uses CTC. We can extract boundaries from CTC alignments.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="k">def</span> <span class="nf">ctc_segmentation</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">text</span><span class="p">,</span> <span class="n">blank_id</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    logits: (T, vocab_size) - CTC output probabilities
    text: Ground truth text
    Returns: List of (char, start_frame, end_frame)
    </span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">logits</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Get most likely path (greedy decoding)
</span>    <span class="n">path</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Collapse repeated characters and remove blanks
</span>    <span class="n">segments</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">prev_char</span> <span class="o">=</span> <span class="n">blank_id</span>
    <span class="n">start_frame</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">char</span> <span class="o">=</span> <span class="n">path</span><span class="p">[</span><span class="n">t</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">char</span> <span class="o">!=</span> <span class="n">blank_id</span> <span class="ow">and</span> <span class="n">char</span> <span class="o">!=</span> <span class="n">prev_char</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">prev_char</span> <span class="o">!=</span> <span class="n">blank_id</span><span class="p">:</span>
                <span class="n">segments</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">prev_char</span><span class="p">,</span> <span class="n">start_frame</span><span class="p">,</span> <span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">start_frame</span> <span class="o">=</span> <span class="n">t</span>
        
        <span class="n">prev_char</span> <span class="o">=</span> <span class="n">char</span>
    
    <span class="c1"># Add last segment
</span>    <span class="k">if</span> <span class="n">prev_char</span> <span class="o">!=</span> <span class="n">blank_id</span><span class="p">:</span>
        <span class="n">segments</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">prev_char</span><span class="p">,</span> <span class="n">start_frame</span><span class="p">,</span> <span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">segments</span>
</code></pre></div></div>

<h2 id="13-deep-dive-endpointing-end-of-query-detection">13. Deep Dive: Endpointing (End-of-Query Detection)</h2>

<p><strong>Problem:</strong> When should the system stop listening?</p>

<p><strong>Naive Approach:</strong> Fixed timeout (e.g., 700ms of silence).
<strong>Problem:</strong> Cuts off slow speakers, feels sluggish for fast speakers.</p>

<p><strong>Adaptive Endpointing:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">AdaptiveEndpointer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">base_timeout</span> <span class="o">=</span> <span class="mf">0.7</span>  <span class="c1"># 700ms
</span>        <span class="n">self</span><span class="p">.</span><span class="n">min_timeout</span> <span class="o">=</span> <span class="mf">0.3</span>
        <span class="n">self</span><span class="p">.</span><span class="n">max_timeout</span> <span class="o">=</span> <span class="mf">1.5</span>
        
    <span class="k">def</span> <span class="nf">compute_timeout</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">speaking_rate</span><span class="p">,</span> <span class="n">prosody_features</span><span class="p">):</span>
        <span class="c1"># speaking_rate: words per second
</span>        <span class="c1"># prosody_features: pitch drop, energy drop
</span>        
        <span class="c1"># Slow speakers get longer timeout
</span>        <span class="n">rate_factor</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">speaking_rate</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">)</span>
        
        <span class="c1"># Falling intonation (end of sentence) -&gt; shorter timeout
</span>        <span class="n">pitch_drop</span> <span class="o">=</span> <span class="n">prosody_features</span><span class="p">[</span><span class="sh">'</span><span class="s">pitch_drop</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">prosody_factor</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="p">(</span><span class="n">pitch_drop</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span>
        
        <span class="n">timeout</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">base_timeout</span> <span class="o">*</span> <span class="n">rate_factor</span> <span class="o">*</span> <span class="n">prosody_factor</span>
        <span class="n">timeout</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">timeout</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">min_timeout</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">max_timeout</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">timeout</span>
</code></pre></div></div>

<h2 id="14-deep-dive-speaker-diarization-boundaries">14. Deep Dive: Speaker Diarization Boundaries</h2>

<p><strong>Pyannote Pipeline:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyannote.audio</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">pyannote/speaker-diarization</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Apply on audio file
</span><span class="n">diarization</span> <span class="o">=</span> <span class="nf">pipeline</span><span class="p">(</span><span class="sh">"</span><span class="s">audio.wav</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Extract speaker segments
</span><span class="k">for</span> <span class="n">turn</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">speaker</span> <span class="ow">in</span> <span class="n">diarization</span><span class="p">.</span><span class="nf">itertracks</span><span class="p">(</span><span class="n">yield_label</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Speaker </span><span class="si">{</span><span class="n">speaker</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">turn</span><span class="p">.</span><span class="n">start</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">s - </span><span class="si">{</span><span class="n">turn</span><span class="p">.</span><span class="n">end</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">s</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Custom Post-Processing:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">merge_short_segments</span><span class="p">(</span><span class="n">diarization</span><span class="p">,</span> <span class="n">min_duration</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Merge segments shorter than min_duration with neighbors</span><span class="sh">"""</span>
    <span class="n">segments</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">diarization</span><span class="p">.</span><span class="nf">itertracks</span><span class="p">(</span><span class="n">yield_label</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="n">merged</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">segments</span><span class="p">):</span>
        <span class="n">turn</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">speaker</span> <span class="o">=</span> <span class="n">segments</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">duration</span> <span class="o">=</span> <span class="n">turn</span><span class="p">.</span><span class="n">end</span> <span class="o">-</span> <span class="n">turn</span><span class="p">.</span><span class="n">start</span>
        
        <span class="k">if</span> <span class="n">duration</span> <span class="o">&lt;</span> <span class="n">min_duration</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Merge with previous segment
</span>            <span class="n">prev_turn</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">prev_speaker</span> <span class="o">=</span> <span class="n">merged</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">merged</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nc">Segment</span><span class="p">(</span><span class="n">prev_turn</span><span class="p">.</span><span class="n">start</span><span class="p">,</span> <span class="n">turn</span><span class="p">.</span><span class="n">end</span><span class="p">),</span> <span class="n">_</span><span class="p">,</span> <span class="n">prev_speaker</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">merged</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">segments</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="k">return</span> <span class="n">merged</span>
</code></pre></div></div>

<h2 id="15-system-design-real-time-podcast-transcription">15. System Design: Real-Time Podcast Transcription</h2>

<p><strong>Requirements:</strong></p>
<ul>
  <li>Transcribe 1-hour podcast in &lt; 5 minutes.</li>
  <li>Accurate speaker labels.</li>
  <li>Word-level timestamps.</li>
</ul>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>VAD:</strong> Silero VAD to remove silence (reduces audio by 30%).</li>
  <li><strong>Diarization:</strong> Pyannote to get speaker segments.</li>
  <li><strong>ASR:</strong> Whisper Large-v2 on each speaker segment.</li>
  <li><strong>Forced Alignment:</strong> MFA to get word timestamps.</li>
  <li><strong>Post-Processing:</strong> Punctuation restoration, capitalization.</li>
</ol>

<p><strong>Pipeline Code:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">transcribe_podcast</span><span class="p">(</span><span class="n">audio_path</span><span class="p">):</span>
    <span class="c1"># 1. VAD
</span>    <span class="n">speech_segments</span> <span class="o">=</span> <span class="nf">silero_vad</span><span class="p">(</span><span class="n">audio_path</span><span class="p">)</span>
    
    <span class="c1"># 2. Diarization
</span>    <span class="n">diarization</span> <span class="o">=</span> <span class="nf">pyannote_diarize</span><span class="p">(</span><span class="n">audio_path</span><span class="p">)</span>
    
    <span class="c1"># 3. ASR per speaker segment
</span>    <span class="n">transcripts</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">turn</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">speaker</span> <span class="ow">in</span> <span class="n">diarization</span><span class="p">.</span><span class="nf">itertracks</span><span class="p">(</span><span class="n">yield_label</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">segment_audio</span> <span class="o">=</span> <span class="nf">extract_segment</span><span class="p">(</span><span class="n">audio_path</span><span class="p">,</span> <span class="n">turn</span><span class="p">.</span><span class="n">start</span><span class="p">,</span> <span class="n">turn</span><span class="p">.</span><span class="n">end</span><span class="p">)</span>
        <span class="n">text</span> <span class="o">=</span> <span class="nf">whisper_transcribe</span><span class="p">(</span><span class="n">segment_audio</span><span class="p">)</span>
        
        <span class="c1"># 4. Forced Alignment
</span>        <span class="n">word_timestamps</span> <span class="o">=</span> <span class="nf">mfa_align</span><span class="p">(</span><span class="n">segment_audio</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>
        
        <span class="n">transcripts</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
            <span class="sh">'</span><span class="s">speaker</span><span class="sh">'</span><span class="p">:</span> <span class="n">speaker</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">start</span><span class="sh">'</span><span class="p">:</span> <span class="n">turn</span><span class="p">.</span><span class="n">start</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">end</span><span class="sh">'</span><span class="p">:</span> <span class="n">turn</span><span class="p">.</span><span class="n">end</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="n">text</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">words</span><span class="sh">'</span><span class="p">:</span> <span class="n">word_timestamps</span>
        <span class="p">})</span>
    
    <span class="k">return</span> <span class="n">transcripts</span>
</code></pre></div></div>

<h2 id="16-production-considerations">16. Production Considerations</h2>

<ol>
  <li><strong>Latency Budget:</strong>
    <ul>
      <li>VAD: &lt; 10ms</li>
      <li>Diarization: Can be offline (batch)</li>
      <li>Forced Alignment: &lt; 1s per minute of audio</li>
    </ul>
  </li>
  <li><strong>Accuracy vs. Speed:</strong>
    <ul>
      <li>For live captions: Use streaming VAD + fast ASR (Conformer-S).</li>
      <li>For archival: Use offline diarization + Whisper Large.</li>
    </ul>
  </li>
  <li><strong>Edge Deployment:</strong>
    <ul>
      <li>VAD runs on-device (DSP or NPU).</li>
      <li>ASR runs in cloud (GPU).</li>
    </ul>
  </li>
</ol>

<h2 id="17-summary">17. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Level</th>
      <th style="text-align: left">Task</th>
      <th style="text-align: left">Technology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Signal</strong></td>
      <td style="text-align: left">Speech vs. Silence</td>
      <td style="text-align: left">WebRTC VAD, Silero</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Speaker</strong></td>
      <td style="text-align: left">Speaker A vs. B</td>
      <td style="text-align: left">Pyannote, x-vectors</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Linguistic</strong></td>
      <td style="text-align: left">Word Timestamps</td>
      <td style="text-align: left">Montreal Forced Aligner, CTC</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Semantic</strong></td>
      <td style="text-align: left">Turn-taking</td>
      <td style="text-align: left">Endpointing Models</td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0035-speech-boundary-detection/">arunbaby.com/speech-tech/0035-speech-boundary-detection</a></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#forced-alignment" class="page__taxonomy-item p-category" rel="tag">forced-alignment</a><span class="sep">, </span>
    
      <a href="/tags/#phonetics" class="page__taxonomy-item p-category" rel="tag">phonetics</a><span class="sep">, </span>
    
      <a href="/tags/#segmentation" class="page__taxonomy-item p-category" rel="tag">segmentation</a><span class="sep">, </span>
    
      <a href="/tags/#vad" class="page__taxonomy-item p-category" rel="tag">vad</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech_tech</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Speech+Boundary+Detection%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0035-speech-boundary-detection%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0035-speech-boundary-detection%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0035-speech-boundary-detection/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0034-dialog-state-tracking/" class="pagination--pager" title="Dialog State Tracking (DST)">Previous</a>
    
    
      <a href="/speech-tech/0036-multi-task-speech-learning/" class="pagination--pager" title="Multi-task Speech Learning">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
