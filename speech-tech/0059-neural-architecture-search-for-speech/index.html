<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Neural Architecture Search (NAS) for Speech - Arun Baby</title>
<meta name="description" content="“Hand-crafting speech architectures is reaching its limits. For the next generation of voice assistants, we don’t build the model—we define the search space and let the computer discover the most efficient physics of sound.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Neural Architecture Search (NAS) for Speech">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0059-neural-architecture-search-for-speech/">


  <meta property="og:description" content="“Hand-crafting speech architectures is reaching its limits. For the next generation of voice assistants, we don’t build the model—we define the search space and let the computer discover the most efficient physics of sound.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Neural Architecture Search (NAS) for Speech">
  <meta name="twitter:description" content="“Hand-crafting speech architectures is reaching its limits. For the next generation of voice assistants, we don’t build the model—we define the search space and let the computer discover the most efficient physics of sound.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0059-neural-architecture-search-for-speech/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T15:12:33+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0059-neural-architecture-search-for-speech/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Neural Architecture Search (NAS) for Speech">
    <meta itemprop="description" content="“Hand-crafting speech architectures is reaching its limits. For the next generation of voice assistants, we don’t build the model—we define the search space and let the computer discover the most efficient physics of sound.”">
    <meta itemprop="datePublished" content="2025-12-29T15:12:33+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0059-neural-architecture-search-for-speech/" itemprop="url">Neural Architecture Search (NAS) for Speech
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction-the-mobile-challenge">1. Introduction: The Mobile Challenge</a></li><li><a href="#2-the-search-space-building-blocks-of-speech">2. The Search Space: Building Blocks of Speech</a><ul><li><a href="#21-convolutional-front-ends">2.1 Convolutional Front-ends</a></li><li><a href="#22-transformerconformer-blocks">2.2 Transformer/Conformer Blocks</a></li><li><a href="#23-the-connectivity">2.3 The Connectivity</a></li></ul></li><li><a href="#3-high-level-architecture-the-performance-aware-searcher">3. High-Level Architecture: The Performance-Aware Searcher</a></li><li><a href="#4-implementation-once-for-all-ofa-search">4. Implementation: Once-for-all (OFA) Search</a><ul><li><a href="#the-logic">The Logic</a></li></ul></li><li><a href="#5-the-reward-function-the-efficiency-frontier">5. The Reward Function: The “Efficiency Frontier”</a></li><li><a href="#6-real-time-implementation-on-device-accuracy">6. Real-time Implementation: On-Device Accuracy</a></li><li><a href="#7-comparative-analysis-hand-crafted-vs-nas-models">7. Comparative Analysis: Hand-crafted vs. NAS Models</a></li><li><a href="#8-failure-modes-in-speech-nas">8. Failure Modes in Speech NAS</a></li><li><a href="#9-real-world-case-study-googles-e-nas-for-voice-assistant">9. Real-World Case Study: Google’s “E-NAS” for Voice Assistant</a></li><li><a href="#10-key-takeaways">10. Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Hand-crafting speech architectures is reaching its limits. For the next generation of voice assistants, we don’t build the model—we define the search space and let the computer discover the most efficient physics of sound.”</strong></p>

<h2 id="1-introduction-the-mobile-challenge">1. Introduction: The Mobile Challenge</h2>

<p>In the world of Speech Tech, we are constantly fighting two warring factions: <strong>Word Error Rate (WER)</strong> and <strong>Real-Time Factor (RTF)</strong>.</p>
<ul>
  <li>A massive model like Whisper-Large has a low WER but can never run on a phone in real-time.</li>
  <li>A tiny model fits on the phone but misses the nuances of accents and noise.</li>
</ul>

<p>Historically, humans designed “Mobile-Optimized” architectures like <strong>MobileNet</strong> or <strong>Cuside-Transformer</strong> by hand. However, sound is complex. The optimal architecture for a “Quiet Home” is different from an “Industrial Warehouse.”</p>

<p><strong>Neural Architecture Search (NAS) for Speech</strong> is the automation of this discovery. It treats a model’s topology (number of heads, kernel sizes, dilation rates) as a <strong>Constraint Satisfaction Problem</strong> (connecting to our <strong>Sudoku Solver</strong> DSA topic). Today, we build the pipeline for discovering speech models that are “Pareto-Optimal”—better accuracy for less compute.</p>

<hr />

<h2 id="2-the-search-space-building-blocks-of-speech">2. The Search Space: Building Blocks of Speech</h2>

<p>NAS depends on the “Architecture Palette.” For speech, we typically search across:</p>

<h3 id="21-convolutional-front-ends">2.1 Convolutional Front-ends</h3>
<ul>
  <li>Kernel sizes (3x3 vs 5x5 vs 7x7).</li>
  <li>Dilated Convolutions: How far should the “context window” reach in a single time step?</li>
  <li>Grouped vs. Depthwise Separable Convolutions (to save parameters).</li>
</ul>

<h3 id="22-transformerconformer-blocks">2.2 Transformer/Conformer Blocks</h3>
<ul>
  <li>Number of attention heads.</li>
  <li>Dimension of the Feed-Forward Network (FFN).</li>
  <li>Placement of the Convolutional module within the Transformer layer.</li>
</ul>

<h3 id="23-the-connectivity">2.3 The Connectivity</h3>
<ul>
  <li>Should we use “Dense” connections (skip connections) or a linear stack?</li>
  <li>Where should we downsample the time-dimension (Striding)?</li>
</ul>

<hr />

<h2 id="3-high-level-architecture-the-performance-aware-searcher">3. High-Level Architecture: The Performance-Aware Searcher</h2>

<p>A production NAS system for speech (like Apple’s research into efficient ASR) follows a three-stage loop:</p>

<ol>
  <li><strong>Search Controller (The Agent)</strong>: Proposes a “Candidate Model” from the search space.</li>
  <li><strong>Training &amp; Evaluation (The Trial)</strong>: Trains the candidate on a subset of the dataset (e.g., Librispeech-100) for a few epochs.</li>
  <li><strong>Hardware Profiler</strong>: Measures the <strong>Latency</strong> on a specific target device (e.g., iPhone 15, Pixel 8) and the <strong>Power Consumption</strong>.</li>
  <li><strong>Reward Function</strong>: Rewards candidates that have the best “WER-Latency” trade-off.</li>
</ol>

<hr />

<h2 id="4-implementation-once-for-all-ofa-search">4. Implementation: Once-for-all (OFA) Search</h2>

<p>One of the most efficient NAS strategies is the <strong>OFA (Once-for-all)</strong> approach. Instead of training 1,000 separate models, we train one “Super-Network” that contains all possible sub-networks.</p>

<h3 id="the-logic">The Logic</h3>
<ol>
  <li><strong>Train the Super-Net</strong>: Ensure that any “Slice” of the network (e.g., using only 4 heads instead of 8) is still functionally valid.</li>
  <li><strong>Architectural Sampling</strong>: Randomly pick sub-networks during training and update their weights.</li>
  <li><strong>The Result</strong>: At the end of training, you have a single set of weights from which you can “extract” the best model for any hardware constraint (one for a high-end server, one for a smartwatch) without re-training.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SpeechSuperNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">max_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">max_layers</span><span class="o">=</span><span class="mi">12</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span>
            <span class="nc">DynamicTransformerBlock</span><span class="p">(</span><span class="n">max_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_layers</span><span class="p">)</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">current_config</span><span class="p">):</span>
        <span class="c1"># Dynamically 'slice' the model based on current search trial
</span>        <span class="n">num_layers</span> <span class="o">=</span> <span class="n">current_config</span><span class="p">[</span><span class="sh">'</span><span class="s">layers</span><span class="sh">'</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="n">current_config</span><span class="p">[</span><span class="sh">'</span><span class="s">heads</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<hr />

<h2 id="5-the-reward-function-the-efficiency-frontier">5. The Reward Function: The “Efficiency Frontier”</h2>

<p>We don’t just want the lowest Word Error Rate (WER). We want to solve for:
$Reward = -WER - \lambda \cdot \log(Latency)$</p>

<p>Where $\lambda$ represents how much we value speed.</p>
<ul>
  <li>If $\lambda$ is high, the system will favor tiny, lightning-fast models.</li>
  <li>If $\lambda$ is low, it will favor heavy, accurate models.</li>
</ul>

<hr />

<h2 id="6-real-time-implementation-on-device-accuracy">6. Real-time Implementation: On-Device Accuracy</h2>

<p>When an architecture is discovered, how is it deployed?</p>
<ol>
  <li><strong>Export to ONNX/CoreML</strong>: Convert the neural graph to a static format optimized for the mobile NPU (Neural Processing Unit).</li>
  <li><strong>Quantization-Aware Discovery</strong>: The NAS system searches for models that perform well even when their weights are compressed from 32-bit floats to 8-bit integers.</li>
  <li><strong>Phonetic Pruning</strong>: Prune layers that the NAS system identifies as redundant for specific acoustic environments.</li>
</ol>

<hr />

<h2 id="7-comparative-analysis-hand-crafted-vs-nas-models">7. Comparative Analysis: Hand-crafted vs. NAS Models</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Metric</th>
      <th style="text-align: left">Hand-crafted (Conformer)</th>
      <th style="text-align: left">NAS-Optimized (S-NAS)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>WER (Noise)</strong></td>
      <td style="text-align: left">5.2%</td>
      <td style="text-align: left">4.8%</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Params</strong></td>
      <td style="text-align: left">120M</td>
      <td style="text-align: left">35M</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Latency (iPhone)</strong></td>
      <td style="text-align: left">120ms</td>
      <td style="text-align: left">40ms</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Search Time</strong></td>
      <td style="text-align: left">3 months (Human)</td>
      <td style="text-align: left">48 hours (GPU)</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="8-failure-modes-in-speech-nas">8. Failure Modes in Speech NAS</h2>

<ol>
  <li><strong>Invalid Topologies</strong>: The searcher proposes a model that is too deep to fit in the GPU’s memory.
    <ul>
      <li><em>Mitigation</em>: Implement “Soft Constraints” in the search controller that reject configurations exceeding a resource budget (The Sudoku Link).</li>
    </ul>
  </li>
  <li><strong>The “Hardware Gap”</strong>: A model that is fast on a CPU might be slow on a DSP (Digital Signal Processor).
    <ul>
      <li><em>Mitigation</em>: Always perform evaluations on the <strong>Physical Hardware</strong>, not a simulator.</li>
    </ul>
  </li>
  <li><strong>Feature Mismatch</strong>: The NAS finds a great model for 16kHz audio, but the production system uses 8kHz.</li>
</ol>

<hr />

<h2 id="9-real-world-case-study-googles-e-nas-for-voice-assistant">9. Real-World Case Study: Google’s “E-NAS” for Voice Assistant</h2>

<p>Google used NAS to design the “New Google Assistant” models.</p>
<ul>
  <li><strong>The Challenge</strong>: The model had to understand voice locally on a phone to eliminate latency.</li>
  <li><strong>The Result</strong>: NAS discovered a “Hydra” architecture—a single shared convolutional trunk with multiple “Heads” for different tasks (ASR, Intent Detection). This reduced parameter count by 75% compared to the original design.</li>
</ul>

<hr />

<h2 id="10-key-takeaways">10. Key Takeaways</h2>

<ol>
  <li><strong>Search is the new Engineering</strong>: (The DSA Link) Automating the search for the “Correct digits in the grid” is the only way to achieve peak efficiency.</li>
  <li><strong>Hardware-in-the-loop</strong>: A speech model is only as good as its speed on the target device.</li>
  <li><strong>NAS is not just for WER</strong>: Use it to optimize for battery life, memory, and even privacy.</li>
  <li><strong>Pruning starts in the Search phase</strong>: (The ML Link) Use AutoML principles to kill poor architectures early.</li>
</ol>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0059-neural-architecture-search-for-speech/">arunbaby.com/speech-tech/0059-neural-architecture-search-for-speech</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#automl" class="page__taxonomy-item p-category" rel="tag">automl</a><span class="sep">, </span>
    
      <a href="/tags/#conformer" class="page__taxonomy-item p-category" rel="tag">conformer</a><span class="sep">, </span>
    
      <a href="/tags/#latency-optimization" class="page__taxonomy-item p-category" rel="tag">latency-optimization</a><span class="sep">, </span>
    
      <a href="/tags/#mobilenet" class="page__taxonomy-item p-category" rel="tag">mobilenet</a><span class="sep">, </span>
    
      <a href="/tags/#nas" class="page__taxonomy-item p-category" rel="tag">nas</a><span class="sep">, </span>
    
      <a href="/tags/#tts" class="page__taxonomy-item p-category" rel="tag">tts</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0059-sudoku-solver/" rel="permalink">Sudoku Solver
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Sudoku Solver is the quintessential backtracking problem—it represents the transition from simple recursion to a multi-constraint search problem where every...</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0059-automl-systems/" rel="permalink">AutoML Systems at Scale
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The ultimate bottleneck in machine learning is not data or compute—it is the human engineer. AutoML Systems aim to automate the ‘grad student descent’—turni...</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0059-agent-benchmarking-deep-dive/" rel="permalink">Agent Benchmarking: A Deep Dive
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“If you cannot measure an agent, you cannot improve it. Benchmarking is the process of defining what it means for a machine to ‘think’ through a task.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Neural+Architecture+Search+%28NAS%29+for+Speech%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0059-neural-architecture-search-for-speech%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0059-neural-architecture-search-for-speech%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0059-neural-architecture-search-for-speech/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0058-conversational-ai-system/" class="pagination--pager" title="Architecting Conversational AI Systems">Previous</a>
    
    
      <a href="/speech-tech/0060-multi-tier-speech-caching/" class="pagination--pager" title="Multi-tier Speech Caching Architecture">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
