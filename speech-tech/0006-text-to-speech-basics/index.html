<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Text-to-Speech (TTS) System Fundamentals - Arun Baby</title>
<meta name="description" content="From text to natural speech: understanding modern neural TTS architectures that power Alexa, Google Assistant, and Siri.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Text-to-Speech (TTS) System Fundamentals">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0006-text-to-speech-basics/">


  <meta property="og:description" content="From text to natural speech: understanding modern neural TTS architectures that power Alexa, Google Assistant, and Siri.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Text-to-Speech (TTS) System Fundamentals">
  <meta name="twitter:description" content="From text to natural speech: understanding modern neural TTS architectures that power Alexa, Google Assistant, and Siri.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0006-text-to-speech-basics/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0006-text-to-speech-basics/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Text-to-Speech (TTS) System Fundamentals">
    <meta itemprop="description" content="From text to natural speech: understanding modern neural TTS architectures that power Alexa, Google Assistant, and Siri.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0006-text-to-speech-basics/" itemprop="url">Text-to-Speech (TTS) System Fundamentals
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          20 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#tts-pipeline-architecture">TTS Pipeline Architecture</a><ul><li><a href="#traditional-two-stage-pipeline">Traditional Two-Stage Pipeline</a></li><li><a href="#why-two-stages">Why Two Stages?</a></li></ul></li><li><a href="#key-components">Key Components</a><ul><li><a href="#1-text-processing-frontend">1. Text Processing (Frontend)</a></li><li><a href="#2-acoustic-model">2. Acoustic Model</a></li><li><a href="#3-vocoder">3. Vocoder</a></li></ul></li><li><a href="#modern-tts-fastspeech-2">Modern TTS: FastSpeech 2</a></li><li><a href="#prosody-control">Prosody Control</a></li><li><a href="#connection-to-evaluation-metrics">Connection to Evaluation Metrics</a><ul><li><a href="#objective-metrics">Objective Metrics</a></li><li><a href="#subjective-metrics">Subjective Metrics</a></li></ul></li><li><a href="#production-considerations">Production Considerations</a><ul><li><a href="#latency">Latency</a></li><li><a href="#multi-speaker-tts">Multi-Speaker TTS</a></li></ul></li><li><a href="#training-data-requirements">Training Data Requirements</a><ul><li><a href="#dataset-characteristics">Dataset Characteristics</a></li><li><a href="#data-preparation-pipeline">Data Preparation Pipeline</a></li><li><a href="#data-quality-challenges">Data Quality Challenges</a></li></ul></li><li><a href="#voice-cloning--few-shot-learning">Voice Cloning &amp; Few-Shot Learning</a><ul><li><a href="#approaches">Approaches</a></li></ul></li><li><a href="#production-deployment-patterns">Production Deployment Patterns</a><ul><li><a href="#pattern-1-caching--dynamic-generation">Pattern 1: Caching + Dynamic Generation</a></li><li><a href="#pattern-2-streaming-tts">Pattern 2: Streaming TTS</a></li><li><a href="#pattern-3-edge-deployment">Pattern 3: Edge Deployment</a></li></ul></li><li><a href="#quality-assessment-in-production">Quality Assessment in Production</a><ul><li><a href="#automated-quality-monitoring">Automated Quality Monitoring</a></li></ul></li><li><a href="#comparative-analysis">Comparative Analysis</a><ul><li><a href="#tacotron-2-vs-fastspeech-2">Tacotron 2 vs FastSpeech 2</a></li><li><a href="#when-to-use-each">When to Use Each</a></li></ul></li><li><a href="#recent-advances-2023-2024">Recent Advances (2023-2024)</a><ul><li><a href="#1-vall-e-zero-shot-voice-cloning">1. VALL-E (Zero-Shot Voice Cloning)</a></li><li><a href="#2-vits-end-to-end-tts">2. VITS (End-to-End TTS)</a></li><li><a href="#3-yourtts-multi-lingual-voice-cloning">3. YourTTS (Multi-lingual Voice Cloning)</a></li><li><a href="#4-bark-generative-audio-model">4. Bark (Generative Audio Model)</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>From text to natural speech: understanding modern neural TTS architectures that power Alexa, Google Assistant, and Siri.</strong></p>

<h2 id="introduction">Introduction</h2>

<p><strong>Text-to-Speech (TTS)</strong> converts written text into spoken audio. Modern TTS systems produce human-like speech quality using deep learning.</p>

<p><strong>Why TTS matters:</strong></p>
<ul>
  <li><strong>Virtual assistants:</strong> Alexa, Google Assistant, Siri</li>
  <li><strong>Accessibility:</strong> Screen readers for visually impaired</li>
  <li><strong>Content creation:</strong> Audiobooks, podcasts, voiceovers</li>
  <li><strong>Conversational AI:</strong> Voice bots, IVR systems</li>
  <li><strong>Education:</strong> Language learning apps</li>
</ul>

<p><strong>Evolution:</strong></p>
<ol>
  <li><strong>Concatenative synthesis</strong> (1990s-2000s): Stitch pre-recorded audio units</li>
  <li><strong>Parametric synthesis</strong> (2000s-2010s): Statistical models (HMM)</li>
  <li><strong>Neural TTS</strong> (2016+): End-to-end deep learning (Tacotron, WaveNet)</li>
  <li><strong>Modern TTS</strong> (2020+): Fast, controllable, expressive (FastSpeech, VITS)</li>
</ol>

<hr />

<h2 id="tts-pipeline-architecture">TTS Pipeline Architecture</h2>

<h3 id="traditional-two-stage-pipeline">Traditional Two-Stage Pipeline</h3>

<p>Most TTS systems use a two-stage approach:</p>

<p><code class="language-plaintext highlighter-rouge">
Text → [Acoustic Model] → Mel Spectrogram → [Vocoder] → Audio Waveform
</code></p>

<p><strong>Stage 1: Acoustic Model (Text → Mel Spectrogram)</strong></p>
<ul>
  <li>Input: Text (characters/phonemes)</li>
  <li>Output: Mel spectrogram (acoustic features)</li>
  <li>Examples: Tacotron 2, FastSpeech 2</li>
</ul>

<p><strong>Stage 2: Vocoder (Mel Spectrogram → Waveform)</strong></p>
<ul>
  <li>Input: Mel spectrogram</li>
  <li>Output: Audio waveform</li>
  <li>Examples: WaveNet, WaveGlow, HiFi-GAN</li>
</ul>

<h3 id="why-two-stages">Why Two Stages?</h3>

<p><strong>Advantages:</strong></p>
<ul>
  <li><strong>Modularity:</strong> Train acoustic model and vocoder separately</li>
  <li><strong>Efficiency:</strong> Mel spectrogram is compressed representation</li>
  <li><strong>Controllability:</strong> Can modify prosody at mel spectrogram level</li>
</ul>

<p><strong>Alternative: End-to-End Models</strong></p>
<ul>
  <li>VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech)</li>
  <li>Directly generates waveform from text</li>
  <li>Faster inference, fewer components</li>
</ul>

<hr />

<h2 id="key-components">Key Components</h2>

<h3 id="1-text-processing-frontend">1. Text Processing (Frontend)</h3>

<p>Transform raw text into model-ready input.</p>

<p>``python
class TextProcessor:
 “””
 Text normalization and phoneme conversion
 “””</p>

<p>def <strong>init</strong>(self):
 self.normalizer = TextNormalizer()
 self.g2p = Grapheme2Phoneme() # Grapheme-to-Phoneme</p>

<p>def process(self, text: str) -&gt; list[str]:
 “””
 Convert text to phoneme sequence</p>

<p>Args:
 text: Raw input text</p>

<p>Returns:
 List of phonemes
 “””
 # 1. Normalize text
 normalized = self.normalizer.normalize(text)
 # “Dr. Smith has $100” → “Doctor Smith has one hundred dollars”</p>

<p># 2. Convert to phonemes
 phonemes = self.g2p.convert(normalized)
 # “hello” → [‘HH’, ‘AH’, ‘L’, ‘OW’]</p>

<p>return phonemes</p>

<p>class TextNormalizer:
 “””
 Normalize text (expand abbreviations, numbers, etc.)
 “””</p>

<p>def normalize(self, text: str) -&gt; str:
 text = self._expand_abbreviations(text)
 text = self._expand_numbers(text)
 text = self._expand_symbols(text)
 return text</p>

<p>def _expand_abbreviations(self, text: str) -&gt; str:
 “"”Dr. → Doctor, Mr. → Mister, etc.”””
 expansions = {
 ‘Dr.’: ‘Doctor’,
 ‘Mr.’: ‘Mister’,
 ‘Mrs.’: ‘Misses’,
 ‘Ms.’: ‘Miss’,
 ‘St.’: ‘Street’,
 }
 for abbr, expansion in expansions.items():
 text = text.replace(abbr, expansion)
 return text</p>

<p>def _expand_numbers(self, text: str) -&gt; str:
 “”“$100 → one hundred dollars”””
 import re</p>

<p># Currency
 text = re.sub(r’$(\d+)’, r’\1 dollars’, text)</p>

<p># Years
 text = re.sub(r’(\d{4})’, self._year_to_words, text)</p>

<p>return text</p>

<p>def _year_to_words(self, match) -&gt; str:
 “"”Convert year to words: 2024 → twenty twenty-four”””
 # Simplified implementation
 return match.group(0) # Placeholder</p>

<p>def _expand_symbols(self, text: str) -&gt; str:
 “””@ → at, % → percent, etc.”””
 symbols = {
 ‘@’: ‘at’,
 ‘%’: ‘percent’,
 ‘#’: ‘number’,
 ‘&amp;’: ‘and’,
 }
 for symbol, expansion in symbols.items():
 text = text.replace(symbol, expansion)
 return text
``</p>

<h3 id="2-acoustic-model">2. Acoustic Model</h3>

<p>Generates mel spectrogram from text/phonemes.</p>

<p><strong>Tacotron 2 Architecture (simplified):</strong></p>

<p><code class="language-plaintext highlighter-rouge">
Input: Phoneme sequence
 ↓
[Character Embeddings]
 ↓
[Encoder] (Bidirectional LSTM)
 ↓
[Attention] (Location-sensitive)
 ↓
[Decoder] (Autoregressive LSTM)
 ↓
[Mel Predictor]
 ↓
Output: Mel Spectrogram
</code></p>

<p>``python
import torch
import torch.nn as nn</p>

<p>class SimplifiedTacotron(nn.Module):
 “””
 Simplified Tacotron-style acoustic model</p>

<p>Real Tacotron 2 is much more complex!
 “””</p>

<p>def <strong>init</strong>(
 self,
 vocab_size: int,
 embedding_dim: int = 512,
 encoder_hidden: int = 256,
 decoder_hidden: int = 1024,
 n_mels: int = 80
 ):
 super().<strong>init</strong>()</p>

<p># Character/phoneme embeddings
 self.embedding = nn.Embedding(vocab_size, embedding_dim)</p>

<p># Encoder
 self.encoder = nn.LSTM(
 embedding_dim,
 encoder_hidden,
 num_layers=3,
 batch_first=True,
 bidirectional=True
 )</p>

<p># Attention
 self.attention = LocationSensitiveAttention(
 encoder_hidden * 2, # Bidirectional
 decoder_hidden
 )</p>

<p># Decoder
 self.decoder = nn.LSTMCell(
 encoder_hidden * 2 + n_mels, # Context + previous mel frame
 decoder_hidden
 )</p>

<p># Mel predictor
 self.mel_predictor = nn.Linear(decoder_hidden, n_mels)</p>

<p># Stop token predictor
 self.stop_predictor = nn.Linear(decoder_hidden, 1)</p>

<p>def forward(self, text, mel_targets=None):
 “””
 Forward pass</p>

<p>Args:
 text: [batch, seq_len] phoneme indices
 mel_targets: [batch, mel_len, n_mels] target mels (training only)</p>

<p>Returns:
 mels: [batch, mel_len, n_mels]
 stop_tokens: [batch, mel_len]
 “””
 # Encode text
 embedded = self.embedding(text) # [batch, seq_len, embed_dim]
 encoder_outputs, _ = self.encoder(embedded) # [batch, seq_len, hidden*2]</p>

<p># Decode (autoregressive)
 if mel_targets is not None:
 # Teacher forcing during training
 return self._decode_teacher_forcing(encoder_outputs, mel_targets)
 else:
 # Autoregressive during inference
 return self._decode_autoregressive(encoder_outputs, max_len=1000)</p>

<p>def _decode_autoregressive(self, encoder_outputs, max_len):
 “"”Autoregressive decoding (inference)”””
 batch_size = encoder_outputs.size(0)
 n_mels = self.mel_predictor.out_features</p>

<p># Initialize
 decoder_hidden = torch.zeros(batch_size, self.decoder.hidden_size)
 decoder_cell = torch.zeros(batch_size, self.decoder.hidden_size)
 attention_context = torch.zeros(batch_size, encoder_outputs.size(2))
 mel_frame = torch.zeros(batch_size, n_mels)</p>

<p>mels = []
 stop_tokens = []</p>

<p>for _ in range(max_len):
 # Compute attention
 attention_context, _ = self.attention(
 decoder_hidden,
 encoder_outputs
 )</p>

<p># Decoder step
 decoder_input = torch.cat([attention_context, mel_frame], dim=1)
 decoder_hidden, decoder_cell = self.decoder(
 decoder_input,
 (decoder_hidden, decoder_cell)
 )</p>

<p># Predict mel frame and stop token
 mel_frame = self.mel_predictor(decoder_hidden)
 stop_token = torch.sigmoid(self.stop_predictor(decoder_hidden))</p>

<p>mels.append(mel_frame)
 stop_tokens.append(stop_token)</p>

<p># Check if should stop
 if (stop_token &gt; 0.5).all():
 break</p>

<p>mels = torch.stack(mels, dim=1) # [batch, mel_len, n_mels]
 stop_tokens = torch.stack(stop_tokens, dim=1) # [batch, mel_len, 1]</p>

<p>return mels, stop_tokens</p>

<p>class LocationSensitiveAttention(nn.Module):
 “””
 Location-sensitive attention (simplified)</p>

<p>Note: Real Tacotron uses cumulative attention features; this
 minimal version omits location convolution for brevity.
 “””</p>

<p>def <strong>init</strong>(self, encoder_dim, decoder_dim, attention_dim=128):
 super().<strong>init</strong>()</p>

<p>self.query_layer = nn.Linear(decoder_dim, attention_dim)
 self.key_layer = nn.Linear(encoder_dim, attention_dim)
 self.value_layer = nn.Linear(attention_dim, 1)</p>

<p># For brevity, location features are omitted in this simplified demo</p>

<p>def forward(self, query, keys):
 “””
 Compute attention context</p>

<p>Args:
 query: [batch, decoder_dim] - current decoder state
 keys: [batch, seq_len, encoder_dim] - encoder outputs</p>

<p>Returns:
 context: [batch, encoder_dim]
 attention_weights: [batch, seq_len]
 “””
 # Compute attention scores
 query_proj = self.query_layer(query).unsqueeze(1) # [batch, 1, attn_dim]
 keys_proj = self.key_layer(keys) # [batch, seq_len, attn_dim]</p>

<p>scores = self.value_layer(torch.tanh(query_proj + keys_proj)).squeeze(2)
 attention_weights = torch.softmax(scores, dim=1) # [batch, seq_len]</p>

<p># Compute context
 context = torch.bmm(
 attention_weights.unsqueeze(1),
 keys
 ).squeeze(1) # [batch, encoder_dim]</p>

<p>return context, attention_weights
``</p>

<h3 id="3-vocoder">3. Vocoder</h3>

<p>Converts mel spectrogram to waveform.</p>

<p><strong>Popular Vocoders:</strong></p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Type</th>
      <th>Quality</th>
      <th>Speed</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>WaveNet</td>
      <td>Autoregressive</td>
      <td>Excellent</td>
      <td>Slow</td>
      <td>Original neural vocoder</td>
    </tr>
    <tr>
      <td>WaveGlow</td>
      <td>Flow-based</td>
      <td>Excellent</td>
      <td>Fast</td>
      <td>Parallel generation</td>
    </tr>
    <tr>
      <td>HiFi-GAN</td>
      <td>GAN-based</td>
      <td>Excellent</td>
      <td>Very Fast</td>
      <td>Current SOTA</td>
    </tr>
    <tr>
      <td>MelGAN</td>
      <td>GAN-based</td>
      <td>Good</td>
      <td>Very Fast</td>
      <td>Lightweight</td>
    </tr>
  </tbody>
</table>

<p><strong>HiFi-GAN Architecture:</strong></p>

<p>``python
import torch
import torch.nn as nn</p>

<p>class HiFiGANGenerator(nn.Module):
 “””
 Simplified HiFi-GAN generator</p>

<p>Upsamples mel spectrogram to waveform
 “””</p>

<p>def <strong>init</strong>(
 self,
 n_mels: int = 80,
 upsample_rates: list[int] = [8, 8, 2, 2],
 upsample_kernel_sizes: list[int] = [16, 16, 4, 4],
 resblock_kernel_sizes: list[int] = [3, 7, 11],
 resblock_dilation_sizes: list[list[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]]
 ):
 super().<strong>init</strong>()</p>

<p>self.num_kernels = len(resblock_kernel_sizes)
 self.num_upsamples = len(upsample_rates)</p>

<p># Initial conv
 self.conv_pre = nn.Conv1d(n_mels, 512, kernel_size=7, padding=3)</p>

<p># Upsampling layers
 self.ups = nn.ModuleList()
 for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):
 self.ups.append(
 nn.ConvTranspose1d(
 512 // (2 ** i),
 512 // (2 ** (i + 1)),
 kernel_size=k,
 stride=u,
 padding=(k - u) // 2
 )
 )</p>

<p># Residual blocks
 self.resblocks = nn.ModuleList()
 for i in range(len(self.ups)):
 ch = 512 // (2 ** (i + 1))
 for k, d in zip(resblock_kernel_sizes, resblock_dilation_sizes):
 self.resblocks.append(ResBlock(ch, k, d))</p>

<p># Final conv
 self.conv_post = nn.Conv1d(ch, 1, kernel_size=7, padding=3)</p>

<p>def forward(self, mel):
 “””
 Generate waveform from mel spectrogram</p>

<p>Args:
 mel: [batch, n_mels, mel_len]</p>

<p>Returns:
 waveform: [batch, 1, audio_len]
 “””
 x = self.conv_pre(mel)</p>

<p>for i, up in enumerate(self.ups):
 x = torch.nn.functional.leaky_relu(x, 0.1)
 x = up(x)</p>

<p># Apply residual blocks
 xs = None
 for j in range(self.num_kernels):
 if xs is None:
 xs = self.resblocks<a href="x">i * self.num_kernels + j</a>
 else:
 xs += self.resblocks<a href="x">i * self.num_kernels + j</a>
 x = xs / self.num_kernels</p>

<p>x = torch.nn.functional.leaky_relu(x)
 x = self.conv_post(x)
 x = torch.tanh(x)</p>

<p>return x</p>

<p>class ResBlock(nn.Module):
 “"”Residual block with dilated convolutions”””</p>

<p>def <strong>init</strong>(self, channels, kernel_size, dilations):
 super().<strong>init</strong>()</p>

<p>self.convs = nn.ModuleList()
 for d in dilations:
 self.convs.append(
 nn.Conv1d(
 channels,
 channels,
 kernel_size=kernel_size,
 dilation=d,
 padding=(kernel_size * d - d) // 2
 )
 )</p>

<p>def forward(self, x):
 for conv in self.convs:
 xt = torch.nn.functional.leaky_relu(x, 0.1)
 xt = conv(xt)
 x = x + xt
 return x
``</p>

<hr />

<h2 id="modern-tts-fastspeech-2">Modern TTS: FastSpeech 2</h2>

<p><strong>Problem with Tacotron:</strong> Autoregressive decoding is slow and can have attention errors.</p>

<p><strong>FastSpeech 2 advantages:</strong></p>
<ul>
  <li><strong>Non-autoregressive:</strong> Generates all mel frames in parallel (much faster)</li>
  <li><strong>Duration prediction:</strong> Predicts phoneme durations explicitly</li>
  <li><strong>Controllability:</strong> Can control pitch, energy, duration</li>
</ul>

<p><strong>Architecture:</strong></p>

<p><code class="language-plaintext highlighter-rouge">
Input: Phoneme sequence
 ↓
[Phoneme Embeddings]
 ↓
[Feed-Forward Transformer]
 ↓
[Duration Predictor] → phoneme durations
[Pitch Predictor] → pitch contour
[Energy Predictor] → energy contour
 ↓
[Length Regulator] (expand phonemes by duration)
 ↓
[Feed-Forward Transformer]
 ↓
Output: Mel Spectrogram
</code></p>

<p><strong>Key innovation: Length Regulator</strong></p>

<p>``python
def length_regulator(phoneme_features, durations):
 “””
 Expand phoneme features based on predicted durations</p>

<p>Args:
 phoneme_features: [batch, phoneme_len, hidden]
 durations: [batch, phoneme_len] - frames per phoneme</p>

<p>Returns:
 expanded: [batch, mel_len, hidden]
 “””
 expanded = []</p>

<p>for batch_idx in range(phoneme_features.size(0)):
 batch_expanded = []</p>

<p>for phoneme_idx in range(phoneme_features.size(1)):
 feature = phoneme_features[batch_idx, phoneme_idx]
 duration = durations[batch_idx, phoneme_idx].int()</p>

<p># Repeat feature ‘duration’ times
 batch_expanded.append(feature.repeat(duration, 1))</p>

<p>batch_expanded = torch.cat(batch_expanded, dim=0)
 expanded.append(batch_expanded)</p>

<p># Pad to max length
 max_len = max(e.size(0) for e in expanded)
 expanded = [torch.nn.functional.pad(e, (0, 0, 0, max_len - e.size(0))) for e in expanded]</p>

<p>return torch.stack(expanded)</p>

<h1 id="example">Example</h1>
<p>phoneme_features = torch.randn(1, 10, 256) # 10 phonemes
durations = torch.tensor([[5, 3, 4, 6, 2, 5, 7, 3, 4, 5]]) # Frames per phoneme</p>

<p>expanded = length_regulator(phoneme_features, durations)
print(f”Input shape: {phoneme_features.shape}”)
print(f”Output shape: {expanded.shape}”) # [1, 44, 256] (sum of durations)
``</p>

<hr />

<h2 id="prosody-control">Prosody Control</h2>

<p><strong>Prosody:</strong> Rhythm, stress, and intonation of speech</p>

<p><strong>Control dimensions:</strong></p>
<ul>
  <li><strong>Pitch:</strong> Fundamental frequency (F0)</li>
  <li><strong>Duration:</strong> Phoneme/word length</li>
  <li><strong>Energy:</strong> Loudness</li>
</ul>

<p>``python
class ProsodyController:
 “””
 Control prosody in TTS generation
 “””</p>

<p>def <strong>init</strong>(self, model):
 self.model = model</p>

<p>def synthesize_with_prosody(
 self,
 text: str,
 pitch_scale: float = 1.0,
 duration_scale: float = 1.0,
 energy_scale: float = 1.0
 ):
 “””
 Generate speech with prosody control</p>

<p>Args:
 text: Input text
 pitch_scale: Multiply pitch by this factor (&gt;1 = higher, &lt;1 = lower)
 duration_scale: Multiply duration by this factor (&gt;1 = slower, &lt;1 = faster)
 energy_scale: Multiply energy by this factor (&gt;1 = louder, &lt;1 = softer)</p>

<p>Returns:
 audio: Generated waveform
 “””
 # Get model predictions
 phonemes = self.model.text_to_phonemes(text)
 mel_spec, pitch, duration, energy = self.model.predict(phonemes)</p>

<p># Apply prosody modifications
 pitch_modified = pitch * pitch_scale
 duration_modified = duration * duration_scale
 energy_modified = energy * energy_scale</p>

<p># Regenerate mel spectrogram with modified prosody
 mel_spec_modified = self.model.synthesize_mel(
 phonemes,
 pitch=pitch_modified,
 duration=duration_modified,
 energy=energy_modified
 )</p>

<p># Vocoder: mel → waveform
 audio = self.model.vocoder(mel_spec_modified)</p>

<p>return audio</p>

<h1 id="usage-example">Usage example</h1>
<p>controller = ProsodyController(tts_model)</p>

<h1 id="happy-speech-higher-pitch-faster">Happy speech: higher pitch, faster</h1>
<p>happy_audio = controller.synthesize_with_prosody(
 “Hello, how are you?”,
 pitch_scale=1.2,
 duration_scale=0.9,
 energy_scale=1.1
)</p>

<h1 id="sad-speech-lower-pitch-slower">Sad speech: lower pitch, slower</h1>
<p>sad_audio = controller.synthesize_with_prosody(
 “Hello, how are you?”,
 pitch_scale=0.8,
 duration_scale=1.2,
 energy_scale=0.9
)
``</p>

<hr />

<h2 id="connection-to-evaluation-metrics">Connection to Evaluation Metrics</h2>

<p>Like ML model evaluation, TTS systems need multiple metrics:</p>

<h3 id="objective-metrics">Objective Metrics</h3>

<p><strong>Mel Cepstral Distortion (MCD):</strong></p>
<ul>
  <li>Measures distance between generated and ground truth mels</li>
  <li>Lower is better</li>
  <li>Correlates with quality but not perfectly</li>
</ul>

<p>``python
import librosa
import numpy as np</p>

<p>def mel_cepstral_distortion(generated_mel, target_mel):
 “””
 Compute MCD between generated and target mel spectrograms</p>

<p>Args:
 generated_mel: [n_mels, time]
 target_mel: [n_mels, time]</p>

<p>Returns:
 MCD score (lower is better)
 “””
 # Align lengths
 min_len = min(generated_mel.shape[1], target_mel.shape[1])
 generated_mel = generated_mel[:, :min_len]
 target_mel = target_mel[:, :min_len]</p>

<p># Compute simple L2 distance over mel frames (proxy for MCD)
 diff = generated_mel - target_mel
 mcd = float(np.linalg.norm(diff, axis=0).mean())</p>

<p>return mcd
``</p>

<p><strong>F0 RMSE:</strong> Root mean squared error of pitch</p>

<p><strong>Duration Accuracy:</strong> How well predicted durations match ground truth</p>

<h3 id="subjective-metrics">Subjective Metrics</h3>

<p><strong>Mean Opinion Score (MOS):</strong></p>
<ul>
  <li>Human raters score quality 1-5</li>
  <li>Gold standard for TTS evaluation</li>
  <li>Expensive and time-consuming</li>
</ul>

<p><strong>MUSHRA Test:</strong> Compare multiple systems side-by-side</p>

<hr />

<h2 id="production-considerations">Production Considerations</h2>

<h3 id="latency">Latency</h3>

<p><strong>Components of TTS latency:</strong></p>
<ol>
  <li><strong>Text processing:</strong> 5-10ms</li>
  <li><strong>Acoustic model:</strong> 50-200ms (depends on text length)</li>
  <li><strong>Vocoder:</strong> 20-100ms</li>
  <li><strong>Total:</strong> 75-310ms</li>
</ol>

<p><strong>Optimization strategies:</strong></p>
<ul>
  <li><strong>Streaming TTS:</strong> Generate audio incrementally</li>
  <li><strong>Model distillation:</strong> Smaller, faster models</li>
  <li><strong>Quantization:</strong> INT8 inference</li>
  <li><strong>Caching:</strong> Pre-generate common phrases</li>
</ul>

<h3 id="multi-speaker-tts">Multi-Speaker TTS</h3>

<p>``python
class MultiSpeakerTTS:
 “””
 TTS supporting multiple voices</p>

<p>Approach 1: Speaker embedding
 Approach 2: Separate models per speaker
 “””</p>

<p>def <strong>init</strong>(self, model, speaker_embeddings):
 self.model = model
 self.speaker_embeddings = speaker_embeddings</p>

<p>def synthesize(self, text: str, speaker_id: int):
 “””
 Generate speech in specific speaker’s voice</p>

<p>Args:
 text: Input text
 speaker_id: Speaker identifier</p>

<p>Returns:
 audio: Waveform in target speaker’s voice
 “””
 # Get speaker embedding
 speaker_emb = self.speaker_embeddings[speaker_id]</p>

<p># Generate with speaker conditioning
 mel = self.model.generate_mel(text, speaker_embedding=speaker_emb)
 audio = self.model.vocoder(mel)</p>

<p>return audio
``</p>

<hr />

<h2 id="training-data-requirements">Training Data Requirements</h2>

<h3 id="dataset-characteristics">Dataset Characteristics</h3>

<p><strong>Typical single-speaker TTS training:</strong></p>
<ul>
  <li><strong>Audio hours:</strong> 10-24 hours of clean speech</li>
  <li><strong>Utterances:</strong> 5,000-15,000 sentences</li>
  <li><strong>Recording quality:</strong> Studio quality, 22 kHz+ sample rate</li>
  <li><strong>Text diversity:</strong> Cover phonetic diversity of language</li>
</ul>

<p><strong>Multi-speaker TTS:</strong></p>
<ul>
  <li><strong>Speakers:</strong> 100-10,000 speakers</li>
  <li><strong>Hours per speaker:</strong> 1-5 hours</li>
  <li><strong>Total hours:</strong> 100-50,000 hours (e.g., LibriTTS: 585 hours, 2,456 speakers)</li>
</ul>

<h3 id="data-preparation-pipeline">Data Preparation Pipeline</h3>

<p>``python
class TTSDataPreparation:
 “””
 Prepare data for TTS training
 “””</p>

<p>def <strong>init</strong>(self, sample_rate=22050):
 self.sample_rate = sample_rate</p>

<p>def prepare_dataset(
 self,
 audio_files: list[str],
 text_files: list[str]
 ) -&gt; list[dict]:
 “””
 Prepare audio-text pairs</p>

<p>Steps:</p>
<ol>
  <li>Text normalization</li>
  <li>Audio preprocessing</li>
  <li>Alignment (forced alignment)</li>
  <li>Feature extraction
 “””
 dataset = []</li>
</ol>

<p>for audio_file, text_file in zip(audio_files, text_files):
 # Load audio
 audio, sr = librosa.load(audio_file, sr=self.sample_rate)</p>

<p># Load text
 with open(text_file) as f:
 text = f.read().strip()</p>

<p># Normalize text
 normalized_text = self.normalize_text(text)</p>

<p># Convert to phonemes
 phonemes = self.text_to_phonemes(normalized_text)</p>

<p># Extract mel spectrogram
 mel = self.extract_mel(audio)</p>

<p># Extract prosody features
 pitch = self.extract_pitch(audio)
 energy = self.extract_energy(audio)</p>

<p># Compute duration (requires forced alignment)
 duration = self.compute_durations(audio, phonemes)</p>

<p>dataset.append({
 ‘audio_path’: audio_file,
 ‘text’: text,
 ‘normalized_text’: normalized_text,
 ‘phonemes’: phonemes,
 ‘mel’: mel,
 ‘pitch’: pitch,
 ‘energy’: energy,
 ‘duration’: duration
 })</p>

<p>return dataset</p>

<p>def extract_mel(self, audio):
 “"”Extract mel spectrogram”””
 mel = librosa.feature.melspectrogram(
 y=audio,
 sr=self.sample_rate,
 n_fft=1024,
 hop_length=256,
 n_mels=80
 )
 mel_db = librosa.power_to_db(mel, ref=np.max)
 return mel_db</p>

<p>def extract_pitch(self, audio):
 “"”Extract pitch (F0) contour”””
 f0, voiced_flag, voiced_probs = librosa.pyin(
 audio,
 fmin=librosa.note_to_hz(‘C2’),
 fmax=librosa.note_to_hz(‘C7’),
 sr=self.sample_rate
 )
 return f0</p>

<p>def extract_energy(self, audio):
 “"”Extract energy (RMS)”””
 energy = librosa.feature.rms(
 y=audio,
 frame_length=1024,
 hop_length=256
 )[0]
 return energy
``</p>

<h3 id="data-quality-challenges">Data Quality Challenges</h3>

<p><strong>1. Noisy Audio:</strong></p>
<ul>
  <li>Background noise degrades quality</li>
  <li>Solution: Use noise reduction, or data augmentation</li>
</ul>

<p><strong>2. Alignment Errors:</strong></p>
<ul>
  <li>Text-audio misalignment breaks training</li>
  <li>Solution: Forced alignment with Montreal Forced Aligner (MFA)</li>
</ul>

<p><strong>3. Prosody Variation:</strong></p>
<ul>
  <li>Inconsistent prosody confuses models</li>
  <li>Solution: Filter outliers, normalize prosody</li>
</ul>

<p><strong>4. Out-of-Domain Text:</strong></p>
<ul>
  <li>Model struggles with unseen words/names</li>
  <li>Solution: Diverse training text, robust G2P</li>
</ul>

<hr />

<h2 id="voice-cloning--few-shot-learning">Voice Cloning &amp; Few-Shot Learning</h2>

<p><strong>Voice cloning:</strong> Generate speech in a target voice with minimal data.</p>

<h3 id="approaches">Approaches</h3>

<p><strong>1. Speaker Embedding (Zero-Shot/Few-Shot)</strong></p>

<p>``python
class SpeakerEncoder(nn.Module):
 “””
 Encode speaker characteristics from reference audio</p>

<p>Architecture: Similar to speaker recognition
 “””</p>

<p>def <strong>init</strong>(self, mel_dim=80, embedding_dim=256):
 super().<strong>init</strong>()</p>

<p># LSTM encoder
 self.encoder = nn.LSTM(
 mel_dim,
 embedding_dim,
 num_layers=3,
 batch_first=True
 )</p>

<p># Projection to speaker embedding
 self.projection = nn.Linear(embedding_dim, embedding_dim)</p>

<p>def forward(self, mel):
 “””
 Extract speaker embedding from mel spectrogram</p>

<p>Args:
 mel: [batch, mel_len, mel_dim]</p>

<p>Returns:
 speaker_embedding: [batch, embedding_dim]
 “””
 _, (hidden, _) = self.encoder(mel)</p>

<p># Use last hidden state
 speaker_emb = self.projection(hidden[-1])</p>

<p># L2 normalize
 speaker_emb = speaker_emb / (speaker_emb.norm(dim=1, keepdim=True) + 1e-8)</p>

<p>return speaker_emb</p>

<p>class VoiceCloningTTS:
 “””
 TTS with voice cloning capability
 “””</p>

<p>def <strong>init</strong>(self, acoustic_model, vocoder, speaker_encoder):
 self.acoustic_model = acoustic_model
 self.vocoder = vocoder
 self.speaker_encoder = speaker_encoder</p>

<p>def clone_voice(
 self,
 text: str,
 reference_audio: torch.Tensor
 ):
 “””
 Generate speech in reference voice</p>

<p>Args:
 text: Text to synthesize
 reference_audio: Audio sample of target voice (3-10 seconds)</p>

<p>Returns:
 synthesized_audio: Speech in target voice
 “””
 # Extract speaker embedding from reference
 reference_mel = self.extract_mel(reference_audio)
 speaker_embedding = self.speaker_encoder(reference_mel)</p>

<p># Generate mel spectrogram conditioned on speaker
 mel = self.acoustic_model.generate(
 text,
 speaker_embedding=speaker_embedding
 )</p>

<p># Vocoder: mel → waveform
 audio = self.vocoder(mel)</p>

<p>return audio</p>

<h1 id="usage">Usage</h1>
<p>tts = VoiceCloningTTS(acoustic_model, vocoder, speaker_encoder)</p>

<h1 id="clone-voice-from-5-second-reference">Clone voice from 5-second reference</h1>
<p>reference_audio = load_audio(“reference_voice.wav”)
cloned_speech = tts.clone_voice(
 “Hello, this is a cloned voice!”,
 reference_audio
)
``</p>

<p><strong>2. Fine-Tuning (10-60 minutes of data)</strong></p>

<p>``python
class VoiceCloner:
 “””
 Fine-tune pre-trained TTS on target voice
 “””</p>

<p>def <strong>init</strong>(self, pretrained_model):
 self.model = pretrained_model</p>

<p>def fine_tune(
 self,
 target_voice_data: list[tuple], # [(audio, text), …]
 num_steps: int = 1000,
 learning_rate: float = 1e-4
 ):
 “””
 Fine-tune model on target voice</p>

<p>Args:
 target_voice_data: Audio-text pairs for target speaker
 num_steps: Training steps
 learning_rate: Learning rate
 “””
 optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)</p>

<p>for step in range(num_steps):
 # Sample batch
 batch = random.sample(target_voice_data, min(8, len(target_voice_data)))</p>

<p># Forward pass
 loss = self.model.compute_loss(batch)</p>

<p># Backward pass
 optimizer.zero_grad()
 loss.backward()
 optimizer.step()</p>

<p>if step % 100 == 0:
 print(f”Step {step}, Loss: {loss.item():.4f}”)</p>

<p>return self.model</p>

<h1 id="usage-1">Usage</h1>
<p>cloner = VoiceCloner(pretrained_tts_model)</p>

<h1 id="collect-30-minutes-of-target-voice">Collect 30 minutes of target voice</h1>
<p>target_data = […] # List of (audio, text) pairs</p>

<h1 id="fine-tune">Fine-tune</h1>
<p>cloned_model = cloner.fine_tune(target_data, num_steps=1000)
``</p>

<hr />

<h2 id="production-deployment-patterns">Production Deployment Patterns</h2>

<h3 id="pattern-1-caching--dynamic-generation">Pattern 1: Caching + Dynamic Generation</h3>

<p>``python
class HybridTTSSystem:
 “””
 Hybrid system: Cache common phrases, generate on-the-fly for novel text
 “””</p>

<p>def <strong>init</strong>(self, tts_model, cache_backend):
 self.tts = tts_model
 self.cache = cache_backend # e.g., Redis
 self.common_phrases = [
 “Welcome”, “Thank you”, “Goodbye”,
 “Please hold”, “One moment please”
 ]
 self._warm_cache()</p>

<p>def _warm_cache(self):
 “"”Pre-generate and cache common phrases”””
 for phrase in self.common_phrases:
 if not self.cache.exists(phrase):
 audio = self.tts.synthesize(phrase)
 self.cache.set(phrase, audio, ttl=86400) # 24-hour TTL</p>

<p>def synthesize(self, text: str):
 “””
 Synthesize with caching</p>

<p>Cache hit: &lt;5ms
 Cache miss: 100-300ms (generate)
 “””
 # Check cache
 cached = self.cache.get(text)
 if cached is not None:
 return cached</p>

<p># Generate
 audio = self.tts.synthesize(text)</p>

<p># Cache if frequently requested
 request_count = self.cache.increment(f”count:{text}”)
 if request_count &gt; 10:
 self.cache.set(text, audio, ttl=3600)</p>

<p>return audio
``</p>

<h3 id="pattern-2-streaming-tts">Pattern 2: Streaming TTS</h3>

<p>``python
class StreamingTTS:
 “””
 Stream audio as it’s generated (reduce latency)
 “””</p>

<p>def <strong>init</strong>(self, acoustic_model, vocoder):
 self.acoustic_model = acoustic_model
 self.vocoder = vocoder</p>

<p>def stream_synthesize(self, text: str):
 “””
 Generate audio in chunks</p>

<p>Yields audio chunks as they’re ready
 “””
 # Generate mel spectrogram
 mel_frames = self.acoustic_model.generate_streaming(text)</p>

<p># Stream vocoder output
 mel_buffer = []
 chunk_size = 50 # mel frames per chunk</p>

<p>for mel_frame in mel_frames:
 mel_buffer.append(mel_frame)</p>

<p>if len(mel_buffer) &gt;= chunk_size:
 # Vocoder: mel chunk → audio chunk
 mel_chunk = torch.stack(mel_buffer)
 audio_chunk = self.vocoder(mel_chunk)</p>

<p>yield audio_chunk</p>

<p># Keep overlap for smoothness
 mel_buffer = mel_buffer[-10:]</p>

<p># Final chunk
 if mel_buffer:
 mel_chunk = torch.stack(mel_buffer)
 audio_chunk = self.vocoder(mel_chunk)
 yield audio_chunk</p>

<h1 id="usage-2">Usage</h1>
<p>streaming_tts = StreamingTTS(acoustic_model, vocoder)</p>

<h1 id="stream-audio">Stream audio</h1>
<p>for audio_chunk in streaming_tts.stream_synthesize(“Long text to synthesize…”):
 # Play audio_chunk immediately
 play_audio(audio_chunk)
 # User starts hearing speech before full generation completes!
``</p>

<h3 id="pattern-3-edge-deployment">Pattern 3: Edge Deployment</h3>

<p>``python
class EdgeTTS:
 “””
 TTS optimized for edge devices (phones, IoT)
 “””</p>

<p>def <strong>init</strong>(self):
 self.model = self.load_optimized_model()</p>

<p>def load_optimized_model(self):
 “””
 Load quantized, pruned model</p>

<p>Techniques:</p>
<ul>
  <li>INT8 quantization (4x smaller, 2-4x faster)</li>
  <li>Knowledge distillation (smaller student model)</li>
  <li>Pruning (remove 30-50% of weights)
 “””
 import torch.quantization</li>
</ul>

<p># Load full precision model
 model = load_full_model()</p>

<p># Quantize to INT8
 # Use dynamic quantization for linear-heavy modules as a safe default
 model_quantized = torch.quantization.quantize_dynamic(
 model,
 {nn.Linear},
 dtype=torch.qint8
 )</p>

<p>return model_quantized</p>

<p>def synthesize_on_device(self, text: str):
 “””
 Synthesize on edge device</p>

<p>Latency: 50-150ms
 Memory: &lt;100MB
 “””
 audio = self.model.generate(text)
 return audio
``</p>

<hr />

<h2 id="quality-assessment-in-production">Quality Assessment in Production</h2>

<h3 id="automated-quality-monitoring">Automated Quality Monitoring</h3>

<p>``python
class TTSQualityMonitor:
 “””
 Monitor TTS quality in production
 “””</p>

<p>def <strong>init</strong>(self):
 self.baseline_mcd = 2.5 # Expected MCD
 self.alert_threshold = 3.5 # Alert if MCD &gt; this</p>

<p>def monitor_synthesis(self, text: str, generated_audio: np.ndarray):
 “””
 Check quality of generated audio</p>

<p>Red flags:</p>
<ul>
  <li>Abnormal duration</li>
  <li>Clipping / distortion</li>
  <li>Silent segments</li>
  <li>MCD drift
 “””
 issues = []</li>
</ul>

<p># Check duration
 expected_duration = len(text.split()) * 0.5 # ~0.5s per word
 actual_duration = len(generated_audio) / 22050
 if abs(actual_duration - expected_duration) / expected_duration &gt; 0.5:
 issues.append(f”Abnormal duration: {actual_duration:.2f}s vs {expected_duration:.2f}s expected”)</p>

<p># Check for clipping
 if np.max(np.abs(generated_audio)) &gt; 0.99:
 issues.append(“Clipping detected”)</p>

<p># Check for silent segments
 rms = librosa.feature.rms(y=generated_audio)[0]
 silent_ratio = (rms &lt; 0.01).sum() / len(rms)
 if silent_ratio &gt; 0.3:
 issues.append(f”Too much silence: {silent_ratio:.1%}”)</p>

<p># Log for drift detection
 self.log_quality_metrics({
 ‘text_length’: len(text),
 ‘audio_duration’: actual_duration,
 ‘max_amplitude’: np.max(np.abs(generated_audio)),
 ‘silent_ratio’: silent_ratio
 })</p>

<p>return issues</p>

<p>def log_quality_metrics(self, metrics: dict):
 “"”Log metrics for drift detection”””
 # Send to monitoring system (Datadog, Prometheus, etc.)
 pass
``</p>

<hr />

<h2 id="comparative-analysis">Comparative Analysis</h2>

<h3 id="tacotron-2-vs-fastspeech-2">Tacotron 2 vs FastSpeech 2</h3>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Tacotron 2</th>
      <th>FastSpeech 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Speed</strong></td>
      <td>Slow (autoregressive)</td>
      <td>Fast (parallel)</td>
    </tr>
    <tr>
      <td><strong>Quality</strong></td>
      <td>Excellent</td>
      <td>Excellent</td>
    </tr>
    <tr>
      <td><strong>Robustness</strong></td>
      <td>Can skip/repeat words</td>
      <td>More robust</td>
    </tr>
    <tr>
      <td><strong>Controllability</strong></td>
      <td>Limited</td>
      <td>Explicit control (pitch, duration)</td>
    </tr>
    <tr>
      <td><strong>Training</strong></td>
      <td>Simpler (no duration model)</td>
      <td>Needs duration labels</td>
    </tr>
    <tr>
      <td><strong>Latency</strong></td>
      <td>200-500ms</td>
      <td>50-150ms</td>
    </tr>
  </tbody>
</table>

<h3 id="when-to-use-each">When to Use Each</h3>

<p><strong>Use Tacotron 2 when:</strong></p>
<ul>
  <li>Maximum naturalness is critical</li>
  <li>Training data is limited (easier to train)</li>
  <li>Latency is acceptable</li>
</ul>

<p><strong>Use FastSpeech 2 when:</strong></p>
<ul>
  <li>Low latency required</li>
  <li>Need prosody control</li>
  <li>Robustness is critical (production systems)</li>
</ul>

<hr />

<h2 id="recent-advances-2023-2024">Recent Advances (2023-2024)</h2>

<h3 id="1-vall-e-zero-shot-voice-cloning">1. VALL-E (Zero-Shot Voice Cloning)</h3>

<p>Microsoft’s VALL-E can clone a voice from a 3-second sample using language model approach.</p>

<p><strong>Key idea:</strong> Treat TTS as conditional language modeling over discrete audio codes.</p>

<h3 id="2-vits-end-to-end-tts">2. VITS (End-to-End TTS)</h3>

<p>Combines acoustic model and vocoder into single model.</p>

<p><strong>Advantages:</strong></p>
<ul>
  <li>Faster training and inference</li>
  <li>Better audio quality</li>
  <li>Simplified pipeline</li>
</ul>

<h3 id="3-yourtts-multi-lingual-voice-cloning">3. YourTTS (Multi-lingual Voice Cloning)</h3>

<p>Zero-shot multi-lingual TTS supporting 16+ languages.</p>

<h3 id="4-bark-generative-audio-model">4. Bark (Generative Audio Model)</h3>

<p>Text-to-audio model that can generate music, sound effects, and speech with emotions.</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Two-stage pipeline</strong> - Acoustic model + vocoder is standard 
✅ <strong>Text processing critical</strong> - Normalization and G2P affect quality 
✅ <strong>Autoregressive vs non-autoregressive</strong> - Tacotron vs FastSpeech trade-offs 
✅ <strong>Prosody control</strong> - Pitch, duration, energy for expressiveness 
✅ <strong>Multiple metrics</strong> - Objective (MCD) and subjective (MOS) both needed 
✅ <strong>Production optimization</strong> - Latency, caching, streaming for real-time use 
✅ <strong>Like climbing stairs</strong> - Build incrementally (phoneme → mel → waveform)</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0006-text-to-speech-basics/">arunbaby.com/speech-tech/0006-text-to-speech-basics</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#neural-tts" class="page__taxonomy-item p-category" rel="tag">neural-tts</a><span class="sep">, </span>
    
      <a href="/tags/#prosody" class="page__taxonomy-item p-category" rel="tag">prosody</a><span class="sep">, </span>
    
      <a href="/tags/#synthesis" class="page__taxonomy-item p-category" rel="tag">synthesis</a><span class="sep">, </span>
    
      <a href="/tags/#tts" class="page__taxonomy-item p-category" rel="tag">tts</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0006-climbing-stairs/" rel="permalink">Climbing Stairs
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          25 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The Fibonacci problem in disguise, teaching the fundamental transition from recursion to dynamic programming to space optimization.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0006-model-evaluation-metrics/" rel="permalink">Model Evaluation Metrics
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How to measure if your ML model is actually good, choosing the right metrics is as important as building the model itself.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0006-agent-frameworks-landscape/" rel="permalink">Agent Frameworks Landscape
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“To Framework or Not to Framework? Navigating the Agent Ecosystem.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Text-to-Speech+%28TTS%29+System+Fundamentals%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0006-text-to-speech-basics%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0006-text-to-speech-basics%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0006-text-to-speech-basics/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0005-speaker-recognition/" class="pagination--pager" title="Speaker Recognition &amp; Verification">Previous</a>
    
    
      <a href="/speech-tech/0007-audio-preprocessing/" class="pagination--pager" title="Audio Preprocessing &amp; Signal Processing">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
