<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>ASR Beam Search Implementation - Arun Baby</title>
<meta name="description" content="Implementing the core decoding logic of modern Speech Recognition systems, handling alignment, blanks, and language models.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="ASR Beam Search Implementation">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0023-asr-beam-search-implementation/">


  <meta property="og:description" content="Implementing the core decoding logic of modern Speech Recognition systems, handling alignment, blanks, and language models.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="ASR Beam Search Implementation">
  <meta name="twitter:description" content="Implementing the core decoding logic of modern Speech Recognition systems, handling alignment, blanks, and language models.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0023-asr-beam-search-implementation/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0023-asr-beam-search-implementation/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="ASR Beam Search Implementation">
    <meta itemprop="description" content="Implementing the core decoding logic of modern Speech Recognition systems, handling alignment, blanks, and language models.">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0023-asr-beam-search-implementation/" itemprop="url">ASR Beam Search Implementation
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem">Problem</a></li><li><a href="#background-a-brief-history-of-decoders">Background: A Brief History of Decoders</a></li><li><a href="#why-greedy-fails-in-speech">Why Greedy Fails in Speech</a></li><li><a href="#high-level-architecture-ctc-decoding-flow">High-Level Architecture: CTC Decoding Flow</a></li><li><a href="#algorithm-ctc-beam-search">Algorithm: CTC Beam Search</a><ul><li><a href="#the-state-space">The State Space</a></li><li><a href="#python-implementation">Python Implementation</a></li></ul></li><li><a href="#adding-a-language-model-lm">Adding a Language Model (LM)</a><ul><li><a href="#kenlm-integration">KenLM Integration</a></li></ul></li><li><a href="#measuring-success-word-error-rate-wer">Measuring Success: Word Error Rate (WER)</a><ul><li><a href="#python-implementation-of-wer">Python Implementation of WER</a></li></ul></li><li><a href="#streaming-asr-the-infinite-loop">Streaming ASR: The Infinite Loop</a></li><li><a href="#hot-word-boosting-contextual-biasing">Hot-Word Boosting (Contextual Biasing)</a></li><li><a href="#debugging-the-decoder">Debugging the Decoder</a></li><li><a href="#appendix-a-the-mathematics-of-ctc">Appendix A: The Mathematics of CTC</a></li><li><a href="#appendix-b-complete-python-decoder-class">Appendix B: Complete Python Decoder Class</a></li><li><a href="#appendix-c-the-asr-troubleshooting-guide">Appendix C: The ASR Troubleshooting Guide</a></li><li><a href="#appendix-d-deep-dive-into-wfst-weighted-finite-state-transducers">Appendix D: Deep Dive into WFST (Weighted Finite State Transducers)</a></li><li><a href="#conclusion">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Implementing the core decoding logic of modern Speech Recognition systems, handling alignment, blanks, and language models.</strong></p>

<h2 id="problem">Problem</h2>

<p>Standard Beam Search (for Transformers) generates one token at a time.
CTC Beam Search processes a sequence of probabilities over time steps.</p>

<p><strong>The Rules of CTC:</strong></p>
<ol>
  <li><strong>Blank Token (<code class="language-plaintext highlighter-rouge">&lt;blk&gt;</code>):</strong> A special token that represents “silence” or “no transition”.</li>
  <li><strong>Collapse Repeats:</strong> <code class="language-plaintext highlighter-rouge">AA</code> -&gt; <code class="language-plaintext highlighter-rouge">A</code>.</li>
  <li><strong>Blanks Separate:</strong> <code class="language-plaintext highlighter-rouge">A &lt;blk&gt; A</code> -&gt; <code class="language-plaintext highlighter-rouge">AA</code>. (This allows us to spell words like “Aaron”).</li>
</ol>

<p><strong>Example:</strong></p>
<ul>
  <li>Output: <code class="language-plaintext highlighter-rouge">h h e e l l l &lt;blk&gt; l l o</code></li>
  <li>Collapse: <code class="language-plaintext highlighter-rouge">h e l &lt;blk&gt; l o</code></li>
  <li>Remove Blanks: <code class="language-plaintext highlighter-rouge">h e l l o</code></li>
</ul>

<h2 id="background-a-brief-history-of-decoders">Background: A Brief History of Decoders</h2>

<p>To understand CTC, we must understand what came before.</p>

<ol>
  <li><strong>HMM-GMM (1980s-2000s):</strong> The “Dark Ages”. We modeled speech as a Hidden Markov Model. The decoder was a massive Viterbi search over a graph of Phonemes -&gt; Triphones -&gt; Words -&gt; Sentences. It was complex, fragile, and required expert linguistic knowledge.</li>
  <li><strong>HMM-DNN (2010-2015):</strong> We replaced the Gaussian Mixture Models (GMMs) with Deep Neural Networks (DNNs) to predict phoneme probabilities. The decoder remained the same Viterbi beast.</li>
  <li><strong>CTC (2006/2015):</strong> Alex Graves introduced CTC. Suddenly, we didn’t need phoneme alignment. The model learned to align itself. The decoder became a simple “Prefix Search”.</li>
  <li><strong>RNN-T (Transducer):</strong> An upgrade to CTC that removes the conditional independence assumption. It’s the standard for streaming ASR today (Siri, Google Assistant).</li>
  <li><strong>Attention (Whisper):</strong> The decoder is just a text generator (like GPT) that attends to the audio. Simple, but hard to stream.</li>
</ol>

<p>Standard Beam Search (for Transformers) generates one token at a time.
CTC Beam Search processes a sequence of probabilities over time steps.</p>

<p><strong>The Rules of CTC:</strong></p>
<ol>
  <li><strong>Blank Token (<code class="language-plaintext highlighter-rouge">&lt;blk&gt;</code>):</strong> A special token that represents “silence” or “no transition”.</li>
  <li><strong>Collapse Repeats:</strong> <code class="language-plaintext highlighter-rouge">AA</code> -&gt; <code class="language-plaintext highlighter-rouge">A</code>.</li>
  <li><strong>Blanks Separate:</strong> <code class="language-plaintext highlighter-rouge">A &lt;blk&gt; A</code> -&gt; <code class="language-plaintext highlighter-rouge">AA</code>. (This allows us to spell words like “Aaron”).</li>
</ol>

<p><strong>Example:</strong></p>
<ul>
  <li>Output: <code class="language-plaintext highlighter-rouge">h h e e l l l &lt;blk&gt; l l o</code></li>
  <li>Collapse: <code class="language-plaintext highlighter-rouge">h e l &lt;blk&gt; l o</code></li>
  <li>Remove Blanks: <code class="language-plaintext highlighter-rouge">h e l l o</code></li>
</ul>

<h2 id="why-greedy-fails-in-speech">Why Greedy Fails in Speech</h2>

<p>Imagine the audio for “The cat”.</p>
<ul>
  <li>Frame 10: P(The) = 0.6, P(A) = 0.4</li>
  <li>Frame 11: P(cat) = 0.4, P(car) = 0.6</li>
</ul>

<p>Greedy might pick “The car”.
But maybe “A cat” was more likely overall if we looked at the whole sequence.
Beam Search allows us to keep “A” alive as a hypothesis until we see “cat”, which confirms it.</p>

<h2 id="high-level-architecture-ctc-decoding-flow">High-Level Architecture: CTC Decoding Flow</h2>

<p><code class="language-plaintext highlighter-rouge">ascii
+-------------+ +-------------+ +-------------+
| Audio Frame | -&gt; | Acoustic Mod| -&gt; | Prob Matrix |
+-------------+ +-------------+ +-------------+
 | (T x V)
 v
 +------------------+
 | CTC Beam Search |
 +------------------+
 |
 +--------------------+--------------------+
 | |
 (Expand Prefix) (Score with LM)
 | |
 v v
 +----------------+ +----------------+
 | New Hypotheses | &lt;--------------------- | Language Model |
 +----------------+ +----------------+
</code></p>

<h2 id="algorithm-ctc-beam-search">Algorithm: CTC Beam Search</h2>

<p>This is more complex than standard Beam Search because we have to track two probabilities for each hypothesis:</p>
<ol>
  <li><code class="language-plaintext highlighter-rouge">P_b</code>: Probability ending in a <strong>Blank</strong>.</li>
  <li><code class="language-plaintext highlighter-rouge">P_nb</code>: Probability ending in a <strong>Non-Blank</strong>.</li>
</ol>

<p>Why? Because if the next token is the <em>same</em> as the last one, the behavior depends on whether there was a blank in between.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">A</code> + <code class="language-plaintext highlighter-rouge">A</code> = <code class="language-plaintext highlighter-rouge">A</code> (Merge)</li>
  <li><code class="language-plaintext highlighter-rouge">A</code> + <code class="language-plaintext highlighter-rouge">&lt;blk&gt;</code> + <code class="language-plaintext highlighter-rouge">A</code> = <code class="language-plaintext highlighter-rouge">AA</code> (No Merge)</li>
</ul>

<h3 id="the-state-space">The State Space</h3>
<p>A hypothesis is defined by the text prefix (e.g., “hel”).
At each time step <code class="language-plaintext highlighter-rouge">t</code>, we update the probabilities of all active prefixes based on the acoustic probabilities <code class="language-plaintext highlighter-rouge">y_t</code>.</p>

<h3 id="python-implementation">Python Implementation</h3>

<p>This is a simplified version of the algorithm used in libraries like <code class="language-plaintext highlighter-rouge">pyctcdecode</code>.</p>

<p>``python
import numpy as np
from collections import defaultdict</p>

<p>def ctc_beam_search(probs, vocab, beam_width=10):
 “””
 probs: (T, V) numpy array of probabilities
 vocab: list of characters (index 0 is <blk>)
 """
 T, V = probs.shape</blk></p>

<p># beam: dict mapping prefix -&gt; probability
 # We work in log domain to avoid underflow
 beam = defaultdict(lambda: float(‘-inf’))</p>

<p># Initialize with empty prefix
 # We track two scores: (score_blank, score_non_blank)
 beam[()] = (0.0, float(‘-inf’))</p>

<p>for t in range(T):
 next_beam = defaultdict(lambda: (float(‘-inf’), float(‘-inf’)))</p>

<p># Get top-K prefixes from previous step
 # Sort by total score (logsumexp of blank and non-blank)
 sorted_prefixes = sorted(
 beam.items(),
 key=lambda x: np.logaddexp(x[1][0], x[1][1]),
 reverse=True
 )[:beam_width]</p>

<p>for prefix, (p_b, p_nb) in sorted_prefixes:
 # 1. Handle Blank Token (Index 0)
 pr_blank = probs[t, 0]
 # If we add a blank, the prefix doesn’t change.
 # New blank score = log(P(blank at t)) + log(P_total at t-1)
 n_p_b, n_p_nb = next_beam[prefix]
 n_p_b = np.logaddexp(n_p_b, pr_blank + np.logaddexp(p_b, p_nb))
 next_beam[prefix] = (n_p_b, n_p_nb)</p>

<p># 2. Handle Non-Blank Tokens
 for v in range(1, V):
 pr_char = probs[t, v]
 char = vocab[v]</p>

<p># Case A: Repeat character (e.g., “l” -&gt; “l”)
 if len(prefix) &gt; 0 and prefix[-1] == char:
 # 1. Merge (from non-blank): “l” + “l” -&gt; “l”
 # We update the NON-blank score of the SAME prefix
 n_p_b, n_p_nb = next_beam[prefix]
 n_p_nb = np.logaddexp(n_p_nb, pr_char + p_nb)
 next_beam[prefix] = (n_p_b, n_p_nb)</p>

<p># 2. No Merge (from blank): “l” + <blk> + "l" -&gt; "ll"
 # We extend the prefix
 new_prefix = prefix + (char,)
 n_p_b, n_p_nb = next_beam[new_prefix]
 n_p_nb = np.logaddexp(n_p_nb, pr_char + p_b)
 next_beam[new_prefix] = (n_p_b, n_p_nb)</blk></p>

<p># Case B: New character
 else:
 new_prefix = prefix + (char,)
 n_p_b, n_p_nb = next_beam[new_prefix]
 # We can come from blank OR non-blank
 n_p_nb = np.logaddexp(n_p_nb, pr_char + np.logaddexp(p_b, p_nb))
 next_beam[new_prefix] = (n_p_b, n_p_nb)</p>

<p>beam = next_beam</p>

<p># Final cleanup: Return top hypothesis
 best_prefix = max(beam.items(), key=lambda x: np.logaddexp(x[1][0], x[1][1]))[0]
 return ““.join(best_prefix)</p>

<h1 id="mock-data">Mock Data</h1>
<h1 id="t3-v3--a-b">T=3, V=3 (<blk>, A, B)</blk></h1>
<p>probs = np.log(np.array([
 [0.1, 0.8, 0.1], # Mostly A
 [0.8, 0.1, 0.1], # Mostly Blank
 [0.1, 0.1, 0.8] # Mostly B
]))
vocab = [‘<blk>', 'A', 'B']
print(ctc_beam_search(probs, vocab)) # Output: "AB"
``</blk></p>

<h2 id="adding-a-language-model-lm">Adding a Language Model (LM)</h2>

<p>The acoustic model (AM) is good at sounds, but bad at grammar.</p>
<ul>
  <li>AM hears: “I want to wreck a nice beach.”</li>
  <li>LM knows: “I want to recognize speech.”</li>
</ul>

<p>We fuse them during the beam search.
<code class="language-plaintext highlighter-rouge">Score = Score_AM + (alpha * Score_LM) + (beta * Word_Count)</code></p>

<ul>
  <li><strong>Alpha:</strong> Weight of the LM (usually 0.5 - 2.0).</li>
  <li><strong>Beta:</strong> Word insertion bonus (encourages longer sentences).</li>
</ul>

<h3 id="kenlm-integration">KenLM Integration</h3>
<p>In production, we use <strong>KenLM</strong>, a highly optimized C++ library for n-gram language models.
We query the LM every time we append a space character (end of a word).</p>

<p>``python</p>
<h1 id="pseudo-code-for-lm-scoring">Pseudo-code for LM scoring</h1>
<p>if char == ‘ ‘:
 word = get_last_word(new_prefix)
 lm_score = lm.score(word)
 n_p_nb += alpha * lm_score
``</p>

<h2 id="measuring-success-word-error-rate-wer">Measuring Success: Word Error Rate (WER)</h2>

<p>In classification, we use Accuracy. In ASR, we use WER.
<code class="language-plaintext highlighter-rouge">WER = (S + D + I) / N</code></p>
<ul>
  <li><strong>S (Substitutions):</strong> “cat” -&gt; “hat”</li>
  <li><strong>D (Deletions):</strong> “the cat” -&gt; “cat”</li>
  <li><strong>I (Insertions):</strong> “cat” -&gt; “the cat”</li>
  <li><strong>N:</strong> Total number of words in the reference.</li>
</ul>

<p><strong>Note:</strong> WER can be &gt; 100% if you insert a lot of garbage!</p>

<h3 id="python-implementation-of-wer">Python Implementation of WER</h3>

<p>This uses the Levenshtein Distance algorithm (Dynamic Programming!).</p>

<p>``python
def calculate_wer(reference, hypothesis):
 r = reference.split()
 h = hypothesis.split()
 n = len(r)
 m = len(h)</p>

<p># DP Matrix
 d = np.zeros((n+1, m+1))</p>

<p>for i in range(n+1): d[i][0] = i
 for j in range(m+1): d[0][j] = j</p>

<p>for i in range(1, n+1):
 for j in range(1, m+1):
 if r[i-1] == h[j-1]:
 d[i][j] = d[i-1][j-1]
 else:
 sub = d[i-1][j-1] + 1
 ins = d[i][j-1] + 1
 rem = d[i-1][j] + 1
 d[i][j] = min(sub, ins, rem)</p>

<p>return d[n][m] / n</p>

<p>ref = “the cat sat on the mat”
hyp = “the cat sat on mat” # Deletion
print(calculate_wer(ref, hyp)) # 1/6 = 16.6%
``</p>

<h2 id="streaming-asr-the-infinite-loop">Streaming ASR: The Infinite Loop</h2>

<p>Standard Beam Search waits for the end of the file. In a live meeting, you can’t wait.
We need <strong>Streaming Decoding</strong>.</p>

<p><strong>Challenges:</strong></p>
<ol>
  <li><strong>Latency:</strong> Users expect text to appear &lt; 200ms after they speak.</li>
  <li><strong>Stability:</strong> The decoder might change its mind. “I want to…” -&gt; “I want two…”. This “flicker” is annoying.</li>
</ol>

<p><strong>Solution:</strong></p>
<ul>
  <li><strong>Partial Results:</strong> Output the current best hypothesis every 100ms.</li>
  <li><strong>Endpointing:</strong> If the user pauses for &gt; 500ms, finalize the sentence and clear the beam history.</li>
  <li><strong>Stability Heuristic:</strong> Only display words that have been stable for 3 frames.</li>
</ul>

<h2 id="hot-word-boosting-contextual-biasing">Hot-Word Boosting (Contextual Biasing)</h2>

<p>If you build an ASR for a medical app, it needs to know “Hydrochlorothiazide”. A generic LM won’t know it.
We use <strong>Contextual Biasing</strong>.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Build a <strong>Trie</strong> of hot-words (e.g., contact names, drug names).</li>
  <li>During Beam Search, traverse the Trie with the current prefix.</li>
  <li>If we are inside a hot-word node, add a bonus score.</li>
</ol>

<p>``python</p>
<h1 id="pseudo-code">Pseudo-code</h1>
<p>if current_prefix in hot_word_trie:
 score += boosting_weight
``</p>

<h2 id="debugging-the-decoder">Debugging the Decoder</h2>

<p>When your WER (Word Error Rate) is high, how do you know if it’s the Model or the Decoder?</p>

<ol>
  <li><strong>Force Alignment:</strong> Feed the <em>correct</em> transcript into the decoder and see its probability. If the probability is high but the decoder didn’t pick it, your <strong>Beam Width</strong> is too small (search error).</li>
  <li><strong>Greedy Check:</strong> If Greedy Search gives garbage, your <strong>Model</strong> is bad (modeling error).</li>
  <li><strong>LM Weight Tuning:</strong> Grid search <code class="language-plaintext highlighter-rouge">alpha</code> and <code class="language-plaintext highlighter-rouge">beta</code> on a validation set. A bad alpha can ruin a perfect acoustic model.</li>
</ol>

<h2 id="appendix-a-the-mathematics-of-ctc">Appendix A: The Mathematics of CTC</h2>

<p>For those who want to understand the “Forward-Backward” algorithm used in CTC training.</p>

<p><strong>Objective:</strong> Maximize <code class="language-plaintext highlighter-rouge">P(Y|X)</code>.
Since many paths map to <code class="language-plaintext highlighter-rouge">Y</code> (e.g., <code class="language-plaintext highlighter-rouge">AA</code> and <code class="language-plaintext highlighter-rouge">A</code> both map to <code class="language-plaintext highlighter-rouge">A</code>), we sum over all valid paths.</p>

<p><code class="language-plaintext highlighter-rouge">P(Y|X) = Sum_{pi in Path(Y)} P(pi|X)</code></p>

<p><strong>Dynamic Programming (Forward Variable alpha):</strong>
<code class="language-plaintext highlighter-rouge">alpha_t(s)</code>: Probability of outputting the first <code class="language-plaintext highlighter-rouge">s</code> characters of <code class="language-plaintext highlighter-rouge">Y</code> after <code class="language-plaintext highlighter-rouge">t</code> time steps.</p>

<p><strong>Transitions:</strong></p>
<ul>
  <li>If <code class="language-plaintext highlighter-rouge">Y[s] == Y[s-2]</code> (repeat char): We can only come from <code class="language-plaintext highlighter-rouge">alpha_{t-1}(s)</code> or <code class="language-plaintext highlighter-rouge">alpha_{t-1}(s-1)</code>.</li>
  <li>If <code class="language-plaintext highlighter-rouge">Y[s] != Y[s-2]</code> (new char): We can also come from <code class="language-plaintext highlighter-rouge">alpha_{t-1}(s-2)</code> (skipping the blank).</li>
</ul>

<p>This O(T * S) algorithm is what allows CTC to be differentiable and trainable via backpropagation.</p>

<h2 id="appendix-b-complete-python-decoder-class">Appendix B: Complete Python Decoder Class</h2>

<p>``python
import numpy as np
from collections import defaultdict</p>

<p>class CTCDecoder:
 def <strong>init</strong>(self, vocab, beam_width=100, alpha=0.5, beta=1.0):
 self.vocab = vocab
 self.beam_width = beam_width
 self.alpha = alpha
 self.beta = beta</p>

<p>def decode(self, probs):
 “””
 probs: (T, V)
 Returns: best_string
 “””
 # Initialization
 beam = defaultdict(lambda: (float(‘-inf’), float(‘-inf’)))
 beam[()] = (0.0, float(‘-inf’))</p>

<p>for t in range(len(probs)):
 next_beam = defaultdict(lambda: (float(‘-inf’), float(‘-inf’)))</p>

<p># Pruning: Only keep top beam_width
 sorted_beam = sorted(
 beam.items(),
 key=lambda x: np.logaddexp(x[1][0], x[1][1]),
 reverse=True
 )[:self.beam_width]</p>

<p>for prefix, (p_b, p_nb) in sorted_beam:
 # … (Same logic as above) …
 # See main article for the core loop
 pass</p>

<p>beam = next_beam</p>

<p>return self._get_best_hypothesis(beam)</p>

<p>def _get_best_hypothesis(self, beam):
 best_prefix = max(beam.items(), key=lambda x: np.logaddexp(x[1][0], x[1][1]))[0]
 return ““.join(best_prefix)
``</p>

<h2 id="appendix-c-the-asr-troubleshooting-guide">Appendix C: The ASR Troubleshooting Guide</h2>

<p><strong>Problem: The decoder outputs nothing.</strong></p>
<ul>
  <li><strong>Cause:</strong> Your blank probability is 1.0 everywhere.</li>
  <li><strong>Fix:</strong> Check your training data. Are your labels aligned? Is the learning rate too high (exploding gradients)?</li>
</ul>

<p><strong>Problem: The decoder repeats words (“hello hello hello”).</strong></p>
<ul>
  <li><strong>Cause:</strong> The model is confident for too many frames.</li>
  <li><strong>Fix:</strong> Increase the blank probability penalty or use a Language Model with a repetition penalty.</li>
</ul>

<p><strong>Problem: WER is 100%.</strong></p>
<ul>
  <li><strong>Cause:</strong> Vocabulary mismatch. Are you using the same char-to-int mapping as training?</li>
  <li><strong>Fix:</strong> Verify <code class="language-plaintext highlighter-rouge">vocab.json</code>.</li>
</ul>

<h2 id="appendix-d-deep-dive-into-wfst-weighted-finite-state-transducers">Appendix D: Deep Dive into WFST (Weighted Finite State Transducers)</h2>

<p>Before Deep Learning took over, ASR was built on <strong>WFSTs</strong>.
Even today, the <strong>Kaldi</strong> toolkit (which powers many production systems) uses them.</p>

<p><strong>What is a WFST?</strong>
It’s a graph where edges have:</p>
<ol>
  <li><strong>Input Label</strong> (e.g., Phoneme)</li>
  <li><strong>Output Label</strong> (e.g., Word)</li>
  <li><strong>Weight</strong> (Probability)</li>
</ol>

<p><strong>The Composition (H o C o L o G):</strong>
We build a massive static graph by composing four smaller graphs:</p>
<ul>
  <li><strong>H (HMM):</strong> Maps HMM states to Context-Dependent Phones.</li>
  <li><strong>C (Context):</strong> Maps Context-Dependent Phones to Phones.</li>
  <li><strong>L (Lexicon):</strong> Maps Phones to Words (Pronunciation Dictionary).</li>
  <li><strong>G (Grammar):</strong> Maps Words to Sentences (Language Model).</li>
</ul>

<p><strong>Decoding:</strong>
Decoding is simply finding the shortest path in the <code class="language-plaintext highlighter-rouge">H o C o L o G</code> graph.
This is extremely fast because the graph is pre-compiled and optimized (determinized and minimized).</p>

<p><strong>Why learn this?</strong>
If you work on <strong>Edge AI</strong> (embedded devices), you might not have the RAM for a Transformer. A WFST decoder is incredibly memory-efficient and fast.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Implementing a CTC Beam Search decoder is a rite of passage. It forces you to understand the probabilistic nature of speech.</p>

<p>While end-to-end models (like Whisper) are replacing complex decoders with simple <code class="language-plaintext highlighter-rouge">model.generate()</code>, understanding <strong>Beam Search</strong> is still crucial for:</p>
<ol>
  <li><strong>Streaming ASR</strong> (where Transformers are too slow).</li>
  <li><strong>Keyword Spotting</strong> (Wake word detection).</li>
  <li><strong>Customization</strong> (Adding hot-words).</li>
</ol>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>CTC</strong> handles the alignment between audio and text.</li>
  <li><strong>Beam Search</strong> keeps multiple hypotheses alive to correct early mistakes.</li>
  <li><strong>LMs</strong> are essential for fixing homophones (“beach” vs “speech”).</li>
</ul>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0023-asr-beam-search-implementation/">arunbaby.com/speech-tech/0023-asr-beam-search-implementation</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#beam-search" class="page__taxonomy-item p-category" rel="tag">beam-search</a><span class="sep">, </span>
    
      <a href="/tags/#ctc" class="page__taxonomy-item p-category" rel="tag">ctc</a><span class="sep">, </span>
    
      <a href="/tags/#decoding" class="page__taxonomy-item p-category" rel="tag">decoding</a><span class="sep">, </span>
    
      <a href="/tags/#kenlm" class="page__taxonomy-item p-category" rel="tag">kenlm</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0023-decode-ways/" rel="permalink">Decode Ways
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          16 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">A deceptive counting problem that teaches the fundamentals of state transitions and connects directly to Beam Search.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0023-beam-search-decoding/" rel="permalink">Beam Search Decoding
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          14 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The industry-standard algorithm for converting probabilistic model outputs into coherent text sequences.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0023-ui-automation-agents/" rel="permalink">UI Automation Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          20 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The ultimate API: The User Interface.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=ASR+Beam+Search+Implementation%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0023-asr-beam-search-implementation%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0023-asr-beam-search-implementation%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0023-asr-beam-search-implementation/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0022-cost-efficient-speech-systems/" class="pagination--pager" title="Cost-efficient Speech Systems">Previous</a>
    
    
      <a href="/speech-tech/0024-speech-tokenization/" class="pagination--pager" title="Speech Tokenization">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
