<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Speech Architecture Search - Arun Baby</title>
<meta name="description" content="Design neural architecture search systems for speech models that automatically discover optimal ASR/TTS architectures—using dynamic programming and path optimization to navigate exponential search spaces.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Speech Architecture Search">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0021-speech-architecture-search/">


  <meta property="og:description" content="Design neural architecture search systems for speech models that automatically discover optimal ASR/TTS architectures—using dynamic programming and path optimization to navigate exponential search spaces.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Speech Architecture Search">
  <meta name="twitter:description" content="Design neural architecture search systems for speech models that automatically discover optimal ASR/TTS architectures—using dynamic programming and path optimization to navigate exponential search spaces.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0021-speech-architecture-search/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0021-speech-architecture-search/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Speech Architecture Search">
    <meta itemprop="description" content="Design neural architecture search systems for speech models that automatically discover optimal ASR/TTS architectures—using dynamic programming and path optimization to navigate exponential search spaces.">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0021-speech-architecture-search/" itemprop="url">Speech Architecture Search
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem-statement">Problem Statement</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li></ul></li><li><a href="#understanding-the-requirements">Understanding the Requirements</a><ul><li><a href="#why-speech-architecture-search">Why Speech Architecture Search?</a></li><li><a href="#speech-architecture-challenges">Speech Architecture Challenges</a></li><li><a href="#the-path-optimization-connection">The Path Optimization Connection</a></li></ul></li><li><a href="#high-level-architecture">High-Level Architecture</a><ul><li><a href="#key-components">Key Components</a></li></ul></li><li><a href="#component-deep-dives">Component Deep-Dives</a><ul><li><a href="#1-speech-specific-search-space">1. Speech-Specific Search Space</a></li><li><a href="#2-architecture-evaluation">2. Architecture Evaluation</a></li><li><a href="#3-search-strategy-for-speech">3. Search Strategy for Speech</a></li><li><a href="#4-multi-objective-optimization">4. Multi-Objective Optimization</a></li></ul></li><li><a href="#scaling-strategies">Scaling Strategies</a><ul><li><a href="#efficient-evaluation">Efficient Evaluation</a></li></ul></li><li><a href="#real-world-case-study-googles-speech-nas">Real-World Case Study: Google’s Speech NAS</a><ul><li><a href="#googles-approach-for-mobile-asr">Google’s Approach for Mobile ASR</a></li><li><a href="#lessons-learned">Lessons Learned</a></li></ul></li><li><a href="#cost-analysis">Cost Analysis</a><ul><li><a href="#nas-vs-manual-design">NAS vs Manual Design</a></li></ul></li><li><a href="#advanced-topics">Advanced Topics</a><ul><li><a href="#1-multi-lingual-nas">1. Multi-Lingual NAS</a></li><li><a href="#2-streaming-aware-nas">2. Streaming-Aware NAS</a></li><li><a href="#3-transfer-from-vision-nas">3. Transfer from Vision NAS</a></li></ul></li><li><a href="#monitoring--debugging">Monitoring &amp; Debugging</a><ul><li><a href="#key-metrics">Key Metrics</a></li><li><a href="#debugging-tools">Debugging Tools</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a><ul><li><a href="#connection-to-thematic-link-dynamic-programming-and-path-optimization">Connection to Thematic Link: Dynamic Programming and Path Optimization</a></li></ul></li></ul>
            </nav>
          </aside>
        
        <p><strong>Design neural architecture search systems for speech models that automatically discover optimal ASR/TTS architectures—using dynamic programming and path optimization to navigate exponential search spaces.</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p>Design a <strong>Speech Architecture Search System</strong> that:</p>

<ol>
  <li><strong>Automatically discovers</strong> ASR/TTS architectures optimized for accuracy, latency, and size</li>
  <li><strong>Searches efficiently</strong> through speech-specific architecture spaces (encoders, decoders, attention)</li>
  <li><strong>Handles speech constraints</strong> (streaming, long sequences, variable-length inputs)</li>
  <li><strong>Optimizes for deployment</strong> (mobile, edge, server, different hardware)</li>
  <li><strong>Supports multi-objective</strong> optimization (WER, latency, params, multilingual capability)</li>
</ol>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Speech-specific search spaces:</strong>
    <ul>
      <li>Encoder architectures (Conformer, Transformer, RNN, CNN)</li>
      <li>Decoder types (CTC, RNN-T, attention-based)</li>
      <li>Attention mechanisms (self-attention, cross-attention, relative positional)</li>
      <li>Feature extraction configs (mel-spec, MFCC, learnable features)</li>
    </ul>
  </li>
  <li><strong>Search strategies:</strong>
    <ul>
      <li>Reinforcement learning</li>
      <li>Evolutionary algorithms</li>
      <li>Differentiable NAS (DARTS for speech)</li>
      <li>Bayesian optimization</li>
      <li>Transfer from vision NAS results</li>
    </ul>
  </li>
  <li><strong>Performance estimation:</strong>
    <ul>
      <li>Train on subset of data (LibriSpeech-100h vs 960h)</li>
      <li>Early stopping based on validation WER</li>
      <li>Weight sharing across architectures</li>
      <li>WER prediction from architecture features</li>
    </ul>
  </li>
  <li><strong>Multi-objective optimization:</strong>
    <ul>
      <li>WER vs latency (for real-time ASR)</li>
      <li>WER vs model size (for on-device)</li>
      <li>WER vs RTF (real-time factor)</li>
      <li>Multi-lingual capability vs params</li>
    </ul>
  </li>
  <li><strong>Streaming-aware search:</strong>
    <ul>
      <li>Architectures must support chunk-wise processing</li>
      <li>Latency measured per chunk, not full utterance</li>
      <li>Look-ahead constraints (for causal models)</li>
    </ul>
  </li>
  <li><strong>Evaluation:</strong>
    <ul>
      <li>WER/CER on multiple test sets</li>
      <li>Latency measurement on target hardware</li>
      <li>Parameter count and memory footprint</li>
      <li>Multi-lingual evaluation</li>
    </ul>
  </li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Efficiency:</strong> Find good architecture in &lt;50 GPU days</li>
  <li><strong>Quality:</strong> WER competitive with hand-designed models</li>
  <li><strong>Generalizability:</strong> Transfer across languages and domains</li>
  <li><strong>Reproducibility:</strong> Same search produces same results</li>
  <li><strong>Practicality:</strong> Discovered models deployable in production</li>
</ol>

<h2 id="understanding-the-requirements">Understanding the Requirements</h2>

<h3 id="why-speech-architecture-search">Why Speech Architecture Search?</h3>

<p><strong>Manual speech model design challenges:</strong></p>
<ul>
  <li>Requires domain expertise (speech signal processing + deep learning)</li>
  <li>Hard to balance accuracy, latency, and size</li>
  <li>Difficult to optimize for specific hardware (mobile, server)</li>
  <li>Time-consuming to explore alternative designs</li>
</ul>

<p><strong>Speech NAS enables:</strong></p>
<ul>
  <li>Automated discovery of novel architectures</li>
  <li>Hardware-specific optimization (mobile, edge TPU, server GPU)</li>
  <li>Multi-lingual model optimization</li>
  <li>Systematic exploration of design space</li>
</ul>

<h3 id="speech-architecture-challenges">Speech Architecture Challenges</h3>

<ol>
  <li><strong>Long sequences:</strong> Audio is 100s-1000s of frames (vs images ~224×224)</li>
  <li><strong>Temporal modeling:</strong> Need strong sequential modeling (RNNs, Transformers)</li>
  <li><strong>Streaming requirements:</strong> Many applications need real-time processing</li>
  <li><strong>Variable length:</strong> Utterances vary from 1s to 60s+</li>
  <li><strong>Multi-lingual:</strong> Same architecture should work across languages</li>
</ol>

<h3 id="the-path-optimization-connection">The Path Optimization Connection</h3>

<p>Just like <strong>Unique Paths</strong> uses DP to count paths through a grid:</p>

<table>
  <thead>
    <tr>
      <th>Unique Paths</th>
      <th>Neural Arch Search</th>
      <th>Speech Arch Search</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>m×n grid</td>
      <td>General model space</td>
      <td>Speech-specific space</td>
    </tr>
    <tr>
      <td>Count paths</td>
      <td>Evaluate architectures</td>
      <td>Evaluate speech models</td>
    </tr>
    <tr>
      <td>DP: paths(i,j) = paths(i-1,j) + paths(i,j-1)</td>
      <td>DP: Build from sub-architectures</td>
      <td>DP: Build from encoder/decoder blocks</td>
    </tr>
    <tr>
      <td>O(m×n) from O(2^(m+n))</td>
      <td>Polynomial from exponential</td>
      <td>Efficient from exhaustive</td>
    </tr>
    <tr>
      <td>Reconstruct optimal path</td>
      <td>Extract best architecture</td>
      <td>Extract best speech model</td>
    </tr>
  </tbody>
</table>

<p>Both use <strong>DP and path optimization</strong> to navigate exponentially large spaces.</p>

<h2 id="high-level-architecture">High-Level Architecture</h2>

<p>``
┌─────────────────────────────────────────────────────────────────┐
│ Speech Architecture Search System │
└─────────────────────────────────────────────────────────────────┘</p>

<p>Search Controller
 ┌────────────────────────────────────┐
 │ Strategy: RL / EA / DARTS │
 │ - Propose speech architectures │
 │ - Encoder + Decoder + Attention │
 └──────────────┬─────────────────────┘
 │
 ┌──────▼──────┐
 │ Speech │
 │ Search │
 │ Space │
 │ │
 │ - Encoder │
 │ - Decoder │
 │ - Attention │
 │ - Features │
 └──────┬──────┘
 │
 ┌──────────────┼──────────────┐
 │ │ │
┌───────▼────────┐ ┌──▼────┐ ┌──────▼──────┐
│ Architecture │ │ WER │ │ Latency │
│ Evaluator │ │Predict│ │ Predictor │
│ │ │ │ │ │
│ - Train ASR/TTS│ │ - Skip│ │ - Hardware │
│ - Measure WER │ │ bad │ │ profile │
│ - Measure RTF │ │ archs│ │ - RTF est │
└───────┬────────┘ └───────┘ └──────┬──────┘
 │ │
 └─────────────┬─────────────┘
 │
 ┌─────────▼────────┐
 │ Distributed │
 │ Training │
 │ - Worker pool │
 │ - GPU cluster │
 │ - Multi-task eval│
 └─────────┬────────┘
 │
 ┌─────────▼────────┐
 │ Results Database │
 │ - Architectures │
 │ - WER scores │
 │ - Latency │
 │ - Pareto front │
 └──────────────────┘
``</p>

<h3 id="key-components">Key Components</h3>

<ol>
  <li><strong>Search Controller:</strong> Proposes speech architectures</li>
  <li><strong>Speech Search Space:</strong> Defines encoder/decoder/attention options</li>
  <li><strong>Architecture Evaluator:</strong> Trains and measures WER/latency</li>
  <li><strong>Performance Predictors:</strong> Estimate WER and latency without full training</li>
  <li><strong>Distributed Training:</strong> Parallel architecture evaluation</li>
  <li><strong>Results Database:</strong> Track all evaluated architectures</li>
</ol>

<h2 id="component-deep-dives">Component Deep-Dives</h2>

<h3 id="1-speech-specific-search-space">1. Speech-Specific Search Space</h3>

<p>Define search space for ASR models:</p>

<p>``python
from dataclasses import dataclass
from typing import List
from enum import Enum</p>

<p>class EncoderType(Enum):
 “"”Encoder architecture options.”””
 CONFORMER = “conformer”
 TRANSFORMER = “transformer”
 LSTM = “lstm”
 BLSTM = “blstm”
 CNN_LSTM = “cnn_lstm”
 CONTEXTNET = “contextnet”</p>

<p>class DecoderType(Enum):
 “"”Decoder architecture options.”””
 CTC = “ctc”
 RNN_T = “rnn_t”
 ATTENTION = “attention”
 TRANSFORMER_DECODER = “transformer_decoder”</p>

<p>class AttentionType(Enum):
 “"”Attention mechanism options.”””
 MULTI_HEAD = “multi_head”
 RELATIVE = “relative”
 LOCAL = “local”
 EFFICIENT = “efficient_attention”</p>

<p>@dataclass
class SpeechArchConfig:
 “””
 Speech model architecture configuration.</p>

<p>Similar to choosing path in Unique Paths:</p>
<ul>
  <li>Each choice (encoder, decoder, etc.) is like a move</li>
  <li>Combination forms complete architecture (path)
 “””
 # Encoder config
 encoder_type: EncoderType
 encoder_layers: int
 encoder_dim: int
 encoder_heads: int # For attention-based encoders</li>
</ul>

<p># Decoder config
 decoder_type: DecoderType
 decoder_layers: int
 decoder_dim: int</p>

<p># Attention config
 attention_type: AttentionType
 attention_dim: int</p>

<p># Feature extraction
 n_mels: int</p>

<p>def count_parameters(self) -&gt; int:
 “"”Estimate parameter count.”””
 # Simplified estimation
 encoder_params = self.encoder_layers * (self.encoder_dim ** 2) * 4
 decoder_params = self.decoder_layers * (self.decoder_dim ** 2) * 4
 return encoder_params + decoder_params</p>

<p>def estimate_flops(self, sequence_length: int = 1000) -&gt; int:
 “"”Estimate FLOPs for sequence of given length.”””
 # Encoder (attention is O(L^2 * D))
 encoder_flops = sequence_length ** 2 * self.encoder_dim * self.encoder_layers</p>

<p># Decoder
 decoder_flops = sequence_length * self.decoder_dim * self.decoder_layers</p>

<p>return encoder_flops + decoder_flops</p>

<p>class SpeechSearchSpace:
 “””
 Search space for speech architectures.</p>

<p>Similar to grid in Unique Paths:</p>
<ul>
  <li>Dimensions: encoder × decoder × attention × features</li>
  <li>Each dimension has multiple choices</li>
  <li>Total space is exponential
 “””</li>
</ul>

<p>def <strong>init</strong>(self):
 # Define choices
 self.encoder_types = list(EncoderType)
 self.decoder_types = list(DecoderType)
 self.encoder_layer_options = [4, 6, 8, 12]
 self.encoder_dim_options = [256, 512, 768]
 self.decoder_layer_options = [1, 2, 4]</p>

<p>def count_total_architectures(self) -&gt; int:
 “””
 Count total architectures (like counting paths).
 “””
 count = (
 len(self.encoder_types) *
 len(self.encoder_layer_options) *
 len(self.encoder_dim_options) *
 len(self.decoder_types) *
 len(self.decoder_layer_options)
 )
 return count</p>

<p>def sample_random_architecture(self) -&gt; SpeechArchConfig:
 “"”Sample random architecture from space.”””
 import random</p>

<p>return SpeechArchConfig(
 encoder_type=random.choice(self.encoder_types),
 encoder_layers=random.choice(self.encoder_layer_options),
 encoder_dim=random.choice(self.encoder_dim_options),
 encoder_heads=8, # Fixed for simplicity
 decoder_type=random.choice(self.decoder_types),
 decoder_layers=random.choice(self.decoder_layer_options),
 decoder_dim=256, # Fixed
 attention_type=AttentionType.MULTI_HEAD,
 attention_dim=256,
 n_mels=80
 )
``</p>

<h3 id="2-architecture-evaluation">2. Architecture Evaluation</h3>

<p>``python
import torch
import torch.nn as nn</p>

<p>def build_speech_model(config: SpeechArchConfig) -&gt; nn.Module:
 “””
 Build speech model from configuration.</p>

<p>Args:
 config: Architecture configuration</p>

<p>Returns:
 PyTorch model
 “””
 # This would integrate with ESPnet or custom implementation
 # Simplified example:</p>

<p>if config.encoder_type == EncoderType.CONFORMER:
 from espnet.nets.pytorch_backend.conformer.encoder import Encoder
 encoder = Encoder(
 idim=config.n_mels,
 attention_dim=config.encoder_dim,
 attention_heads=config.encoder_heads,
 linear_units=config.encoder_dim * 4,
 num_blocks=config.encoder_layers
 )
 elif config.encoder_type == EncoderType.TRANSFORMER:
 from espnet.nets.pytorch_backend.transformer.encoder import Encoder
 encoder = Encoder(
 idim=config.n_mels,
 attention_dim=config.encoder_dim,
 attention_heads=config.encoder_heads,
 linear_units=config.encoder_dim * 4,
 num_blocks=config.encoder_layers
 )
 else:
 # LSTM, CNN-LSTM, etc.
 encoder = create_encoder(config)</p>

<p># Build decoder
 if config.decoder_type == DecoderType.CTC:
 decoder = nn.Linear(config.encoder_dim, num_tokens)
 elif config.decoder_type == DecoderType.RNN_T:
 decoder = create_rnnt_decoder(config)
 else:
 decoder = create_attention_decoder(config)</p>

<p># Combine into full model
 model = SpeechModel(encoder=encoder, decoder=decoder)</p>

<p>return model</p>

<p>def evaluate_speech_architecture(
 config: SpeechArchConfig,
 train_subset: str = “librispeech-100h”,
 val_subset: str = “librispeech-dev”,
 max_epochs: int = 20
) -&gt; Dict:
 “””
 Evaluate speech architecture.</p>

<p>Args:
 config: Architecture to evaluate
 train_subset: Training data subset
 val_subset: Validation data
 max_epochs: Max training epochs</p>

<p>Returns:
 Dictionary with WER, latency, params, etc.
 “””
 # Build model
 model = build_speech_model(config)</p>

<p># Count parameters
 num_params = sum(p.numel() for p in model.parameters())</p>

<p># Train
 best_wer = train_and_evaluate(
 model,
 train_data=train_subset,
 val_data=val_subset,
 max_epochs=max_epochs
 )</p>

<p># Measure latency
 latency_ms = measure_inference_latency(model)</p>

<p># Measure RTF (real-time factor)
 rtf = measure_rtf(model)</p>

<p>return {
 “config”: config,
 “wer”: best_wer,
 “latency_ms”: latency_ms,
 “rtf”: rtf,
 “params”: num_params,
 “flops”: config.estimate_flops()
 }
``</p>

<h3 id="3-search-strategy-for-speech">3. Search Strategy for Speech</h3>

<p>``python
class SpeechNASController:
 “””
 NAS controller for speech architectures.</p>

<p>Uses DP-like building:</p>
<ul>
  <li>Build encoder → choose decoder → optimize jointly</li>
  <li>Like building path: choose direction at each step
 “””</li>
</ul>

<p>def <strong>init</strong>(self, search_space: SpeechSearchSpace):
 self.search_space = search_space
 self.evaluated_archs = {}
 self.best_archs = []</p>

<p>def search_with_evolutionary(self, population_size: int = 20, generations: int = 50):
 “””
 Evolutionary search for speech architectures.</p>

<p>Similar to exploring paths in Unique Paths:</p>
<ul>
  <li>Generate population (multiple paths)</li>
  <li>Evaluate fitness (WER)</li>
  <li>Mutate and crossover (create new paths)</li>
  <li>Select best (optimal paths)
 “””
 # Initialize population
 population = [
 self.search_space.sample_random_architecture()
 for _ in range(population_size)
 ]</li>
</ul>

<p>for generation in range(generations):
 # Evaluate all architectures
 fitness_scores = []</p>

<p>for arch in population:
 if encode_architecture(arch) not in self.evaluated_archs:
 result = evaluate_speech_architecture(arch)
 self.evaluated_archs[encode_architecture(arch)] = result
 fitness = 1.0 / (result[‘wer’] + 0.01) # Lower WER = higher fitness
 else:
 result = self.evaluated_archs[encode_architecture(arch)]
 fitness = 1.0 / (result[‘wer’] + 0.01)</p>

<p>fitness_scores.append((arch, fitness, result))</p>

<p># Sort by fitness
 fitness_scores.sort(key=lambda x: x[1], reverse=True)</p>

<p># Track best
 self.best_archs.append(fitness_scores[0])</p>

<p># Selection: keep top 50%
 survivors = [arch for arch, _, _ in fitness_scores[:population_size // 2]]</p>

<p># Mutation and crossover to create next generation
 offspring = []</p>

<p>while len(offspring) &lt; population_size // 2:
 # Select parents
 parent1 = random.choice(survivors)
 parent2 = random.choice(survivors)</p>

<p># Crossover
 child = self._crossover(parent1, parent2)</p>

<p># Mutation
 if random.random() &lt; 0.3:
 child = self._mutate(child)</p>

<p>offspring.append(child)</p>

<p># New population
 population = survivors + offspring</p>

<p># Return best architecture found
 best = max(self.best_archs, key=lambda x: x[1])
 return best[0], best[2]</p>

<p>def _crossover(self, arch1: SpeechArchConfig, arch2: SpeechArchConfig) -&gt; SpeechArchConfig:
 “””
 Crossover two architectures.</p>

<p>Randomly inherit components from parents.
 “””
 return SpeechArchConfig(
 encoder_type=random.choice([arch1.encoder_type, arch2.encoder_type]),
 encoder_layers=random.choice([arch1.encoder_layers, arch2.encoder_layers]),
 encoder_dim=random.choice([arch1.encoder_dim, arch2.encoder_dim]),
 encoder_heads=random.choice([arch1.encoder_heads, arch2.encoder_heads]),
 decoder_type=random.choice([arch1.decoder_type, arch2.decoder_type]),
 decoder_layers=random.choice([arch1.decoder_layers, arch2.decoder_layers]),
 decoder_dim=random.choice([arch1.decoder_dim, arch2.decoder_dim]),
 attention_type=random.choice([arch1.attention_type, arch2.attention_type]),
 attention_dim=random.choice([arch1.attention_dim, arch2.attention_dim]),
 n_mels=random.choice([arch1.n_mels, arch2.n_mels])
 )</p>

<p>def _mutate(self, arch: SpeechArchConfig) -&gt; SpeechArchConfig:
 “””
 Mutate architecture.</p>

<p>Randomly change one component.
 “””
 mutation_choice = random.randint(0, 3)</p>

<p>new_arch = SpeechArchConfig(**arch.<strong>dict</strong>)</p>

<p>if mutation_choice == 0:
 # Mutate encoder
 new_arch.encoder_layers = random.choice(self.search_space.encoder_layer_options)
 elif mutation_choice == 1:
 # Mutate encoder dim
 new_arch.encoder_dim = random.choice(self.search_space.encoder_dim_options)
 elif mutation_choice == 2:
 # Mutate decoder
 new_arch.decoder_layers = random.choice(self.search_space.decoder_layer_options)
 else:
 # Mutate encoder type
 new_arch.encoder_type = random.choice(self.search_space.encoder_types)</p>

<p>return new_arch
``</p>

<h3 id="4-multi-objective-optimization">4. Multi-Objective Optimization</h3>

<p>``python
class MultiObjectiveSpeechNAS:
 “””
 Multi-objective NAS for speech.</p>

<p>Optimize for:</p>
<ul>
  <li>WER (minimize)</li>
  <li>Latency (minimize)</li>
  <li>Model size (minimize)</li>
</ul>

<p>Find Pareto frontier of optimal trade-offs.
 “””</p>

<p>def <strong>init</strong>(self, search_space: SpeechSearchSpace):
 self.search_space = search_space
 self.pareto_front = []</p>

<p>def search(self, num_candidates: int = 100):
 “"”Search for Pareto-optimal architectures.”””
 evaluated = []</p>

<p>for i in range(num_candidates):
 # Sample architecture
 arch = self.search_space.sample_random_architecture()</p>

<p># Evaluate
 result = evaluate_speech_architecture(arch)</p>

<p>evaluated.append({
 “arch”: arch,
 “wer”: result[‘wer’],
 “latency”: result[‘latency_ms’],
 “params”: result[‘params’]
 })</p>

<p># Find Pareto frontier
 self.pareto_front = self._compute_pareto_front(evaluated)</p>

<p>return self.pareto_front</p>

<p>def _compute_pareto_front(self, candidates: List[Dict]) -&gt; List[Dict]:
 “””
 Compute Pareto frontier.</p>

<p>An architecture is Pareto-optimal if no other architecture
 is better in all objectives.
 “””
 pareto = []</p>

<p>for i, cand1 in enumerate(candidates):
 is_dominated = False</p>

<p>for j, cand2 in enumerate(candidates):
 if i == j:
 continue</p>

<p># Check if cand2 dominates cand1
 # (better or equal in all objectives, strictly better in at least one)
 if (cand2[‘wer’] &lt;= cand1[‘wer’] and
 cand2[‘latency’] &lt;= cand1[‘latency’] and
 cand2[‘params’] &lt;= cand1[‘params’] and
 (cand2[‘wer’] &lt; cand1[‘wer’] or
 cand2[‘latency’] &lt; cand1[‘latency’] or
 cand2[‘params’] &lt; cand1[‘params’])):
 is_dominated = True
 break</p>

<p>if not is_dominated:
 pareto.append(cand1)</p>

<p>return pareto</p>

<p>def select_for_target(self, max_latency_ms: float, max_params: int) -&gt; Optional[Dict]:
 “””
 Select best architecture meeting constraints.</p>

<p>Args:
 max_latency_ms: Maximum acceptable latency
 max_params: Maximum model size</p>

<p>Returns:
 Best architecture meeting constraints, or None
 “””
 candidates = [
 arch for arch in self.pareto_front
 if arch[‘latency’] &lt;= max_latency_ms and arch[‘params’] &lt;= max_params
 ]</p>

<p>if not candidates:
 return None</p>

<p># Return lowest WER among candidates
 return min(candidates, key=lambda x: x[‘wer’])
``</p>

<h2 id="scaling-strategies">Scaling Strategies</h2>

<h3 id="efficient-evaluation">Efficient Evaluation</h3>

<p><strong>1. Progressive training:</strong></p>

<p>``python
def progressive_evaluation(arch: SpeechArchConfig):
 “””
 Evaluate architecture progressively.</p>

<p>Start with small dataset/short training.
 Only continue if promising.
 “””
 # Stage 1: Train on LibriSpeech-100h for 5 epochs
 wer_stage1 = quick_train(arch, data=”librispeech-100h”, epochs=5)</p>

<p>if wer_stage1 &gt; 0.20: # 20% WER threshold
 return {“wer”: wer_stage1, “early_stopped”: True}</p>

<p># Stage 2: Train on LibriSpeech-100h for 20 epochs
 wer_stage2 = quick_train(arch, data=”librispeech-100h”, epochs=20)</p>

<p>if wer_stage2 &gt; 0.10:
 return {“wer”: wer_stage2, “early_stopped”: True}</p>

<p># Stage 3: Full training on LibriSpeech-960h
 wer_final = full_train(arch, data=”librispeech-960h”, epochs=100)</p>

<p>return {“wer”: wer_final, “early_stopped”: False}
``</p>

<p><strong>2. Weight sharing (supernet for speech):</strong></p>

<p>``python
class SpeechSuperNet(nn.Module):
 “””
 Super-network for speech NAS.</p>

<p>Contains all possible operations.
 Different architectures share weights.
 “””</p>

<p>def <strong>init</strong>(self, search_space: SpeechSearchSpace):
 super().<strong>init</strong>()</p>

<p># Create all encoder options
 self.encoders = nn.ModuleDict({
 enc_type.value: create_encoder(enc_type, max_layers=12, max_dim=768)
 for enc_type in EncoderType
 })</p>

<p># Create all decoder options
 self.decoders = nn.ModuleDict({
 dec_type.value: create_decoder(dec_type)
 for dec_type in DecoderType
 })</p>

<p>def forward(self, audio_features: torch.Tensor, arch: SpeechArchConfig):
 “"”Forward with specific architecture.”””
 # Select encoder
 encoder = self.encoders[arch.encoder_type.value]</p>

<p># Select decoder
 decoder = self.decoders[arch.decoder_type.value]</p>

<p># Forward pass
 encoder_out = encoder(audio_features)
 output = decoder(encoder_out)</p>

<p>return output
``</p>

<h2 id="real-world-case-study-googles-speech-nas">Real-World Case Study: Google’s Speech NAS</h2>

<h3 id="googles-approach-for-mobile-asr">Google’s Approach for Mobile ASR</h3>

<p><strong>Goal:</strong> Find ASR architecture for on-device deployment with &lt;100ms latency.</p>

<p><strong>Search space:</strong></p>
<ul>
  <li>Encoder: RNN, LSTM, GRU, Conformer variants</li>
  <li>Layers: 2-8</li>
  <li>Hidden dim: 128-512</li>
  <li>Decoder: CTC, RNN-T</li>
</ul>

<p><strong>Search strategy:</strong></p>
<ul>
  <li>Reinforcement learning controller</li>
  <li>Multi-objective: WER + latency + model size</li>
  <li>Progressive training (100h → 960h dataset)</li>
</ul>

<p><strong>Results:</strong></p>
<ul>
  <li><strong>Discovered architecture:</strong> 4-layer Conformer + RNN-T</li>
  <li><strong>WER:</strong> 5.2% on LibriSpeech test-clean (vs 6.1% baseline)</li>
  <li><strong>Latency:</strong> 85ms on Pixel 6 (vs 120ms baseline)</li>
  <li><strong>Size:</strong> 45M params (vs 80M baseline LSTM)</li>
  <li><strong>Search cost:</strong> 80 GPU days (vs months of manual tuning)</li>
</ul>

<p><strong>Key insights:</strong></p>
<ul>
  <li>Conformer with fewer layers beats deep LSTM</li>
  <li>RNN-T decoder better latency than attention for streaming</li>
  <li>Smaller models with better architecture beat larger hand-designed ones</li>
</ul>

<h3 id="lessons-learned">Lessons Learned</h3>

<ol>
  <li><strong>Speech-specific constraints matter:</strong> Streaming, variable length, long sequences</li>
  <li><strong>Multi-objective is essential:</strong> Can’t just optimize WER</li>
  <li><strong>Progressive evaluation saves compute:</strong> 80% of candidates filtered early</li>
  <li><strong>Transfer works:</strong> ImageNet NAS insights transfer to speech (depth vs width)</li>
  <li><strong>Hardware-in-the-loop:</strong> Measure latency on actual target device</li>
</ol>

<h2 id="cost-analysis">Cost Analysis</h2>

<h3 id="nas-vs-manual-design">NAS vs Manual Design</h3>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Time</th>
      <th>GPU Cost</th>
      <th>Quality (WER)</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Manual design</td>
      <td>6 months</td>
      <td>50 GPU days</td>
      <td>6.5%</td>
      <td>Expert-dependent</td>
    </tr>
    <tr>
      <td>Random search</td>
      <td>N/A</td>
      <td>500 GPU days</td>
      <td>7.0%</td>
      <td>Baseline</td>
    </tr>
    <tr>
      <td>Evolutionary NAS</td>
      <td>2 months</td>
      <td>100 GPU days</td>
      <td>5.8%</td>
      <td>Robust</td>
    </tr>
    <tr>
      <td>RL-based NAS</td>
      <td>1 month</td>
      <td>80 GPU days</td>
      <td>5.2%</td>
      <td>Google’s approach</td>
    </tr>
    <tr>
      <td>DARTS for speech</td>
      <td>2 weeks</td>
      <td>10 GPU days</td>
      <td>6.0%</td>
      <td>Fast but less stable</td>
    </tr>
    <tr>
      <td>Transfer + fine-tune</td>
      <td>1 week</td>
      <td>5 GPU days</td>
      <td>5.5%</td>
      <td>Use vision NAS results</td>
    </tr>
  </tbody>
</table>

<p><strong>ROI:</strong></p>
<ul>
  <li>Manual: <code class="language-plaintext highlighter-rouge">120K (engineer time) + </code>15K (GPUs) = $135K</li>
  <li>NAS: <code class="language-plaintext highlighter-rouge">40K (engineer time) + </code>24K (GPUs) = $64K</li>
  <li><strong>Savings:</strong> $71K + better model + faster iteration</li>
</ul>

<h2 id="advanced-topics">Advanced Topics</h2>

<h3 id="1-multi-lingual-nas">1. Multi-Lingual NAS</h3>

<p>Search for architectures that work across languages:</p>

<p>``python
def multi_lingual_nas(languages: List[str] = [“en”, “zh”, “es”]):
 “””
 Search for architecture that works well across languages.</p>

<p>Fitness = average WER across all languages.
 “””
 def evaluate_multilingual(arch: SpeechArchConfig) -&gt; float:
 wers = []</p>

<p>for lang in languages:
 wer = train_and_evaluate(
 arch,
 train_data=f”common_voice_{lang}”,
 val_data=f”common_voice_{lang}_dev”
 )
 wers.append(wer)</p>

<p># Average WER across languages
 return sum(wers) / len(wers)</p>

<p># Search with multi-lingual fitness
 # … (use evolutionary or RL search)
``</p>

<h3 id="2-streaming-aware-nas">2. Streaming-Aware NAS</h3>

<p>Optimize for streaming ASR:</p>

<p>``python
def streaming_aware_evaluation(arch: SpeechArchConfig) -&gt; Dict:
 “””
 Evaluate architecture for streaming capability.</p>

<p>Metrics:</p>
<ul>
  <li>Per-chunk latency (not full utterance)</li>
  <li>Look-ahead requirement</li>
  <li>Chunk size vs WER trade-off
 “””
 model = build_speech_model(arch)</li>
</ul>

<p># Test streaming performance
 chunk_size_ms = 100 # 100ms chunks</p>

<p>chunk_latency = measure_chunk_latency(model, chunk_size_ms)
 streaming_wer = evaluate_streaming_wer(model, chunk_size_ms)</p>

<p>return {
 “chunk_latency_ms”: chunk_latency,
 “streaming_wer”: streaming_wer,
 “supports_streaming”: chunk_latency &lt; chunk_size_ms
 }
``</p>

<h3 id="3-transfer-from-vision-nas">3. Transfer from Vision NAS</h3>

<p>Leverage insights from ImageNet NAS:</p>

<p>``python
def transfer_vision_to_speech(vision_arch_config):
 “””
 Transfer successful vision architectures to speech.</p>

<p>Example: EfficientNet principles → EfficientConformer</p>
<ul>
  <li>Depth scaling</li>
  <li>Width scaling</li>
  <li>Compound scaling
 “””
 # Extract architectural principles
 depth_factor = vision_arch_config.depth_coefficient
 width_factor = vision_arch_config.width_coefficient</li>
</ul>

<p># Apply to speech
 speech_config = SpeechArchConfig(
 encoder_type=EncoderType.CONFORMER,
 encoder_layers=int(6 * depth_factor),
 encoder_dim=int(256 * width_factor),
 encoder_heads=8,
 decoder_type=DecoderType.RNN_T,
 decoder_layers=2,
 decoder_dim=256,
 attention_type=AttentionType.RELATIVE,
 attention_dim=256,
 n_mels=80
 )</p>

<p>return speech_config
``</p>

<h2 id="monitoring--debugging">Monitoring &amp; Debugging</h2>

<h3 id="key-metrics">Key Metrics</h3>

<p><strong>Search Progress:</strong></p>
<ul>
  <li>Best WER found so far vs iterations</li>
  <li>Pareto frontier evolution</li>
  <li>Architecture diversity (entropy of designs explored)</li>
  <li>GPU utilization during search</li>
</ul>

<p><strong>Architecture Analysis:</strong></p>
<ul>
  <li>Most common encoder/decoder types in top performers</li>
  <li>Depth vs width trade-offs</li>
  <li>Correlation between architecture features and WER</li>
</ul>

<p><strong>Resource Tracking:</strong></p>
<ul>
  <li>Total GPU hours consumed</li>
  <li>Average training time per architecture</li>
  <li>Early stopping rate (% of archs stopped early)</li>
</ul>

<h3 id="debugging-tools">Debugging Tools</h3>

<ul>
  <li>Visualize architecture graphs</li>
  <li>Compare top-N architectures side-by-side</li>
  <li>Ablation studies (which components matter most?)</li>
  <li>Error analysis (where do discovered archs fail?)</li>
</ul>

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Speech NAS automates</strong> architecture design for ASR/TTS models</p>

<p>✅ <strong>Search space is exponential</strong> - like paths in a grid, need smart search</p>

<p>✅ <strong>DP and smart search</strong> make NAS practical - from infeasible to 50-100 GPU days</p>

<p>✅ <strong>Multi-objective optimization</strong> essential - WER, latency, size must be balanced</p>

<p>✅ <strong>Progressive evaluation</strong> saves compute - filter bad candidates early</p>

<p>✅ <strong>Weight sharing</strong> (supernet) enables evaluating 1000s of architectures</p>

<p>✅ <strong>Speech-specific constraints</strong> - streaming, variable length, multi-lingual</p>

<p>✅ <strong>Transfer from vision</strong> accelerates speech NAS</p>

<p>✅ <strong>Hardware-aware search</strong> critical for deployment</p>

<p>✅ <strong>Same DP pattern</strong> as Unique Paths - build optimal solution from sub-solutions</p>

<h3 id="connection-to-thematic-link-dynamic-programming-and-path-optimization">Connection to Thematic Link: Dynamic Programming and Path Optimization</h3>

<p>All three topics use <strong>DP to optimize paths through exponential spaces</strong>:</p>

<p><strong>DSA (Unique Paths):</strong></p>
<ul>
  <li>Navigate m×n grid using DP</li>
  <li>Recurrence: paths(i,j) = paths(i-1,j) + paths(i,j-1)</li>
  <li>Build solution from optimal sub-solutions</li>
</ul>

<p><strong>ML System Design (Neural Architecture Search):</strong></p>
<ul>
  <li>Navigate exponential architecture space</li>
  <li>Use DP/RL/gradient methods to find optimal</li>
  <li>Build full model from optimal components</li>
</ul>

<p><strong>Speech Tech (Speech Architecture Search):</strong></p>
<ul>
  <li>Navigate encoder×decoder×attention space</li>
  <li>Use DP-inspired search to find optimal speech models</li>
  <li>Build ASR/TTS from optimal sub-architectures</li>
</ul>

<p>The <strong>unifying principle</strong>: decompose exponentially large search spaces into manageable subproblems, solve optimally using DP or DP-inspired methods, and construct the best overall solution.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0021-speech-architecture-search/">arunbaby.com/speech-tech/0021-speech-architecture-search</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#automl" class="page__taxonomy-item p-category" rel="tag">automl</a><span class="sep">, </span>
    
      <a href="/tags/#model-optimization" class="page__taxonomy-item p-category" rel="tag">model-optimization</a><span class="sep">, </span>
    
      <a href="/tags/#nas" class="page__taxonomy-item p-category" rel="tag">nas</a><span class="sep">, </span>
    
      <a href="/tags/#neural-architecture-search" class="page__taxonomy-item p-category" rel="tag">neural-architecture-search</a><span class="sep">, </span>
    
      <a href="/tags/#speech-recognition" class="page__taxonomy-item p-category" rel="tag">speech-recognition</a><span class="sep">, </span>
    
      <a href="/tags/#tts" class="page__taxonomy-item p-category" rel="tag">tts</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0021-unique-paths/" rel="permalink">Unique Paths
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master grid path counting with dynamic programming—the same optimization technique used in neural architecture search and speech model design.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0021-neural-architecture-search/" rel="permalink">Neural Architecture Search
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Design neural architecture search systems that automatically discover optimal model architectures using dynamic programming and path optimization—the same pr...</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0021-vision-agent-fundamentals/" rel="permalink">Vision Agent Fundamentals
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Giving eyes to the brain: How Agents see the world.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Speech+Architecture+Search%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0021-speech-architecture-search%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0021-speech-architecture-search%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0021-speech-architecture-search/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0020-adaptive-speech-models/" class="pagination--pager" title="Adaptive Speech Models">Previous</a>
    
    
      <a href="/speech-tech/0022-cost-efficient-speech-systems/" class="pagination--pager" title="Cost-efficient Speech Systems">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
