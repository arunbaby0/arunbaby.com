<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Audio Feature Extraction for Speech ML - Arun Baby</title>
<meta name="description" content="How to transform raw audio waveforms into ML-ready features that capture speech characteristics for robust model training.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Audio Feature Extraction for Speech ML">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0003-audio-feature-extraction/">


  <meta property="og:description" content="How to transform raw audio waveforms into ML-ready features that capture speech characteristics for robust model training.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Audio Feature Extraction for Speech ML">
  <meta name="twitter:description" content="How to transform raw audio waveforms into ML-ready features that capture speech characteristics for robust model training.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0003-audio-feature-extraction/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0003-audio-feature-extraction/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Audio Feature Extraction for Speech ML">
    <meta itemprop="description" content="How to transform raw audio waveforms into ML-ready features that capture speech characteristics for robust model training.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0003-audio-feature-extraction/" itemprop="url">Audio Feature Extraction for Speech ML
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          29 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#problem-definition">Problem Definition</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li></ul></li><li><a href="#audio-basics">Audio Basics</a><ul><li><a href="#waveform-representation">Waveform Representation</a></li></ul></li><li><a href="#feature-1-mel-frequency-cepstral-coefficients-mfccs">Feature 1: Mel-Frequency Cepstral Coefficients (MFCCs)</a><ul><li><a href="#why-mfccs">Why MFCCs?</a></li><li><a href="#how-mfccs-work">How MFCCs Work</a></li><li><a href="#implementation">Implementation</a></li><li><a href="#visualizing-mfccs">Visualizing MFCCs</a></li></ul></li><li><a href="#feature-2-mel-spectrograms">Feature 2: Mel-Spectrograms</a><ul><li><a href="#what-is-a-spectrogram">What is a Spectrogram?</a></li><li><a href="#mel-spectrogram-vs-mfcc">Mel-Spectrogram vs MFCC</a></li><li><a href="#implementation-1">Implementation</a></li><li><a href="#visualizing-mel-spectrogram">Visualizing Mel-Spectrogram</a></li></ul></li><li><a href="#feature-3-raw-spectrograms-stft">Feature 3: Raw Spectrograms (STFT)</a><ul><li><a href="#implementation-2">Implementation</a></li></ul></li><li><a href="#feature-4-time-domain-features">Feature 4: Time-Domain Features</a><ul><li><a href="#implementation-3">Implementation</a></li></ul></li><li><a href="#feature-5-pitch--formants">Feature 5: Pitch &amp; Formants</a><ul><li><a href="#pitch-extraction">Pitch Extraction</a></li></ul></li><li><a href="#production-feature-pipeline">Production Feature Pipeline</a><ul><li><a href="#unified-feature-extractor">Unified Feature Extractor</a></li></ul></li><li><a href="#handling-variable-length-audio">Handling Variable-Length Audio</a><ul><li><a href="#strategy-1-paddingtruncation">Strategy 1: Padding/Truncation</a></li><li><a href="#strategy-2-temporal-pooling">Strategy 2: Temporal Pooling</a></li></ul></li><li><a href="#real-time-feature-extraction">Real-Time Feature Extraction</a><ul><li><a href="#streaming-feature-extractor">Streaming Feature Extractor</a></li></ul></li><li><a href="#performance-optimization">Performance Optimization</a><ul><li><a href="#1-caching-features">1. Caching Features</a></li><li><a href="#2-parallel-processing">2. Parallel Processing</a></li></ul></li><li><a href="#advanced-feature-types">Advanced Feature Types</a><ul><li><a href="#1-learned-features-embeddings">1. Learned Features (Embeddings)</a></li><li><a href="#2-filter-bank-features-fbank">2. Filter Bank Features (FBank)</a></li><li><a href="#3-prosodic-features">3. Prosodic Features</a></li></ul></li><li><a href="#feature-quality--validation">Feature Quality &amp; Validation</a><ul><li><a href="#feature-quality-metrics">Feature Quality Metrics</a></li></ul></li><li><a href="#connection-to-data-preprocessing-pipeline">Connection to Data Preprocessing Pipeline</a><ul><li><a href="#parallel-concepts">Parallel Concepts</a></li><li><a href="#unified-preprocessing-framework">Unified Preprocessing Framework</a></li></ul></li><li><a href="#production-best-practices">Production Best Practices</a><ul><li><a href="#1-feature-versioning">1. Feature Versioning</a></li><li><a href="#2-error-handling">2. Error Handling</a></li><li><a href="#3-monitoring-feature-quality">3. Monitoring Feature Quality</a></li></ul></li><li><a href="#data-augmentation-in-feature-space">Data Augmentation in Feature Space</a><ul><li><a href="#specaugment">SpecAugment</a></li></ul></li><li><a href="#batch-feature-extraction-for-training">Batch Feature Extraction for Training</a><ul><li><a href="#batch-extraction-pipeline">Batch Extraction Pipeline</a></li></ul></li><li><a href="#real-world-systems">Real-World Systems</a><ul><li><a href="#kaldi-traditional-asr-feature-pipeline">Kaldi: Traditional ASR Feature Pipeline</a></li><li><a href="#pytorch-modern-deep-learning-pipeline">PyTorch: Modern Deep Learning Pipeline</a></li><li><a href="#google-production-asr-feature-extraction">Google: Production ASR Feature Extraction</a></li></ul></li><li><a href="#choosing-the-right-features">Choosing the Right Features</a><ul><li><a href="#feature-selection-guide">Feature Selection Guide</a></li><li><a href="#combining-features">Combining Features</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>How to transform raw audio waveforms into ML-ready features that capture speech characteristics for robust model training.</strong></p>

<h2 id="introduction">Introduction</h2>

<p>Raw audio waveforms are high-dimensional, noisy, and difficult for ML models to learn from directly. <strong>Feature extraction</strong> transforms audio into compact, informative representations that:</p>

<ul>
  <li>Capture important speech characteristics</li>
  <li>Reduce dimensionality (16kHz audio = 16,000 samples/sec → ~40 features)</li>
  <li>Provide invariance to irrelevant variations (volume, recording device)</li>
  <li>Enable efficient model training</li>
</ul>

<p><strong>Why it matters:</strong></p>
<ul>
  <li><strong>Improves accuracy:</strong> Good features → better models</li>
  <li><strong>Reduces compute:</strong> Lower dimensionality = faster training/inference</li>
  <li><strong>Enables transfer learning:</strong> Pre-extracted features work across tasks</li>
  <li><strong>Production efficiency:</strong> Feature extraction can be cached</li>
</ul>

<p><strong>What you’ll learn:</strong></p>
<ul>
  <li>Core audio features (MFCCs, spectrograms, mel-scale)</li>
  <li>Time-domain vs frequency-domain features</li>
  <li>Production-grade extraction pipelines</li>
  <li>Optimization for real-time processing</li>
  <li>Feature engineering for speech tasks</li>
</ul>

<hr />

<h2 id="problem-definition">Problem Definition</h2>

<p>Design a feature extraction pipeline for speech ML systems.</p>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Feature Types</strong>
    <ul>
      <li>Time-domain features (energy, zero-crossing rate)</li>
      <li>Frequency-domain features (spectrograms, MFCCs)</li>
      <li>Temporal features (deltas, delta-deltas)</li>
      <li>Learned features (embeddings)</li>
    </ul>
  </li>
  <li><strong>Input Handling</strong>
    <ul>
      <li>Support multiple sample rates (8kHz, 16kHz, 48kHz)</li>
      <li>Handle variable-length audio</li>
      <li>Process both mono and stereo</li>
      <li>Support batch processing</li>
    </ul>
  </li>
  <li><strong>Output Format</strong>
    <ul>
      <li>Fixed-size feature vectors</li>
      <li>Variable-length sequences</li>
      <li>2D/3D tensors for neural networks</li>
    </ul>
  </li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Performance</strong>
    <ul>
      <li>Real-time: Extract features &lt; 10ms for 1 sec audio</li>
      <li>Batch: Process 10K files/hour on single machine</li>
      <li>Memory: &lt; 100MB RAM for streaming</li>
    </ul>
  </li>
  <li><strong>Quality</strong>
    <ul>
      <li>Robust to noise</li>
      <li>Consistent across devices</li>
      <li>Reproducible (deterministic)</li>
    </ul>
  </li>
  <li><strong>Flexibility</strong>
    <ul>
      <li>Configurable parameters</li>
      <li>Support multiple backends (librosa, torchaudio)</li>
      <li>Easy to extend with new features</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="audio-basics">Audio Basics</h2>

<h3 id="waveform-representation">Waveform Representation</h3>

<p>``python
import numpy as np
import librosa
import matplotlib.pyplot as plt</p>

<h1 id="load-audio">Load audio</h1>
<p>audio, sr = librosa.load(‘speech.wav’, sr=16000)</p>

<p>print(f”Sample rate: {sr} Hz”)
print(f”Duration: {len(audio) / sr:.2f} seconds”)
print(f”Shape: {audio.shape}”)
print(f”Range: [{audio.min():.3f}, {audio.max():.3f}]”)</p>

<h1 id="visualize-waveform">Visualize waveform</h1>
<p>plt.figure(figsize=(12, 4))
time = np.arange(len(audio)) / sr
plt.plot(time, audio)
plt.xlabel(‘Time (s)’)
plt.ylabel(‘Amplitude’)
plt.title(‘Audio Waveform’)
plt.show()
``</p>

<p><strong>Key properties:</strong></p>
<ul>
  <li><strong>Sample rate (sr):</strong> Samples per second (e.g., 16000 Hz = 16000 samples/sec)</li>
  <li><strong>Duration:</strong> <code class="language-plaintext highlighter-rouge">len(audio) / sr</code> seconds</li>
  <li><strong>Amplitude:</strong> Typically normalized to [-1, 1]</li>
</ul>

<hr />

<h2 id="feature-1-mel-frequency-cepstral-coefficients-mfccs">Feature 1: Mel-Frequency Cepstral Coefficients (MFCCs)</h2>

<p><strong>MFCCs</strong> are the most widely used features in speech recognition.</p>

<h3 id="why-mfccs">Why MFCCs?</h3>

<ol>
  <li><strong>Mimic human hearing:</strong> Use mel scale (perceptual frequency scale)</li>
  <li><strong>Compact:</strong> Represent spectral envelope with 13-40 coefficients</li>
  <li><strong>Robust:</strong> Less sensitive to pitch variations</li>
  <li><strong>Proven:</strong> Gold standard for ASR for decades</li>
</ol>

<h3 id="how-mfccs-work">How MFCCs Work</h3>

<p>``
Audio Waveform
 ↓</p>
<ol>
  <li>Pre-emphasis (boost high frequencies)
 ↓</li>
  <li>Frame the signal (25ms windows, 10ms hop)
 ↓</li>
  <li>Apply window function (Hamming)
 ↓</li>
  <li>FFT (Fast Fourier Transform)
 ↓</li>
  <li>Mel filterbank (map to mel scale)
 ↓</li>
  <li>Log (compress dynamic range)
 ↓</li>
  <li>DCT (Discrete Cosine Transform)
 ↓
MFCCs (13-40 coefficients per frame)
``</li>
</ol>

<h3 id="implementation">Implementation</h3>

<p>``python
import librosa
import numpy as np</p>

<p>class MFCCExtractor:
 “””
 Extract MFCC features from audio</p>

<p>Standard configuration for speech recognition
 “””</p>

<p>def <strong>init</strong>(
 self,
 sr=16000,
 n_mfcc=40,
 n_fft=512,
 hop_length=160, # 10ms at 16kHz
 n_mels=40,
 fmin=20,
 fmax=8000
 ):
 self.sr = sr
 self.n_mfcc = n_mfcc
 self.n_fft = n_fft
 self.hop_length = hop_length
 self.n_mels = n_mels
 self.fmin = fmin
 self.fmax = fmax</p>

<p>def extract(self, audio: np.ndarray) -&gt; np.ndarray:
 “””
 Extract MFCCs</p>

<p>Args:
 audio: Audio waveform (1D array)</p>

<p>Returns:
 MFCCs: (n_mfcc, time_steps)
 “””
 # Extract MFCCs
 mfccs = librosa.feature.mfcc(
 y=audio,
 sr=self.sr,
 n_mfcc=self.n_mfcc,
 n_fft=self.n_fft,
 hop_length=self.hop_length,
 n_mels=self.n_mels,
 fmin=self.fmin,
 fmax=self.fmax
 )</p>

<p>return mfccs # Shape: (n_mfcc, time)</p>

<p>def extract_with_deltas(self, audio: np.ndarray) -&gt; np.ndarray:
 “””
 Extract MFCCs + deltas + delta-deltas</p>

<p>Deltas capture temporal dynamics</p>

<p>Returns:
 Features: (n_mfcc * 3, time_steps)
 “””
 # MFCCs
 mfccs = self.extract(audio)</p>

<p># Delta (first derivative)
 delta = librosa.feature.delta(mfccs)</p>

<p># Delta-delta (second derivative)
 delta2 = librosa.feature.delta(mfccs, order=2)</p>

<p># Stack
 features = np.vstack([mfccs, delta, delta2]) # (120, time)</p>

<p>return features</p>

<h1 id="usage">Usage</h1>
<p>extractor = MFCCExtractor()
mfccs = extractor.extract(audio)
print(f”MFCCs shape: {mfccs.shape}”) # (40, time_steps)</p>

<h1 id="with-deltas">With deltas</h1>
<p>features = extractor.extract_with_deltas(audio)
print(f”MFCCs+deltas shape: {features.shape}”) # (120, time_steps)
``</p>

<h3 id="visualizing-mfccs">Visualizing MFCCs</h3>

<p>``python
import matplotlib.pyplot as plt</p>

<p>def plot_mfccs(mfccs, sr, hop_length):
 “"”Visualize MFCC features”””
 plt.figure(figsize=(12, 6))</p>

<p># Convert frame indices to time
 times = librosa.frames_to_time(
 np.arange(mfccs.shape[1]),
 sr=sr,
 hop_length=hop_length
 )</p>

<p>plt.imshow(
 mfccs,
 aspect=’auto’,
 origin=’lower’,
 extent=[times[0], times[-1], 0, mfccs.shape[0]],
 cmap=’viridis’
 )</p>

<p>plt.colorbar(format=’%+2.0f dB’)
 plt.xlabel(‘Time (s)’)
 plt.ylabel(‘MFCC Coefficient’)
 plt.title(‘MFCC Features’)
 plt.tight_layout()
 plt.show()</p>

<p>plot_mfccs(mfccs, sr=16000, hop_length=160)
``</p>

<hr />

<h2 id="feature-2-mel-spectrograms">Feature 2: Mel-Spectrograms</h2>

<p><strong>Mel-spectrograms</strong> preserve more temporal detail than MFCCs.</p>

<h3 id="what-is-a-spectrogram">What is a Spectrogram?</h3>

<p>A <strong>spectrogram</strong> shows how the frequency content of a signal changes over time.</p>

<ul>
  <li><strong>X-axis:</strong> Time</li>
  <li><strong>Y-axis:</strong> Frequency</li>
  <li><strong>Color:</strong> Magnitude (energy)</li>
</ul>

<h3 id="mel-spectrogram-vs-mfcc">Mel-Spectrogram vs MFCC</h3>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>Mel-Spectrogram</th>
      <th>MFCC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Dimensions</td>
      <td>(n_mels, time)</td>
      <td>(n_mfcc, time)</td>
    </tr>
    <tr>
      <td>Information</td>
      <td>Full spectrum</td>
      <td>Spectral envelope</td>
    </tr>
    <tr>
      <td>Size</td>
      <td>40-128 bins</td>
      <td>13-40 coefficients</td>
    </tr>
    <tr>
      <td>Use case</td>
      <td>CNNs, deep learning</td>
      <td>Traditional ASR</td>
    </tr>
    <tr>
      <td>Temporal resolution</td>
      <td>Higher</td>
      <td>Lower (due to DCT)</td>
    </tr>
  </tbody>
</table>

<h3 id="implementation-1">Implementation</h3>

<p>``python
class MelSpectrogramExtractor:
 “””
 Extract log mel-spectrogram features</p>

<p>Popular for deep learning models (CNNs, Transformers)
 “””</p>

<p>def <strong>init</strong>(
 self,
 sr=16000,
 n_fft=512,
 hop_length=160,
 n_mels=80,
 fmin=0,
 fmax=8000
 ):
 self.sr = sr
 self.n_fft = n_fft
 self.hop_length = hop_length
 self.n_mels = n_mels
 self.fmin = fmin
 self.fmax = fmax</p>

<p>def extract(self, audio: np.ndarray) -&gt; np.ndarray:
 “””
 Extract log mel-spectrogram</p>

<p>Returns:
 Log mel-spectrogram: (n_mels, time_steps)
 “””
 # Compute mel spectrogram
 mel_spec = librosa.feature.melspectrogram(
 y=audio,
 sr=self.sr,
 n_fft=self.n_fft,
 hop_length=self.hop_length,
 n_mels=self.n_mels,
 fmin=self.fmin,
 fmax=self.fmax
 )</p>

<p># Convert to log scale (dB)
 log_mel = librosa.power_to_db(mel_spec, ref=np.max)</p>

<p>return log_mel # Shape: (n_mels, time)</p>

<p>def extract_normalized(self, audio: np.ndarray) -&gt; np.ndarray:
 “””
 Extract and normalize to [0, 1]</p>

<p>Better for neural networks
 “””
 log_mel = self.extract(audio)</p>

<p># Normalize to [0, 1]
 log_mel_norm = (log_mel - log_mel.min()) / (log_mel.max() - log_mel.min() + 1e-8)</p>

<p>return log_mel_norm</p>

<h1 id="usage-1">Usage</h1>
<p>mel_extractor = MelSpectrogramExtractor(n_mels=80)
mel_spec = mel_extractor.extract(audio)
print(f”Mel-spectrogram shape: {mel_spec.shape}”) # (80, time_steps)
``</p>

<h3 id="visualizing-mel-spectrogram">Visualizing Mel-Spectrogram</h3>

<p>``python
def plot_mel_spectrogram(mel_spec, sr, hop_length):
 “"”Visualize mel-spectrogram”””
 plt.figure(figsize=(12, 6))</p>

<p>librosa.display.specshow(
 mel_spec,
 sr=sr,
 hop_length=hop_length,
 x_axis=’time’,
 y_axis=’mel’,
 cmap=’viridis’
 )</p>

<p>plt.colorbar(format=’%+2.0f dB’)
 plt.title(‘Mel-Spectrogram’)
 plt.tight_layout()
 plt.show()</p>

<p>plot_mel_spectrogram(mel_spec, sr=16000, hop_length=160)
``</p>

<hr />

<h2 id="feature-3-raw-spectrograms-stft">Feature 3: Raw Spectrograms (STFT)</h2>

<p><strong>Short-Time Fourier Transform (STFT)</strong> provides the highest frequency resolution.</p>

<h3 id="implementation-2">Implementation</h3>

<p>``python
class STFTExtractor:
 “””
 Extract raw STFT features</p>

<p>Used when you need full frequency resolution
 “””</p>

<p>def <strong>init</strong>(
 self,
 n_fft=512,
 hop_length=160,
 win_length=400
 ):
 self.n_fft = n_fft
 self.hop_length = hop_length
 self.win_length = win_length</p>

<p>def extract(self, audio: np.ndarray) -&gt; np.ndarray:
 “””
 Extract magnitude spectrogram</p>

<p>Returns:
 Spectrogram: (n_fft//2 + 1, time_steps)
 “””
 # Compute STFT
 stft = librosa.stft(
 audio,
 n_fft=self.n_fft,
 hop_length=self.hop_length,
 win_length=self.win_length
 )</p>

<p># Get magnitude
 magnitude = np.abs(stft)</p>

<p># Convert to dB
 magnitude_db = librosa.amplitude_to_db(magnitude, ref=np.max)</p>

<p>return magnitude_db # Shape: (n_fft//2 + 1, time)</p>

<p>def extract_with_phase(self, audio: np.ndarray):
 “””
 Extract magnitude and phase</p>

<p>Phase information useful for reconstruction
 “””
 stft = librosa.stft(
 audio,
 n_fft=self.n_fft,
 hop_length=self.hop_length,
 win_length=self.win_length
 )</p>

<p>magnitude = np.abs(stft)
 phase = np.angle(stft)</p>

<p>return magnitude, phase</p>

<h1 id="usage-2">Usage</h1>
<p>stft_extractor = STFTExtractor()
spectrogram = stft_extractor.extract(audio)
print(f”Spectrogram shape: {spectrogram.shape}”) # (257, time_steps)
``</p>

<hr />

<h2 id="feature-4-time-domain-features">Feature 4: Time-Domain Features</h2>

<p>Simple but effective features computed directly from waveform.</p>

<h3 id="implementation-3">Implementation</h3>

<p>``python
class TimeDomainExtractor:
 “””
 Extract time-domain features</p>

<p>Fast to compute, useful for simple tasks
 “””</p>

<p>def extract_energy(self, audio: np.ndarray, frame_length=400, hop_length=160):
 “””
 Frame-wise energy (RMS)</p>

<p>Captures loudness/volume over time
 “””
 energy = librosa.feature.rms(
 y=audio,
 frame_length=frame_length,
 hop_length=hop_length
 )[0]</p>

<p>return energy</p>

<p>def extract_zero_crossing_rate(self, audio: np.ndarray, frame_length=400, hop_length=160):
 “””
 Zero-crossing rate</p>

<p>Measures how often signal crosses zero
 High ZCR → noisy/unvoiced
 Low ZCR → tonal/voiced
 “””
 zcr = librosa.feature.zero_crossing_rate(
 audio,
 frame_length=frame_length,
 hop_length=hop_length
 )[0]</p>

<p>return zcr</p>

<p>def extract_all(self, audio: np.ndarray):
 “"”Extract all time-domain features”””
 energy = self.extract_energy(audio)
 zcr = self.extract_zero_crossing_rate(audio)</p>

<p># Stack features
 features = np.vstack([energy, zcr]) # (2, time)</p>

<p>return features</p>

<h1 id="usage-3">Usage</h1>
<p>time_extractor = TimeDomainExtractor()
time_features = time_extractor.extract_all(audio)
print(f”Time-domain features shape: {time_features.shape}”) # (2, time_steps)
``</p>

<hr />

<h2 id="feature-5-pitch--formants">Feature 5: Pitch &amp; Formants</h2>

<p><strong>Pitch</strong> and <strong>formants</strong> are linguistic features important for speech.</p>

<h3 id="pitch-extraction">Pitch Extraction</h3>

<p>``python
class PitchExtractor:
 “””
 Extract fundamental frequency (F0)</p>

<p>Important for:</p>
<ul>
  <li>Speaker recognition</li>
  <li>Emotion detection</li>
  <li>Prosody modeling
 “””</li>
</ul>

<p>def <strong>init</strong>(self, sr=16000, fmin=80, fmax=400):
 self.sr = sr
 self.fmin = fmin # Typical male voice
 self.fmax = fmax # Typical female voice</p>

<p>def extract_f0(self, audio: np.ndarray, hop_length=160):
 “””
 Extract pitch (fundamental frequency)</p>

<p>Returns:
 f0: Pitch values (Hz) per frame
 voiced_flag: Boolean array (voiced vs unvoiced)
 “””
 # Extract pitch using YIN algorithm
 f0 = librosa.yin(
 audio,
 fmin=self.fmin,
 fmax=self.fmax,
 sr=self.sr,
 hop_length=hop_length
 )</p>

<p># Detect voiced regions (f0 &gt; 0)
 voiced_flag = f0 &gt; 0</p>

<p>return f0, voiced_flag</p>

<p>def extract_pitch_features(self, audio: np.ndarray):
 “””
 Extract pitch statistics</p>

<p>Useful for speaker/emotion recognition
 “””
 f0, voiced = self.extract_f0(audio)</p>

<p># Statistics on voiced frames
 voiced_f0 = f0[voiced]</p>

<p>if len(voiced_f0) &gt; 0:
 features = {
 ‘mean_pitch’: np.mean(voiced_f0),
 ‘std_pitch’: np.std(voiced_f0),
 ‘min_pitch’: np.min(voiced_f0),
 ‘max_pitch’: np.max(voiced_f0),
 ‘pitch_range’: np.max(voiced_f0) - np.min(voiced_f0),
 ‘voiced_ratio’: np.sum(voiced) / len(voiced)
 }
 else:
 features = {k: 0.0 for k in [‘mean_pitch’, ‘std_pitch’, ‘min_pitch’, ‘max_pitch’, ‘pitch_range’, ‘voiced_ratio’]}</p>

<p>return features</p>

<h1 id="usage-4">Usage</h1>
<p>pitch_extractor = PitchExtractor()
f0, voiced = pitch_extractor.extract_f0(audio)
print(f”Pitch shape: {f0.shape}”)</p>

<p>pitch_stats = pitch_extractor.extract_pitch_features(audio)
print(f”Pitch statistics: {pitch_stats}”)
``</p>

<hr />

<h2 id="production-feature-pipeline">Production Feature Pipeline</h2>

<p>Combine all features into a unified pipeline.</p>

<h3 id="unified-feature-extractor">Unified Feature Extractor</h3>

<p>``python
from dataclasses import dataclass
from typing import Dict, List, Optional
import json</p>

<p>@dataclass
class FeatureConfig:
 “"”Configuration for feature extraction”””
 sr: int = 16000
 feature_types: List[str] = None # [‘mfcc’, ‘mel’, ‘pitch’]</p>

<p># MFCC config
 n_mfcc: int = 40</p>

<p># Mel-spectrogram config
 n_mels: int = 80</p>

<p># Common config
 n_fft: int = 512
 hop_length: int = 160 # 10ms</p>

<p># Normalization
 normalize: bool = True</p>

<p>def <strong>post_init</strong>(self):
 if self.feature_types is None:
 self.feature_types = [‘mfcc’]</p>

<p>class AudioFeatureExtractor:
 “””
 Production-grade audio feature extractor</p>

<p>Supports multiple feature types, caching, and batch processing
 “””</p>

<p>def <strong>init</strong>(self, config: FeatureConfig):
 self.config = config</p>

<p># Initialize extractors
 self.mfcc_extractor = MFCCExtractor(
 sr=config.sr,
 n_mfcc=config.n_mfcc,
 n_fft=config.n_fft,
 hop_length=config.hop_length
 )</p>

<p>self.mel_extractor = MelSpectrogramExtractor(
 sr=config.sr,
 n_mels=config.n_mels,
 n_fft=config.n_fft,
 hop_length=config.hop_length
 )</p>

<p>self.pitch_extractor = PitchExtractor(sr=config.sr)
 self.time_extractor = TimeDomainExtractor()</p>

<p>def extract(self, audio: np.ndarray) -&gt; Dict[str, np.ndarray]:
 “””
 Extract features based on config</p>

<p>Args:
 audio: Audio waveform</p>

<p>Returns:
 Dictionary of features
 “””
 features = {}</p>

<p>if ‘mfcc’ in self.config.feature_types:
 mfccs = self.mfcc_extractor.extract_with_deltas(audio)
 if self.config.normalize:
 mfccs = self._normalize(mfccs)
 features[‘mfcc’] = mfccs</p>

<p>if ‘mel’ in self.config.feature_types:
 mel = self.mel_extractor.extract(audio)
 if self.config.normalize:
 mel = self._normalize(mel)
 features[‘mel’] = mel</p>

<p>if ‘pitch’ in self.config.feature_types:
 f0, voiced = self.pitch_extractor.extract_f0(audio, hop_length=self.config.hop_length)
 features[‘pitch’] = f0
 features[‘voiced’] = voiced.astype(np.float32)</p>

<p>if ‘time’ in self.config.feature_types:
 time_feats = self.time_extractor.extract_all(audio)
 if self.config.normalize:
 time_feats = self._normalize(time_feats)
 features[‘time’] = time_feats</p>

<p>return features</p>

<p>def _normalize(self, features: np.ndarray) -&gt; np.ndarray:
 “””
 Normalize features (mean=0, std=1) per coefficient
 “””
 mean = np.mean(features, axis=1, keepdims=True)
 std = np.std(features, axis=1, keepdims=True) + 1e-8</p>

<p>normalized = (features - mean) / std</p>

<p>return normalized</p>

<p>def extract_from_file(self, audio_path: str) -&gt; Dict[str, np.ndarray]:
 “””
 Extract features from audio file
 “””
 audio, sr = librosa.load(audio_path, sr=self.config.sr)
 return self.extract(audio)</p>

<p>def extract_batch(self, audio_list: List[np.ndarray]) -&gt; List[Dict[str, np.ndarray]]:
 “””
 Extract features from batch of audio
 “””
 return [self.extract(audio) for audio in audio_list]</p>

<p>def save_config(self, path: str):
 “"”Save feature extraction config”””
 with open(path, ‘w’) as f:
 json.dump(self.config.<strong>dict</strong>, f, indent=2)</p>

<p>@staticmethod
 def load_config(path: str) -&gt; FeatureConfig:
 “"”Load feature extraction config”””
 with open(path, ‘r’) as f:
 config_dict = json.load(f)
 return FeatureConfig(**config_dict)</p>

<h1 id="usage-5">Usage</h1>
<p>config = FeatureConfig(
 feature_types=[‘mfcc’, ‘mel’, ‘pitch’],
 n_mfcc=40,
 n_mels=80,
 normalize=True
)</p>

<p>extractor = AudioFeatureExtractor(config)</p>

<h1 id="extract-features">Extract features</h1>
<p>features = extractor.extract(audio)
print(“Extracted features:”, features.keys())
for name, feat in features.items():
 print(f” {name}: {feat.shape}”)</p>

<h1 id="save-config-for-reproducibility">Save config for reproducibility</h1>
<p>extractor.save_config(‘feature_config.json’)
``</p>

<hr />

<h2 id="handling-variable-length-audio">Handling Variable-Length Audio</h2>

<p>Different audio clips have different durations. Need to handle this for ML.</p>

<h3 id="strategy-1-paddingtruncation">Strategy 1: Padding/Truncation</h3>

<p>``python
class VariableLengthHandler:
 “””
 Handle variable-length audio
 “””</p>

<p>def pad_or_truncate(self, features: np.ndarray, target_length: int) -&gt; np.ndarray:
 “””
 Pad or truncate features to fixed length</p>

<p>Args:
 features: (n_features, time)
 target_length: Target time dimension</p>

<p>Returns:
 Fixed-length features: (n_features, target_length)
 “””
 current_length = features.shape[1]</p>

<p>if current_length &lt; target_length:
 # Pad with zeros
 pad_width = ((0, 0), (0, target_length - current_length))
 features = np.pad(features, pad_width, mode=’constant’)
 elif current_length &gt; target_length:
 # Truncate (take first target_length frames)
 features = features[:, :target_length]</p>

<p>return features</p>

<p>def create_mask(self, features: np.ndarray, target_length: int) -&gt; np.ndarray:
 “””
 Create attention mask for padded features</p>

<p>Returns:
 Mask: (target_length,) - 1 for real frames, 0 for padding
 “””
 current_length = features.shape[1]</p>

<p>mask = np.zeros(target_length)
 mask[:min(current_length, target_length)] = 1</p>

<p>return mask
``</p>

<h3 id="strategy-2-temporal-pooling">Strategy 2: Temporal Pooling</h3>

<p>``python
class TemporalPooler:
 “””
 Pool variable-length features to fixed size
 “””</p>

<p>def mean_pool(self, features: np.ndarray) -&gt; np.ndarray:
 “””
 Average pool over time</p>

<p>Args:
 features: (n_features, time)</p>

<p>Returns:
 Pooled: (n_features,)
 “””
 return np.mean(features, axis=1)</p>

<p>def max_pool(self, features: np.ndarray) -&gt; np.ndarray:
 “"”Max pool over time”””
 return np.max(features, axis=1)</p>

<p>def stats_pool(self, features: np.ndarray) -&gt; np.ndarray:
 “””
 Statistical pooling: mean + std</p>

<p>Returns:
 Pooled: (n_features * 2,)
 “””
 mean = np.mean(features, axis=1)
 std = np.std(features, axis=1)</p>

<p>return np.concatenate([mean, std])
``</p>

<hr />

<h2 id="real-time-feature-extraction">Real-Time Feature Extraction</h2>

<p>For streaming applications, need incremental feature extraction.</p>

<h3 id="streaming-feature-extractor">Streaming Feature Extractor</h3>

<p>``python
from collections import deque</p>

<p>class StreamingFeatureExtractor:
 “””
 Extract features from streaming audio</p>

<p>Use case: Real-time ASR, voice assistants
 “””</p>

<p>def <strong>init</strong>(
 self,
 sr=16000,
 frame_length_ms=25,
 hop_length_ms=10,
 buffer_duration_ms=500
 ):
 self.sr = sr
 self.frame_length = int(sr * frame_length_ms / 1000)
 self.hop_length = int(sr * hop_length_ms / 1000)
 self.buffer_length = int(sr * buffer_duration_ms / 1000)</p>

<p># Circular buffer for audio
 self.buffer = deque(maxlen=self.buffer_length)</p>

<p># Feature extractor
 self.extractor = MFCCExtractor(
 sr=sr,
 hop_length=self.hop_length
 )</p>

<p>def add_audio_chunk(self, audio_chunk: np.ndarray):
 “””
 Add new audio chunk to buffer</p>

<p>Args:
 audio_chunk: New audio samples
 “””
 self.buffer.extend(audio_chunk)</p>

<p>def extract_latest(self) -&gt; Optional[np.ndarray]:
 “””
 Extract features from current buffer</p>

<p>Returns:
 Features or None if buffer too small
 “””
 if len(self.buffer) &lt; self.frame_length:
 return None</p>

<p># Convert buffer to array
 audio = np.array(self.buffer)</p>

<p># Extract features
 features = self.extractor.extract(audio)</p>

<p>return features</p>

<p>def reset(self):
 “"”Clear buffer”””
 self.buffer.clear()</p>

<h1 id="usage-6">Usage</h1>
<p>streaming_extractor = StreamingFeatureExtractor()</p>

<h1 id="simulate-streaming-100ms-chunks">Simulate streaming (100ms chunks)</h1>
<p>chunk_size = 1600 # 100ms at 16kHz</p>

<p>for i in range(0, len(audio), chunk_size):
 chunk = audio[i:i+chunk_size]</p>

<p># Add to buffer
 streaming_extractor.add_audio_chunk(chunk)</p>

<p># Extract features
 features = streaming_extractor.extract_latest()</p>

<p>if features is not None:
 print(f”Chunk {i//chunk_size}: features shape = {features.shape}”)
 # Process features (send to model, etc.)
``</p>

<hr />

<h2 id="performance-optimization">Performance Optimization</h2>

<h3 id="1-caching-features">1. Caching Features</h3>

<p>``python
import os
import pickle
import hashlib</p>

<p>class CachedFeatureExtractor:
 “””
 Cache extracted features to disk</p>

<p>Avoid re-extracting for same audio
 “””</p>

<p>def <strong>init</strong>(self, extractor: AudioFeatureExtractor, cache_dir=’./feature_cache’):
 self.extractor = extractor
 self.cache_dir = cache_dir
 os.makedirs(cache_dir, exist_ok=True)</p>

<p>def _get_cache_path(self, audio_path: str) -&gt; str:
 “"”Generate cache file path based on audio path hash”””
 path_hash = hashlib.md5(audio_path.encode()).hexdigest()
 return os.path.join(self.cache_dir, f”{path_hash}.pkl”)</p>

<p>def extract_from_file(self, audio_path: str, use_cache=True) -&gt; Dict[str, np.ndarray]:
 “””
 Extract features with caching
 “””
 cache_path = self._get_cache_path(audio_path)</p>

<p># Check cache
 if use_cache and os.path.exists(cache_path):
 with open(cache_path, ‘rb’) as f:
 features = pickle.load(f)
 return features</p>

<p># Extract features
 features = self.extractor.extract_from_file(audio_path)</p>

<p># Save to cache
 with open(cache_path, ‘wb’) as f:
 pickle.dump(features, f)</p>

<p>return features
``</p>

<h3 id="2-parallel-processing">2. Parallel Processing</h3>

<p>``python
from multiprocessing import Pool
from functools import partial</p>

<p>class ParallelFeatureExtractor:
 “””
 Extract features from multiple files in parallel
 “””</p>

<p>def <strong>init</strong>(self, extractor: AudioFeatureExtractor, n_workers=4):
 self.extractor = extractor
 self.n_workers = n_workers</p>

<p>def extract_from_files(self, audio_paths: List[str]) -&gt; List[Dict[str, np.ndarray]]:
 “””
 Extract features from multiple files in parallel
 “””
 with Pool(self.n_workers) as pool:
 features_list = pool.map(
 self.extractor.extract_from_file,
 audio_paths
 )</p>

<p>return features_list</p>

<h1 id="usage-7">Usage</h1>
<p>parallel_extractor = ParallelFeatureExtractor(extractor, n_workers=8)
audio_files = [‘file1.wav’, ‘file2.wav’, …] # 1000s of files
features = parallel_extractor.extract_from_files(audio_files)
``</p>

<hr />

<h2 id="advanced-feature-types">Advanced Feature Types</h2>

<h3 id="1-learned-features-embeddings">1. Learned Features (Embeddings)</h3>

<p>Instead of hand-crafted features, learn representations from data.</p>

<p>``python
import torch
import torch.nn as nn</p>

<p>class AudioEmbeddingExtractor(nn.Module):
 “””
 Extract learned audio embeddings</p>

<p>Use pre-trained models (wav2vec, HuBERT) as feature extractors
 “””</p>

<p>def <strong>init</strong>(self, model_name=’facebook/wav2vec2-base’):
 super().<strong>init</strong>()
 from transformers import Wav2Vec2Model</p>

<p># Load pre-trained model
 self.model = Wav2Vec2Model.from_pretrained(model_name)
 self.model.eval() # Freeze for feature extraction</p>

<p>def extract(self, audio: np.ndarray, sr=16000) -&gt; np.ndarray:
 “””
 Extract contextualized embeddings</p>

<p>Returns:
 Embeddings: (time_steps, hidden_dim)
 typically (time, 768) for base model
 “””
 # Convert to tensor
 audio_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)</p>

<p># Extract features
 with torch.no_grad():
 outputs = self.model(audio_tensor)
 embeddings = outputs.last_hidden_state[0] # (time, 768)</p>

<p>return embeddings.numpy()</p>

<h1 id="usage---much-more-powerful-than-mfccs-for-transfer-learning">Usage - MUCH more powerful than MFCCs for transfer learning</h1>
<p>embedding_extractor = AudioEmbeddingExtractor()
embeddings = embedding_extractor.extract(audio)
print(f”Embeddings shape: {embeddings.shape}”) # (time, 768)
``</p>

<p><strong>Comparison:</strong></p>

<table>
  <thead>
    <tr>
      <th>Feature Type</th>
      <th>Dimension</th>
      <th>Training Required</th>
      <th>Transfer Learning</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MFCCs</td>
      <td>40-120</td>
      <td>No</td>
      <td>Poor</td>
      <td>Baseline</td>
    </tr>
    <tr>
      <td>Mel-spectrogram</td>
      <td>80-128</td>
      <td>No</td>
      <td>Good</td>
      <td>+5-10%</td>
    </tr>
    <tr>
      <td>Wav2Vec embeddings</td>
      <td>768</td>
      <td>Yes (pre-trained)</td>
      <td>Excellent</td>
      <td>+15-25%</td>
    </tr>
  </tbody>
</table>

<h3 id="2-filter-bank-features-fbank">2. Filter Bank Features (FBank)</h3>

<p>Alternative to MFCCs - skip the DCT step.</p>

<p>``python
class FilterbankExtractor:
 “””
 Extract log mel-filterbank features</p>

<p>Similar to mel-spectrograms, popular in modern ASR
 “””</p>

<p>def <strong>init</strong>(self, sr=16000, n_mels=80, n_fft=512, hop_length=160):
 self.sr = sr
 self.n_mels = n_mels
 self.n_fft = n_fft
 self.hop_length = hop_length</p>

<p>def extract(self, audio: np.ndarray) -&gt; np.ndarray:
 “””
 Extract log filter bank energies</p>

<p>Returns:
 FBank: (n_mels, time_steps)
 “””
 # Mel spectrogram
 mel_spec = librosa.feature.melspectrogram(
 y=audio,
 sr=self.sr,
 n_fft=self.n_fft,
 hop_length=self.hop_length,
 n_mels=self.n_mels
 )</p>

<p># Log
 log_mel = librosa.power_to_db(mel_spec, ref=np.max)</p>

<p>return log_mel</p>

<h1 id="fbank-vs-mfcc">FBank vs MFCC:</h1>
<h1 id="--fbank-keep-all-mel-bins-80-128">- FBank: Keep all mel bins (80-128)</h1>
<h1 id="--mfcc-compress-to-13-40-via-dct">- MFCC: Compress to 13-40 via DCT</h1>
<p>#</p>
<h1 id="fbank-often-works-better-with-neural-networks">FBank often works better with neural networks</h1>
<p>``</p>

<h3 id="3-prosodic-features">3. Prosodic Features</h3>

<p>Capture rhythm, stress, and intonation.</p>

<p>``python
class ProsodicFeatureExtractor:
 “””
 Extract prosodic features for emotion, speaker ID, etc.
 “””</p>

<p>def extract_intensity_contour(self, audio, sr=16000, hop_length=160):
 “””
 Intensity (loudness) over time
 “””
 intensity = librosa.feature.rms(y=audio, hop_length=hop_length)[0]</p>

<p># Convert to dB
 intensity_db = librosa.amplitude_to_db(intensity, ref=np.max)</p>

<p>return intensity_db</p>

<p>def extract_speaking_rate(self, audio, sr=16000):
 “””
 Estimate speaking rate (syllables per second)</p>

<p>Approximation: count peaks in energy envelope
 “””
 # Energy envelope
 energy = librosa.feature.rms(y=audio, hop_length=160)[0]</p>

<p># Find peaks (local maxima)
 from scipy.signal import find_peaks</p>

<p>peaks, _ = find_peaks(energy, distance=10, prominence=0.1)</p>

<p># Speaking rate
 duration = len(audio) / sr
 syllables_per_sec = len(peaks) / duration</p>

<p>return syllables_per_sec</p>

<p>def extract_all_prosodic(self, audio, sr=16000):
 “"”Extract all prosodic features”””</p>

<p># Pitch
 pitch_extractor = PitchExtractor(sr=sr)
 pitch_stats = pitch_extractor.extract_pitch_features(audio)</p>

<p># Intensity
 intensity = self.extract_intensity_contour(audio, sr)</p>

<p># Speaking rate
 speaking_rate = self.extract_speaking_rate(audio, sr)</p>

<p>return {
 **pitch_stats,
 ‘mean_intensity’: np.mean(intensity),
 ‘std_intensity’: np.std(intensity),
 ‘speaking_rate’: speaking_rate
 }
``</p>

<hr />

<h2 id="feature-quality--validation">Feature Quality &amp; Validation</h2>

<p>Ensure extracted features are high quality.</p>

<h3 id="feature-quality-metrics">Feature Quality Metrics</h3>

<p>``python
class FeatureQualityChecker:
 “””
 Validate quality of extracted features
 “””</p>

<p>def check_for_nans(self, features: Dict[str, np.ndarray]) -&gt; bool:
 “"”Check for NaN/Inf values”””
 for name, feat in features.items():
 if np.isnan(feat).any() or np.isinf(feat).any():
 print(f”⚠️ {name} contains NaN/Inf”)
 return False
 return True</p>

<p>def check_dynamic_range(self, features: Dict[str, np.ndarray]) -&gt; Dict[str, float]:
 “””
 Check dynamic range of features</p>

<p>Low dynamic range → feature not informative
 “””
 ranges = {}</p>

<p>for name, feat in features.items():
 feat_range = feat.max() - feat.min()
 ranges[name] = feat_range</p>

<p>if feat_range &lt; 1e-6:
 print(f”⚠️ {name} has very low dynamic range: {feat_range}”)</p>

<p>return ranges</p>

<p>def check_feature_statistics(self, features_batch: List[np.ndarray]):
 “””
 Check statistics across batch</p>

<p>Ensure features are properly normalized
 “””
 # Stack all features
 all_features = np.concatenate(features_batch, axis=1) # (n_features, total_time)</p>

<p># Per-feature statistics
 mean_per_feature = np.mean(all_features, axis=1)
 std_per_feature = np.std(all_features, axis=1)</p>

<p>print(“Feature Statistics:”)
 print(f” Mean range: [{mean_per_feature.min():.3f}, {mean_per_feature.max():.3f}]”)
 print(f” Std range: [{std_per_feature.min():.3f}, {std_per_feature.max():.3f}]”)</p>

<p># Check if normalized
 if np.abs(mean_per_feature).max() &gt; 0.1:
 print(“⚠️ Features not centered (mean far from 0)”)</p>

<p>if np.abs(std_per_feature - 1.0).max() &gt; 0.2:
 print(“⚠️ Features not standardized (std far from 1)”)
``</p>

<hr />

<h2 id="connection-to-data-preprocessing-pipeline">Connection to Data Preprocessing Pipeline</h2>

<p>Feature extraction for speech is analogous to data preprocessing for ML systems (seeML).</p>

<h3 id="parallel-concepts">Parallel Concepts</h3>

<table>
  <thead>
    <tr>
      <th>Speech Feature Extraction</th>
      <th>ML Data Preprocessing</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Handle missing audio</td>
      <td>Handle missing values</td>
    </tr>
    <tr>
      <td>Normalize features (mean=0, std=1)</td>
      <td>Normalize numerical features</td>
    </tr>
    <tr>
      <td>Pad/truncate variable length</td>
      <td>Handle variable-length sequences</td>
    </tr>
    <tr>
      <td>Validate audio quality</td>
      <td>Schema validation</td>
    </tr>
    <tr>
      <td>Cache extracted features</td>
      <td>Cache preprocessed data</td>
    </tr>
    <tr>
      <td>Batch processing</td>
      <td>Distributed data processing</td>
    </tr>
  </tbody>
</table>

<h3 id="unified-preprocessing-framework">Unified Preprocessing Framework</h3>

<p>``python
class UnifiedPreprocessor:
 “””
 Combined preprocessing for multimodal ML</p>

<p>Example: Speech + text + metadata
 “””</p>

<p>def <strong>init</strong>(self):
 # Audio features
 self.audio_extractor = AudioFeatureExtractor(
 FeatureConfig(feature_types=[‘mfcc’, ‘mel’])
 )</p>

<p># Text features (from transcripts)
 from sklearn.feature_extraction.text import TfidfVectorizer
 self.text_vectorizer = TfidfVectorizer(max_features=1000)</p>

<p># Numerical features
 from sklearn.preprocessing import StandardScaler
 self.numerical_scaler = StandardScaler()</p>

<p>def preprocess_sample(self, audio, text, metadata):
 “””
 Preprocess multimodal sample</p>

<p>Args:
 audio: Audio waveform
 text: Transcript or description
 metadata: User/item metadata (dict)</p>

<p>Returns:
 Combined feature vector
 “””
 # Extract audio features
 audio_features = self.audio_extractor.extract(audio)
 audio_pooled = np.mean(audio_features[‘mfcc’], axis=1) # (n_mfcc,)</p>

<p># Extract text features
 text_features = self.text_vectorizer.transform([text]).toarray()[0] # (1000,)</p>

<p># Process metadata
 metadata_array = np.array([
 metadata[‘user_age’],
 metadata[‘user_gender’],
 metadata[‘device_type’]
 ])
 metadata_scaled = self.numerical_scaler.transform([metadata_array])[0]</p>

<p># Concatenate all features
 combined = np.concatenate([
 audio_pooled, # (40,)
 text_features, # (1000,)
 metadata_scaled # (3,)
 ]) # Total: (1043,)</p>

<p>return combined
``</p>

<hr />

<h2 id="production-best-practices">Production Best Practices</h2>

<h3 id="1-feature-versioning">1. Feature Versioning</h3>

<p>Track feature extraction versions for reproducibility.</p>

<p>``python
class VersionedFeatureExtractor:
 “””
 Version feature extraction logic</p>

<p>Critical for:</p>
<ul>
  <li>A/B testing different features</li>
  <li>Rollback if new features hurt performance</li>
  <li>Reproducibility
 “””</li>
</ul>

<p>VERSION = “1.2.0”</p>

<p>def <strong>init</strong>(self, config: FeatureConfig):
 self.config = config
 self.extractor = AudioFeatureExtractor(config)</p>

<p>def extract_with_metadata(self, audio_path: str):
 “””
 Extract features with version metadata
 “””
 features = self.extractor.extract_from_file(audio_path)</p>

<p>metadata = {
 ‘version’: self.VERSION,
 ‘config’: self.config.<strong>dict</strong>,
 ‘timestamp’: datetime.now().isoformat(),
 ‘audio_path’: audio_path
 }</p>

<p>return {
 ‘features’: features,
 ‘metadata’: metadata
 }</p>

<p>def save_features(self, features, output_path):
 “"”Save features with version info”””
 np.savez_compressed(
 output_path,
 **features[‘features’],
 metadata=json.dumps(features[‘metadata’])
 )
``</p>

<h3 id="2-error-handling">2. Error Handling</h3>

<p>Robust feature extraction handles failures gracefully.</p>

<p>``python
class RobustFeatureExtractor:
 “””
 Feature extractor with error handling
 “””</p>

<p>def <strong>init</strong>(self, extractor: AudioFeatureExtractor):
 self.extractor = extractor</p>

<p>def extract_safe(self, audio_path: str) -&gt; Optional[Dict]:
 “””
 Extract features with error handling
 “””
 try:
 # Load audio
 audio, sr = librosa.load(audio_path, sr=self.extractor.config.sr)</p>

<p># Validate
 if len(audio) == 0:
 logger.warning(f”Empty audio: {audio_path}”)
 return None</p>

<p>if len(audio) &lt; self.extractor.config.sr * 0.1: # &lt; 100ms
 logger.warning(f”Audio too short: {audio_path}”)
 return None</p>

<p># Extract
 features = self.extractor.extract(audio)</p>

<p># Quality check
 quality_checker = FeatureQualityChecker()
 if not quality_checker.check_for_nans(features):
 logger.error(f”Feature extraction failed (NaN): {audio_path}”)
 return None</p>

<p>return features</p>

<p>except Exception as e:
 logger.error(f”Feature extraction error for {audio_path}: {e}”)
 return None</p>

<p>def extract_batch_robust(self, audio_paths: List[str]) -&gt; List[Dict]:
 “””
 Extract from batch, skipping failures
 “””
 results = []
 failures = []</p>

<p>for path in audio_paths:
 features = self.extract_safe(path)
 if features is not None:
 results.append({‘path’: path, ‘features’: features})
 else:
 failures.append(path)</p>

<p>success_rate = len(results) / len(audio_paths)
 logger.info(f”Feature extraction: {len(results)}/{len(audio_paths)} succeeded ({success_rate:.1%})”)</p>

<p>if failures:
 logger.warning(f”Failed files: {failures[:10]}”) # Log first 10</p>

<p>return results
``</p>

<h3 id="3-monitoring-feature-quality">3. Monitoring Feature Quality</h3>

<p>Track feature statistics over time to detect issues.</p>

<p>``python
class FeatureMonitor:
 “””
 Monitor feature quality in production
 “””</p>

<p>def <strong>init</strong>(self, expected_stats: Dict[str, Dict]):
 “””
 Args:
 expected_stats: Expected statistics per feature type
 {
 ‘mfcc’: {‘mean_range’: [-5, 5], ‘std_range’: [0.5, 2.0]},
 ‘mel’: {‘mean_range’: [-80, 0], ‘std_range’: [10, 30]}
 }
 “””
 self.expected_stats = expected_stats</p>

<p>def validate_features(self, features: Dict[str, np.ndarray]) -&gt; List[str]:
 “””
 Validate extracted features against expected statistics</p>

<p>Returns:
 List of warnings
 “””
 warnings = []</p>

<p>for feat_name, feat_values in features.items():
 if feat_name not in self.expected_stats:
 continue</p>

<p>expected = self.expected_stats[feat_name]</p>

<p># Check mean
 actual_mean = np.mean(feat_values)
 expected_mean_range = expected[‘mean_range’]</p>

<p>if not (expected_mean_range[0] &lt;= actual_mean &lt;= expected_mean_range[1]):
 warnings.append(
 f”{feat_name}: mean {actual_mean:.2f} outside expected range {expected_mean_range}”
 )</p>

<p># Check std
 actual_std = np.std(feat_values)
 expected_std_range = expected[‘std_range’]</p>

<p>if not (expected_std_range[0] &lt;= actual_std &lt;= expected_std_range[1]):
 warnings.append(
 f”{feat_name}: std {actual_std:.2f} outside expected range {expected_std_range}”
 )</p>

<p>return warnings</p>

<p>def compute_statistics(self, features_batch: List[Dict[str, np.ndarray]]):
 “””
 Compute statistics across batch</p>

<p>Use to establish baseline expected_stats
 “””
 stats = {}</p>

<p># Get feature names from first sample
 feature_names = features_batch[0].keys()</p>

<p>for feat_name in feature_names:
 # Collect all values
 all_values = np.concatenate([
 f[feat_name].flatten() for f in features_batch
 ])</p>

<p>stats[feat_name] = {
 ‘mean’: np.mean(all_values),
 ‘std’: np.std(all_values),
 ‘min’: np.min(all_values),
 ‘max’: np.max(all_values),
 ‘percentiles’: {
 ‘25’: np.percentile(all_values, 25),
 ‘50’: np.percentile(all_values, 50),
 ‘75’: np.percentile(all_values, 75),
 ‘95’: np.percentile(all_values, 95)
 }
 }</p>

<p>return stats
``</p>

<hr />

<h2 id="data-augmentation-in-feature-space">Data Augmentation in Feature Space</h2>

<p>Augment features directly for training robustness.</p>

<h3 id="specaugment">SpecAugment</h3>

<p>``python
class SpecAugment:
 “””
 SpecAugment: Data augmentation on spectrograms</p>

<p>Proposed in “SpecAugment: A Simple Data Augmentation Method for ASR” (Google, 2019)</p>

<p>Improves ASR accuracy by 10-20% on many benchmarks
 “””</p>

<p>def <strong>init</strong>(
 self,
 time_mask_param=70,
 freq_mask_param=15,
 num_time_masks=2,
 num_freq_masks=2
 ):
 self.time_mask_param = time_mask_param
 self.freq_mask_param = freq_mask_param
 self.num_time_masks = num_time_masks
 self.num_freq_masks = num_freq_masks</p>

<p>def time_mask(self, spec: np.ndarray) -&gt; np.ndarray:
 “””
 Mask random time region</p>

<p>Sets random time frames to zero
 “””
 spec = spec.copy()
 time_length = spec.shape[1]</p>

<p>for _ in range(self.num_time_masks):
 t = np.random.randint(0, min(self.time_mask_param, time_length))
 t0 = np.random.randint(0, time_length - t)
 spec[:, t0:t0+t] = 0</p>

<p>return spec</p>

<p>def freq_mask(self, spec: np.ndarray) -&gt; np.ndarray:
 “””
 Mask random frequency region</p>

<p>Sets random frequency bins to zero
 “””
 spec = spec.copy()
 freq_length = spec.shape[0]</p>

<p>for _ in range(self.num_freq_masks):
 f = np.random.randint(0, min(self.freq_mask_param, freq_length))
 f0 = np.random.randint(0, freq_length - f)
 spec[f0:f0+f, :] = 0</p>

<p>return spec</p>

<p>def augment(self, spec: np.ndarray) -&gt; np.ndarray:
 “"”Apply both time and freq masking”””
 spec = self.time_mask(spec)
 spec = self.freq_mask(spec)
 return spec</p>

<h1 id="usage-during-training">Usage during training</h1>
<p>augmenter = SpecAugment()</p>

<p>for audio, label in train_loader:
 # Extract features
 mel_spec = mel_extractor.extract(audio)</p>

<p># Augment
 mel_spec_aug = augmenter.augment(mel_spec)</p>

<p># Train model
 train_model(mel_spec_aug, label)
``</p>

<hr />

<h2 id="batch-feature-extraction-for-training">Batch Feature Extraction for Training</h2>

<p>Extract features for entire dataset efficiently.</p>

<h3 id="batch-extraction-pipeline">Batch Extraction Pipeline</h3>

<p>``python
import os
from pathlib import Path
from tqdm import tqdm
import h5py</p>

<p>class BatchFeatureExtractor:
 “””
 Extract features for large audio datasets</p>

<p>Use case: Prepare training data</p>
<ul>
  <li>Extract once, train many times</li>
  <li>Save features to disk (HDF5 format)
 “””</li>
</ul>

<p>def <strong>init</strong>(self, extractor: AudioFeatureExtractor, n_workers=8):
 self.extractor = extractor
 self.n_workers = n_workers</p>

<p>def extract_dataset(
 self,
 audio_dir: str,
 output_path: str,
 max_length_frames: int = 1000
 ):
 “””
 Extract features for all audio files in directory</p>

<p>Args:
 audio_dir: Directory containing .wav files
 output_path: HDF5 file to save features
 max_length_frames: Pad/truncate to this length
 “””
 # Find all audio files
 audio_files = list(Path(audio_dir).rglob(‘*.wav’))
 print(f”Found {len(audio_files)} audio files”)</p>

<p># Create HDF5 file
 with h5py.File(output_path, ‘w’) as hf:
 # Pre-allocate datasets
 # (We’ll store features for each type)
 feature_dim = self.extractor.config.n_mfcc * 3 # MFCCs + deltas</p>

<p>features_dataset = hf.create_dataset(
 ‘features’,
 shape=(len(audio_files), feature_dim, max_length_frames),
 dtype=’float32’
 )</p>

<p>lengths_dataset = hf.create_dataset(
 ‘lengths’,
 shape=(len(audio_files),),
 dtype=’int32’
 )</p>

<p># Store file paths
 paths_dataset = hf.create_dataset(
 ‘paths’,
 shape=(len(audio_files),),
 dtype=h5py.string_dtype()
 )</p>

<p># Extract features
 for idx, audio_path in enumerate(tqdm(audio_files)):
 try:
 # Load audio
 audio, sr = librosa.load(str(audio_path), sr=self.extractor.config.sr)</p>

<p># Extract features
 features = self.extractor.extract(audio)</p>

<p># Get MFCCs with deltas
 mfcc_deltas = features[‘mfcc’] # (120, time)</p>

<p># Pad or truncate
 handler = VariableLengthHandler()
 mfcc_fixed = handler.pad_or_truncate(mfcc_deltas, max_length_frames)</p>

<p># Store
 features_dataset[idx] = mfcc_fixed
 lengths_dataset[idx] = min(mfcc_deltas.shape[1], max_length_frames)
 paths_dataset[idx] = str(audio_path)</p>

<p>except Exception as e:
 logger.error(f”Failed to process {audio_path}: {e}”)
 # Store zeros for failed files
 features_dataset[idx] = np.zeros((feature_dim, max_length_frames))
 lengths_dataset[idx] = 0
 paths_dataset[idx] = str(audio_path)</p>

<p>print(f”Features saved to {output_path}”)</p>

<h1 id="usage-8">Usage</h1>
<p>batch_extractor = BatchFeatureExtractor(extractor, n_workers=8)
batch_extractor.extract_dataset(
 audio_dir=’./data/train/’,
 output_path=’./features/train_features.h5’,
 max_length_frames=1000
)</p>

<h1 id="load-for-training">Load for training</h1>
<p>with h5py.File(‘./features/train_features.h5’, ‘r’) as hf:
 features = hf[‘features’][:] # (N, feature_dim, max_length)
 lengths = hf[‘lengths’][:] # (N,)
 paths = hf[‘paths’][:] # (N,)
``</p>

<hr />

<h2 id="real-world-systems">Real-World Systems</h2>

<h3 id="kaldi-traditional-asr-feature-pipeline">Kaldi: Traditional ASR Feature Pipeline</h3>

<p>Kaldi is the industry standard for traditional ASR.</p>

<p><strong>Feature extraction:</strong>
``bash</p>
<h1 id="kaldi-feature-extraction-mfcc--pitch">Kaldi feature extraction (MFCC + pitch)</h1>
<p>compute-mfcc-feats –config=conf/mfcc.conf scp:wav.scp ark:mfcc.ark
compute-and-process-kaldi-pitch-feats scp:wav.scp ark:pitch.ark</p>

<h1 id="combine-features">Combine features</h1>
<p>paste-feats ark:mfcc.ark ark:pitch.ark ark:features.ark
``</p>

<p><strong>Configuration (mfcc.conf):</strong>
<code class="language-plaintext highlighter-rouge">
--use-energy=true
--num-mel-bins=40
--num-ceps=40
--low-freq=20
--high-freq=8000
--sample-frequency=16000
</code></p>

<h3 id="pytorch-modern-deep-learning-pipeline">PyTorch: Modern Deep Learning Pipeline</h3>

<p>``python
import torchaudio
import torch</p>

<p>class TorchAudioExtractor:
 “””
 Feature extraction using torchaudio</p>

<p>Benefits:</p>
<ul>
  <li>GPU acceleration</li>
  <li>Differentiable (can backprop through features)</li>
  <li>Integrated with PyTorch training
 “””</li>
</ul>

<p>def <strong>init</strong>(self, sr=16000, n_mfcc=40, n_mels=80):
 self.sr = sr
 self.n_mfcc = n_mfcc
 self.n_mels = n_mels</p>

<p># Create transforms (can move to GPU)
 self.mfcc_transform = torchaudio.transforms.MFCC(
 sample_rate=sr,
 n_mfcc=n_mfcc,
 melkwargs={‘n_mels’: 40, ‘n_fft’: 512, ‘hop_length’: 160}
 )</p>

<p>self.mel_transform = torchaudio.transforms.MelSpectrogram(
 sample_rate=sr,
 n_fft=512,
 hop_length=160,
 n_mels=n_mels
 )</p>

<p># Amplitude → dB conversion
 self.db_transform = torchaudio.transforms.AmplitudeToDB()</p>

<p>def to(self, device):
 “””
 Move transforms to a device (CPU/GPU) and return self.
 “””
 self.mfcc_transform = self.mfcc_transform.to(device)
 self.mel_transform = self.mel_transform.to(device)
 self.db_transform = self.db_transform.to(device)
 return self</p>

<p>def extract(self, audio: torch.Tensor) -&gt; Dict[str, torch.Tensor]:
 “””
 Extract features (GPU-accelerated if audio on GPU)</p>

<p>Args:
 audio: (batch, time) or (time,)</p>

<p>Returns:
 Dictionary of features
 “””
 if audio.ndim == 1:
 audio = audio.unsqueeze(0) # Add batch dimension</p>

<p># Extract
 mfccs = self.mfcc_transform(audio) # (batch, n_mfcc, time)
 mel = self.mel_transform(audio) # (batch, n_mels, time)
 mel_db = self.db_transform(mel)</p>

<p>return {
 ‘mfcc’: mfccs,
 ‘mel’: mel_db
 }</p>

<h1 id="usage-with-gpu">Usage with GPU</h1>
<p>device = torch.device(‘cuda’ if torch.cuda.is_available() else ‘cpu’)</p>

<p>extractor = TorchAudioExtractor().to(device)</p>

<h1 id="load-audio-1">Load audio</h1>
<p>audio, sr = torchaudio.load(‘speech.wav’)
audio = audio.to(device)</p>

<h1 id="extract-on-gpu">Extract (on GPU)</h1>
<p>features = extractor.extract(audio)
``</p>

<h3 id="google-production-asr-feature-extraction">Google: Production ASR Feature Extraction</h3>

<p><strong>Stack:</strong></p>
<ul>
  <li><strong>Input:</strong> 16kHz audio</li>
  <li><strong>Features:</strong> 80-bin log mel-filterbank</li>
  <li><strong>Augmentation:</strong> SpecAugment</li>
  <li><strong>Normalization:</strong> Per-utterance mean/variance normalization</li>
  <li><strong>Model:</strong> Transformer encoder-decoder</li>
</ul>

<p><strong>Key optimizations:</strong></p>
<ul>
  <li>Precompute features for training data</li>
  <li>On-the-fly extraction for inference</li>
  <li>GPU-accelerated extraction for real-time systems</li>
</ul>

<hr />

<h2 id="choosing-the-right-features">Choosing the Right Features</h2>

<p>Different tasks need different features.</p>

<h3 id="feature-selection-guide">Feature Selection Guide</h3>

<table>
  <thead>
    <tr>
      <th>Task</th>
      <th>Best Features</th>
      <th>Why</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ASR (traditional)</strong></td>
      <td>MFCCs + deltas</td>
      <td>Captures phonetic content</td>
    </tr>
    <tr>
      <td><strong>ASR (deep learning)</strong></td>
      <td>Mel-spectrograms</td>
      <td>Works well with CNNs</td>
    </tr>
    <tr>
      <td><strong>Speaker Recognition</strong></td>
      <td>MFCCs + pitch + prosody</td>
      <td>Speaker identity in pitch/prosody</td>
    </tr>
    <tr>
      <td><strong>Emotion Recognition</strong></td>
      <td>Prosodic + spectral</td>
      <td>Emotion in prosody + voice quality</td>
    </tr>
    <tr>
      <td><strong>Keyword Spotting</strong></td>
      <td>Mel-spectrograms</td>
      <td>Simple, fast with CNNs</td>
    </tr>
    <tr>
      <td><strong>Speech Enhancement</strong></td>
      <td>STFT magnitude + phase</td>
      <td>Need phase for reconstruction</td>
    </tr>
    <tr>
      <td><strong>Voice Activity Detection</strong></td>
      <td>Energy + ZCR</td>
      <td>Simple features sufficient</td>
    </tr>
  </tbody>
</table>

<h3 id="combining-features">Combining Features</h3>

<p>``python
class MultiFeatureExtractor:
 “””
 Combine multiple feature types</p>

<p>Different features capture different aspects
 “””</p>

<p>def <strong>init</strong>(self):
 self.mfcc_ext = MFCCExtractor()
 self.pitch_ext = PitchExtractor()
 self.prosody_ext = ProsodicFeatureExtractor()</p>

<p>def extract_combined(self, audio):
 “””
 Extract and combine multiple feature types
 “””
 # MFCCs (40, time)
 mfccs = self.mfcc_ext.extract(audio)</p>

<p># Pitch (time,)
 pitch, voiced = self.pitch_ext.extract_f0(audio)
 pitch = pitch.reshape(1, -1) # (1, time)</p>

<p># Energy (1, time)
 energy = librosa.feature.rms(y=audio, hop_length=160)</p>

<p># Align all features to same time dimension
 min_time = min(mfccs.shape[1], pitch.shape[1], energy.shape[1])</p>

<p>mfccs = mfccs[:, :min_time]
 pitch = pitch[:, :min_time]
 energy = energy[:, :min_time]</p>

<p># Stack
 combined = np.vstack([mfccs, pitch, energy]) # (42, time)</p>

<p>return combined
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>MFCCs</strong> are standard for speech recognition - compact and robust 
✅ <strong>Mel-spectrograms</strong> work better with deep learning (CNNs, Transformers) 
✅ <strong>Delta features</strong> capture temporal dynamics - critical for accuracy 
✅ <strong>Normalize features</strong> for stable training (mean=0, std=1) 
✅ <strong>Handle variable length</strong> with padding, pooling, or attention masks 
✅ <strong>Cache features</strong> for repeated use - major speedup in training 
✅ <strong>Streaming extraction</strong> possible with circular buffers 
✅ <strong>Parallel processing</strong> speeds up batch feature extraction 
✅ <strong>SpecAugment</strong> improves robustness through feature-space augmentation 
✅ <strong>Monitor feature quality</strong> to detect pipeline issues early 
✅ <strong>Version features</strong> for reproducibility and A/B testing 
✅ <strong>Choose features based on task</strong> - no one-size-fits-all</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0003-audio-feature-extraction/">arunbaby.com/speech-tech/0003-audio-feature-extraction</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#audio-processing" class="page__taxonomy-item p-category" rel="tag">audio-processing</a><span class="sep">, </span>
    
      <a href="/tags/#feature-extraction" class="page__taxonomy-item p-category" rel="tag">feature-extraction</a><span class="sep">, </span>
    
      <a href="/tags/#mfcc" class="page__taxonomy-item p-category" rel="tag">mfcc</a><span class="sep">, </span>
    
      <a href="/tags/#spectrograms" class="page__taxonomy-item p-category" rel="tag">spectrograms</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0003-merge-sorted-lists/" rel="permalink">Merge Two Sorted Lists
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          29 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The pointer manipulation pattern that powers merge sort, data pipeline merging, and multi-source stream processing.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0003-data-preprocessing/" rel="permalink">Data Preprocessing Pipeline Design
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          28 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How to build production-grade pipelines that clean, transform, and validate billions of data points before training.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0003-prompt-engineering-for-agents/" rel="permalink">Prompt Engineering for Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Programming with English: The High-Level Language of 2024.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Audio+Feature+Extraction+for+Speech+ML%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0003-audio-feature-extraction%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0003-audio-feature-extraction%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0003-audio-feature-extraction/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0002-speech-classification/" class="pagination--pager" title="Speech Command Classification">Previous</a>
    
    
      <a href="/speech-tech/0004-voice-activity-detection/" class="pagination--pager" title="Voice Activity Detection (VAD)">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
