<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Speech Emotion Recognition - Arun Baby</title>
<meta name="description" content="“Teaching machines to hear feelings.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Speech Emotion Recognition">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/">


  <meta property="og:description" content="“Teaching machines to hear feelings.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Speech Emotion Recognition">
  <meta name="twitter:description" content="“Teaching machines to hear feelings.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:07:50+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Speech Emotion Recognition">
    <meta itemprop="description" content="“Teaching machines to hear feelings.”">
    <meta itemprop="datePublished" content="2025-12-31T10:07:50+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/" itemprop="url">Speech Emotion Recognition
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction">1. Introduction</a></li><li><a href="#2-challenges-in-ser">2. Challenges in SER</a></li><li><a href="#3-acoustic-features-for-ser">3. Acoustic Features for SER</a><ul><li><a href="#31-prosodic-features">3.1. Prosodic Features</a></li><li><a href="#32-spectral-features">3.2. Spectral Features</a></li><li><a href="#33-voice-quality-features">3.3. Voice Quality Features</a></li></ul></li><li><a href="#4-traditional-ml-approaches">4. Traditional ML Approaches</a><ul><li><a href="#41-feature-extraction--classifier">4.1. Feature Extraction + Classifier</a></li><li><a href="#42-opensmile-features">4.2. openSMILE Features</a></li></ul></li><li><a href="#5-deep-learning-approaches">5. Deep Learning Approaches</a><ul><li><a href="#51-cnn-on-spectrograms">5.1. CNN on Spectrograms</a></li><li><a href="#52-lstmgru-on-sequences">5.2. LSTM/GRU on Sequences</a></li><li><a href="#53-transformer-based-models">5.3. Transformer-Based Models</a></li></ul></li><li><a href="#6-datasets">6. Datasets</a><ul><li><a href="#61-iemocap">6.1. IEMOCAP</a></li><li><a href="#62-ravdess">6.2. RAVDESS</a></li><li><a href="#63-crema-d">6.3. CREMA-D</a></li><li><a href="#64-cmu-mosei">6.4. CMU-MOSEI</a></li><li><a href="#65-emodb-german">6.5. EmoDB (German)</a></li></ul></li><li><a href="#7-evaluation-metrics">7. Evaluation Metrics</a></li><li><a href="#8-system-design-call-center-emotion-analytics">8. System Design: Call Center Emotion Analytics</a></li><li><a href="#9-multimodal-emotion-recognition">9. Multimodal Emotion Recognition</a><ul><li><a href="#91-early-fusion">9.1. Early Fusion</a></li><li><a href="#92-late-fusion">9.2. Late Fusion</a></li><li><a href="#93-cross-modal-attention">9.3. Cross-Modal Attention</a></li></ul></li><li><a href="#10-real-time-considerations">10. Real-Time Considerations</a></li><li><a href="#11-interview-questions">11. Interview Questions</a></li><li><a href="#12-common-mistakes">12. Common Mistakes</a></li><li><a href="#13-future-trends">13. Future Trends</a></li><li><a href="#14-conclusion">14. Conclusion</a></li><li><a href="#15-training-pipeline">15. Training Pipeline</a><ul><li><a href="#151-data-preprocessing">15.1. Data Preprocessing</a></li><li><a href="#152-data-loading">15.2. Data Loading</a></li><li><a href="#153-training-loop">15.3. Training Loop</a></li></ul></li><li><a href="#16-data-augmentation">16. Data Augmentation</a></li><li><a href="#17-handling-class-imbalance">17. Handling Class Imbalance</a></li><li><a href="#18-dimensional-emotion-recognition">18. Dimensional Emotion Recognition</a></li><li><a href="#19-production-deployment">19. Production Deployment</a><ul><li><a href="#191-model-export">19.1. Model Export</a></li><li><a href="#192-inference-service">19.2. Inference Service</a></li><li><a href="#193-streaming-processing">19.3. Streaming Processing</a></li></ul></li><li><a href="#20-mastery-checklist">20. Mastery Checklist</a></li><li><a href="#21-conclusion">21. Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Teaching machines to hear feelings.”</strong></p>

<h2 id="1-introduction">1. Introduction</h2>

<p><strong>Speech Emotion Recognition (SER)</strong> is the task of identifying the emotional state of a speaker from their voice.</p>

<p><strong>Emotions Typically Recognized:</strong></p>
<ul>
  <li><strong>Basic:</strong> Happy, Sad, Angry, Fear, Disgust, Surprise, Neutral.</li>
  <li><strong>Dimensional:</strong> Valence (positive/negative), Arousal (activation level), Dominance.</li>
</ul>

<p><strong>Applications:</strong></p>
<ul>
  <li><strong>Customer Service:</strong> Detect frustrated callers, route to specialists.</li>
  <li><strong>Mental Health:</strong> Monitor emotional state over time.</li>
  <li><strong>Human-Robot Interaction:</strong> Empathetic responses.</li>
  <li><strong>Gaming:</strong> Adaptive game difficulty based on player emotion.</li>
  <li><strong>Automotive:</strong> Detect driver stress or drowsiness.</li>
</ul>

<h2 id="2-challenges-in-ser">2. Challenges in SER</h2>

<p><strong>1. Subjectivity:</strong></p>
<ul>
  <li>Same utterance can be perceived differently.</li>
  <li>Cultural differences in emotional expression.</li>
</ul>

<p><strong>2. Speaker Variability:</strong></p>
<ul>
  <li>Emotional expression varies by person.</li>
  <li>Age, gender, and language effects.</li>
</ul>

<p><strong>3. Context Dependency:</strong></p>
<ul>
  <li>“Really?” can be surprised, sarcastic, or angry.</li>
  <li>Need context to disambiguate.</li>
</ul>

<p><strong>4. Data Scarcity:</strong></p>
<ul>
  <li>Labeled emotional speech is expensive to collect.</li>
  <li>Acted vs spontaneous speech differs.</li>
</ul>

<p><strong>5. Class Imbalance:</strong></p>
<ul>
  <li>Neutral is often dominant.</li>
  <li>Extreme emotions (rage, despair) are rare.</li>
</ul>

<h2 id="3-acoustic-features-for-ser">3. Acoustic Features for SER</h2>

<h3 id="31-prosodic-features">3.1. Prosodic Features</h3>

<p><strong>Pitch (F0):</strong></p>
<ul>
  <li>Higher pitch → excitement, anger.</li>
  <li>Lower pitch → sadness, boredom.</li>
</ul>

<p><strong>Energy:</strong></p>
<ul>
  <li>Higher energy → anger, happiness.</li>
  <li>Lower energy → sadness.</li>
</ul>

<p><strong>Speaking Rate:</strong></p>
<ul>
  <li>Faster → excitement, nervousness.</li>
  <li>Slower → sadness, hesitation.</li>
</ul>

<h3 id="32-spectral-features">3.2. Spectral Features</h3>

<p><strong>MFCCs:</strong></p>
<ul>
  <li>Standard speech features.</li>
  <li>13-40 coefficients + deltas.</li>
</ul>

<p><strong>Mel Spectrogram:</strong></p>
<ul>
  <li>Raw input for CNNs.</li>
  <li>Captures timbral qualities.</li>
</ul>

<p><strong>Formants:</strong></p>
<ul>
  <li>Vowel quality changes with emotion.</li>
</ul>

<h3 id="33-voice-quality-features">3.3. Voice Quality Features</h3>

<p><strong>Jitter and Shimmer:</strong></p>
<ul>
  <li>Irregularities in pitch and amplitude.</li>
  <li>Higher in stressed/emotional speech.</li>
</ul>

<p><strong>Harmonic-to-Noise Ratio (HNR):</strong></p>
<ul>
  <li>Clarity of voice.</li>
  <li>Lower in breathy or tense speech.</li>
</ul>

<h2 id="4-traditional-ml-approaches">4. Traditional ML Approaches</h2>

<h3 id="41-feature-extraction--classifier">4.1. Feature Extraction + Classifier</h3>

<p><strong>Pipeline:</strong></p>
<ol>
  <li>Extract hand-crafted features (openSMILE).</li>
  <li>Train SVM, Random Forest, or GMM.</li>
</ol>

<p>``python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler</p>

<h1 id="extract-features-using-opensmile-or-librosa">Extract features (using openSMILE or librosa)</h1>
<p>X_train = extract_features(train_audio)
X_test = extract_features(test_audio)</p>

<h1 id="scale-features">Scale features</h1>
<p>scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)</p>

<h1 id="train-svm">Train SVM</h1>
<p>clf = SVC(kernel=’rbf’, C=1.0)
clf.fit(X_train, y_train)</p>

<h1 id="predict">Predict</h1>
<p>predictions = clf.predict(X_test)
``</p>

<h3 id="42-opensmile-features">4.2. openSMILE Features</h3>

<p><strong>openSMILE</strong> extracts thousands of features:</p>
<ul>
  <li>eGeMAPS: 88 features (standardized for emotion).</li>
  <li>ComParE: 6373 features (comprehensive).</li>
</ul>

<p>``python
import opensmile</p>

<p>smile = opensmile.Smile(
 feature_set=opensmile.FeatureSet.eGeMAPSv02,
 feature_level=opensmile.FeatureLevel.Functionals
)</p>

<p>features = smile.process_file(‘audio.wav’)
``</p>

<h2 id="5-deep-learning-approaches">5. Deep Learning Approaches</h2>

<h3 id="51-cnn-on-spectrograms">5.1. CNN on Spectrograms</h3>

<p><strong>Architecture:</strong></p>
<ol>
  <li>Convert audio to mel spectrogram.</li>
  <li>Treat as image, apply 2D CNN.</li>
  <li>Global pooling + dense layers.</li>
</ol>

<p>``python
class EmotionCNN(nn.Module):
 def <strong>init</strong>(self, num_classes=7):
 super().<strong>init</strong>()
 self.conv = nn.Sequential(
 nn.Conv2d(1, 32, kernel_size=3, padding=1),
 nn.ReLU(),
 nn.MaxPool2d(2),
 nn.Conv2d(32, 64, kernel_size=3, padding=1),
 nn.ReLU(),
 nn.MaxPool2d(2),
 nn.Conv2d(64, 128, kernel_size=3, padding=1),
 nn.ReLU(),
 nn.AdaptiveAvgPool2d(1)
 )
 self.fc = nn.Linear(128, num_classes)</p>

<p>def forward(self, x):
 x = self.conv(x)
 x = x.view(x.size(0), -1)
 x = self.fc(x)
 return x
``</p>

<h3 id="52-lstmgru-on-sequences">5.2. LSTM/GRU on Sequences</h3>

<p><strong>Architecture:</strong></p>
<ol>
  <li>Extract frame-level features (MFCCs).</li>
  <li>Feed to bidirectional LSTM.</li>
  <li>Attention or pooling over time.</li>
</ol>

<p>``python
class EmotionLSTM(nn.Module):
 def <strong>init</strong>(self, input_dim=40, hidden_dim=128, num_classes=7):
 super().<strong>init</strong>()
 self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=True, batch_first=True)
 self.attention = nn.Linear(hidden_dim * 2, 1)
 self.fc = nn.Linear(hidden_dim * 2, num_classes)</p>

<p>def forward(self, x):
 # x: (batch, time, features)
 lstm_out, _ = self.lstm(x)</p>

<p># Attention
 attn_weights = F.softmax(self.attention(lstm_out), dim=1)
 context = torch.sum(attn_weights * lstm_out, dim=1)</p>

<p>return self.fc(context)
``</p>

<h3 id="53-transformer-based-models">5.3. Transformer-Based Models</h3>

<p><strong>Using Pretrained Models:</strong></p>
<ul>
  <li><strong>Wav2Vec 2.0:</strong> Self-supervised audio representations.</li>
  <li><strong>HuBERT:</strong> Hidden unit BERT for speech.</li>
  <li><strong>WavLM:</strong> Microsoft’s large speech model.</li>
</ul>

<p>``python
from transformers import Wav2Vec2Model, Wav2Vec2Processor</p>

<p>class EmotionWav2Vec(nn.Module):
 def <strong>init</strong>(self, num_classes=7):
 super().<strong>init</strong>()
 self.wav2vec = Wav2Vec2Model.from_pretrained(“facebook/wav2vec2-base”)
 self.classifier = nn.Linear(768, num_classes)</p>

<p>def forward(self, input_values):
 outputs = self.wav2vec(input_values)
 hidden = outputs.last_hidden_state.mean(dim=1)
 return self.classifier(hidden)
``</p>

<h2 id="6-datasets">6. Datasets</h2>

<h3 id="61-iemocap">6.1. IEMOCAP</h3>

<ul>
  <li>12 hours of audiovisual data.</li>
  <li>5 sessions, 10 actors.</li>
  <li>Emotions: Angry, Happy, Sad, Neutral, Excited, Frustrated.</li>
  <li>Gold standard for SER research.</li>
</ul>

<h3 id="62-ravdess">6.2. RAVDESS</h3>

<ul>
  <li>24 actors (12 male, 12 female).</li>
  <li>7 emotions + calm.</li>
  <li>Acted speech and song.</li>
</ul>

<h3 id="63-crema-d">6.3. CREMA-D</h3>

<ul>
  <li>7,442 clips from 91 actors.</li>
  <li>6 emotions.</li>
  <li>Diverse ethnic backgrounds.</li>
</ul>

<h3 id="64-cmu-mosei">6.4. CMU-MOSEI</h3>

<ul>
  <li>23,453 video clips.</li>
  <li>Multimodal: text, audio, video.</li>
  <li>Sentiment and emotion labels.</li>
</ul>

<h3 id="65-emodb-german">6.5. EmoDB (German)</h3>

<ul>
  <li>535 utterances.</li>
  <li>10 actors, 7 emotions.</li>
  <li>Classic dataset for SER.</li>
</ul>

<h2 id="7-evaluation-metrics">7. Evaluation Metrics</h2>

<p><strong>Classification Metrics:</strong></p>
<ul>
  <li><strong>Accuracy:</strong> Overall correct predictions.</li>
  <li><strong>Weighted F1:</strong> Accounts for class imbalance.</li>
  <li><strong>Unweighted Accuracy (UA):</strong> Average recall across classes.</li>
  <li><strong>Confusion Matrix:</strong> Understand per-class performance.</li>
</ul>

<p><strong>For Dimensional Emotions:</strong></p>
<ul>
  <li><strong>CCC (Concordance Correlation Coefficient):</strong> Agreement measure.</li>
  <li><strong>MSE/MAE:</strong> For valence/arousal prediction.</li>
</ul>

<h2 id="8-system-design-call-center-emotion-analytics">8. System Design: Call Center Emotion Analytics</h2>

<p><strong>Scenario:</strong> Detect customer emotions during support calls.</p>

<p><strong>Requirements:</strong></p>
<ul>
  <li>Real-time analysis.</li>
  <li>Handle noisy telephony audio.</li>
  <li>Alert supervisors on negative emotions.</li>
</ul>

<p><strong>Architecture:</strong>
<code class="language-plaintext highlighter-rouge">
┌─────────────────┐
│ Phone Call │
│ (Audio Stream)│
└────────┬────────┘
 │
┌────────▼────────┐
│ Voice Activity │
│ Detection │
└────────┬────────┘
 │
┌────────▼────────┐
│ Speaker │
│ Diarization │
└────────┬────────┘
 │
┌────────▼────────┐
│ Emotion │
│ Recognition │
└────────┬────────┘
 │
 ├──────────────┐
 │ │
┌────────▼────────┐ ┌▼────────────────┐
│ Dashboard │ │ Alert System │
│ (Real-time) │ │ (Supervisor) │
└─────────────────┘ └─────────────────┘
</code></p>

<p><strong>Implementation Details:</strong></p>
<ul>
  <li>Process in 3-second windows.</li>
  <li>Apply noise reduction first.</li>
  <li>Track emotion trajectory over call.</li>
  <li>Trigger alert if anger/frustration persists.</li>
</ul>

<h2 id="9-multimodal-emotion-recognition">9. Multimodal Emotion Recognition</h2>

<p><strong>Combine modalities for better accuracy:</strong></p>
<ul>
  <li><strong>Audio:</strong> Voice, prosody.</li>
  <li><strong>Text:</strong> Transcribed words, sentiment.</li>
  <li><strong>Video:</strong> Facial expressions, body language.</li>
</ul>

<h3 id="91-early-fusion">9.1. Early Fusion</h3>

<p><strong>Concatenate features before classification:</strong>
<code class="language-plaintext highlighter-rouge">python
audio_features = audio_encoder(audio)
text_features = text_encoder(text)
combined = torch.cat([audio_features, text_features], dim=1)
output = classifier(combined)
</code></p>

<h3 id="92-late-fusion">9.2. Late Fusion</h3>

<p><strong>Combine predictions from each modality:</strong>
<code class="language-plaintext highlighter-rouge">python
audio_pred = audio_model(audio)
text_pred = text_model(text)
combined_pred = (audio_pred + text_pred) / 2
</code></p>

<h3 id="93-cross-modal-attention">9.3. Cross-Modal Attention</h3>

<p><strong>Let modalities attend to each other:</strong>
``python
class CrossModalAttention(nn.Module):
 def <strong>init</strong>(self, dim):
 super().<strong>init</strong>()
 self.query = nn.Linear(dim, dim)
 self.key = nn.Linear(dim, dim)
 self.value = nn.Linear(dim, dim)</p>

<p>def forward(self, x1, x2):
 # x1 attends to x2
 q = self.query(x1)
 k = self.key(x2)
 v = self.value(x2)</p>

<p>attn = F.softmax(torch.bmm(q, k.transpose(1, 2)) / math.sqrt(q.size(-1)), dim=-1)
 return torch.bmm(attn, v)
``</p>

<h2 id="10-real-time-considerations">10. Real-Time Considerations</h2>

<p><strong>Latency Requirements:</strong></p>
<ul>
  <li>Call center: &lt;500ms per segment.</li>
  <li>Gaming: &lt;100ms for responsiveness.</li>
</ul>

<p><strong>Optimization Strategies:</strong></p>
<ol>
  <li><strong>Streaming:</strong> Process overlapping windows.</li>
  <li><strong>Model Pruning:</strong> Reduce model size.</li>
  <li><strong>Quantization:</strong> INT8 inference.</li>
  <li><strong>GPU Batching:</strong> Process multiple calls together.</li>
</ol>

<h2 id="11-interview-questions">11. Interview Questions</h2>

<ol>
  <li><strong>Features for SER:</strong> What acoustic features capture emotion?</li>
  <li><strong>IEMOCAP:</strong> Describe the dataset and common practices.</li>
  <li><strong>Class Imbalance:</strong> How do you handle it in SER?</li>
  <li><strong>Multimodal Fusion:</strong> Early vs late vs attention fusion?</li>
  <li><strong>Real-Time Design:</strong> Design an emotion detector for virtual meetings.</li>
</ol>

<h2 id="12-common-mistakes">12. Common Mistakes</h2>

<ul>
  <li><strong>Ignoring Speaker Effects:</strong> Train with speaker-independent splits.</li>
  <li><strong>Leaking Speakers:</strong> Same speaker in train and test.</li>
  <li><strong>Wrong Metrics:</strong> Use weighted/unweighted accuracy for imbalanced data.</li>
  <li><strong>Acted vs Spontaneous:</strong> Models trained on acted data fail on real speech.</li>
  <li><strong>Ignoring Context:</strong> Sentence-level emotion misses conversational dynamics.</li>
</ul>

<h2 id="13-future-trends">13. Future Trends</h2>

<p><strong>1. Self-Supervised Pretraining:</strong></p>
<ul>
  <li>Wav2Vec, HuBERT for emotion.</li>
  <li>Less labeled data needed.</li>
</ul>

<p><strong>2. Personalized Emotion Recognition:</strong></p>
<ul>
  <li>Adapt to individual expression patterns.</li>
  <li>Few-shot learning.</li>
</ul>

<p><strong>3. Continuous Emotion Tracking:</strong></p>
<ul>
  <li>Not discrete labels, but continuous trajectories.</li>
  <li>Valence-arousal-dominance space.</li>
</ul>

<p><strong>4. Explainable SER:</strong></p>
<ul>
  <li>Which parts of audio indicate emotion.</li>
  <li>Attention visualization.</li>
</ul>

<h2 id="14-conclusion">14. Conclusion</h2>

<p>Speech Emotion Recognition is a challenging but impactful task. It requires understanding of both speech processing and machine learning.</p>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Features:</strong> Prosody, spectral, voice quality.</li>
  <li><strong>Models:</strong> CNN on spectrograms, LSTM on sequences, Transformers.</li>
  <li><strong>Data:</strong> IEMOCAP is the gold standard.</li>
  <li><strong>Evaluation:</strong> Weighted F1 for imbalanced classes.</li>
  <li><strong>Multimodal:</strong> Combining audio + text improves accuracy.</li>
</ul>

<p>As AI becomes more empathetic, SER will be central to human-computer interaction. Master it to build systems that truly understand their users.</p>

<h2 id="15-training-pipeline">15. Training Pipeline</h2>

<h3 id="151-data-preprocessing">15.1. Data Preprocessing</h3>

<p>``python
import librosa
import numpy as np</p>

<p>def preprocess_audio(audio_path, target_sr=16000, max_duration=10):
 # Load audio
 audio, sr = librosa.load(audio_path, sr=target_sr)</p>

<p># Trim silence
 audio, _ = librosa.effects.trim(audio, top_db=20)</p>

<p># Pad or truncate
 max_samples = target_sr * max_duration
 if len(audio) &gt; max_samples:
 audio = audio[:max_samples]
 else:
 audio = np.pad(audio, (0, max_samples - len(audio)))</p>

<p># Compute mel spectrogram
 mel = librosa.feature.melspectrogram(
 y=audio, sr=target_sr, n_mels=80, hop_length=160
 )
 log_mel = np.log(mel + 1e-8)</p>

<p>return log_mel
``</p>

<h3 id="152-data-loading">15.2. Data Loading</h3>

<p>``python
from torch.utils.data import Dataset, DataLoader</p>

<p>class EmotionDataset(Dataset):
 def <strong>init</strong>(self, audio_paths, labels):
 self.audio_paths = audio_paths
 self.labels = labels</p>

<p>def <strong>len</strong>(self):
 return len(self.audio_paths)</p>

<p>def <strong>getitem</strong>(self, idx):
 mel = preprocess_audio(self.audio_paths[idx])
 label = self.labels[idx]
 return torch.tensor(mel).unsqueeze(0), label</p>

<h1 id="create-dataloaders">Create dataloaders</h1>
<p>train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
``</p>

<h3 id="153-training-loop">15.3. Training Loop</h3>

<p>``python
model = EmotionCNN(num_classes=7)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)</p>

<p>for epoch in range(100):
 model.train()
 for mel, labels in train_loader:
 outputs = model(mel)
 loss = criterion(outputs, labels)</p>

<p>optimizer.zero_grad()
 loss.backward()
 optimizer.step()</p>

<p># Validation
 model.eval()
 correct = 0
 total = 0
 with torch.no_grad():
 for mel, labels in val_loader:
 outputs = model(mel)
 _, predicted = torch.max(outputs, 1)
 total += labels.size(0)
 correct += (predicted == labels).sum().item()</p>

<p>print(f”Epoch {epoch}, Val Accuracy: {100 * correct / total:.2f}%”)
``</p>

<h2 id="16-data-augmentation">16. Data Augmentation</h2>

<p><strong>Audio Augmentations:</strong>
``python
import audiomentations as A</p>

<p>augment = A.Compose([
 A.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),
 A.TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),
 A.PitchShift(min_semitones=-4, max_semitones=4, p=0.5),
 A.Shift(min_fraction=-0.5, max_fraction=0.5, p=0.5),
])</p>

<p>def augment_audio(audio, sr):
 return augment(samples=audio, sample_rate=sr)
``</p>

<p><strong>SpecAugment:</strong>
``python
def spec_augment(mel, freq_mask=10, time_mask=20):
 # Frequency masking
 f0 = np.random.randint(0, mel.shape[0] - freq_mask)
 mel[f0:f0+freq_mask, :] = 0</p>

<p># Time masking
 t0 = np.random.randint(0, mel.shape[1] - time_mask)
 mel[:, t0:t0+time_mask] = 0</p>

<p>return mel
``</p>

<h2 id="17-handling-class-imbalance">17. Handling Class Imbalance</h2>

<p><strong>Strategies:</strong></p>
<ol>
  <li>
    <p><strong>Weighted Loss:</strong>
<code class="language-plaintext highlighter-rouge">python
class_weights = compute_class_weights(labels)
criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights))
</code></p>
  </li>
  <li>
    <p><strong>Oversampling:</strong>
<code class="language-plaintext highlighter-rouge">python
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler()
X_resampled, y_resampled = ros.fit_resample(X, y)
</code></p>
  </li>
  <li>
    <p><strong>Focal Loss:</strong>
``python
class FocalLoss(nn.Module):
 def <strong>init</strong>(self, gamma=2):
 super().<strong>init</strong>()
 self.gamma = gamma</p>
  </li>
</ol>

<p>def forward(self, inputs, targets):
 ce_loss = F.cross_entropy(inputs, targets, reduction=’none’)
 pt = torch.exp(-ce_loss)
 focal_loss = (1 - pt) ** self.gamma * ce_loss
 return focal_loss.mean()
``</p>

<h2 id="18-dimensional-emotion-recognition">18. Dimensional Emotion Recognition</h2>

<p><strong>Valence-Arousal-Dominance (VAD) Model:</strong></p>
<ul>
  <li><strong>Valence:</strong> Positive (happy) to Negative (sad).</li>
  <li><strong>Arousal:</strong> Active (excited) to Passive (calm).</li>
  <li><strong>Dominance:</strong> Dominant to Submissive.</li>
</ul>

<p><strong>Regression Instead of Classification:</strong>
``python
class EmotionVADRegressor(nn.Module):
 def <strong>init</strong>(self):
 super().<strong>init</strong>()
 self.encoder = Wav2Vec2Model.from_pretrained(“facebook/wav2vec2-base”)
 self.regressor = nn.Linear(768, 3) # Predict V, A, D</p>

<p>def forward(self, x):
 features = self.encoder(x).last_hidden_state.mean(dim=1)
 return self.regressor(features)</p>

<h1 id="training-with-mse-loss">Training with MSE loss</h1>
<p>criterion = nn.MSELoss()
output = model(audio)
loss = criterion(output, torch.tensor([valence, arousal, dominance]))
``</p>

<p><strong>Evaluation Metric (CCC):</strong>
``python
def concordance_correlation_coefficient(pred, target):
 mean_pred = pred.mean()
 mean_target = target.mean()
 var_pred = pred.var()
 var_target = target.var()
 covar = ((pred - mean_pred) * (target - mean_target)).mean()</p>

<p>ccc = 2 * covar / (var_pred + var_target + (mean_pred - mean_target)**2)
 return ccc
``</p>

<h2 id="19-production-deployment">19. Production Deployment</h2>

<h3 id="191-model-export">19.1. Model Export</h3>

<p>``python</p>
<h1 id="export-to-onnx">Export to ONNX</h1>
<p>dummy_input = torch.randn(1, 1, 80, 400)
torch.onnx.export(model, dummy_input, “emotion_model.onnx”)</p>

<h1 id="or-torchscript">Or TorchScript</h1>
<p>scripted = torch.jit.script(model)
scripted.save(“emotion_model.pt”)
``</p>

<h3 id="192-inference-service">19.2. Inference Service</h3>

<p>``python
from fastapi import FastAPI, UploadFile
import soundfile as sf</p>

<p>app = FastAPI()</p>

<p>@app.post(“/predict”)
async def predict_emotion(file: UploadFile):
 # Read audio
 audio, sr = sf.read(file.file)</p>

<p># Preprocess
 mel = preprocess_audio_from_array(audio, sr)</p>

<p># Predict
 with torch.no_grad():
 output = model(torch.tensor(mel).unsqueeze(0))
 emotion_idx = output.argmax().item()</p>

<p>emotions = [“angry”, “happy”, “sad”, “neutral”, “fear”, “disgust”, “surprise”]
 return {“emotion”: emotions[emotion_idx]}
``</p>

<h3 id="193-streaming-processing">19.3. Streaming Processing</h3>

<p>``python
class StreamingEmotionDetector:
 def <strong>init</strong>(self, model, window_size=3.0, hop_size=1.0, sr=16000):
 self.model = model
 self.window_samples = int(window_size * sr)
 self.hop_samples = int(hop_size * sr)
 self.buffer = []</p>

<p>def process_chunk(self, audio_chunk):
 self.buffer.extend(audio_chunk)</p>

<p>results = []
 while len(self.buffer) &gt;= self.window_samples:
 window = self.buffer[:self.window_samples]
 emotion = self.predict(window)
 results.append(emotion)
 self.buffer = self.buffer[self.hop_samples:]</p>

<p>return results</p>

<p>def predict(self, audio):
 mel = compute_mel(audio)
 with torch.no_grad():
 output = self.model(torch.tensor(mel).unsqueeze(0))
 return output.argmax().item()
``</p>

<h2 id="20-mastery-checklist">20. Mastery Checklist</h2>

<p><strong>Mastery Checklist:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Extract prosodic features (F0, energy)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Extract spectral features (MFCC, mel spectrogram)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Train CNN on spectrograms</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Train LSTM with attention</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Fine-tune Wav2Vec2 for emotion</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Handle class imbalance (weighted loss, oversampling)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement multimodal fusion</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Evaluate with weighted F1 and UA</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Deploy real-time emotion detector</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Understand dimensional emotion models</li>
</ul>

<h2 id="21-conclusion">21. Conclusion</h2>

<p>Speech Emotion Recognition bridges the gap between AI and human emotional intelligence. It’s a challenging task that requires:</p>
<ul>
  <li><strong>Domain Knowledge:</strong> Understanding how emotions manifest in speech.</li>
  <li><strong>ML Expertise:</strong> Selecting and training appropriate models.</li>
  <li><strong>Data Engineering:</strong> Handling imbalanced, subjective labels.</li>
  <li><strong>System Design:</strong> Building real-time, production-ready systems.</li>
</ul>

<p><strong>The Path Forward:</strong></p>
<ol>
  <li>Start with IEMOCAP and a CNN baseline.</li>
  <li>Upgrade to Wav2Vec2 for better features.</li>
  <li>Add multimodal (text) for improved accuracy.</li>
  <li>Deploy with streaming for real-time applications.</li>
</ol>

<p>As AI assistants become more prevalent, emotional intelligence will be a key differentiator. Systems that understand and respond to human emotions will create more natural, empathetic interactions. Master SER to be at the forefront of this revolution.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/">arunbaby.com/speech-tech/0045-speech-emotion-recognition</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#classification" class="page__taxonomy-item p-category" rel="tag">classification</a><span class="sep">, </span>
    
      <a href="/tags/#deep-learning" class="page__taxonomy-item p-category" rel="tag">deep-learning</a><span class="sep">, </span>
    
      <a href="/tags/#emotion" class="page__taxonomy-item p-category" rel="tag">emotion</a><span class="sep">, </span>
    
      <a href="/tags/#multimodal" class="page__taxonomy-item p-category" rel="tag">multimodal</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0045-sliding-window-maximum/" rel="permalink">Sliding Window Maximum
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Finding the king of every window.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0045-rag-systems/" rel="permalink">RAG Systems
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Grounding LLMs in facts, not hallucinations.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0045-data-leakage-prevention/" rel="permalink">Data Leakage Prevention
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Prevent leaks by design: minimize data access, redact outputs and logs, and enforce least privilege for tools and memory.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Speech+Emotion+Recognition%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0045-speech-emotion-recognition%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0045-speech-emotion-recognition%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0044-voice-conversion/" class="pagination--pager" title="Voice Conversion">Previous</a>
    
    
      <a href="/speech-tech/0046-cross-lingual-speech-transfer/" class="pagination--pager" title="Cross-Lingual Speech Transfer">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
