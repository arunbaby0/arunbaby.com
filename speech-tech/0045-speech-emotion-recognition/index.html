<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Speech Emotion Recognition - Arun Baby</title>
<meta name="description" content="“Teaching machines to hear feelings.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Speech Emotion Recognition">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/">


  <meta property="og:description" content="“Teaching machines to hear feelings.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Speech Emotion Recognition">
  <meta name="twitter:description" content="“Teaching machines to hear feelings.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-21T18:25:51+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Speech Emotion Recognition">
    <meta itemprop="description" content="“Teaching machines to hear feelings.”">
    <meta itemprop="datePublished" content="2025-12-21T18:25:51+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/" itemprop="url">Speech Emotion Recognition
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          11 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction">1. Introduction</a></li><li><a href="#2-challenges-in-ser">2. Challenges in SER</a></li><li><a href="#3-acoustic-features-for-ser">3. Acoustic Features for SER</a><ul><li><a href="#31-prosodic-features">3.1. Prosodic Features</a></li><li><a href="#32-spectral-features">3.2. Spectral Features</a></li><li><a href="#33-voice-quality-features">3.3. Voice Quality Features</a></li></ul></li><li><a href="#4-traditional-ml-approaches">4. Traditional ML Approaches</a><ul><li><a href="#41-feature-extraction--classifier">4.1. Feature Extraction + Classifier</a></li><li><a href="#42-opensmile-features">4.2. openSMILE Features</a></li></ul></li><li><a href="#5-deep-learning-approaches">5. Deep Learning Approaches</a><ul><li><a href="#51-cnn-on-spectrograms">5.1. CNN on Spectrograms</a></li><li><a href="#52-lstmgru-on-sequences">5.2. LSTM/GRU on Sequences</a></li><li><a href="#53-transformer-based-models">5.3. Transformer-Based Models</a></li></ul></li><li><a href="#6-datasets">6. Datasets</a><ul><li><a href="#61-iemocap">6.1. IEMOCAP</a></li><li><a href="#62-ravdess">6.2. RAVDESS</a></li><li><a href="#63-crema-d">6.3. CREMA-D</a></li><li><a href="#64-cmu-mosei">6.4. CMU-MOSEI</a></li><li><a href="#65-emodb-german">6.5. EmoDB (German)</a></li></ul></li><li><a href="#7-evaluation-metrics">7. Evaluation Metrics</a></li><li><a href="#8-system-design-call-center-emotion-analytics">8. System Design: Call Center Emotion Analytics</a></li><li><a href="#9-multimodal-emotion-recognition">9. Multimodal Emotion Recognition</a><ul><li><a href="#91-early-fusion">9.1. Early Fusion</a></li><li><a href="#92-late-fusion">9.2. Late Fusion</a></li><li><a href="#93-cross-modal-attention">9.3. Cross-Modal Attention</a></li></ul></li><li><a href="#10-real-time-considerations">10. Real-Time Considerations</a></li><li><a href="#11-interview-questions">11. Interview Questions</a></li><li><a href="#12-common-mistakes">12. Common Mistakes</a></li><li><a href="#13-future-trends">13. Future Trends</a></li><li><a href="#14-conclusion">14. Conclusion</a></li><li><a href="#15-training-pipeline">15. Training Pipeline</a><ul><li><a href="#151-data-preprocessing">15.1. Data Preprocessing</a></li><li><a href="#152-data-loading">15.2. Data Loading</a></li><li><a href="#153-training-loop">15.3. Training Loop</a></li></ul></li><li><a href="#16-data-augmentation">16. Data Augmentation</a></li><li><a href="#17-handling-class-imbalance">17. Handling Class Imbalance</a></li><li><a href="#18-dimensional-emotion-recognition">18. Dimensional Emotion Recognition</a></li><li><a href="#19-production-deployment">19. Production Deployment</a><ul><li><a href="#191-model-export">19.1. Model Export</a></li><li><a href="#192-inference-service">19.2. Inference Service</a></li><li><a href="#193-streaming-processing">19.3. Streaming Processing</a></li></ul></li><li><a href="#20-mastery-checklist">20. Mastery Checklist</a></li><li><a href="#21-conclusion">21. Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Teaching machines to hear feelings.”</strong></p>

<h2 id="1-introduction">1. Introduction</h2>

<p><strong>Speech Emotion Recognition (SER)</strong> is the task of identifying the emotional state of a speaker from their voice.</p>

<p><strong>Emotions Typically Recognized:</strong></p>
<ul>
  <li><strong>Basic:</strong> Happy, Sad, Angry, Fear, Disgust, Surprise, Neutral.</li>
  <li><strong>Dimensional:</strong> Valence (positive/negative), Arousal (activation level), Dominance.</li>
</ul>

<p><strong>Applications:</strong></p>
<ul>
  <li><strong>Customer Service:</strong> Detect frustrated callers, route to specialists.</li>
  <li><strong>Mental Health:</strong> Monitor emotional state over time.</li>
  <li><strong>Human-Robot Interaction:</strong> Empathetic responses.</li>
  <li><strong>Gaming:</strong> Adaptive game difficulty based on player emotion.</li>
  <li><strong>Automotive:</strong> Detect driver stress or drowsiness.</li>
</ul>

<h2 id="2-challenges-in-ser">2. Challenges in SER</h2>

<p><strong>1. Subjectivity:</strong></p>
<ul>
  <li>Same utterance can be perceived differently.</li>
  <li>Cultural differences in emotional expression.</li>
</ul>

<p><strong>2. Speaker Variability:</strong></p>
<ul>
  <li>Emotional expression varies by person.</li>
  <li>Age, gender, and language effects.</li>
</ul>

<p><strong>3. Context Dependency:</strong></p>
<ul>
  <li>“Really?” can be surprised, sarcastic, or angry.</li>
  <li>Need context to disambiguate.</li>
</ul>

<p><strong>4. Data Scarcity:</strong></p>
<ul>
  <li>Labeled emotional speech is expensive to collect.</li>
  <li>Acted vs spontaneous speech differs.</li>
</ul>

<p><strong>5. Class Imbalance:</strong></p>
<ul>
  <li>Neutral is often dominant.</li>
  <li>Extreme emotions (rage, despair) are rare.</li>
</ul>

<h2 id="3-acoustic-features-for-ser">3. Acoustic Features for SER</h2>

<h3 id="31-prosodic-features">3.1. Prosodic Features</h3>

<p><strong>Pitch (F0):</strong></p>
<ul>
  <li>Higher pitch → excitement, anger.</li>
  <li>Lower pitch → sadness, boredom.</li>
</ul>

<p><strong>Energy:</strong></p>
<ul>
  <li>Higher energy → anger, happiness.</li>
  <li>Lower energy → sadness.</li>
</ul>

<p><strong>Speaking Rate:</strong></p>
<ul>
  <li>Faster → excitement, nervousness.</li>
  <li>Slower → sadness, hesitation.</li>
</ul>

<h3 id="32-spectral-features">3.2. Spectral Features</h3>

<p><strong>MFCCs:</strong></p>
<ul>
  <li>Standard speech features.</li>
  <li>13-40 coefficients + deltas.</li>
</ul>

<p><strong>Mel Spectrogram:</strong></p>
<ul>
  <li>Raw input for CNNs.</li>
  <li>Captures timbral qualities.</li>
</ul>

<p><strong>Formants:</strong></p>
<ul>
  <li>Vowel quality changes with emotion.</li>
</ul>

<h3 id="33-voice-quality-features">3.3. Voice Quality Features</h3>

<p><strong>Jitter and Shimmer:</strong></p>
<ul>
  <li>Irregularities in pitch and amplitude.</li>
  <li>Higher in stressed/emotional speech.</li>
</ul>

<p><strong>Harmonic-to-Noise Ratio (HNR):</strong></p>
<ul>
  <li>Clarity of voice.</li>
  <li>Lower in breathy or tense speech.</li>
</ul>

<h2 id="4-traditional-ml-approaches">4. Traditional ML Approaches</h2>

<h3 id="41-feature-extraction--classifier">4.1. Feature Extraction + Classifier</h3>

<p><strong>Pipeline:</strong></p>
<ol>
  <li>Extract hand-crafted features (openSMILE).</li>
  <li>Train SVM, Random Forest, or GMM.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Extract features (using openSMILE or librosa)
</span><span class="n">X_train</span> <span class="o">=</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">train_audio</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">test_audio</span><span class="p">)</span>

<span class="c1"># Scale features
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Train SVM
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict
</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="42-opensmile-features">4.2. openSMILE Features</h3>

<p><strong>openSMILE</strong> extracts thousands of features:</p>
<ul>
  <li>eGeMAPS: 88 features (standardized for emotion).</li>
  <li>ComParE: 6373 features (comprehensive).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">opensmile</span>

<span class="n">smile</span> <span class="o">=</span> <span class="n">opensmile</span><span class="p">.</span><span class="nc">Smile</span><span class="p">(</span>
    <span class="n">feature_set</span><span class="o">=</span><span class="n">opensmile</span><span class="p">.</span><span class="n">FeatureSet</span><span class="p">.</span><span class="n">eGeMAPSv02</span><span class="p">,</span>
    <span class="n">feature_level</span><span class="o">=</span><span class="n">opensmile</span><span class="p">.</span><span class="n">FeatureLevel</span><span class="p">.</span><span class="n">Functionals</span>
<span class="p">)</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">smile</span><span class="p">.</span><span class="nf">process_file</span><span class="p">(</span><span class="sh">'</span><span class="s">audio.wav</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="5-deep-learning-approaches">5. Deep Learning Approaches</h2>

<h3 id="51-cnn-on-spectrograms">5.1. CNN on Spectrograms</h3>

<p><strong>Architecture:</strong></p>
<ol>
  <li>Convert audio to mel spectrogram.</li>
  <li>Treat as image, apply 2D CNN.</li>
  <li>Global pooling + dense layers.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EmotionCNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<h3 id="52-lstmgru-on-sequences">5.2. LSTM/GRU on Sequences</h3>

<p><strong>Architecture:</strong></p>
<ol>
  <li>Extract frame-level features (MFCCs).</li>
  <li>Feed to bidirectional LSTM.</li>
  <li>Attention or pooling over time.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EmotionLSTM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (batch, time, features)
</span>        <span class="n">lstm_out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">lstm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Attention
</span>        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">attention</span><span class="p">(</span><span class="n">lstm_out</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">attn_weights</span> <span class="o">*</span> <span class="n">lstm_out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="53-transformer-based-models">5.3. Transformer-Based Models</h3>

<p><strong>Using Pretrained Models:</strong></p>
<ul>
  <li><strong>Wav2Vec 2.0:</strong> Self-supervised audio representations.</li>
  <li><strong>HuBERT:</strong> Hidden unit BERT for speech.</li>
  <li><strong>WavLM:</strong> Microsoft’s large speech model.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">Wav2Vec2Model</span><span class="p">,</span> <span class="n">Wav2Vec2Processor</span>

<span class="k">class</span> <span class="nc">EmotionWav2Vec</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">7</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">wav2vec</span> <span class="o">=</span> <span class="n">Wav2Vec2Model</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">facebook/wav2vec2-base</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_values</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">wav2vec</span><span class="p">(</span><span class="n">input_values</span><span class="p">)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="6-datasets">6. Datasets</h2>

<h3 id="61-iemocap">6.1. IEMOCAP</h3>

<ul>
  <li>12 hours of audiovisual data.</li>
  <li>5 sessions, 10 actors.</li>
  <li>Emotions: Angry, Happy, Sad, Neutral, Excited, Frustrated.</li>
  <li>Gold standard for SER research.</li>
</ul>

<h3 id="62-ravdess">6.2. RAVDESS</h3>

<ul>
  <li>24 actors (12 male, 12 female).</li>
  <li>7 emotions + calm.</li>
  <li>Acted speech and song.</li>
</ul>

<h3 id="63-crema-d">6.3. CREMA-D</h3>

<ul>
  <li>7,442 clips from 91 actors.</li>
  <li>6 emotions.</li>
  <li>Diverse ethnic backgrounds.</li>
</ul>

<h3 id="64-cmu-mosei">6.4. CMU-MOSEI</h3>

<ul>
  <li>23,453 video clips.</li>
  <li>Multimodal: text, audio, video.</li>
  <li>Sentiment and emotion labels.</li>
</ul>

<h3 id="65-emodb-german">6.5. EmoDB (German)</h3>

<ul>
  <li>535 utterances.</li>
  <li>10 actors, 7 emotions.</li>
  <li>Classic dataset for SER.</li>
</ul>

<h2 id="7-evaluation-metrics">7. Evaluation Metrics</h2>

<p><strong>Classification Metrics:</strong></p>
<ul>
  <li><strong>Accuracy:</strong> Overall correct predictions.</li>
  <li><strong>Weighted F1:</strong> Accounts for class imbalance.</li>
  <li><strong>Unweighted Accuracy (UA):</strong> Average recall across classes.</li>
  <li><strong>Confusion Matrix:</strong> Understand per-class performance.</li>
</ul>

<p><strong>For Dimensional Emotions:</strong></p>
<ul>
  <li><strong>CCC (Concordance Correlation Coefficient):</strong> Agreement measure.</li>
  <li><strong>MSE/MAE:</strong> For valence/arousal prediction.</li>
</ul>

<h2 id="8-system-design-call-center-emotion-analytics">8. System Design: Call Center Emotion Analytics</h2>

<p><strong>Scenario:</strong> Detect customer emotions during support calls.</p>

<p><strong>Requirements:</strong></p>
<ul>
  <li>Real-time analysis.</li>
  <li>Handle noisy telephony audio.</li>
  <li>Alert supervisors on negative emotions.</li>
</ul>

<p><strong>Architecture:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────┐
│   Phone Call    │
│   (Audio Stream)│
└────────┬────────┘
         │
┌────────▼────────┐
│  Voice Activity │
│    Detection    │
└────────┬────────┘
         │
┌────────▼────────┐
│  Speaker        │
│  Diarization    │
└────────┬────────┘
         │
┌────────▼────────┐
│  Emotion        │
│  Recognition    │
└────────┬────────┘
         │
         ├──────────────┐
         │              │
┌────────▼────────┐    ┌▼────────────────┐
│   Dashboard     │    │  Alert System   │
│   (Real-time)   │    │  (Supervisor)   │
└─────────────────┘    └─────────────────┘
</code></pre></div></div>

<p><strong>Implementation Details:</strong></p>
<ul>
  <li>Process in 3-second windows.</li>
  <li>Apply noise reduction first.</li>
  <li>Track emotion trajectory over call.</li>
  <li>Trigger alert if anger/frustration persists.</li>
</ul>

<h2 id="9-multimodal-emotion-recognition">9. Multimodal Emotion Recognition</h2>

<p><strong>Combine modalities for better accuracy:</strong></p>
<ul>
  <li><strong>Audio:</strong> Voice, prosody.</li>
  <li><strong>Text:</strong> Transcribed words, sentiment.</li>
  <li><strong>Video:</strong> Facial expressions, body language.</li>
</ul>

<h3 id="91-early-fusion">9.1. Early Fusion</h3>

<p><strong>Concatenate features before classification:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">audio_features</span> <span class="o">=</span> <span class="nf">audio_encoder</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
<span class="n">text_features</span> <span class="o">=</span> <span class="nf">text_encoder</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">audio_features</span><span class="p">,</span> <span class="n">text_features</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">classifier</span><span class="p">(</span><span class="n">combined</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="92-late-fusion">9.2. Late Fusion</h3>

<p><strong>Combine predictions from each modality:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">audio_pred</span> <span class="o">=</span> <span class="nf">audio_model</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
<span class="n">text_pred</span> <span class="o">=</span> <span class="nf">text_model</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">combined_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">audio_pred</span> <span class="o">+</span> <span class="n">text_pred</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
</code></pre></div></div>

<h3 id="93-cross-modal-attention">9.3. Cross-Modal Attention</h3>

<p><strong>Let modalities attend to each other:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CrossModalAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">query</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">key</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="c1"># x1 attends to x2
</span>        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        
        <span class="n">attn</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">bmm</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="10-real-time-considerations">10. Real-Time Considerations</h2>

<p><strong>Latency Requirements:</strong></p>
<ul>
  <li>Call center: &lt;500ms per segment.</li>
  <li>Gaming: &lt;100ms for responsiveness.</li>
</ul>

<p><strong>Optimization Strategies:</strong></p>
<ol>
  <li><strong>Streaming:</strong> Process overlapping windows.</li>
  <li><strong>Model Pruning:</strong> Reduce model size.</li>
  <li><strong>Quantization:</strong> INT8 inference.</li>
  <li><strong>GPU Batching:</strong> Process multiple calls together.</li>
</ol>

<h2 id="11-interview-questions">11. Interview Questions</h2>

<ol>
  <li><strong>Features for SER:</strong> What acoustic features capture emotion?</li>
  <li><strong>IEMOCAP:</strong> Describe the dataset and common practices.</li>
  <li><strong>Class Imbalance:</strong> How do you handle it in SER?</li>
  <li><strong>Multimodal Fusion:</strong> Early vs late vs attention fusion?</li>
  <li><strong>Real-Time Design:</strong> Design an emotion detector for virtual meetings.</li>
</ol>

<h2 id="12-common-mistakes">12. Common Mistakes</h2>

<ul>
  <li><strong>Ignoring Speaker Effects:</strong> Train with speaker-independent splits.</li>
  <li><strong>Leaking Speakers:</strong> Same speaker in train and test.</li>
  <li><strong>Wrong Metrics:</strong> Use weighted/unweighted accuracy for imbalanced data.</li>
  <li><strong>Acted vs Spontaneous:</strong> Models trained on acted data fail on real speech.</li>
  <li><strong>Ignoring Context:</strong> Sentence-level emotion misses conversational dynamics.</li>
</ul>

<h2 id="13-future-trends">13. Future Trends</h2>

<p><strong>1. Self-Supervised Pretraining:</strong></p>
<ul>
  <li>Wav2Vec, HuBERT for emotion.</li>
  <li>Less labeled data needed.</li>
</ul>

<p><strong>2. Personalized Emotion Recognition:</strong></p>
<ul>
  <li>Adapt to individual expression patterns.</li>
  <li>Few-shot learning.</li>
</ul>

<p><strong>3. Continuous Emotion Tracking:</strong></p>
<ul>
  <li>Not discrete labels, but continuous trajectories.</li>
  <li>Valence-arousal-dominance space.</li>
</ul>

<p><strong>4. Explainable SER:</strong></p>
<ul>
  <li>Which parts of audio indicate emotion.</li>
  <li>Attention visualization.</li>
</ul>

<h2 id="14-conclusion">14. Conclusion</h2>

<p>Speech Emotion Recognition is a challenging but impactful task. It requires understanding of both speech processing and machine learning.</p>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Features:</strong> Prosody, spectral, voice quality.</li>
  <li><strong>Models:</strong> CNN on spectrograms, LSTM on sequences, Transformers.</li>
  <li><strong>Data:</strong> IEMOCAP is the gold standard.</li>
  <li><strong>Evaluation:</strong> Weighted F1 for imbalanced classes.</li>
  <li><strong>Multimodal:</strong> Combining audio + text improves accuracy.</li>
</ul>

<p>As AI becomes more empathetic, SER will be central to human-computer interaction. Master it to build systems that truly understand their users.</p>

<h2 id="15-training-pipeline">15. Training Pipeline</h2>

<h3 id="151-data-preprocessing">15.1. Data Preprocessing</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">librosa</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">preprocess_audio</span><span class="p">(</span><span class="n">audio_path</span><span class="p">,</span> <span class="n">target_sr</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span> <span class="n">max_duration</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Load audio
</span>    <span class="n">audio</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">audio_path</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">target_sr</span><span class="p">)</span>
    
    <span class="c1"># Trim silence
</span>    <span class="n">audio</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">effects</span><span class="p">.</span><span class="nf">trim</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">top_db</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    
    <span class="c1"># Pad or truncate
</span>    <span class="n">max_samples</span> <span class="o">=</span> <span class="n">target_sr</span> <span class="o">*</span> <span class="n">max_duration</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">max_samples</span><span class="p">:</span>
        <span class="n">audio</span> <span class="o">=</span> <span class="n">audio</span><span class="p">[:</span><span class="n">max_samples</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">audio</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_samples</span> <span class="o">-</span> <span class="nf">len</span><span class="p">(</span><span class="n">audio</span><span class="p">)))</span>
    
    <span class="c1"># Compute mel spectrogram
</span>    <span class="n">mel</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">feature</span><span class="p">.</span><span class="nf">melspectrogram</span><span class="p">(</span>
        <span class="n">y</span><span class="o">=</span><span class="n">audio</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">target_sr</span><span class="p">,</span> <span class="n">n_mels</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">hop_length</span><span class="o">=</span><span class="mi">160</span>
    <span class="p">)</span>
    <span class="n">log_mel</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">mel</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">log_mel</span>
</code></pre></div></div>

<h3 id="152-data-loading">15.2. Data Loading</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>

<span class="k">class</span> <span class="nc">EmotionDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio_paths</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">audio_paths</span> <span class="o">=</span> <span class="n">audio_paths</span>
        <span class="n">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
    
    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">audio_paths</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">mel</span> <span class="o">=</span> <span class="nf">preprocess_audio</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">audio_paths</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">mel</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">label</span>

<span class="c1"># Create dataloaders
</span><span class="n">train_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="153-training-loop">15.3. Training Loop</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">EmotionCNN</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">mel</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">mel</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    
    <span class="c1"># Validation
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">mel</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">mel</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s">, Val Accuracy: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="16-data-augmentation">16. Data Augmentation</h2>

<p><strong>Audio Augmentations:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">audiomentations</span> <span class="k">as</span> <span class="n">A</span>

<span class="n">augment</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
    <span class="n">A</span><span class="p">.</span><span class="nc">AddGaussianNoise</span><span class="p">(</span><span class="n">min_amplitude</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">max_amplitude</span><span class="o">=</span><span class="mf">0.015</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">A</span><span class="p">.</span><span class="nc">TimeStretch</span><span class="p">(</span><span class="n">min_rate</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">max_rate</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">A</span><span class="p">.</span><span class="nc">PitchShift</span><span class="p">(</span><span class="n">min_semitones</span><span class="o">=-</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_semitones</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">A</span><span class="p">.</span><span class="nc">Shift</span><span class="p">(</span><span class="n">min_fraction</span><span class="o">=-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_fraction</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
<span class="p">])</span>

<span class="k">def</span> <span class="nf">augment_audio</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">sr</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">augment</span><span class="p">(</span><span class="n">samples</span><span class="o">=</span><span class="n">audio</span><span class="p">,</span> <span class="n">sample_rate</span><span class="o">=</span><span class="n">sr</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>SpecAugment:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">spec_augment</span><span class="p">(</span><span class="n">mel</span><span class="p">,</span> <span class="n">freq_mask</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">time_mask</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
    <span class="c1"># Frequency masking
</span>    <span class="n">f0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">mel</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">freq_mask</span><span class="p">)</span>
    <span class="n">mel</span><span class="p">[</span><span class="n">f0</span><span class="p">:</span><span class="n">f0</span><span class="o">+</span><span class="n">freq_mask</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="c1"># Time masking
</span>    <span class="n">t0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">mel</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">time_mask</span><span class="p">)</span>
    <span class="n">mel</span><span class="p">[:,</span> <span class="n">t0</span><span class="p">:</span><span class="n">t0</span><span class="o">+</span><span class="n">time_mask</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">return</span> <span class="n">mel</span>
</code></pre></div></div>

<h2 id="17-handling-class-imbalance">17. Handling Class Imbalance</h2>

<p><strong>Strategies:</strong></p>
<ol>
  <li><strong>Weighted Loss:</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">class_weights</span> <span class="o">=</span> <span class="nf">compute_class_weights</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">class_weights</span><span class="p">))</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Oversampling:</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">RandomOverSampler</span>
<span class="n">ros</span> <span class="o">=</span> <span class="nc">RandomOverSampler</span><span class="p">()</span>
<span class="n">X_resampled</span><span class="p">,</span> <span class="n">y_resampled</span> <span class="o">=</span> <span class="n">ros</span><span class="p">.</span><span class="nf">fit_resample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Focal Loss:</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FocalLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
    
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">ce_loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cross_entropy</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">pt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">ce_loss</span><span class="p">)</span>
    <span class="n">focal_loss</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pt</span><span class="p">)</span> <span class="o">**</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">ce_loss</span>
    <span class="k">return</span> <span class="n">focal_loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="18-dimensional-emotion-recognition">18. Dimensional Emotion Recognition</h2>

<p><strong>Valence-Arousal-Dominance (VAD) Model:</strong></p>
<ul>
  <li><strong>Valence:</strong> Positive (happy) to Negative (sad).</li>
  <li><strong>Arousal:</strong> Active (excited) to Passive (calm).</li>
  <li><strong>Dominance:</strong> Dominant to Submissive.</li>
</ul>

<p><strong>Regression Instead of Classification:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EmotionVADRegressor</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Wav2Vec2Model</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">facebook/wav2vec2-base</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">regressor</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Predict V, A, D
</span>    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">last_hidden_state</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">regressor</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="c1"># Training with MSE loss
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">valence</span><span class="p">,</span> <span class="n">arousal</span><span class="p">,</span> <span class="n">dominance</span><span class="p">]))</span>
</code></pre></div></div>

<p><strong>Evaluation Metric (CCC):</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">concordance_correlation_coefficient</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="n">mean_pred</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
    <span class="n">mean_target</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
    <span class="n">var_pred</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="nf">var</span><span class="p">()</span>
    <span class="n">var_target</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="nf">var</span><span class="p">()</span>
    <span class="n">covar</span> <span class="o">=</span> <span class="p">((</span><span class="n">pred</span> <span class="o">-</span> <span class="n">mean_pred</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">mean_target</span><span class="p">)).</span><span class="nf">mean</span><span class="p">()</span>
    
    <span class="n">ccc</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">covar</span> <span class="o">/</span> <span class="p">(</span><span class="n">var_pred</span> <span class="o">+</span> <span class="n">var_target</span> <span class="o">+</span> <span class="p">(</span><span class="n">mean_pred</span> <span class="o">-</span> <span class="n">mean_target</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ccc</span>
</code></pre></div></div>

<h2 id="19-production-deployment">19. Production Deployment</h2>

<h3 id="191-model-export">19.1. Model Export</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Export to ONNX
</span><span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">onnx</span><span class="p">.</span><span class="nf">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_input</span><span class="p">,</span> <span class="sh">"</span><span class="s">emotion_model.onnx</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Or TorchScript
</span><span class="n">scripted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="nf">script</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">scripted</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="sh">"</span><span class="s">emotion_model.pt</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="192-inference-service">19.2. Inference Service</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">fastapi</span> <span class="kn">import</span> <span class="n">FastAPI</span><span class="p">,</span> <span class="n">UploadFile</span>
<span class="kn">import</span> <span class="n">soundfile</span> <span class="k">as</span> <span class="n">sf</span>

<span class="n">app</span> <span class="o">=</span> <span class="nc">FastAPI</span><span class="p">()</span>

<span class="nd">@app.post</span><span class="p">(</span><span class="sh">"</span><span class="s">/predict</span><span class="sh">"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">predict_emotion</span><span class="p">(</span><span class="nb">file</span><span class="p">:</span> <span class="n">UploadFile</span><span class="p">):</span>
    <span class="c1"># Read audio
</span>    <span class="n">audio</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">sf</span><span class="p">.</span><span class="nf">read</span><span class="p">(</span><span class="nb">file</span><span class="p">.</span><span class="nb">file</span><span class="p">)</span>
    
    <span class="c1"># Preprocess
</span>    <span class="n">mel</span> <span class="o">=</span> <span class="nf">preprocess_audio_from_array</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">sr</span><span class="p">)</span>
    
    <span class="c1"># Predict
</span>    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">mel</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="n">emotion_idx</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nf">argmax</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
    
    <span class="n">emotions</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">angry</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">happy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sad</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">neutral</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">fear</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">disgust</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">surprise</span><span class="sh">"</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">emotion</span><span class="sh">"</span><span class="p">:</span> <span class="n">emotions</span><span class="p">[</span><span class="n">emotion_idx</span><span class="p">]}</span>
</code></pre></div></div>

<h3 id="193-streaming-processing">19.3. Streaming Processing</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">StreamingEmotionDetector</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">hop_size</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16000</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">window_samples</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">window_size</span> <span class="o">*</span> <span class="n">sr</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hop_samples</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">hop_size</span> <span class="o">*</span> <span class="n">sr</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">def</span> <span class="nf">process_chunk</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio_chunk</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">audio_chunk</span><span class="p">)</span>
        
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">window_samples</span><span class="p">:</span>
            <span class="n">window</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">[:</span><span class="n">self</span><span class="p">.</span><span class="n">window_samples</span><span class="p">]</span>
            <span class="n">emotion</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
            <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">emotion</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nb">buffer</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">hop_samples</span><span class="p">:]</span>
        
        <span class="k">return</span> <span class="n">results</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">audio</span><span class="p">):</span>
        <span class="n">mel</span> <span class="o">=</span> <span class="nf">compute_mel</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">mel</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">.</span><span class="nf">argmax</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="20-mastery-checklist">20. Mastery Checklist</h2>

<p><strong>Mastery Checklist:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Extract prosodic features (F0, energy)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Extract spectral features (MFCC, mel spectrogram)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Train CNN on spectrograms</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Train LSTM with attention</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Fine-tune Wav2Vec2 for emotion</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Handle class imbalance (weighted loss, oversampling)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement multimodal fusion</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Evaluate with weighted F1 and UA</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Deploy real-time emotion detector</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Understand dimensional emotion models</li>
</ul>

<h2 id="21-conclusion">21. Conclusion</h2>

<p>Speech Emotion Recognition bridges the gap between AI and human emotional intelligence. It’s a challenging task that requires:</p>
<ul>
  <li><strong>Domain Knowledge:</strong> Understanding how emotions manifest in speech.</li>
  <li><strong>ML Expertise:</strong> Selecting and training appropriate models.</li>
  <li><strong>Data Engineering:</strong> Handling imbalanced, subjective labels.</li>
  <li><strong>System Design:</strong> Building real-time, production-ready systems.</li>
</ul>

<p><strong>The Path Forward:</strong></p>
<ol>
  <li>Start with IEMOCAP and a CNN baseline.</li>
  <li>Upgrade to Wav2Vec2 for better features.</li>
  <li>Add multimodal (text) for improved accuracy.</li>
  <li>Deploy with streaming for real-time applications.</li>
</ol>

<p>As AI assistants become more prevalent, emotional intelligence will be a key differentiator. Systems that understand and respond to human emotions will create more natural, empathetic interactions. Master SER to be at the forefront of this revolution.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/">arunbaby.com/speech-tech/0045-speech-emotion-recognition</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#classification" class="page__taxonomy-item p-category" rel="tag">classification</a><span class="sep">, </span>
    
      <a href="/tags/#deep-learning" class="page__taxonomy-item p-category" rel="tag">deep-learning</a><span class="sep">, </span>
    
      <a href="/tags/#emotion" class="page__taxonomy-item p-category" rel="tag">emotion</a><span class="sep">, </span>
    
      <a href="/tags/#multimodal" class="page__taxonomy-item p-category" rel="tag">multimodal</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Speech+Emotion+Recognition%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0045-speech-emotion-recognition%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0045-speech-emotion-recognition%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0045-speech-emotion-recognition/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0044-voice-conversion/" class="pagination--pager" title="Voice Conversion">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
