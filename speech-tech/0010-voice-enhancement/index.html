<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Voice Enhancement &amp; Noise Reduction - Arun Baby</title>
<meta name="description" content="Build systems that enhance voice quality by removing noise, improving intelligibility, and optimizing audio for speech applications.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Voice Enhancement &amp; Noise Reduction">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0010-voice-enhancement/">


  <meta property="og:description" content="Build systems that enhance voice quality by removing noise, improving intelligibility, and optimizing audio for speech applications.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Voice Enhancement &amp; Noise Reduction">
  <meta name="twitter:description" content="Build systems that enhance voice quality by removing noise, improving intelligibility, and optimizing audio for speech applications.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0010-voice-enhancement/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0010-voice-enhancement/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Voice Enhancement &amp; Noise Reduction">
    <meta itemprop="description" content="Build systems that enhance voice quality by removing noise, improving intelligibility, and optimizing audio for speech applications.">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0010-voice-enhancement/" itemprop="url">Voice Enhancement &amp; Noise Reduction
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          27 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#problem-formulation">Problem Formulation</a><ul><li><a href="#inputoutput">Input/Output</a></li><li><a href="#quality-metrics">Quality Metrics</a></li></ul></li><li><a href="#classical-methods">Classical Methods</a><ul><li><a href="#1-spectral-subtraction">1. Spectral Subtraction</a></li><li><a href="#2-wiener-filtering">2. Wiener Filtering</a></li></ul></li><li><a href="#deep-learning-approaches">Deep Learning Approaches</a><ul><li><a href="#1-mask-based-enhancement">1. Mask-Based Enhancement</a></li><li><a href="#2-end-to-end-waveform-enhancement">2. End-to-End Waveform Enhancement</a></li></ul></li><li><a href="#real-time-enhancement">Real-Time Enhancement</a><ul><li><a href="#streaming-enhancement-system">Streaming Enhancement System</a></li></ul></li><li><a href="#multi-channel-enhancement">Multi-Channel Enhancement</a><ul><li><a href="#beamforming">Beamforming</a></li></ul></li><li><a href="#connection-to-caching-ml">Connection to Caching (ML)</a></li><li><a href="#understanding-audio-enhancement-fundamentals">Understanding Audio Enhancement Fundamentals</a><ul><li><a href="#why-enhancement-is-critical">Why Enhancement is Critical</a></li><li><a href="#frequency-domain-analysis">Frequency Domain Analysis</a></li></ul></li><li><a href="#advanced-deep-learning-enhancement">Advanced Deep Learning Enhancement</a><ul><li><a href="#state-of-the-art-architectures">State-of-the-Art Architectures</a></li><li><a href="#real-time-enhancement-with-onnx">Real-Time Enhancement with ONNX</a></li></ul></li><li><a href="#production-quality-assurance">Production Quality Assurance</a><ul><li><a href="#automated-quality-metrics">Automated Quality Metrics</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Build systems that enhance voice quality by removing noise, improving intelligibility, and optimizing audio for speech applications.</strong></p>

<h2 id="introduction">Introduction</h2>

<p><strong>Voice enhancement</strong> improves speech quality by:</p>
<ul>
  <li>Removing background noise (traffic, wind, keyboard)</li>
  <li>Suppressing reverberation</li>
  <li>Normalizing volume levels</li>
  <li>Enhancing speech intelligibility</li>
  <li>Removing artifacts and distortion</li>
</ul>

<p><strong>Critical for:</strong></p>
<ul>
  <li>Video conferencing (Zoom, Teams, Meet)</li>
  <li>Voice assistants (Alexa, Siri, Google Assistant)</li>
  <li>Podcast/content creation</li>
  <li>Hearing aids</li>
  <li>Telecommunication</li>
  <li>Speech recognition systems</li>
</ul>

<p><strong>Key challenges:</strong></p>
<ul>
  <li>Real-time processing (&lt; 50ms latency)</li>
  <li>Preserving speech quality</li>
  <li>Handling diverse noise types</li>
  <li>Low computational cost</li>
  <li>Avoiding artifacts</li>
</ul>

<hr />

<h2 id="problem-formulation">Problem Formulation</h2>

<h3 id="inputoutput">Input/Output</h3>

<p>``
Input: Noisy speech signal
 y(t) = s(t) + n(t)
 where:
 s(t) = clean speech
 n(t) = noise</p>

<p>Output: Enhanced speech signal
 ŝ(t) ≈ s(t)</p>

<p>Goal: Minimize ‖ŝ(t) - s(t)‖ while maintaining naturalness
``</p>

<h3 id="quality-metrics">Quality Metrics</h3>

<p>``python
import numpy as np
from scipy import signal</p>

<p>def calculate_snr(clean_speech, noisy_speech):
 “””
 Calculate Signal-to-Noise Ratio</p>

<p>SNR = 10 * log10(P_signal / P_noise)</p>

<p>Higher is better (typically 10-30 dB)
 “””
 signal_power = np.mean(clean_speech ** 2)
 noise = noisy_speech - clean_speech
 noise_power = np.mean(noise ** 2)</p>

<p>if noise_power == 0:
 return float(‘inf’)</p>

<p>snr = 10 * np.log10(signal_power / noise_power)
 return snr</p>

<p>def calculate_pesq(reference, degraded, sr=16000):
 “””
 Calculate PESQ (Perceptual Evaluation of Speech Quality)</p>

<p>Range: -0.5 to 4.5 (higher is better)
 Industry standard for speech quality
 “””
 from pesq import pesq</p>

<p># PESQ requires 8kHz or 16kHz
 if sr not in [8000, 16000]:
 raise ValueError(“PESQ requires sr=8000 or sr=16000”)</p>

<p>mode = ‘nb’ if sr == 8000 else ‘wb’
 score = pesq(sr, reference, degraded, mode)</p>

<p>return score</p>

<p>def calculate_stoi(clean, enhanced, sr=16000):
 “””
 Calculate STOI (Short-Time Objective Intelligibility)</p>

<p>Range: 0 to 1 (higher is better)
 Correlates well with speech intelligibility
 “””
 from pystoi import stoi</p>

<p>score = stoi(clean, enhanced, sr, extended=False)
 return score</p>

<h1 id="usage">Usage</h1>
<p>clean = np.random.randn(16000) # 1 second at 16kHz
noisy = clean + 0.1 * np.random.randn(16000)</p>

<p>snr = calculate_snr(clean, noisy)
print(f”SNR: {snr:.2f} dB”)</p>

<h1 id="pesq_score--calculate_pesqclean-noisy-sr16000">pesq_score = calculate_pesq(clean, noisy, sr=16000)</h1>
<h1 id="printfpesq-pesq_score2f">print(f”PESQ: {pesq_score:.2f}”)</h1>
<p>``</p>

<hr />

<h2 id="classical-methods">Classical Methods</h2>

<h3 id="1-spectral-subtraction">1. Spectral Subtraction</h3>

<p><strong>Subtract noise spectrum from noisy spectrum</strong></p>

<p>``python
import librosa
import numpy as np</p>

<p>class SpectralSubtraction:
 “””
 Classic spectral subtraction for noise reduction</p>

<p>Steps:</p>
<ol>
  <li>Estimate noise spectrum (from silence periods)</li>
  <li>Subtract from noisy spectrum</li>
  <li>Half-wave rectification</li>
  <li>Reconstruct signal
 “””</li>
</ol>

<p>def <strong>init</strong>(self, n_fft=512, hop_length=128):
 self.n_fft = n_fft
 self.hop_length = hop_length
 self.noise_profile = None</p>

<p>def estimate_noise(self, noise_audio, sr=16000):
 “””
 Estimate noise spectrum from noise-only segment</p>

<p>Args:
 noise_audio: Audio containing only noise
 “””
 # STFT of noise
 noise_stft = librosa.stft(
 noise_audio,
 n_fft=self.n_fft,
 hop_length=self.hop_length
 )</p>

<p># Average magnitude spectrum
 self.noise_profile = np.mean(np.abs(noise_stft), axis=1, keepdims=True)</p>

<p>def enhance(self, noisy_audio, alpha=2.0, beta=0.002):
 “””
 Apply spectral subtraction</p>

<p>Args:
 noisy_audio: Noisy speech signal
 alpha: Over-subtraction factor (higher = more aggressive)
 beta: Spectral floor (prevents negative values)</p>

<p>Returns:
 Enhanced audio
 “””
 if self.noise_profile is None:
 raise ValueError(“Must estimate noise first”)</p>

<p># STFT of noisy signal
 noisy_stft = librosa.stft(
 noisy_audio,
 n_fft=self.n_fft,
 hop_length=self.hop_length
 )</p>

<p># Magnitude and phase
 mag = np.abs(noisy_stft)
 phase = np.angle(noisy_stft)</p>

<p># Spectral subtraction
 enhanced_mag = mag - alpha * self.noise_profile</p>

<p># Half-wave rectification with spectral floor
 enhanced_mag = np.maximum(enhanced_mag, beta * mag)</p>

<p># Reconstruct with original phase
 enhanced_stft = enhanced_mag * np.exp(1j * phase)</p>

<p># Inverse STFT
 enhanced_audio = librosa.istft(
 enhanced_stft,
 hop_length=self.hop_length
 )</p>

<p>return enhanced_audio</p>

<h1 id="usage-1">Usage</h1>
<p>sr = 16000</p>

<h1 id="load-noisy-speech">Load noisy speech</h1>
<p>noisy_speech, _ = librosa.load(‘noisy_speech.wav’, sr=sr)</p>

<h1 id="estimate-noise-from-first-05-seconds-assumed-to-be-silence">Estimate noise from first 0.5 seconds (assumed to be silence)</h1>
<p>noise_segment = noisy_speech[:int(0.5 * sr)]</p>

<p>enhancer = SpectralSubtraction()
enhancer.estimate_noise(noise_segment)</p>

<h1 id="enhance-full-audio">Enhance full audio</h1>
<p>enhanced = enhancer.enhance(noisy_speech, alpha=2.0)</p>

<h1 id="save-result">Save result</h1>
<p>import soundfile as sf
sf.write(‘enhanced_speech.wav’, enhanced, sr)
``</p>

<h3 id="2-wiener-filtering">2. Wiener Filtering</h3>

<p><strong>Optimal filter in MMSE sense</strong></p>

<p>``python
class WienerFilter:
 “””
 Wiener filtering for speech enhancement</p>

<p>Minimizes mean squared error between clean and enhanced speech
 “””</p>

<p>def <strong>init</strong>(self, n_fft=512, hop_length=128):
 self.n_fft = n_fft
 self.hop_length = hop_length
 self.noise_psd = None</p>

<p>def estimate_noise_psd(self, noise_audio):
 “"”Estimate noise power spectral density”””
 noise_stft = librosa.stft(
 noise_audio,
 n_fft=self.n_fft,
 hop_length=self.hop_length
 )</p>

<p># Power spectral density
 self.noise_psd = np.mean(np.abs(noise_stft) ** 2, axis=1, keepdims=True)</p>

<p>def enhance(self, noisy_audio, a_priori_snr=None):
 “””
 Apply Wiener filtering</p>

<p>Wiener gain: H = S / (S + N)
 where S = signal PSD, N = noise PSD
 “””
 if self.noise_psd is None:
 raise ValueError(“Must estimate noise PSD first”)</p>

<p># STFT
 noisy_stft = librosa.stft(
 noisy_audio,
 n_fft=self.n_fft,
 hop_length=self.hop_length
 )</p>

<p># Noisy PSD
 noisy_psd = np.abs(noisy_stft) ** 2</p>

<p># Estimate clean speech PSD
 speech_psd = np.maximum(noisy_psd - self.noise_psd, 0)</p>

<p># Wiener gain
 wiener_gain = speech_psd / (speech_psd + self.noise_psd + 1e-10)</p>

<p># Apply gain
 enhanced_stft = wiener_gain * noisy_stft</p>

<p># Inverse STFT
 enhanced_audio = librosa.istft(
 enhanced_stft,
 hop_length=self.hop_length
 )</p>

<p>return enhanced_audio</p>

<h1 id="usage-2">Usage</h1>
<p>wiener = WienerFilter()
wiener.estimate_noise_psd(noise_segment)
enhanced = wiener.enhance(noisy_speech)
``</p>

<hr />

<h2 id="deep-learning-approaches">Deep Learning Approaches</h2>

<h3 id="1-mask-based-enhancement">1. Mask-Based Enhancement</h3>

<p><strong>Learn ideal ratio mask (IRM) or ideal binary mask (IBM)</strong></p>

<p>``python
import torch
import torch.nn as nn</p>

<p>class MaskEstimationNet(nn.Module):
 “””
 Neural network for mask estimation</p>

<p>Predicts time-frequency mask to apply to noisy spectrogram
 “””</p>

<p>def <strong>init</strong>(self, n_fft=512, hidden_dim=128):
 super().<strong>init</strong>()</p>

<p>self.n_freq = n_fft // 2 + 1</p>

<p># Bidirectional LSTM
 self.lstm = nn.LSTM(
 input_size=self.n_freq,
 hidden_size=hidden_dim,
 num_layers=2,
 batch_first=True,
 bidirectional=True
 )</p>

<p># Mask prediction
 self.mask_fc = nn.Sequential(
 nn.Linear(hidden_dim * 2, hidden_dim),
 nn.ReLU(),
 nn.Dropout(0.2),
 nn.Linear(hidden_dim, self.n_freq),
 nn.Sigmoid() # Mask values in [0, 1]
 )</p>

<p>def forward(self, noisy_mag):
 “””
 Args:
 noisy_mag: Noisy magnitude spectrogram [batch, time, freq]</p>

<p>Returns:
 mask: Predicted mask [batch, time, freq]
 “””
 # LSTM
 lstm_out, _ = self.lstm(noisy_mag)</p>

<p># Predict mask
 mask = self.mask_fc(lstm_out)</p>

<p>return mask</p>

<p>class MaskBasedEnhancer:
 “””
 Speech enhancement using learned mask
 “””</p>

<p>def <strong>init</strong>(self, model, n_fft=512, hop_length=128):
 self.model = model
 self.model.eval()</p>

<p>self.n_fft = n_fft
 self.hop_length = hop_length</p>

<p>def enhance(self, noisy_audio):
 “””
 Enhance audio using learned mask</p>

<p>Steps:</p>
<ol>
  <li>Compute noisy spectrogram</li>
  <li>Predict mask with neural network</li>
  <li>Apply mask</li>
  <li>Reconstruct audio
 “””
 # STFT
 noisy_stft = librosa.stft(
 noisy_audio,
 n_fft=self.n_fft,
 hop_length=self.hop_length
 )</li>
</ol>

<p># Magnitude and phase
 noisy_mag = np.abs(noisy_stft)
 phase = np.angle(noisy_stft)</p>

<p># Normalize magnitude
 mag_mean = np.mean(noisy_mag)
 mag_std = np.std(noisy_mag)
 noisy_mag_norm = (noisy_mag - mag_mean) / (mag_std + 1e-8)</p>

<p># Predict mask
 with torch.no_grad():
 # Transpose to [1, time, freq]
 mag_tensor = torch.FloatTensor(noisy_mag_norm.T).unsqueeze(0)</p>

<p>mask = self.model(mag_tensor)</p>

<p># Back to numpy
 mask = mask.squeeze(0).numpy().T</p>

<p># Apply mask
 enhanced_mag = noisy_mag * mask</p>

<p># Reconstruct
 enhanced_stft = enhanced_mag * np.exp(1j * phase)
 enhanced_audio = librosa.istft(
 enhanced_stft,
 hop_length=self.hop_length
 )</p>

<p>return enhanced_audio</p>

<h1 id="usage-3">Usage</h1>
<p>model = MaskEstimationNet(n_fft=512)
enhancer = MaskBasedEnhancer(model)</p>

<h1 id="enhance">Enhance</h1>
<p>enhanced = enhancer.enhance(noisy_speech)
``</p>

<h3 id="2-end-to-end-waveform-enhancement">2. End-to-End Waveform Enhancement</h3>

<p><strong>Direct waveform→waveform mapping</strong></p>

<p>``python
class ConvTasNet(nn.Module):
 “””
 Conv-TasNet for speech enhancement</p>

<p>End-to-end time-domain speech separation
 Based on: “Conv-TasNet: Surpassing Ideal Time-Frequency Masking”
 “””</p>

<p>def <strong>init</strong>(self, N=256, L=20, B=256, H=512, P=3, X=8, R=3):
 “””
 Args:
 N: Number of filters in autoencoder
 L: Length of filters (ms)
 B: Number of channels in bottleneck
 H: Number of channels in conv blocks
 P: Kernel size in conv blocks
 X: Number of conv blocks in each repeat
 R: Number of repeats
 “””
 super().<strong>init</strong>()</p>

<p># Encoder (waveform → features)
 self.encoder = nn.Conv1d(1, N, L, stride=L//2, padding=L//2)</p>

<p># Separator (TCN blocks)
 self.separator = self._build_separator(N, B, H, P, X, R)</p>

<p># Decoder (features → waveform)
 self.decoder = nn.ConvTranspose1d(N, 1, L, stride=L//2, padding=L//2)</p>

<p>def _build_separator(self, N, B, H, P, X, R):
 “"”Build temporal convolutional network”””
 layers = []</p>

<p># Layer normalization
 layers.append(nn.LayerNorm(N))</p>

<p># Bottleneck
 layers.append(nn.Conv1d(N, B, 1))</p>

<p># TCN blocks
 for r in range(R):
 for x in range(X):
 dilation = 2 ** x
 layers.append(
 TemporalConvBlock(B, H, P, dilation)
 )</p>

<p># Output projection
 layers.append(nn.PReLU())
 layers.append(nn.Conv1d(B, N, 1))</p>

<p>return nn.Sequential(*layers)</p>

<p>def forward(self, mixture):
 “””
 Args:
 mixture: Noisy waveform [batch, 1, samples]</p>

<p>Returns:
 estimated_clean: Enhanced waveform [batch, 1, samples]
 “””
 # Encode
 encoded = self.encoder(mixture) # [batch, N, T]</p>

<p># Separate
 mask = self.separator(encoded) # [batch, N, T]</p>

<p># Apply mask
 separated = encoded * mask</p>

<p># Decode
 estimated = self.decoder(separated) # [batch, 1, samples]</p>

<p>return estimated</p>

<p>class TemporalConvBlock(nn.Module):
 “””
 Temporal convolutional block with dilated convolutions
 “””</p>

<p>def <strong>init</strong>(self, in_channels, hidden_channels, kernel_size, dilation):
 super().<strong>init</strong>()</p>

<p>self.conv1 = nn.Conv1d(
 in_channels, hidden_channels,
 kernel_size, dilation=dilation,
 padding=dilation * (kernel_size - 1) // 2
 )
 self.prelu1 = nn.PReLU()
 self.norm1 = nn.GroupNorm(1, hidden_channels)</p>

<p>self.conv2 = nn.Conv1d(
 hidden_channels, in_channels,
 1
 )
 self.prelu2 = nn.PReLU()
 self.norm2 = nn.GroupNorm(1, in_channels)</p>

<p>def forward(self, x):
 “””
 Args:
 x: [batch, channels, time]
 “””
 residual = x</p>

<p>out = self.conv1(x)
 out = self.prelu1(out)
 out = self.norm1(out)</p>

<p>out = self.conv2(out)
 out = self.prelu2(out)
 out = self.norm2(out)</p>

<p>return out + residual</p>

<h1 id="usage-4">Usage</h1>
<p>model = ConvTasNet()
noisy_tensor = torch.randn(1, 1, 16000) # 1 second
enhanced_tensor = model(noisy_tensor)
``</p>

<hr />

<h2 id="real-time-enhancement">Real-Time Enhancement</h2>

<h3 id="streaming-enhancement-system">Streaming Enhancement System</h3>

<p>``python
import numpy as np
from collections import deque</p>

<p>class StreamingEnhancer:
 “””
 Real-time streaming speech enhancement</p>

<p>Requirements:</p>
<ul>
  <li>Low latency (&lt; 50ms)</li>
  <li>Causal processing</li>
  <li>Minimal buffering
 “””</li>
</ul>

<p>def <strong>init</strong>(self, model, chunk_size=512, overlap=256, sr=16000):
 “””
 Args:
 chunk_size: Samples per chunk
 overlap: Overlap between chunks (for smooth transitions)
 “””
 self.model = model
 self.chunk_size = chunk_size
 self.overlap = overlap
 self.sr = sr</p>

<p># Circular buffer for overlap-add
 self.buffer = deque(maxlen=overlap)
 self.output_buffer = deque(maxlen=overlap)</p>

<p>self.processed_chunks = 0</p>

<p>def process_chunk(self, audio_chunk):
 “””
 Process single audio chunk</p>

<p>Args:
 audio_chunk: Audio samples [chunk_size]</p>

<p>Returns:
 Enhanced audio chunk
 “””
 # Add previous overlap
 if len(self.buffer) &gt; 0:
 input_chunk = np.concatenate([
 np.array(self.buffer),
 audio_chunk
 ])
 else:
 input_chunk = audio_chunk</p>

<p># Enhance
 enhanced = self._enhance_chunk(input_chunk)</p>

<p># Overlap-add with linear cross-fade
 if len(self.output_buffer) &gt; 0:
 # Smooth transition
 overlap_region = min(len(self.output_buffer), self.overlap)
 for i in range(overlap_region):
 weight = i / overlap_region
 enhanced[i] = (1 - weight) * self.output_buffer[i] + weight * enhanced[i]</p>

<p># Save overlap for next chunk
 self.buffer.clear()
 self.buffer.extend(audio_chunk[-self.overlap:])</p>

<p>self.output_buffer.clear()
 self.output_buffer.extend(enhanced[-self.overlap:])</p>

<p>self.processed_chunks += 1</p>

<p># Return non-overlap part
 return enhanced[:-self.overlap] if len(enhanced) &gt; self.overlap else enhanced</p>

<p>def _enhance_chunk(self, audio_chunk):
 “"”Enhance using model”””
 # Convert to tensor
 audio_tensor = torch.FloatTensor(audio_chunk).unsqueeze(0).unsqueeze(0)</p>

<p># Enhance
 with torch.no_grad():
 enhanced_tensor = self.model(audio_tensor)</p>

<p># Back to numpy
 enhanced = enhanced_tensor.squeeze().numpy()</p>

<p>return enhanced</p>

<p>def get_latency_ms(self):
 “"”Calculate processing latency”””
 return (self.chunk_size / self.sr) * 1000</p>

<h1 id="usage-for-real-time-processing">Usage for real-time processing</h1>
<p>model = ConvTasNet()
enhancer = StreamingEnhancer(model, chunk_size=512, overlap=256, sr=16000)</p>

<p>print(f”Latency: {enhancer.get_latency_ms():.2f} ms”)</p>

<h1 id="process-audio-stream">Process audio stream</h1>
<p>import sounddevice as sd</p>

<p>def audio_callback(indata, outdata, frames, time, status):
 “"”Real-time audio callback”””
 # Get input chunk
 input_chunk = indata[:, 0]</p>

<p># Enhance
 enhanced_chunk = enhancer.process_chunk(input_chunk)</p>

<p># Output
 outdata[:len(enhanced_chunk), 0] = enhanced_chunk</p>

<p>if status:
 print(f”Status: {status}”)</p>

<h1 id="start-real-time-processing">Start real-time processing</h1>
<p>with sd.Stream(
 samplerate=16000,
 channels=1,
 callback=audio_callback,
 blocksize=512
):
 print(“Processing audio in real-time… Press Ctrl+C to stop”)
 sd.sleep(10000)
``</p>

<hr />

<h2 id="multi-channel-enhancement">Multi-Channel Enhancement</h2>

<h3 id="beamforming">Beamforming</h3>

<p>``python
class BeamformerEnhancer:
 “””
 Beamforming for multi-microphone enhancement</p>

<p>Uses spatial information to enhance target speech
 “””</p>

<p>def <strong>init</strong>(self, n_mics=4, sr=16000):
 self.n_mics = n_mics
 self.sr = sr</p>

<p>def delay_and_sum(self, multi_channel_audio, target_direction=0):
 “””
 Delay-and-sum beamforming</p>

<p>Args:
 multi_channel_audio: [n_mics, n_samples]
 target_direction: Target angle in degrees (0 = front)</p>

<p>Returns:
 Enhanced single-channel audio
 “””
 n_samples = multi_channel_audio.shape[1]</p>

<p># Calculate delays for each microphone
 # (Simplified: assumes linear array)
 mic_spacing = 0.05 # 5cm between mics
 speed_of_sound = 343 # m/s</p>

<p>delays = []
 for i in range(self.n_mics):
 distance_diff = i * mic_spacing * np.sin(np.deg2rad(target_direction))
 delay_samples = int(distance_diff / speed_of_sound * self.sr)
 delays.append(delay_samples)</p>

<p># Align and sum
 aligned_signals = []
 for i, delay in enumerate(delays):
 sig = multi_channel_audio[i]
 if delay &gt; 0:
 # Delay by pre-pending zeros
 padded = np.concatenate([np.zeros(delay, dtype=sig.dtype), sig])
 aligned = padded[:n_samples]
 elif delay &lt; 0:
 # Advance by removing first samples
 aligned = sig[-delay:]
 if aligned.shape[0] &lt; n_samples:
 aligned = np.pad(aligned, (0, n_samples - aligned.shape[0]), mode=’constant’)
 else:
 aligned = sig
 aligned_signals.append(aligned)</p>

<p># Sum aligned signals
 enhanced = np.mean(aligned_signals, axis=0)</p>

<p>return enhanced</p>

<p>def mvdr_beamformer(self, multi_channel_audio, noise_segment):
 “””
 MVDR (Minimum Variance Distortionless Response) beamformer</p>

<p>Optimal beamformer for known noise covariance
 “””
 # Compute noise covariance matrix
 noise_cov = self._compute_covariance(noise_segment)</p>

<p># Compute signal+noise covariance
 signal_noise_cov = self._compute_covariance(multi_channel_audio)</p>

<p># MVDR weights
 # w = R_n^{-1} * a / (a^H * R_n^{-1} * a)
 # where a is steering vector</p>

<p># Simplified: assume steering vector points to channel 0
 steering_vector = np.zeros((self.n_mics, 1))
 steering_vector[0] = 1</p>

<p># Compute weights
 inv_noise_cov = np.linalg.pinv(noise_cov)
 numerator = inv_noise_cov @ steering_vector
 denominator = steering_vector.T @ inv_noise_cov @ steering_vector</p>

<p>weights = numerator / (denominator + 1e-10)</p>

<p># Apply weights
 enhanced = weights.T @ multi_channel_audio</p>

<p>return enhanced.squeeze()</p>

<p>def _compute_covariance(self, signal):
 “"”Compute covariance matrix”””
 # [n_mics, n_samples] → [n_mics, n_mics]
 cov = signal @ signal.T / signal.shape[1]
 return cov</p>

<h1 id="usage-5">Usage</h1>
<p>beamformer = BeamformerEnhancer(n_mics=4, sr=16000)</p>

<h1 id="multi-channel-recording">Multi-channel recording</h1>
<p>multi_ch_audio = np.random.randn(4, 16000) # 4 mics, 1 second</p>

<h1 id="enhance-using-delay-and-sum">Enhance using delay-and-sum</h1>
<p>enhanced_ds = beamformer.delay_and_sum(multi_ch_audio, target_direction=0)</p>

<h1 id="or-using-mvdr">Or using MVDR</h1>
<p>noise_segment = multi_ch_audio[:, :8000] # First 0.5 seconds
enhanced_mvdr = beamformer.mvdr_beamformer(multi_ch_audio, noise_segment)
``</p>

<hr />

<h2 id="connection-to-caching-ml">Connection to Caching (ML)</h2>

<p>Voice enhancement benefits from caching strategies:</p>

<p>``python
class EnhancementCache:
 “””
 Cache enhanced audio segments</p>

<p>Connection toML:</p>
<ul>
  <li>Cache expensive enhancement operations</li>
  <li>LRU for frequently accessed segments</li>
  <li>TTL for time-sensitive applications
 “””</li>
</ul>

<p>def <strong>init</strong>(self, capacity=1000):
 from collections import OrderedDict
 self.cache = OrderedDict()
 self.capacity = capacity</p>

<p>self.hits = 0
 self.misses = 0</p>

<p>def get_enhanced(self, audio_segment, model):
 “””
 Get enhanced audio with caching</p>

<p>Args:
 audio_segment: Raw audio
 model: Enhancement model</p>

<p>Returns:
 Enhanced audio
 “””
 # Create cache key (hash of audio)
 cache_key = hash(audio_segment.tobytes())</p>

<p># Check cache
 if cache_key in self.cache:
 self.hits += 1
 self.cache.move_to_end(cache_key) # Mark as recently used
 return self.cache[cache_key]</p>

<p># Compute enhancement
 self.misses += 1
 enhanced = model.enhance(audio_segment)</p>

<p># Cache result
 self.cache[cache_key] = enhanced</p>

<p># Evict if over capacity
 if len(self.cache) &gt; self.capacity:
 self.cache.popitem(last=False)</p>

<p>return enhanced</p>

<p>def get_hit_rate(self):
 “"”Calculate cache hit rate”””
 total = self.hits + self.misses
 return self.hits / total if total &gt; 0 else 0</p>

<h1 id="usage-6">Usage</h1>
<p>cache = EnhancementCache(capacity=1000)
model = ConvTasNet()</p>

<h1 id="process-with-caching">Process with caching</h1>
<p>for audio_segment in audio_stream:
 enhanced = cache.get_enhanced(audio_segment, model)</p>

<p>print(f”Cache hit rate: {cache.get_hit_rate():.2%}”)
``</p>

<hr />

<h2 id="understanding-audio-enhancement-fundamentals">Understanding Audio Enhancement Fundamentals</h2>

<h3 id="why-enhancement-is-critical">Why Enhancement is Critical</h3>

<p>Voice enhancement is the foundation of any production speech system. Poor audio quality cascades through the entire pipeline:</p>

<p>``python
class AudioQualityImpactAnalyzer:
 “””
 Analyze impact of audio quality on downstream tasks</p>

<p>Demonstrates how SNR affects ASR accuracy, speaker recognition, etc.
 “””</p>

<p>def <strong>init</strong>(self, asr_model, speaker_model):
 self.asr_model = asr_model
 self.speaker_model = speaker_model</p>

<p>def evaluate_quality_impact(self, clean_audio, noisy_audio, transcript):
 “””
 Compare performance on clean vs noisy audio</p>

<p>Returns:
 Dictionary with metrics for both conditions
 “””
 # ASR on clean audio
 clean_prediction = self.asr_model.transcribe(clean_audio)
 clean_wer = self._calculate_wer(transcript, clean_prediction)</p>

<p># ASR on noisy audio
 noisy_prediction = self.asr_model.transcribe(noisy_audio)
 noisy_wer = self._calculate_wer(transcript, noisy_prediction)</p>

<p># Speaker embedding quality
 clean_embedding = self.speaker_model.extract_embedding(clean_audio)
 noisy_embedding = self.speaker_model.extract_embedding(noisy_audio)</p>

<p># Embedding similarity (should be close for same speaker)
 similarity = np.dot(clean_embedding, noisy_embedding) / (
 np.linalg.norm(clean_embedding) * np.linalg.norm(noisy_embedding)
 )</p>

<p># Calculate SNR
 snr_db = self._calculate_snr(clean_audio, noisy_audio)</p>

<p>return {
 ‘snr_db’: snr_db,
 ‘clean_wer’: clean_wer,
 ‘noisy_wer’: noisy_wer,
 ‘wer_degradation’: noisy_wer - clean_wer,
 ‘embedding_similarity’: similarity,
 ‘relative_performance’: clean_wer / noisy_wer if noisy_wer &gt; 0 else 1.0
 }</p>

<p>def _calculate_wer(self, reference, hypothesis):
 “"”Calculate Word Error Rate”””
 import editdistance</p>

<p>ref_words = reference.lower().split()
 hyp_words = hypothesis.lower().split()</p>

<p>distance = editdistance.eval(ref_words, hyp_words)
 wer = distance / len(ref_words) if len(ref_words) &gt; 0 else 0</p>

<p>return wer</p>

<p>def _calculate_snr(self, clean, noisy):
 “"”Calculate Signal-to-Noise Ratio”””
 noise = noisy - clean</p>

<p>signal_power = np.mean(clean ** 2)
 noise_power = np.mean(noise ** 2)</p>

<p>if noise_power == 0:
 return float(‘inf’)</p>

<p>snr = 10 * np.log10(signal_power / noise_power)
 return snr</p>

<h1 id="demo-impact-analysis">Demo impact analysis</h1>
<p>print(“=”<em>60)
print(“AUDIO QUALITY IMPACT ANALYSIS”)
print(“=”</em>60)</p>

<h1 id="simulate-different-snr-levels">Simulate different SNR levels</h1>
<p>snr_levels = [-5, 0, 5, 10, 15, 20]</p>

<p>for snr_target in snr_levels:
 # Add noise at specific SNR
 noisy = add_noise_at_snr(clean_audio, noise, snr_target)</p>

<p># Evaluate
 results = analyzer.evaluate_quality_impact(clean_audio, noisy, transcript)</p>

<p>print(f”\nSNR: {snr_target} dB”)
 print(f” WER (clean): {results[‘clean_wer’]:.2%}”)
 print(f” WER (noisy): {results[‘noisy_wer’]:.2%}”)
 print(f” Degradation: {results[‘wer_degradation’]:.2%}”)
 print(f” Speaker Sim: {results[‘embedding_similarity’]:.3f}”)
``</p>

<h3 id="frequency-domain-analysis">Frequency Domain Analysis</h3>

<p>Understanding audio in frequency domain is crucial for enhancement:</p>

<p>``python
class FrequencyDomainAnalyzer:
 “””
 Analyze and visualize audio in frequency domain</p>

<p>Essential for understanding what noise reduction does
 “””</p>

<p>def <strong>init</strong>(self, sr=16000):
 self.sr = sr</p>

<p>def analyze_spectrum(self, audio):
 “””
 Compute and visualize spectrum</p>

<p>Returns:
 frequencies, magnitudes, phases
 “””
 # Compute FFT
 n_fft = 2048
 fft = np.fft.rfft(audio, n=n_fft)</p>

<p># Magnitude and phase
 magnitude = np.abs(fft)
 phase = np.angle(fft)</p>

<p># Frequency bins
 frequencies = np.fft.rfftfreq(n_fft, 1/self.sr)</p>

<p>return frequencies, magnitude, phase</p>

<p>def compare_spectra(self, clean, noisy, enhanced):
 “””
 Compare spectra before and after enhancement
 “””
 import matplotlib.pyplot as plt</p>

<p># Compute spectra
 freq_clean, mag_clean, _ = self.analyze_spectrum(clean)
 freq_noisy, mag_noisy, _ = self.analyze_spectrum(noisy)
 freq_enhanced, mag_enhanced, _ = self.analyze_spectrum(enhanced)</p>

<p># Plot
 fig, axes = plt.subplots(3, 1, figsize=(12, 10))</p>

<p># Clean
 axes[0].plot(freq_clean, 20 * np.log10(mag_clean + 1e-10))
 axes[0].set_title(‘Clean Audio Spectrum’)
 axes[0].set_ylabel(‘Magnitude (dB)’)
 axes[0].grid(True)</p>

<p># Noisy
 axes[1].plot(freq_noisy, 20 * np.log10(mag_noisy + 1e-10), color=’red’)
 axes[1].set_title(‘Noisy Audio Spectrum’)
 axes[1].set_ylabel(‘Magnitude (dB)’)
 axes[1].grid(True)</p>

<p># Enhanced
 axes[2].plot(freq_enhanced, 20 * np.log10(mag_enhanced + 1e-10), color=’green’)
 axes[2].set_title(‘Enhanced Audio Spectrum’)
 axes[2].set_xlabel(‘Frequency (Hz)’)
 axes[2].set_ylabel(‘Magnitude (dB)’)
 axes[2].grid(True)</p>

<p>plt.tight_layout()
 plt.savefig(‘spectrum_comparison.png’)
 plt.close()</p>

<p>def compute_spectral_features(self, audio):
 “””
 Compute spectral features for quality assessment
 “””
 freq, mag, _ = self.analyze_spectrum(audio)</p>

<p># Spectral centroid
 centroid = np.sum(freq * mag) / np.sum(mag)</p>

<p># Spectral bandwidth
 bandwidth = np.sqrt(np.sum(((freq - centroid) ** 2) * mag) / np.sum(mag))</p>

<p># Spectral flatness (Wiener entropy)
 geometric_mean = np.exp(np.mean(np.log(mag + 1e-10)))
 arithmetic_mean = np.mean(mag)
 flatness = geometric_mean / arithmetic_mean</p>

<p># Spectral rolloff (95% of energy)
 cumsum = np.cumsum(mag)
 rolloff_idx = np.where(cumsum &gt;= 0.95 * cumsum[-1])[0][0]
 rolloff = freq[rolloff_idx]</p>

<p>return {
 ‘centroid_hz’: centroid,
 ‘bandwidth_hz’: bandwidth,
 ‘flatness’: flatness,
 ‘rolloff_hz’: rolloff
 }</p>

<h1 id="usage-7">Usage</h1>
<p>analyzer = FrequencyDomainAnalyzer(sr=16000)</p>

<h1 id="analyze-audio">Analyze audio</h1>
<p>features = analyzer.compute_spectral_features(audio)
print(“Spectral Features:”)
print(f” Centroid: {features[‘centroid_hz’]:.1f} Hz”)
print(f” Bandwidth: {features[‘bandwidth_hz’]:.1f} Hz”)
print(f” Flatness: {features[‘flatness’]:.3f}”)
print(f” Rolloff: {features[‘rolloff_hz’]:.1f} Hz”)</p>

<h1 id="compare-beforeafter">Compare before/after</h1>
<p>analyzer.compare_spectra(clean_audio, noisy_audio, enhanced_audio)
``</p>

<hr />

<h2 id="advanced-deep-learning-enhancement">Advanced Deep Learning Enhancement</h2>

<h3 id="state-of-the-art-architectures">State-of-the-Art Architectures</h3>

<p>``python
class ConvTasNetEnhancer(nn.Module):
 “””
 Conv-TasNet for speech enhancement</p>

<p>Architecture:</p>
<ol>
  <li>Encoder: Waveform → Feature representation</li>
  <li>Separator: Mask estimation using temporal convolutions</li>
  <li>Decoder: Masked features → Enhanced waveform</li>
</ol>

<p>Advantages over STFT-based methods:</p>
<ul>
  <li>Operates on raw waveform</li>
  <li>Learnable basis functions</li>
  <li>Better phase reconstruction
 “””</li>
</ul>

<p>def <strong>init</strong>(
 self,
 n_src=1,
 n_filters=512,
 kernel_size=16,
 stride=8,
 n_blocks=8,
 n_repeats=3,
 bn_chan=128,
 hid_chan=512,
 skip_chan=128
 ):
 super().<strong>init</strong>()</p>

<p># Encoder: 1D conv
 self.encoder = nn.Conv1d(
 1,
 n_filters,
 kernel_size=kernel_size,
 stride=stride,
 padding=kernel_size // 2
 )</p>

<p># Separator: TCN blocks
 self.separator = TemporalConvNet(
 n_filters,
 n_src,
 n_blocks=n_blocks,
 n_repeats=n_repeats,
 bn_chan=bn_chan,
 hid_chan=hid_chan,
 skip_chan=skip_chan
 )</p>

<p># Decoder: 1D transposed conv
 self.decoder = nn.ConvTranspose1d(
 n_filters,
 1,
 kernel_size=kernel_size,
 stride=stride,
 padding=kernel_size // 2
 )</p>

<p>def forward(self, waveform):
 “””
 Enhance waveform</p>

<p>Args:
 waveform: [batch, time]</p>

<p>Returns:
 enhanced: [batch, time]
 “””
 # Add channel dimension
 x = waveform.unsqueeze(1) # [batch, 1, time]</p>

<p># Encode
 encoded = self.encoder(x) # [batch, n_filters, time’]</p>

<p># Separate (estimate mask)
 masks = self.separator(encoded) # [batch, n_src, n_filters, time’]</p>

<p># Apply mask
 masked = encoded.unsqueeze(1) * masks # [batch, n_src, n_filters, time’]</p>

<p># Decode
 enhanced = self.decoder(masked.squeeze(1)) # [batch, 1, time]</p>

<p># Remove channel dimension
 enhanced = enhanced.squeeze(1) # [batch, time]</p>

<p># Trim to original length
 if enhanced.shape[-1] != waveform.shape[-1]:
 enhanced = enhanced[…, :waveform.shape[-1]]</p>

<p>return enhanced</p>

<p>class TemporalConvNet(nn.Module):
 “””
 Temporal Convolutional Network for Conv-TasNet</p>

<p>Stack of dilated conv blocks with skip connections
 “””</p>

<p>def <strong>init</strong>(
 self,
 n_filters,
 n_src,
 n_blocks=8,
 n_repeats=3,
 bn_chan=128,
 hid_chan=512,
 skip_chan=128
 ):
 super().<strong>init</strong>()</p>

<p># Layer norm
 self.layer_norm = nn.GroupNorm(1, n_filters)</p>

<p># Bottleneck
 self.bottleneck = nn.Conv1d(n_filters, bn_chan, 1)</p>

<p># TCN blocks
 self.blocks = nn.ModuleList()
 for r in range(n_repeats):
 for b in range(n_blocks):
 dilation = 2 ** b
 self.blocks.append(
 TCNBlock(
 bn_chan,
 hid_chan,
 skip_chan,
 kernel_size=3,
 dilation=dilation
 )
 )</p>

<p># Output
 self.output = nn.Sequential(
 nn.PReLU(),
 nn.Conv1d(skip_chan, n_filters, 1),
 nn.Sigmoid() # Mask should be [0, 1]
 )</p>

<p>def forward(self, x):
 “””
 Args:
 x: [batch, n_filters, time]</p>

<p>Returns:
 masks: [batch, n_src, n_filters, time]
 “””
 # Normalize
 x = self.layer_norm(x)</p>

<p># Bottleneck
 x = self.bottleneck(x) # [batch, bn_chan, time]</p>

<p># Accumulate skip connections
 skip_sum = 0</p>

<p>for block in self.blocks:
 x, skip = block(x)
 skip_sum = skip_sum + skip</p>

<p># Output mask
 masks = self.output(skip_sum)</p>

<p># Unsqueeze for n_src dimension
 masks = masks.unsqueeze(1) # [batch, 1, n_filters, time]</p>

<p>return masks</p>

<p>class TCNBlock(nn.Module):
 “"”Single TCN block with dilated convolution”””</p>

<p>def <strong>init</strong>(self, in_chan, hid_chan, skip_chan, kernel_size=3, dilation=1):
 super().<strong>init</strong>()</p>

<p>self.conv1 = nn.Conv1d(
 in_chan,
 hid_chan,
 1
 )</p>

<p>self.prelu1 = nn.PReLU()</p>

<p>self.norm1 = nn.GroupNorm(1, hid_chan)</p>

<p>self.depthwise_conv = nn.Conv1d(
 hid_chan,
 hid_chan,
 kernel_size,
 padding=(kernel_size - 1) * dilation // 2,
 dilation=dilation,
 groups=hid_chan
 )</p>

<p>self.prelu2 = nn.PReLU()</p>

<p>self.norm2 = nn.GroupNorm(1, hid_chan)</p>

<p>self.conv2 = nn.Conv1d(hid_chan, in_chan, 1)</p>

<p>self.skip_conv = nn.Conv1d(hid_chan, skip_chan, 1)</p>

<p>def forward(self, x):
 “””
 Args:
 x: [batch, in_chan, time]</p>

<p>Returns:
 output: [batch, in_chan, time]
 skip: [batch, skip_chan, time]
 “””
 residual = x</p>

<p># 1x1 conv
 x = self.conv1(x)
 x = self.prelu1(x)
 x = self.norm1(x)</p>

<p># Depthwise conv
 x = self.depthwise_conv(x)
 x = self.prelu2(x)
 x = self.norm2(x)</p>

<p># Skip connection
 skip = self.skip_conv(x)</p>

<p># Output
 x = self.conv2(x)</p>

<p># Residual
 output = x + residual</p>

<p>return output, skip</p>

<h1 id="training-conv-tasnet">Training Conv-TasNet</h1>
<p>class ConvTasNetTrainer:
 “””
 Train Conv-TasNet for speech enhancement
 “””</p>

<p>def <strong>init</strong>(self, model, device=’cuda’):
 self.model = model.to(device)
 self.device = device</p>

<p># Optimizer
 self.optimizer = torch.optim.Adam(
 self.model.parameters(),
 lr=1e-3
 )</p>

<p># Learning rate scheduler
 self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
 self.optimizer,
 mode=’min’,
 factor=0.5,
 patience=3
 )</p>

<p>def train_epoch(self, train_loader):
 “"”Train one epoch”””
 self.model.train()</p>

<p>total_loss = 0</p>

<p>for batch_idx, (noisy, clean) in enumerate(train_loader):
 noisy = noisy.to(self.device)
 clean = clean.to(self.device)</p>

<p># Forward
 enhanced = self.model(noisy)</p>

<p># Loss: SI-SNR (Scale-Invariant SNR)
 loss = self._si_snr_loss(enhanced, clean)</p>

<p># Backward
 self.optimizer.zero_grad()
 loss.backward()</p>

<p># Gradient clipping
 torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)</p>

<p>self.optimizer.step()</p>

<p>total_loss += loss.item()</p>

<p>if batch_idx % 100 == 0:
 print(f”Batch {batch_idx}, Loss: {loss.item():.4f}”)</p>

<p>return total_loss / len(train_loader)</p>

<p>def _si_snr_loss(self, estimate, target):
 “””
 Scale-Invariant Signal-to-Noise Ratio loss</p>

<p>Better than MSE for speech enhancement
 “””
 # Zero-mean
 estimate_zm = estimate - estimate.mean(dim=-1, keepdim=True)
 target_zm = target - target.mean(dim=-1, keepdim=True)</p>

<p># &lt;s’, s&gt;s / ||s||^2
 dot = (estimate_zm * target_zm).sum(dim=-1, keepdim=True)
 target_energy = (target_zm ** 2).sum(dim=-1, keepdim=True)
 projection = dot * target_zm / (target_energy + 1e-8)</p>

<p># Noise
 noise = estimate_zm - projection</p>

<p># SI-SNR
 si_snr = 10 * torch.log10(
 (projection ** 2).sum(dim=-1) / (noise ** 2).sum(dim=-1) + 1e-8
 )</p>

<p># Negative for loss (we want to maximize SI-SNR)
 return -si_snr.mean()</p>

<h1 id="usage-8">Usage</h1>
<p>model = ConvTasNetEnhancer()
trainer = ConvTasNetTrainer(model, device=’cuda’)</p>

<h1 id="train">Train</h1>
<p>for epoch in range(num_epochs):
 train_loss = trainer.train_epoch(train_loader)
 val_loss = trainer.validate(val_loader)</p>

<p>print(f”Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}”)</p>

<p>trainer.scheduler.step(val_loss)
``</p>

<h3 id="real-time-enhancement-with-onnx">Real-Time Enhancement with ONNX</h3>

<p>``python
class RealTimeONNXEnhancer:
 “””
 Real-time enhancement using ONNX Runtime</p>

<p>Optimized for production deployment
 “””</p>

<p>def <strong>init</strong>(self, onnx_model_path, chunk_size=4800):
 “””
 Args:
 onnx_model_path: Path to exported ONNX model
 chunk_size: Audio chunk size (samples)
 “””
 import onnxruntime as ort</p>

<p>self.chunk_size = chunk_size</p>

<p># Load ONNX model
 self.session = ort.InferenceSession(
 onnx_model_path,
 providers=[‘CUDAExecutionProvider’, ‘CPUExecutionProvider’]
 )</p>

<p># Get input/output names
 self.input_name = self.session.get_inputs()[0].name
 self.output_name = self.session.get_outputs()[0].name</p>

<p># State for streaming
 self.reset_state()</p>

<p>def reset_state(self):
 “"”Reset streaming state”””
 self.overlap_buffer = np.zeros(self.chunk_size // 2, dtype=np.float32)</p>

<p>def enhance_chunk(self, audio_chunk):
 “””
 Enhance single audio chunk with overlap-add</p>

<p>Args:
 audio_chunk: [chunk_size] numpy array</p>

<p>Returns:
 enhanced_chunk: [chunk_size] numpy array
 “””
 # Prepare input (batch dimension)
 input_data = audio_chunk.astype(np.float32)[np.newaxis, :]</p>

<p># Run inference
 enhanced = self.session.run(
 [self.output_name],
 {self.input_name: input_data}
 )[0][0]</p>

<p># Overlap-add
 overlap_size = len(self.overlap_buffer)
 enhanced[:overlap_size] += self.overlap_buffer</p>

<p># Save overlap for next chunk
 self.overlap_buffer = enhanced[-overlap_size:].copy()</p>

<p># Return without overlap region
 return enhanced[:-overlap_size]</p>

<p>def enhance_stream(self, audio_stream):
 “””
 Enhance audio stream in real-time</p>

<p>Generator that yields enhanced chunks
 “””
 for chunk in audio_stream:
 # Ensure correct size
 if len(chunk) != self.chunk_size:
 # Pad or skip
 continue</p>

<p># Enhance
 enhanced = self.enhance_chunk(chunk)</p>

<p>yield enhanced</p>

<h1 id="export-pytorch-model-to-onnx">Export PyTorch model to ONNX</h1>
<p>def export_to_onnx(pytorch_model, onnx_path, chunk_size=4800):
 “””
 Export trained PyTorch model to ONNX
 “””
 pytorch_model.eval()</p>

<p># Dummy input
 dummy_input = torch.randn(1, chunk_size)</p>

<p># Export
 torch.onnx.export(
 pytorch_model,
 dummy_input,
 onnx_path,
 input_names=[‘audio_input’],
 output_names=[‘audio_output’],
 dynamic_axes={
 ‘audio_input’: {1: ‘time’},
 ‘audio_output’: {1: ‘time’}
 },
 opset_version=14
 )</p>

<p>print(f”Model exported to {onnx_path}”)</p>

<h1 id="usage-9">Usage</h1>
<h1 id="export-model">Export model</h1>
<p>export_to_onnx(trained_model, ‘convtasnet_enhancer.onnx’)</p>

<h1 id="create-real-time-enhancer">Create real-time enhancer</h1>
<p>enhancer = RealTimeONNXEnhancer(‘convtasnet_enhancer.onnx’, chunk_size=4800)</p>

<h1 id="stream-audio">Stream audio</h1>
<p>def audio_stream_generator():
 “"”Generate audio chunks from microphone/file”””
 # Implementation depends on audio source
 pass</p>

<h1 id="enhance-stream">Enhance stream</h1>
<p>for enhanced_chunk in enhancer.enhance_stream(audio_stream_generator()):
 # Play or save enhanced audio
 pass
``</p>

<hr />

<h2 id="production-quality-assurance">Production Quality Assurance</h2>

<h3 id="automated-quality-metrics">Automated Quality Metrics</h3>

<p>``python
class EnhancementQualityAssurance:
 “””
 Automated quality assurance for enhancement pipeline</p>

<p>Monitors:</p>
<ul>
  <li>SNR improvement</li>
  <li>Speech intelligibility</li>
  <li>Artifacts</li>
  <li>Latency
 “””</li>
</ul>

<p>def <strong>init</strong>(self):
 self.metrics_history = []</p>

<p>def assess_quality(self, original, enhanced, reference=None):
 “””
 Comprehensive quality assessment</p>

<p>Args:
 original: Noisy input
 enhanced: Enhanced output
 reference: Clean reference (if available)</p>

<p>Returns:
 Quality metrics dictionary
 “””
 metrics = {}</p>

<p># SNR improvement (requires reference)
 if reference is not None:
 original_snr = self._compute_snr(original, reference)
 enhanced_snr = self._compute_snr(enhanced, reference)
 metrics[‘snr_improvement_db’] = enhanced_snr - original_snr</p>

<p># PESQ (Perceptual Evaluation of Speech Quality)
 from pesq import pesq
 metrics[‘pesq_original’] = pesq(16000, reference, original, ‘wb’)
 metrics[‘pesq_enhanced’] = pesq(16000, reference, enhanced, ‘wb’)
 metrics[‘pesq_improvement’] = (
 metrics[‘pesq_enhanced’] - metrics[‘pesq_original’]
 )</p>

<p># STOI (Short-Time Objective Intelligibility)
 from pystoi import stoi
 metrics[‘stoi_original’] = stoi(reference, original, 16000)
 metrics[‘stoi_enhanced’] = stoi(reference, enhanced, 16000)
 metrics[‘stoi_improvement’] = (
 metrics[‘stoi_enhanced’] - metrics[‘stoi_original’]
 )</p>

<p># Artifact detection (no reference needed)
 metrics[‘artifact_score’] = self._detect_artifacts(enhanced)</p>

<p># Spectral distortion
 metrics[‘spectral_distortion’] = self._compute_spectral_distortion(
 original, enhanced
 )</p>

<p># Dynamic range
 metrics[‘dynamic_range_db’] = 20 * np.log10(
 np.max(np.abs(enhanced)) / (np.mean(np.abs(enhanced)) + 1e-8)
 )</p>

<p># Clipping detection
 metrics[‘clipping_ratio’] = np.mean(np.abs(enhanced) &gt; 0.99)</p>

<p># Overall quality score
 metrics[‘quality_score’] = self._compute_overall_score(metrics)</p>

<p>self.metrics_history.append(metrics)</p>

<p>return metrics</p>

<p>def _compute_snr(self, signal, reference):
 “"”Compute SNR”””
 noise = signal - reference
 signal_power = np.mean(reference ** 2)
 noise_power = np.mean(noise ** 2)</p>

<p>if noise_power == 0:
 return float(‘inf’)</p>

<p>snr_db = 10 * np.log10(signal_power / noise_power)
 return snr_db</p>

<p>def _detect_artifacts(self, audio):
 “””
 Detect musical noise and other artifacts</p>

<p>Returns:
 Artifact score (0-1, lower is better)
 “””
 # Compute spectrogram
 S = librosa.stft(audio)
 magnitude = np.abs(S)</p>

<p># Temporal variation
 temporal_diff = np.diff(magnitude, axis=1)
 temporal_variance = np.var(temporal_diff)</p>

<p># Spectral variation
 spectral_diff = np.diff(magnitude, axis=0)
 spectral_variance = np.var(spectral_diff)</p>

<p># High variance indicates artifacts
 artifact_score = (temporal_variance + spectral_variance) / 2</p>

<p># Normalize to [0, 1]
 artifact_score = np.clip(artifact_score / 100, 0, 1)</p>

<p>return artifact_score</p>

<p>def _compute_spectral_distortion(self, original, enhanced):
 “””
 Compute spectral distortion</p>

<p>Measures how much the spectrum changed
 “””
 # Compute spectrograms
 S_orig = np.abs(librosa.stft(original))
 S_enh = np.abs(librosa.stft(enhanced))</p>

<p># Log magnitude
 S_orig_db = librosa.amplitude_to_db(S_orig + 1e-10)
 S_enh_db = librosa.amplitude_to_db(S_enh + 1e-10)</p>

<p># MSE in log domain
 distortion = np.mean((S_orig_db - S_enh_db) ** 2)</p>

<p>return distortion</p>

<p>def _compute_overall_score(self, metrics):
 “””
 Compute overall quality score</p>

<p>Weighted combination of metrics
 “””
 score = 0.0</p>

<p># PESQ improvement (if available)
 if ‘pesq_improvement’ in metrics:
 score += 0.4 * np.clip(metrics[‘pesq_improvement’] / 2, 0, 1)</p>

<p># STOI improvement (if available)
 if ‘stoi_improvement’ in metrics:
 score += 0.4 * np.clip(metrics[‘stoi_improvement’], 0, 1)</p>

<p># Artifact penalty
 score -= 0.2 * metrics[‘artifact_score’]</p>

<p># Normalize to [0, 1]
 score = np.clip(score, 0, 1)</p>

<p>return score</p>

<p>def generate_report(self):
 “"”Generate quality assurance report”””
 if not self.metrics_history:
 print(“No metrics recorded”)
 return</p>

<p># Aggregate metrics
 avg_metrics = {}
 for key in self.metrics_history[0].keys():
 values = [m[key] for m in self.metrics_history if key in m]
 avg_metrics[key] = np.mean(values)</p>

<p>print(“\n” + “=”<em>60)
 print(“ENHANCEMENT QUALITY ASSURANCE REPORT”)
 print(“=”</em>60)
 print(f”Samples Evaluated: {len(self.metrics_history)}”)
 print(f”\nAverage Metrics:”)</p>

<p>for key, value in avg_metrics.items():
 print(f” {key:30s}: {value:.4f}”)</p>

<p># Pass/fail criteria
 print(f”\n{‘Criterion’:&lt;30s} {‘Status’:&gt;10s}”)
 print(“-“ * 42)</p>

<p>checks = [
 (‘SNR Improvement’, avg_metrics.get(‘snr_improvement_db’, 0) &gt; 3, ‘&gt;3 dB’),
 (‘PESQ Improvement’, avg_metrics.get(‘pesq_improvement’, 0) &gt; 0.5, ‘&gt;0.5’),
 (‘STOI Improvement’, avg_metrics.get(‘stoi_improvement’, 0) &gt; 0.1, ‘&gt;0.1’),
 (‘Artifact Score’, avg_metrics.get(‘artifact_score’, 1) &lt; 0.3, ‘&lt;0.3’),
 (‘Clipping Ratio’, avg_metrics.get(‘clipping_ratio’, 1) &lt; 0.01, ‘&lt;1%’),
 ]</p>

<p>all_passed = True
 for name, passed, threshold in checks:
 status = “✓ PASS” if passed else “✗ FAIL”
 all_passed = all_passed and passed
 print(f” {name:&lt;30s} {status:&gt;10s} ({threshold})”)</p>

<p>print(“-“ * 42)
 print(f” {‘Overall Result’:&lt;30s} {‘✓ PASS’ if all_passed else ‘✗ FAIL’:&gt;10s}”)
 print(“=”*60)</p>

<h1 id="usage-10">Usage</h1>
<p>qa = EnhancementQualityAssurance()</p>

<h1 id="evaluate-multiple-files">Evaluate multiple files</h1>
<p>for noisy_file, clean_file in test_pairs:
 noisy_audio, _ = librosa.load(noisy_file, sr=16000)
 clean_audio, _ = librosa.load(clean_file, sr=16000)</p>

<p># Enhance
 enhanced_audio = enhancer.enhance(noisy_audio)</p>

<p># Assess quality
 metrics = qa.assess_quality(noisy_audio, enhanced_audio, clean_audio)</p>

<h1 id="generate-report">Generate report</h1>
<p>qa.generate_report()
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Multiple approaches</strong> - Classical (spectral subtraction, Wiener) and deep learning 
✅ <strong>Quality metrics</strong> - PESQ, STOI, SNR for evaluation 
✅ <strong>Real-time processing</strong> - Streaming with low latency &lt; 50ms 
✅ <strong>Multi-channel</strong> - Beamforming for spatial enhancement 
✅ <strong>Caching benefits</strong> - Reduce computational cost for repeated segments 
✅ <strong>Trade-offs</strong> - Quality vs latency vs computational cost 
✅ <strong>Production considerations</strong> - Monitoring, fallback, quality control</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0010-voice-enhancement/">arunbaby.com/speech-tech/0010-voice-enhancement</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#audio-quality" class="page__taxonomy-item p-category" rel="tag">audio-quality</a><span class="sep">, </span>
    
      <a href="/tags/#deep-learning" class="page__taxonomy-item p-category" rel="tag">deep-learning</a><span class="sep">, </span>
    
      <a href="/tags/#noise-reduction" class="page__taxonomy-item p-category" rel="tag">noise-reduction</a><span class="sep">, </span>
    
      <a href="/tags/#signal-processing" class="page__taxonomy-item p-category" rel="tag">signal-processing</a><span class="sep">, </span>
    
      <a href="/tags/#speech-enhancement" class="page__taxonomy-item p-category" rel="tag">speech-enhancement</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0010-reverse-linked-list/" rel="permalink">Reverse Linked List
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          27 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master linked list manipulation through reversal - a fundamental pattern for understanding pointer logic and in-place algorithms.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0010-caching-strategies/" rel="permalink">Caching Strategies for ML Systems
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          26 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Design efficient caching layers for ML systems to reduce latency, save compute costs, and improve user experience at scale.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0010-document-processing/" rel="permalink">Document Processing for Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Garbage In, Garbage Out. The Art of Reading Messy Data.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Voice+Enhancement+%26+Noise+Reduction%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0010-voice-enhancement%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0010-voice-enhancement%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0010-voice-enhancement/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0009-keyword-spotting/" class="pagination--pager" title="Real-time Keyword Spotting">Previous</a>
    
    
      <a href="/speech-tech/0011-speech-separation/" class="pagination--pager" title="Speech Separation">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
