<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Speech Separation - Arun Baby</title>
<meta name="description" content="Separate overlapping speakers with 99%+ accuracy: Deep learning solves the cocktail party problem for meeting transcription and voice assistants.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Speech Separation">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0011-speech-separation/">


  <meta property="og:description" content="Separate overlapping speakers with 99%+ accuracy: Deep learning solves the cocktail party problem for meeting transcription and voice assistants.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Speech Separation">
  <meta name="twitter:description" content="Separate overlapping speakers with 99%+ accuracy: Deep learning solves the cocktail party problem for meeting transcription and voice assistants.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0011-speech-separation/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0011-speech-separation/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Speech Separation">
    <meta itemprop="description" content="Separate overlapping speakers with 99%+ accuracy: Deep learning solves the cocktail party problem for meeting transcription and voice assistants.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0011-speech-separation/" itemprop="url">Speech Separation
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#problem-statement">Problem Statement</a><ul><li><a href="#the-cocktail-party-problem">The Cocktail Party Problem</a></li><li><a href="#problem-formulation">Problem Formulation</a></li></ul></li><li><a href="#understanding-speech-separation">Understanding Speech Separation</a><ul><li><a href="#why-is-speech-separation-hard">Why is Speech Separation Hard?</a></li><li><a href="#the-human-cocktail-party-effect">The Human Cocktail Party Effect</a></li><li><a href="#the-core-mathematical-challenge">The Core Mathematical Challenge</a></li><li><a href="#challenges">Challenges</a></li><li><a href="#traditional-approaches">Traditional Approaches</a></li><li><a href="#deep-learning-revolution">Deep Learning Revolution</a></li></ul></li><li><a href="#solution-1-conv-tasnet-architecture">Solution 1: Conv-TasNet Architecture</a><ul><li><a href="#architecture-overview">Architecture Overview</a></li><li><a href="#implementation">Implementation</a></li><li><a href="#training-with-permutation-invariant-loss">Training with Permutation Invariant Loss</a></li></ul></li><li><a href="#evaluation-metrics">Evaluation Metrics</a><ul><li><a href="#signal-to-distortion-ratio-sdr">Signal-to-Distortion Ratio (SDR)</a></li></ul></li><li><a href="#real-time-separation-pipeline">Real-Time Separation Pipeline</a><ul><li><a href="#streaming-speech-separation">Streaming Speech Separation</a></li></ul></li><li><a href="#integration-with-downstream-tasks">Integration with Downstream Tasks</a><ul><li><a href="#speech-separation--asr-pipeline">Speech Separation + ASR Pipeline</a></li></ul></li><li><a href="#advanced-topics">Advanced Topics</a><ul><li><a href="#unknown-number-of-speakers">Unknown Number of Speakers</a></li><li><a href="#multi-channel-separation">Multi-Channel Separation</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Separate overlapping speakers with 99%+ accuracy: Deep learning solves the cocktail party problem for meeting transcription and voice assistants.</strong></p>

<h2 id="problem-statement">Problem Statement</h2>

<p><strong>Speech Separation</strong> (also called <strong>Source Separation</strong> or <strong>Speaker Separation</strong>) is the task of isolating individual speech sources from a mixture of overlapping speakers.</p>

<h3 id="the-cocktail-party-problem">The Cocktail Party Problem</h3>

<p>Humans can focus on a single speaker in a noisy, multi-speaker environment (like a cocktail party). Teaching machines to do the same is a fundamental challenge in speech processing.</p>

<p><strong>Applications:</strong></p>
<ul>
  <li>Meeting transcription with overlapping speech</li>
  <li>Voice assistants in multi-speaker environments</li>
  <li>Hearing aids for selective attention</li>
  <li>Call center audio analysis</li>
  <li>Video conferencing quality improvement</li>
</ul>

<h3 id="problem-formulation">Problem Formulation</h3>

<p><strong>Input:</strong> Mixed audio with N speakers 
<strong>Output:</strong> N separated audio streams, one per speaker</p>

<p>``
Mixed Audio:
 Speaker 1 + Speaker 2 + … + Speaker N + Noise</p>

<p>Goal:
 → Separated Speaker 1
 → Separated Speaker 2
 → …
 → Separated Speaker N
``</p>

<hr />

<h2 id="understanding-speech-separation">Understanding Speech Separation</h2>

<h3 id="why-is-speech-separation-hard">Why is Speech Separation Hard?</h3>

<p>Let’s understand the fundamental challenge with a simple analogy:</p>

<p><strong>The Paint Mixing Problem</strong></p>

<p>Imagine you have:</p>
<ul>
  <li>Red paint (Speaker 1)</li>
  <li>Blue paint (Speaker 2)</li>
  <li>You mix them → Purple paint (Mixed audio)</li>
</ul>

<p><strong>Challenge</strong>: Given purple paint, separate back into red and blue!</p>

<p>This seems impossible because mixing is <strong>information-destructive</strong>. But speech separation works because:</p>

<ol>
  <li><strong>Speech has structure</strong>: Not random noise, but patterns (phonemes, pitch, timing)</li>
  <li><strong>Speakers differ</strong>: Different voice characteristics (pitch, timbre, accent)</li>
  <li><strong>Deep learning</strong>: Can learn these patterns from thousands of examples</li>
</ol>

<h3 id="the-human-cocktail-party-effect">The Human Cocktail Party Effect</h3>

<p>At a party with multiple conversations, you can focus on one person and “tune out” others. How?</p>

<p><strong>Human brain uses:</strong></p>
<ul>
  <li><strong>Spatial cues</strong>: Sound comes from different directions</li>
  <li><strong>Voice characteristics</strong>: Pitch, timbre, speaking style</li>
  <li><strong>Linguistic context</strong>: Grammar, meaning help predict words</li>
  <li><strong>Visual cues</strong>: Lip reading, body language</li>
</ul>

<p><strong>ML models use:</strong></p>
<ul>
  <li>❌ No spatial cues (single microphone input)</li>
  <li>✅ Voice characteristics (learned from data)</li>
  <li>✅ Temporal patterns (speaking rhythm)</li>
  <li>✅ Spectral patterns (frequency differences)</li>
</ul>

<h3 id="the-core-mathematical-challenge">The Core Mathematical Challenge</h3>

<p><strong>Input</strong>: Mixed waveform <code class="language-plaintext highlighter-rouge">M(t) = S1(t) + S2(t) + ... + Sn(t)</code></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">M(t)</code>: What we hear (mixture)</li>
  <li><code class="language-plaintext highlighter-rouge">S1(t), S2(t), ...</code>: Individual speakers (what we want)</li>
</ul>

<p><strong>Goal</strong>: Find a function <code class="language-plaintext highlighter-rouge">f</code> such that:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">f(M)</code> → <code class="language-plaintext highlighter-rouge">[S1, S2, ..., Sn]</code></li>
</ul>

<p><strong>Why this is hard:</strong></p>
<ol>
  <li><strong>Underdetermined problem</strong>: One equation (mixture), N unknowns (sources)</li>
  <li><strong>Non-linear mixing</strong>: In reality, it’s not just addition (room acoustics, etc.)</li>
  <li><strong>Unknown N</strong>: We often don’t know how many speakers there are</li>
  <li><strong>Permutation ambiguity</strong>: Output order doesn’t matter (Speaker 1 could be output 2)</li>
</ol>

<h3 id="challenges">Challenges</h3>

<p><strong>1. Permutation Problem - The Hardest Part</strong></p>

<p>When you train a model:</p>

<p>``
Attempt 1:
Ground truth: [Speaker A, Speaker B]
Model output: [Speaker A, Speaker B] ✓ Matches!</p>

<p>Attempt 2:
Ground truth: [Speaker A, Speaker B] 
Model output: [Speaker B, Speaker A] ✓ Also correct! Just different order!
``</p>

<p><strong>The problem</strong>: Standard loss (MSE) would say Attempt 2 is wrong!</p>

<p>``python</p>
<h1 id="this-would-incorrectly-penalize-attempt-2">This would incorrectly penalize Attempt 2</h1>
<p>loss = mse(output[0], speaker_A) + mse(output[1], speaker_B)
``</p>

<p><strong>Solution</strong>: Try all permutations, use best one (Permutation Invariant Training)</p>

<p>``python</p>
<h1 id="try-both-orderings-pick-better-one">Try both orderings, pick better one</h1>
<p>loss1 = mse(output[0], speaker_A) + mse(output[1], speaker_B)
loss2 = mse(output[0], speaker_B) + mse(output[1], speaker_A)
loss = min(loss1, loss2) # Use better permutation
``</p>

<p><strong>2. Number of Speakers</strong></p>

<table>
  <thead>
    <tr>
      <th>Scenario</th>
      <th>Difficulty</th>
      <th>Solution</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Fixed N (always 2 speakers)</td>
      <td>Easy</td>
      <td>Train model for N=2</td>
    </tr>
    <tr>
      <td>Variable N (2-5 speakers)</td>
      <td>Hard</td>
      <td>Separate approaches: 1) Train multiple models, 2) Train one model + speaker counting</td>
    </tr>
    <tr>
      <td>Unknown N</td>
      <td>Very Hard</td>
      <td>Need speaker counting + adaptive separation</td>
    </tr>
  </tbody>
</table>

<p><strong>3. Overlapping Speech</strong></p>

<p>``
Scenario 1: Sequential (Easy)
Time: 0s 1s 2s 3s 4s
Speaker A: “Hello” 
Speaker B: “Hi”
 ↑ No overlap, trivial!</p>

<p>Scenario 2: Partial Overlap (Medium)
Time: 0s 1s 2s 3s 4s
Speaker A: “Hello there”
Speaker B: “Hi how are you”
 ↑ Some overlap</p>

<p>Scenario 3: Complete Overlap (Hard)
Time: 0s 1s 2s 3s 4s
Speaker A: “Hello there”
Speaker B: “Hi how are you”
 ↑ Both speaking simultaneously!
``</p>

<p><strong>Why complete overlap is hard:</strong></p>
<ul>
  <li>Maximum information loss</li>
  <li>Voices blend in frequency domain</li>
  <li>Harder to find distinguishing features</li>
</ul>

<p><strong>4. Quality Metrics</strong></p>

<p>How do we measure separation quality?</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>What it Measures</th>
      <th>Good Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>SDR</strong> (Signal-to-Distortion Ratio)</td>
      <td>Overall quality</td>
      <td>&gt; 10 dB</td>
    </tr>
    <tr>
      <td><strong>SIR</strong> (Signal-to-Interference)</td>
      <td>How well other speakers removed</td>
      <td>&gt; 15 dB</td>
    </tr>
    <tr>
      <td><strong>SAR</strong> (Signal-to-Artifacts)</td>
      <td>Artificial noise introduced</td>
      <td>&gt; 10 dB</td>
    </tr>
    <tr>
      <td><strong>SI-SDR</strong> (Scale-Invariant SDR)</td>
      <td>Quality regardless of volume</td>
      <td>&gt; 15 dB</td>
    </tr>
  </tbody>
</table>

<p><strong>Intuition</strong>: Higher dB = Better separation</p>

<p><code class="language-plaintext highlighter-rouge">
SI-SDR = 0 dB → No separation (output = input)
SI-SDR = 10 dB → Good separation (10x better signal)
SI-SDR = 20 dB → Excellent (100x better signal!)
</code></p>

<h3 id="traditional-approaches">Traditional Approaches</h3>

<p><strong>Independent Component Analysis (ICA):</strong></p>
<ul>
  <li>Assumes statistical independence</li>
  <li>Works for determined/overdetermined cases</li>
  <li>Limited by linear mixing assumption</li>
</ul>

<p><strong>Beamforming:</strong></p>
<ul>
  <li>Uses spatial information from microphone array</li>
  <li>Requires known speaker locations</li>
  <li>Hardware-dependent</li>
</ul>

<p><strong>Non-Negative Matrix Factorization (NMF):</strong></p>
<ul>
  <li>Factorizes spectrogram into basis and activation</li>
  <li>Interpretable but limited capacity</li>
</ul>

<h3 id="deep-learning-revolution">Deep Learning Revolution</h3>

<p>Modern approaches use end-to-end deep learning:</p>
<ul>
  <li><strong>TasNet</strong> (Time-domain Audio Separation Network)</li>
  <li><strong>Conv-TasNet</strong> (Convolutional TasNet)</li>
  <li><strong>Dual-Path RNN</strong></li>
  <li><strong>SepFormer</strong> (Transformer-based)</li>
</ul>

<hr />

<h2 id="solution-1-conv-tasnet-architecture">Solution 1: Conv-TasNet Architecture</h2>

<h3 id="architecture-overview">Architecture Overview</h3>

<p>Conv-TasNet is the gold standard for speech separation:</p>

<p><code class="language-plaintext highlighter-rouge">
┌──────────────────────────────────────────────────────────┐
│ CONV-TASNET ARCHITECTURE │
├──────────────────────────────────────────────────────────┤
│ │
│ Input Waveform │
│ [batch, time] │
│ │ │
│ ▼ │
│ ┌─────────────┐ │
│ │ Encoder │ (1D Conv) │
│ │ 512 filters│ Learns time-domain basis functions │
│ └──────┬──────┘ │
│ │ │
│ ▼ │
│ [batch, 512, time'] │
│ │ │
│ ▼ │
│ ┌─────────────┐ │
│ │ Separator │ (TCN blocks) │
│ │ Temporal │ Estimates masks for each speaker │
│ │ Convolution│ │
│ │ Network │ │
│ └──────┬──────┘ │
│ │ │
│ ▼ │
│ [batch, n_speakers, 512, time'] │
│ (Masks for each speaker) │
│ │ │
│ ▼ │
│ ┌─────────────┐ │
│ │ Apply Mask │ (Element-wise multiply) │
│ └──────┬──────┘ │
│ │ │
│ ▼ │
│ [batch, n_speakers, 512, time'] │
│ (Masked representations) │
│ │ │
│ ▼ │
│ ┌─────────────┐ │
│ │ Decoder │ (1D Transposed Conv) │
│ │ n_speakers │ Reconstructs waveforms │
│ │ outputs │ │
│ └──────┬──────┘ │
│ │ │
│ ▼ │
│ Separated Waveforms │
│ [batch, n_speakers, time] │
│ │
└──────────────────────────────────────────────────────────┘
</code></p>

<h3 id="implementation">Implementation</h3>

<p>``python
import torch
import torch.nn as nn
import numpy as np</p>

<p>class ConvTasNet(nn.Module):
 “””
 Conv-TasNet for speech separation</p>

<p>Paper: “Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking
 for Speech Separation” (Luo &amp; Mesgarani, 2019)</p>

<p>Architecture:</p>
<ol>
  <li>Encoder: Waveform → learned representation</li>
  <li>Separator: Mask estimation with TCN</li>
  <li>Decoder: Masked representation → waveforms
 “””</li>
</ol>

<p>def <strong>init</strong>(
 self,
 n_src=2,
 n_filters=512,
 kernel_size=16,
 stride=8,
 n_blocks=8,
 n_repeats=3,
 bn_chan=128,
 hid_chan=512,
 skip_chan=128
 ):
 “””
 Args:
 n_src: Number of sources (speakers)
 n_filters: Number of filters in encoder
 kernel_size: Encoder/decoder kernel size
 stride: Encoder/decoder stride
 n_blocks: Number of TCN blocks per repeat
 n_repeats: Number of times to repeat TCN blocks
 bn_chan: Bottleneck channels
 hid_chan: Hidden channels in TCN
 skip_chan: Skip connection channels
 “””
 super().<strong>init</strong>()</p>

<p>self.n_src = n_src</p>

<p># Encoder: waveform → representation
 self.encoder = nn.Conv1d(
 1,
 n_filters,
 kernel_size=kernel_size,
 stride=stride,
 padding=kernel_size // 2,
 bias=False
 )</p>

<p># Separator: Temporal Convolutional Network
 self.separator = TemporalConvNet(
 n_filters,
 n_src,
 n_blocks=n_blocks,
 n_repeats=n_repeats,
 bn_chan=bn_chan,
 hid_chan=hid_chan,
 skip_chan=skip_chan
 )</p>

<p># Decoder: representation → waveform
 self.decoder = nn.ConvTranspose1d(
 n_filters,
 1,
 kernel_size=kernel_size,
 stride=stride,
 padding=kernel_size // 2,
 bias=False
 )</p>

<p>def forward(self, mixture):
 “””
 Separate mixture into sources</p>

<p>Args:
 mixture: [batch, time] mixed waveform</p>

<p>Returns:
 separated: [batch, n_src, time] separated waveforms
 “””
 batch_size = mixture.size(0)</p>

<p># Add channel dimension
 mixture = mixture.unsqueeze(1) # [batch, 1, time]</p>

<p># Encode
 encoded = self.encoder(mixture) # [batch, n_filters, time’]</p>

<p># Estimate masks
 masks = self.separator(encoded) # [batch, n_src, n_filters, time’]</p>

<p># Apply masks
 masked = encoded.unsqueeze(1) * masks # [batch, n_src, n_filters, time’]</p>

<p># Decode each source
 separated = []</p>

<p>for src_idx in range(self.n_src):
 src_masked = masked[:, src_idx, :, :] # [batch, n_filters, time’]
 src_waveform = self.decoder(src_masked) # [batch, 1, time]
 separated.append(src_waveform.squeeze(1)) # [batch, time]</p>

<p># Stack sources
 separated = torch.stack(separated, dim=1) # [batch, n_src, time]</p>

<p># Trim to original length
 if separated.size(-1) != mixture.size(-1):
 separated = separated[…, :mixture.size(-1)]</p>

<p>return separated</p>

<p>class TemporalConvNet(nn.Module):
 “””
 Temporal Convolutional Network for mask estimation</p>

<p>Stack of dilated 1D conv blocks with skip connections
 “””</p>

<p>def <strong>init</strong>(
 self,
 n_filters,
 n_src,
 n_blocks=8,
 n_repeats=3,
 bn_chan=128,
 hid_chan=512,
 skip_chan=128
 ):
 super().<strong>init</strong>()</p>

<p># Layer normalization
 self.layer_norm = nn.GroupNorm(1, n_filters)</p>

<p># Bottleneck (reduce dimensionality)
 self.bottleneck = nn.Conv1d(n_filters, bn_chan, 1)</p>

<p># TCN blocks
 self.tcn_blocks = nn.ModuleList()</p>

<p>for r in range(n_repeats):
 for b in range(n_blocks):
 dilation = 2 ** b
 self.tcn_blocks.append(
 TCNBlock(
 bn_chan,
 hid_chan,
 skip_chan,
 kernel_size=3,
 dilation=dilation
 )
 )</p>

<p># Output projection
 self.output = nn.Sequential(
 nn.PReLU(),
 nn.Conv1d(skip_chan, n_filters * n_src, 1),
 )</p>

<p>self.n_filters = n_filters
 self.n_src = n_src</p>

<p>def forward(self, x):
 “””
 Estimate masks for each source</p>

<p>Args:
 x: [batch, n_filters, time’]</p>

<p>Returns:
 masks: [batch, n_src, n_filters, time’]
 “””
 batch_size, n_filters, time = x.size()</p>

<p># Normalize
 x = self.layer_norm(x)</p>

<p># Bottleneck
 x = self.bottleneck(x) # [batch, bn_chan, time’]</p>

<p># Accumulate skip connections
 skip_sum = 0</p>

<p>for block in self.tcn_blocks:
 x, skip = block(x)
 skip_sum = skip_sum + skip</p>

<p># Output masks
 masks = self.output(skip_sum) # [batch, n_filters * n_src, time’]</p>

<p># Reshape to [batch, n_src, n_filters, time’]
 masks = masks.view(batch_size, self.n_src, self.n_filters, time)</p>

<p># Apply non-linearity (ReLU for masking)
 masks = torch.relu(masks)</p>

<p>return masks</p>

<p>class TCNBlock(nn.Module):
 “””
 Single TCN block with dilated depthwise-separable convolution
 “””</p>

<p>def <strong>init</strong>(self, in_chan, hid_chan, skip_chan, kernel_size=3, dilation=1):
 super().<strong>init</strong>()</p>

<p># 1x1 conv
 self.conv1x1_1 = nn.Conv1d(in_chan, hid_chan, 1)
 self.prelu1 = nn.PReLU()
 self.norm1 = nn.GroupNorm(1, hid_chan)</p>

<p># Depthwise conv with dilation
 self.depthwise_conv = nn.Conv1d(
 hid_chan,
 hid_chan,
 kernel_size,
 padding=(kernel_size - 1) * dilation // 2,
 dilation=dilation,
 groups=hid_chan # Depthwise
 )
 self.prelu2 = nn.PReLU()
 self.norm2 = nn.GroupNorm(1, hid_chan)</p>

<p># 1x1 conv
 self.conv1x1_2 = nn.Conv1d(hid_chan, in_chan, 1)</p>

<p># Skip connection
 self.skip_conv = nn.Conv1d(hid_chan, skip_chan, 1)</p>

<p>def forward(self, x):
 “””
 Args:
 x: [batch, in_chan, time]</p>

<p>Returns:
 output: [batch, in_chan, time]
 skip: [batch, skip_chan, time]
 “””
 residual = x</p>

<p># 1x1 conv
 x = self.conv1x1_1(x)
 x = self.prelu1(x)
 x = self.norm1(x)</p>

<p># Depthwise conv
 x = self.depthwise_conv(x)
 x = self.prelu2(x)
 x = self.norm2(x)</p>

<p># Skip connection
 skip = self.skip_conv(x)</p>

<p># 1x1 conv
 x = self.conv1x1_2(x)</p>

<p># Residual connection
 output = x + residual</p>

<p>return output, skip</p>

<h1 id="example-usage">Example usage</h1>
<p>model = ConvTasNet(n_src=2, n_filters=512)</p>

<h1 id="mixed-waveform-2-speakers">Mixed waveform (2 speakers)</h1>
<p>mixture = torch.randn(4, 16000) # [batch=4, time=16000 (1 second at 16kHz)]</p>

<h1 id="separate">Separate</h1>
<p>separated = model(mixture) # [4, 2, 16000]</p>

<p>print(f”Input shape: {mixture.shape}”)
print(f”Output shape: {separated.shape}”)
print(f”Separated speaker 1: {separated[:, 0, :].shape}”)
print(f”Separated speaker 2: {separated[:, 1, :].shape}”)
``</p>

<h3 id="training-with-permutation-invariant-loss">Training with Permutation Invariant Loss</h3>

<p>``python
import torch
import torch.nn as nn
import torch.nn.functional as F</p>

<p>class PermutationInvariantLoss(nn.Module):
 “””
 Permutation Invariant Training (PIT) loss</p>

<p>Problem: Model outputs are in arbitrary order
 Solution: Try all permutations, use best one</p>

<p>For 2 speakers:</p>
<ul>
  <li>Try (output1→target1, output2→target2)</li>
  <li>Try (output1→target2, output2→target1)</li>
  <li>Use permutation with lower loss
 “””</li>
</ul>

<p>def <strong>init</strong>(self, loss_fn=’si_sdr’):
 super().<strong>init</strong>()
 self.loss_fn = loss_fn</p>

<p>def forward(self, estimated, target):
 “””
 Compute PIT loss</p>

<p>Args:
 estimated: [batch, n_src, time]
 target: [batch, n_src, time]</p>

<p>Returns:
 loss: scalar
 “””
 batch_size, n_src, time = estimated.size()</p>

<p># Generate all permutations
 import itertools
 perms = list(itertools.permutations(range(n_src)))</p>

<p># Compute loss for each permutation
 perm_losses = []</p>

<p>for perm in perms:
 # Reorder estimated according to permutation
 estimated_perm = estimated[:, perm, :]</p>

<p># Compute loss
 if self.loss_fn == ‘si_sdr’:
 loss = self._si_sdr_loss(estimated_perm, target)
 elif self.loss_fn == ‘mse’:
 loss = F.mse_loss(estimated_perm, target)
 else:
 raise ValueError(f”Unknown loss function: {self.loss_fn}”)</p>

<p>perm_losses.append(loss)</p>

<p># Stack losses
 # [n_perms], take minimum (best permutation)
 perm_losses = torch.stack(perm_losses)
 return perm_losses.min()</p>

<p>def _si_sdr_loss(self, estimated, target):
 “””
 Scale-Invariant Signal-to-Distortion Ratio loss</p>

<p>Better than MSE for speech separation
 “””
 # Zero-mean
 estimated = estimated - estimated.mean(dim=-1, keepdim=True)
 target = target - target.mean(dim=-1, keepdim=True)</p>

<p># Project estimated onto target
 dot = (estimated * target).sum(dim=-1, keepdim=True)
 target_energy = (target ** 2).sum(dim=-1, keepdim=True) + 1e-8
 projection = dot * target / target_energy</p>

<p># Noise (estimation error)
 noise = estimated - projection</p>

<p># SI-SDR
 si_sdr = 10 * torch.log10(
 (projection ** 2).sum(dim=-1) / ((noise ** 2).sum(dim=-1) + 1e-8)
 )</p>

<p># Negative for loss (we want to maximize SI-SDR)
 return -si_sdr.mean()</p>

<h1 id="training-loop">Training loop</h1>
<p>model = ConvTasNet(n_src=2)
criterion = PermutationInvariantLoss(loss_fn=’si_sdr’)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)</p>

<p>def train_epoch(model, train_loader, criterion, optimizer):
 “"”Train one epoch”””
 model.train()
 total_loss = 0</p>

<p>for batch_idx, (mixture, target) in enumerate(train_loader):
 # mixture: [batch, time]
 # target: [batch, n_src, time]</p>

<p># Forward
 estimated = model(mixture)</p>

<p># Loss with PIT
 loss = criterion(estimated, target)</p>

<p># Backward
 optimizer.zero_grad()
 loss.backward()</p>

<p># Gradient clipping
 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)</p>

<p>optimizer.step()</p>

<p>total_loss += loss.item()</p>

<p>if batch_idx % 10 == 0:
 print(f”Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}”)</p>

<p>return total_loss / len(train_loader)</p>

<h1 id="train">Train</h1>
<h1 id="for-epoch-in-rangenum_epochs">for epoch in range(num_epochs):</h1>
<h1 id="train_loss--train_epochmodel-train_loader-criterion-optimizer">train_loss = train_epoch(model, train_loader, criterion, optimizer)</h1>
<h1 id="printfepoch-epoch-train-loss--train_loss4f">print(f”Epoch {epoch}: Train Loss = {train_loss:.4f}”)</h1>
<p>``</p>

<hr />

<h2 id="evaluation-metrics">Evaluation Metrics</h2>

<h3 id="signal-to-distortion-ratio-sdr">Signal-to-Distortion Ratio (SDR)</h3>

<p>``python
def compute_sdr(estimated, target):
 “””
 Compute Signal-to-Distortion Ratio</p>

<table>
  <tbody>
    <tr>
      <td>SDR = 10 * log10(</td>
      <td> </td>
      <td>target</td>
      <td> </td>
      <td>^2 /</td>
      <td> </td>
      <td>target - estimated</td>
      <td> </td>
      <td>^2)</td>
    </tr>
  </tbody>
</table>

<p>Higher is better. Good: &gt; 10 dB, Great: &gt; 15 dB
 “””
 target = target - target.mean()
 estimated = estimated - estimated.mean()</p>

<p>signal_power = np.sum(target ** 2)
 distortion = target - estimated
 distortion_power = np.sum(distortion ** 2) + 1e-10</p>

<p>sdr = 10 * np.log10(signal_power / distortion_power)</p>

<p>return sdr</p>

<p>def compute_si_sdr(estimated, target):
 “””
 Compute Scale-Invariant SDR</p>

<p>Invariant to scaling of the signal
 “””
 # Zero-mean
 estimated = estimated - estimated.mean()
 target = target - target.mean()</p>

<p># Project estimated onto target
 alpha = np.dot(estimated, target) / (np.dot(target, target) + 1e-10)
 projection = alpha * target</p>

<p># Noise
 noise = estimated - projection</p>

<p># SI-SDR
 si_sdr = 10 * np.log10(
 np.sum(projection ** 2) / (np.sum(noise ** 2) + 1e-10)
 )</p>

<p>return si_sdr</p>

<p>def compute_sir(estimated, target, interference):
 “””
 Compute Signal-to-Interference Ratio</p>

<p>Measures how well interfering speakers are suppressed
 “””
 target = target - target.mean()
 estimated = estimated - estimated.mean()</p>

<p># Project estimated onto target
 s_target = np.dot(estimated, target) / (np.dot(target, target) + 1e-10) * target</p>

<p># Interference
 e_interf = 0
 for interf in interference:
 interf = interf - interf.mean()
 e_interf += np.dot(estimated, interf) / (np.dot(interf, interf) + 1e-10) * interf</p>

<p># SIR
 sir = 10 * np.log10(
 np.sum(s_target ** 2) / (np.sum(e_interf ** 2) + 1e-10)
 )</p>

<p>return sir</p>

<h1 id="comprehensive-evaluation">Comprehensive evaluation</h1>
<p>def evaluate_separation(model, test_loader):
 “””
 Evaluate separation quality</p>

<p>Returns metrics for each source
 “””
 model.eval()</p>

<p>all_sdr = []
 all_si_sdr = []</p>

<p>with torch.no_grad():
 for mixture, targets in test_loader:
 # Separate
 estimated = model(mixture)</p>

<p># Convert to numpy
 estimated_np = estimated.cpu().numpy()
 targets_np = targets.cpu().numpy()</p>

<p>batch_size, n_src, time = estimated_np.shape</p>

<p># Compute metrics for each sample and source
 for b in range(batch_size):
 for s in range(n_src):
 est = estimated_np[b, s, :]
 tgt = targets_np[b, s, :]</p>

<p>sdr = compute_sdr(est, tgt)
 si_sdr = compute_si_sdr(est, tgt)</p>

<p>all_sdr.append(sdr)
 all_si_sdr.append(si_sdr)</p>

<p>results = {
 ‘sdr_mean’: np.mean(all_sdr),
 ‘sdr_std’: np.std(all_sdr),
 ‘si_sdr_mean’: np.mean(all_si_sdr),
 ‘si_sdr_std’: np.std(all_si_sdr)
 }</p>

<p>print(“=”<em>60)
 print(“SEPARATION EVALUATION RESULTS”)
 print(“=”</em>60)
 print(f”SDR: {results[‘sdr_mean’]:.2f} ± {results[‘sdr_std’]:.2f} dB”)
 print(f”SI-SDR: {results[‘si_sdr_mean’]:.2f} ± {results[‘si_sdr_std’]:.2f} dB”)
 print(“=”*60)</p>

<p>return results</p>

<h1 id="example">Example</h1>
<h1 id="results--evaluate_separationmodel-test_loader">results = evaluate_separation(model, test_loader)</h1>
<p>``</p>

<hr />

<h2 id="real-time-separation-pipeline">Real-Time Separation Pipeline</h2>

<h3 id="streaming-speech-separation">Streaming Speech Separation</h3>

<p>``python
class StreamingSpeechSeparator:
 “””
 Real-time speech separation for streaming audio</p>

<p>Challenges:</p>
<ul>
  <li>Causal processing (no future context)</li>
  <li>Low latency (&lt; 50ms)</li>
  <li>State management across chunks
 “””</li>
</ul>

<p>def <strong>init</strong>(self, model, chunk_size=4800, overlap=1200):
 “””
 Args:
 model: Trained separation model
 chunk_size: Samples per chunk (300ms at 16kHz)
 overlap: Overlap between chunks (75ms at 16kHz)
 “””
 self.model = model
 self.model.eval()</p>

<p>self.chunk_size = chunk_size
 self.overlap = overlap
 self.hop_size = chunk_size - overlap</p>

<p># Buffer for overlapping
 self.input_buffer = np.zeros(overlap)
 self.output_buffers = [np.zeros(overlap) for _ in range(model.n_src)]</p>

<p>def process_chunk(self, audio_chunk):
 “””
 Process single audio chunk</p>

<p>Args:
 audio_chunk: [chunk_size] numpy array</p>

<p>Returns:
 separated_chunks: list of [hop_size] arrays, one per speaker
 “””
 # Concatenate with buffer
 full_chunk = np.concatenate([self.input_buffer, audio_chunk])</p>

<p># Ensure correct size
 if len(full_chunk) &lt; self.chunk_size:
 full_chunk = np.pad(
 full_chunk,
 (0, self.chunk_size - len(full_chunk)),
 mode=’constant’
 )</p>

<p># Convert to tensor
 with torch.no_grad():
 chunk_tensor = torch.from_numpy(full_chunk).float().unsqueeze(0)</p>

<p># Separate
 separated = self.model(chunk_tensor) # [1, n_src, chunk_size]</p>

<p># Convert back to numpy
 separated_np = separated[0].cpu().numpy() # [n_src, chunk_size]</p>

<p># Overlap-add
 result_chunks = []</p>

<p>for src_idx in range(self.model.n_src):
 src_audio = separated_np[src_idx]</p>

<p># Add overlap from previous chunk
 src_audio[:self.overlap] += self.output_buffers[src_idx]</p>

<p># Extract output (without overlap)
 output_chunk = src_audio[:self.hop_size]
 result_chunks.append(output_chunk)</p>

<p># Save overlap for next chunk
 self.output_buffers[src_idx] = src_audio[-self.overlap:]</p>

<p># Update input buffer
 self.input_buffer = audio_chunk[-self.overlap:]</p>

<p>return result_chunks</p>

<p>def reset(self):
 “"”Reset state for new stream”””
 self.input_buffer = np.zeros(self.overlap)
 self.output_buffers = [np.zeros(self.overlap) for _ in range(self.model.n_src)]</p>

<h1 id="example-real-time-separation-server">Example: Real-time separation server</h1>
<p>from fastapi import FastAPI, WebSocket
import asyncio</p>

<p>app = FastAPI()</p>

<h1 id="load-model">Load model</h1>
<p>model = ConvTasNet(n_src=2)
model.load_state_dict(torch.load(‘convtasnet_separation.pth’))
separator = StreamingSpeechSeparator(model, chunk_size=4800, overlap=1200)</p>

<p>@app.websocket(“/separate”)
async def websocket_separation(websocket: WebSocket):
 “””
 WebSocket endpoint for real-time separation</p>

<p>Client sends audio chunks, receives separated streams
 “””
 await websocket.accept()</p>

<p>try:
 while True:
 # Receive audio chunk
 data = await websocket.receive_bytes()</p>

<p># Decode audio (assuming 16-bit PCM)
 audio_chunk = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0</p>

<p># Separate
 separated_chunks = separator.process_chunk(audio_chunk)</p>

<p># Send separated streams
 for src_idx, src_chunk in enumerate(separated_chunks):
 # Encode back to 16-bit PCM
 src_bytes = (src_chunk * 32768).astype(np.int16).tobytes()</p>

<p>await websocket.send_json({
 ‘speaker_id’: src_idx,
 ‘audio’: src_bytes.hex()
 })</p>

<p>except Exception as e:
 print(f”WebSocket error: {e}”)
 finally:
 separator.reset()
 await websocket.close()</p>

<h1 id="run-server">Run server</h1>
<h1 id="uvicornrunapp-host0000-port8000">uvicorn.run(app, host=’0.0.0.0’, port=8000)</h1>
<p>``</p>

<hr />

<h2 id="integration-with-downstream-tasks">Integration with Downstream Tasks</h2>

<h3 id="speech-separation--asr-pipeline">Speech Separation + ASR Pipeline</h3>

<p>``python
class SeparationASRPipeline:
 “””
 Combined pipeline: Separate speakers → Transcribe each</p>

<p>Use case: Meeting transcription with overlapping speech
 “””</p>

<p>def <strong>init</strong>(self, separation_model, asr_model):
 self.separator = separation_model
 self.asr = asr_model</p>

<p>def transcribe_multi_speaker(self, audio):
 “””
 Transcribe audio with multiple speakers</p>

<p>Args:
 audio: Mixed audio</p>

<p>Returns:
 List of (speaker_id, transcript) tuples
 “””
 # Separate speakers
 with torch.no_grad():
 audio_tensor = torch.from_numpy(audio).float().unsqueeze(0)
 separated = self.separator(audio_tensor)[0] # [n_src, time]</p>

<p># Transcribe each speaker
 transcripts = []</p>

<p>for speaker_id in range(separated.size(0)):
 speaker_audio = separated[speaker_id].cpu().numpy()</p>

<p># Transcribe
 transcript = self.asr.transcribe(speaker_audio)</p>

<p>transcripts.append({
 ‘speaker_id’: speaker_id,
 ‘transcript’: transcript,
 ‘audio_length_sec’: len(speaker_audio) / 16000
 })</p>

<p>return transcripts</p>

<p>def transcribe_with_diarization(self, audio):
 “””
 Transcribe with speaker diarization</p>

<p>Diarization: Who spoke when?
 Separation: Isolate each speaker’s audio
 ASR: Transcribe each speaker
 “””
 # Separate speakers
 with torch.no_grad():
 audio_tensor = torch.from_numpy(audio).float().unsqueeze(0)
 separated = self.separator(audio_tensor)[0] # [n_src, time]</p>

<p># Speaker diarization on each separated stream
 diarization_results = []</p>

<p>for speaker_id in range(separated.size(0)):
 speaker_audio = separated[speaker_id].cpu().numpy()
 # Voice Activity Detection
 vad_segments = self._detect_voice_activity(speaker_audio)</p>

<p># Transcribe active segments
 for segment in vad_segments:
 start_idx = int(segment[‘start’] * 16000)
 end_idx = int(segment[‘end’] * 16000)</p>

<p>segment_audio = speaker_audio[start_idx:end_idx]
 transcript = self.asr.transcribe(segment_audio)</p>

<p>diarization_results.append({
 ‘speaker_id’: speaker_id,
 ‘start_time’: segment[‘start’],
 ‘end_time’: segment[‘end’],
 ‘transcript’: transcript
 })</p>

<p># Sort by start time
 diarization_results.sort(key=lambda x: x[‘start_time’])</p>

<p>return diarization_results</p>

<p>def _detect_voice_activity(self, audio, frame_duration=0.03):
 “””
 Simple energy-based VAD</p>

<p>Returns list of (start, end) segments with voice activity
 “””
 import librosa</p>

<p># Compute energy
 frame_length = int(frame_duration * 16000)
 energy = librosa.feature.rms(
 y=audio,
 frame_length=frame_length,
 hop_length=frame_length // 2
 )[0]</p>

<p># Threshold
 threshold = np.mean(energy) * 0.5</p>

<p># Find voice segments
 is_voice = energy &gt; threshold</p>

<p>segments = []
 in_segment = False
 start = 0</p>

<p>for i, voice in enumerate(is_voice):
 if voice and not in_segment:
 start = i * frame_duration / 2
 in_segment = True
 elif not voice and in_segment:
 end = i * frame_duration / 2
 segments.append({‘start’: start, ‘end’: end})
 in_segment = False</p>

<p>return segments</p>

<h1 id="example-usage-1">Example usage</h1>
<p>separation_model = ConvTasNet(n_src=2)
separation_model.load_state_dict(torch.load(‘separation_model.pth’))</p>

<h1 id="mock-asr-model">Mock ASR model</h1>
<p>class MockASR:
 def transcribe(self, audio):
 return f”Transcribed {len(audio)} samples”</p>

<p>asr_model = MockASR()</p>

<p>pipeline = SeparationASRPipeline(separation_model, asr_model)</p>

<h1 id="transcribe-multi-speaker-audio">Transcribe multi-speaker audio</h1>
<p>audio = np.random.randn(16000 * 10) # 10 seconds
results = pipeline.transcribe_multi_speaker(audio)</p>

<p>print(“Transcription results:”)
for result in results:
 print(f”Speaker {result[‘speaker_id’]}: {result[‘transcript’]}”)
``</p>

<hr />

<h2 id="advanced-topics">Advanced Topics</h2>

<h3 id="unknown-number-of-speakers">Unknown Number of Speakers</h3>

<p>``python
class AdaptiveSeparationModel(nn.Module):
 “””
 Separate audio with unknown number of speakers</p>

<p>Approach:</p>
<ol>
  <li>Estimate number of speakers</li>
  <li>Separate into estimated number of sources</li>
  <li>Filter empty sources
 “””</li>
</ol>

<p>def <strong>init</strong>(self, max_speakers=10):
 super().<strong>init</strong>()</p>

<p>self.max_speakers = max_speakers</p>

<p># Speaker counting network
 self.speaker_counter = nn.Sequential(
 nn.Conv1d(1, 128, kernel_size=3, stride=2),
 nn.ReLU(),
 nn.Conv1d(128, 256, kernel_size=3, stride=2),
 nn.ReLU(),
 nn.AdaptiveAvgPool1d(1),
 nn.Flatten(),
 nn.Linear(256, max_speakers + 1), # 0 to max_speakers
 nn.Softmax(dim=-1)
 )</p>

<p># Separation models for different numbers of speakers
 self.separators = nn.ModuleList([
 ConvTasNet(n_src=n) for n in range(1, max_speakers + 1)
 ])</p>

<p>def forward(self, mixture):
 “””
 Separate with adaptive number of sources</p>

<p>Args:
 mixture: [batch, time]</p>

<p>Returns:
 separated: list of [batch, time] tensors (one per active speaker)
 “””
 # Estimate number of speakers
 mixture_1d = mixture.unsqueeze(1) # [batch, 1, time]
 speaker_probs = self.speaker_counter(mixture_1d) # [batch, max_speakers + 1]</p>

<p>n_speakers = speaker_probs.argmax(dim=-1) # [batch]</p>

<p># For simplicity, use max in batch (in practice, process per sample)
 max_n_speakers = n_speakers.max().item()</p>

<p>if max_n_speakers == 0:
 return []</p>

<p># Separate using appropriate model
 separator = self.separators[max_n_speakers - 1]
 separated = separator(mixture) # [batch, n_src, time]</p>

<p>return separated</p>

<h1 id="example-1">Example</h1>
<p>model = AdaptiveSeparationModel(max_speakers=5)</p>

<h1 id="test-with-2-speakers">Test with 2 speakers</h1>
<p>mixture = torch.randn(1, 16000)
separated = model(mixture)</p>

<p>print(f”Estimated sources: {separated.size(1)}”)
``</p>

<h3 id="multi-channel-separation">Multi-Channel Separation</h3>

<p>``python
class MultiChannelSeparator(nn.Module):
 “””
 Use multiple microphones for better separation</p>

<p>Microphone array provides spatial information
 “””</p>

<p>def <strong>init</strong>(self, n_channels, n_src):
 super().<strong>init</strong>()</p>

<p>self.n_channels = n_channels
 self.n_src = n_src</p>

<p># Encoder for each channel
 self.encoders = nn.ModuleList([
 nn.Conv1d(1, 256, kernel_size=16, stride=8)
 for _ in range(n_channels)
 ])</p>

<p># Cross-channel attention
 self.cross_channel_attention = nn.MultiheadAttention(
 embed_dim=256 * n_channels,
 num_heads=8
 )</p>

<p># Separator
 self.separator = TemporalConvNet(
 256 * n_channels,
 n_src,
 n_blocks=8,
 n_repeats=3,
 bn_chan=128,
 hid_chan=512,
 skip_chan=128
 )</p>

<p># Decoder
 self.decoder = nn.ConvTranspose1d(256, 1, kernel_size=16, stride=8)</p>

<p>def forward(self, multi_channel_mixture):
 “””
 Separate using multi-channel input</p>

<p>Args:
 multi_channel_mixture: [batch, n_channels, time]</p>

<p>Returns:
 separated: [batch, n_src, time]
 “””
 batch_size, n_channels, time = multi_channel_mixture.size()</p>

<p># Encode each channel
 encoded_channels = []</p>

<p>for ch in range(n_channels):
 ch_audio = multi_channel_mixture[:, ch:ch+1, :] # [batch, 1, time]
 ch_encoded = self.encoders<a href="ch_audio">ch</a> # [batch, 256, time’]
 encoded_channels.append(ch_encoded)</p>

<p># Concatenate channels
 encoded = torch.cat(encoded_channels, dim=1) # [batch, 256 * n_channels, time’]</p>

<p># Cross-channel attention
 # Reshape for attention: [time’, batch, 256 * n_channels]
 encoded_t = encoded.permute(2, 0, 1)
 attended, _ = self.cross_channel_attention(encoded_t, encoded_t, encoded_t)
 attended = attended.permute(1, 2, 0) # [batch, 256 * n_channels, time’]</p>

<p># Separate
 masks = self.separator(attended) # [batch, n_src, 256 * n_channels, time’]</p>

<p># Apply masks and decode
 separated = []</p>

<p>for src_idx in range(self.n_src):
 masked = attended * masks[:, src_idx, :, :]</p>

<p># Take first 256 channels for decoding
 masked_single = masked[:, :256, :]</p>

<p>src_waveform = self.decoder(masked_single).squeeze(1)
 separated.append(src_waveform)</p>

<p>separated = torch.stack(separated, dim=1)</p>

<p>return separated</p>

<h1 id="example-4-microphone-array">Example: 4-microphone array</h1>
<p>model = MultiChannelSeparator(n_channels=4, n_src=2)</p>

<h1 id="4-channel-input">4-channel input</h1>
<p>multi_channel_audio = torch.randn(1, 4, 16000)</p>

<p>separated = model(multi_channel_audio)
print(f”Separated shape: {separated.shape}”) # [1, 2, 16000]
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Conv-TasNet</strong> - State-of-the-art time-domain separation 
✅ <strong>PIT loss</strong> - Handle output permutation problem 
✅ <strong>SI-SDR metric</strong> - Scale-invariant quality measure 
✅ <strong>Real-time streaming</strong> - Chunk-based processing with overlap-add 
✅ <strong>Integration with ASR</strong> - End-to-end meeting transcription</p>

<p><strong>Performance Targets:</strong></p>
<ul>
  <li>SI-SDR improvement: &gt; 15 dB</li>
  <li>Real-time factor: &lt; 0.1 (10x faster than real-time)</li>
  <li>Latency: &lt; 50ms for streaming</li>
  <li>Works with 2-5 overlapping speakers</li>
</ul>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0011-speech-separation/">arunbaby.com/speech-tech/0011-speech-separation</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#cocktail-party-problem" class="page__taxonomy-item p-category" rel="tag">cocktail-party-problem</a><span class="sep">, </span>
    
      <a href="/tags/#conv-tasnet" class="page__taxonomy-item p-category" rel="tag">conv-tasnet</a><span class="sep">, </span>
    
      <a href="/tags/#deep-learning" class="page__taxonomy-item p-category" rel="tag">deep-learning</a><span class="sep">, </span>
    
      <a href="/tags/#multi-speaker" class="page__taxonomy-item p-category" rel="tag">multi-speaker</a><span class="sep">, </span>
    
      <a href="/tags/#source-separation" class="page__taxonomy-item p-category" rel="tag">source-separation</a><span class="sep">, </span>
    
      <a href="/tags/#speaker-diarization" class="page__taxonomy-item p-category" rel="tag">speaker-diarization</a><span class="sep">, </span>
    
      <a href="/tags/#speech-separation" class="page__taxonomy-item p-category" rel="tag">speech-separation</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0011-lru-cache/" rel="permalink">LRU Cache
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          27 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master LRU cache design: O(1) get/put with hash map + doubly linked list. Critical for interviews and production caching systems.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0011-content-delivery-network/" rel="permalink">Content Delivery Networks (CDN)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Design a global CDN for ML systems: Edge caching reduces latency from 500ms to 50ms. Critical for real-time predictions worldwide.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0011-vector-search-for-agents/" rel="permalink">Vector Search for Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Finding a Needle in a High-Dimensional Haystack: The Mathematics of Recall.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Speech+Separation%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0011-speech-separation%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0011-speech-separation%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0011-speech-separation/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0010-voice-enhancement/" class="pagination--pager" title="Voice Enhancement &amp; Noise Reduction">Previous</a>
    
    
      <a href="/speech-tech/0012-multi-speaker-asr/" class="pagination--pager" title="Multi-Speaker ASR">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
