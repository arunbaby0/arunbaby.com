<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Real-time Keyword Spotting - Arun Baby</title>
<meta name="description" content="Build lightweight models that detect specific keywords in audio streams with minimal latency and power consumption for voice interfaces.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Real-time Keyword Spotting">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0009-keyword-spotting/">


  <meta property="og:description" content="Build lightweight models that detect specific keywords in audio streams with minimal latency and power consumption for voice interfaces.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Real-time Keyword Spotting">
  <meta name="twitter:description" content="Build lightweight models that detect specific keywords in audio streams with minimal latency and power consumption for voice interfaces.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0009-keyword-spotting/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T16:05:30+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0009-keyword-spotting/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Real-time Keyword Spotting">
    <meta itemprop="description" content="Build lightweight models that detect specific keywords in audio streams with minimal latency and power consumption for voice interfaces.">
    <meta itemprop="datePublished" content="2025-12-29T16:05:30+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0009-keyword-spotting/" itemprop="url">Real-time Keyword Spotting
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          25 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#problem-formulation">Problem Formulation</a><ul><li><a href="#task-definition">Task Definition</a></li><li><a href="#challenges">Challenges</a></li></ul></li><li><a href="#system-architecture">System Architecture</a></li><li><a href="#model-architectures">Model Architectures</a><ul><li><a href="#approach-1-cnn-based-kws">Approach 1: CNN-based KWS</a></li><li><a href="#approach-2-rnn-based-kws">Approach 2: RNN-based KWS</a></li><li><a href="#approach-3-temporal-convolutional-network">Approach 3: Temporal Convolutional Network</a></li></ul></li><li><a href="#feature-extraction-pipeline">Feature Extraction Pipeline</a></li><li><a href="#training-pipeline">Training Pipeline</a><ul><li><a href="#data-preparation">Data Preparation</a></li><li><a href="#training-loop">Training Loop</a></li></ul></li><li><a href="#deployment-optimization">Deployment Optimization</a><ul><li><a href="#model-quantization">Model Quantization</a></li><li><a href="#tensorflow-lite-conversion">TensorFlow Lite Conversion</a></li></ul></li><li><a href="#real-time-inference">Real-time Inference</a><ul><li><a href="#streaming-kws-system">Streaming KWS System</a></li></ul></li><li><a href="#connection-to-binary-search-dsa">Connection to Binary Search (DSA)</a></li><li><a href="#advanced-model-architectures">Advanced Model Architectures</a><ul><li><a href="#1-attention-based-kws">1. Attention-Based KWS</a></li><li><a href="#2-res-net-based-kws">2. Res-Net Based KWS</a></li><li><a href="#3-transformer-based-kws">3. Transformer-Based KWS</a></li></ul></li><li><a href="#data-augmentation-strategies">Data Augmentation Strategies</a><ul><li><a href="#advanced-audio-augmentation">Advanced Audio Augmentation</a></li></ul></li><li><a href="#production-deployment-patterns">Production Deployment Patterns</a><ul><li><a href="#multi-stage-detection-pipeline">Multi-Stage Detection Pipeline</a></li><li><a href="#on-device-learning">On-Device Learning</a></li></ul></li><li><a href="#real-world-integration-examples">Real-World Integration Examples</a><ul><li><a href="#smart-speaker-integration">Smart Speaker Integration</a></li><li><a href="#mobile-app-integration">Mobile App Integration</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Build lightweight models that detect specific keywords in audio streams with minimal latency and power consumption for voice interfaces.</strong></p>

<h2 id="introduction">Introduction</h2>

<p><strong>Keyword spotting (KWS)</strong> detects specific words or phrases in continuous audio streams, enabling voice-activated interfaces.</p>

<p><strong>Common applications:</strong></p>
<ul>
  <li>Wake word detection (â€œHey Siriâ€, â€œAlexaâ€, â€œOK Googleâ€)</li>
  <li>Voice commands (â€œPlayâ€, â€œStopâ€, â€œNextâ€)</li>
  <li>Accessibility features (voice navigation)</li>
  <li>Security (speaker verification)</li>
</ul>

<p><strong>Key requirements:</strong></p>
<ul>
  <li><strong>Ultra-low latency:</strong> &lt; 50ms detection time</li>
  <li><strong>Low power:</strong> Run continuously on battery</li>
  <li><strong>Small model:</strong> Fit on edge devices (&lt; 1MB)</li>
  <li><strong>High accuracy:</strong> &lt; 1% false acceptance rate</li>
  <li><strong>Noise robust:</strong> Work in real-world conditions</li>
</ul>

<hr />

<h2 id="problem-formulation">Problem Formulation</h2>

<h3 id="task-definition">Task Definition</h3>

<p>Given audio input, classify whether a target keyword is present:</p>

<p>``
Input: Audio waveform (e.g., 1 second, 16kHz = 16,000 samples)
Output: {keyword, no_keyword}</p>

<p>Example:
 Audio: â€œHey Siri, whatâ€™s the weather?â€
 Output: keyword=â€hey_siriâ€, timestamp=0.0s
``</p>

<h3 id="challenges">Challenges</h3>

<ol>
  <li><strong>Always-on constraint:</strong> Must run 24/7 without draining battery</li>
  <li><strong>False positives:</strong> Accidental activations frustrate users</li>
  <li><strong>False negatives:</strong> Missed detections break user experience</li>
  <li><strong>Noise robustness:</strong> Background noise, music, TV</li>
  <li><strong>Speaker variability:</strong> Different accents, ages, genders</li>
</ol>

<hr />

<h2 id="system-architecture">System Architecture</h2>

<p><code class="language-plaintext highlighter-rouge">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Microphone Input â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚ Continuous audio stream
 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Audio Preprocessing â”‚
 â”‚ - Noise reduction â”‚
 â”‚ - Normalization â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Feature Extraction â”‚
 â”‚ - MFCC / Mel-spec â”‚
 â”‚ - Sliding window â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ KWS Model (Tiny NN) â”‚
 â”‚ - CNN / RNN / TCN â”‚
 â”‚ - &lt; 1MB, &lt; 10ms â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Post-processing â”‚
 â”‚ - Threshold / Smooth â”‚
 â”‚ - Reject false pos â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Trigger Event â”‚
 â”‚ (Wake system) â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></p>

<hr />

<h2 id="model-architectures">Model Architectures</h2>

<h3 id="approach-1-cnn-based-kws">Approach 1: CNN-based KWS</h3>

<p><strong>Small convolutional network on spectrograms</strong></p>

<p>``python
import torch
import torch.nn as nn</p>

<p>class KeywordSpottingCNN(nn.Module):
 â€œâ€â€
 Lightweight CNN for keyword spotting</p>

<p>Input: Mel-spectrogram (n_mels, time_steps)
 Output: Keyword confidence score</p>

<p>Model size: ~100KB
 Inference time: ~5ms on CPU
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, n_mels=40, n_classes=2):
 super().<strong>init</strong>()</p>

<p># Convolutional layers
 self.conv1 = nn.Sequential(
 nn.Conv2d(1, 64, kernel_size=3, padding=1),
 nn.BatchNorm2d(64),
 nn.ReLU(),
 nn.MaxPool2d(2)
 )</p>

<p>self.conv2 = nn.Sequential(
 nn.Conv2d(64, 64, kernel_size=3, padding=1),
 nn.BatchNorm2d(64),
 nn.ReLU(),
 nn.MaxPool2d(2)
 )</p>

<p># Global average pooling
 self.gap = nn.AdaptiveAvgPool2d((1, 1))</p>

<p># Classifier
 self.fc = nn.Linear(64, n_classes)</p>

<p>def forward(self, x):
 â€œâ€â€
 Args:
 x: [batch, 1, n_mels, time_steps]</p>

<p>Returns:
 [batch, n_classes]
 â€œâ€â€
 x = self.conv1(x)
 x = self.conv2(x)
 x = self.gap(x)
 x = x.view(x.size(0), -1)
 x = self.fc(x)
 return x</p>

<h1 id="create-model">Create model</h1>
<p>model = KeywordSpottingCNN(n_mels=40, n_classes=2)</p>

<h1 id="count-parameters">Count parameters</h1>
<p>n_params = sum(p.numel() for p in model.parameters())
print(fâ€Model parameters: {n_params:,}â€) # ~30K parameters</p>

<h1 id="estimate-model-size">Estimate model size</h1>
<p>model_size_mb = n_params * 4 / (1024 ** 2) # 4 bytes per float32
print(fâ€Model size: {model_size_mb:.2f} MBâ€)
``</p>

<h3 id="approach-2-rnn-based-kws">Approach 2: RNN-based KWS</h3>

<p><strong>Temporal modeling with GRU</strong></p>

<p>``python
class KeywordSpottingGRU(nn.Module):
 â€œâ€â€
 GRU-based keyword spotting</p>

<p>Better for temporal patterns, slightly larger
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, n_mels=40, hidden_size=64, n_layers=2, n_classes=2):
 super().<strong>init</strong>()</p>

<p>self.gru = nn.GRU(
 input_size=n_mels,
 hidden_size=hidden_size,
 num_layers=n_layers,
 batch_first=True,
 bidirectional=False # Unidirectional for streaming
 )</p>

<p>self.fc = nn.Linear(hidden_size, n_classes)</p>

<p>def forward(self, x):
 â€œâ€â€
 Args:
 x: [batch, time_steps, n_mels]</p>

<p>Returns:
 [batch, n_classes]
 â€œâ€â€
 # GRU forward pass
 out, hidden = self.gru(x)</p>

<p># Use last hidden state
 x = out[:, -1, :]</p>

<p># Classifier
 x = self.fc(x)
 return x</p>

<p>model_gru = KeywordSpottingGRU(n_mels=40, hidden_size=64)
``</p>

<h3 id="approach-3-temporal-convolutional-network">Approach 3: Temporal Convolutional Network</h3>

<p><strong>Efficient temporal modeling</strong></p>

<p>``python
class TemporalBlock(nn.Module):
 â€œ"â€Single temporal convolutional blockâ€â€â€</p>

<p>def <strong>init</strong>(self, n_inputs, n_outputs, kernel_size, dilation):
 super().<strong>init</strong>()</p>

<p>self.conv1 = nn.Conv1d(
 n_inputs, n_outputs, kernel_size,
 padding=(kernel_size-1) * dilation // 2,
 dilation=dilation
 )
 self.relu = nn.ReLU()
 self.dropout = nn.Dropout(0.2)</p>

<p># Residual connection
 self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) <br />
 if n_inputs != n_outputs else None</p>

<p>def forward(self, x):
 out = self.conv1(x)
 out = self.relu(out)
 out = self.dropout(out)</p>

<p>res = x if self.downsample is None else self.downsample(x)
 return out + res</p>

<p>class KeywordSpottingTCN(nn.Module):
 â€œâ€â€
 Temporal Convolutional Network for KWS</p>

<p>Combines efficiency of CNNs with temporal modeling
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, n_mels=40, n_classes=2):
 super().<strong>init</strong>()</p>

<p>self.blocks = nn.Sequential(
 TemporalBlock(n_mels, 64, kernel_size=3, dilation=1),
 TemporalBlock(64, 64, kernel_size=3, dilation=2),
 TemporalBlock(64, 64, kernel_size=3, dilation=4),
 )</p>

<p>self.gap = nn.AdaptiveAvgPool1d(1)
 self.fc = nn.Linear(64, n_classes)</p>

<p>def forward(self, x):
 â€œâ€â€
 Args:
 x: [batch, time_steps, n_mels]</p>

<p>Returns:
 [batch, n_classes]
 â€œâ€â€
 # Transpose for conv1d: [batch, n_mels, time_steps]
 x = x.transpose(1, 2)</p>

<p># Temporal blocks
 x = self.blocks(x)</p>

<p># Global average pooling
 x = self.gap(x).squeeze(-1)</p>

<p># Classifier
 x = self.fc(x)
 return x</p>

<p>model_tcn = KeywordSpottingTCN(n_mels=40)
``</p>

<hr />

<h2 id="feature-extraction-pipeline">Feature Extraction Pipeline</h2>

<p>``python
import librosa
import numpy as np</p>

<p>class KeywordSpottingFeatureExtractor:
 â€œâ€â€
 Extract features for keyword spotting</p>

<p>Optimized for real-time processing
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, sample_rate=16000, window_size_ms=30, 
 hop_size_ms=10, n_mels=40):
 self.sample_rate = sample_rate
 self.n_fft = int(sample_rate * window_size_ms / 1000)
 self.hop_length = int(sample_rate * hop_size_ms / 1000)
 self.n_mels = n_mels</p>

<p># Precompute mel filterbank
 self.mel_basis = librosa.filters.mel(
 sr=sample_rate,
 n_fft=self.n_fft,
 n_mels=n_mels,
 fmin=0,
 fmax=sample_rate / 2
 )</p>

<p>def extract(self, audio):
 â€œâ€â€
 Extract mel-spectrogram features</p>

<p>Args:
 audio: Audio samples [n_samples]</p>

<p>Returns:
 Mel-spectrogram [n_mels, time_steps]
 â€œâ€â€
 # Compute STFT
 stft = librosa.stft(
 audio,
 n_fft=self.n_fft,
 hop_length=self.hop_length,
 window=â€™hannâ€™
 )</p>

<p># Power spectrogram
 power = np.abs(stft) ** 2</p>

<p># Apply mel filterbank on power
 mel_power = np.dot(self.mel_basis, power)</p>

<p># Log compression (power â†’ dB)
 mel_db = librosa.power_to_db(mel_power, ref=np.max)</p>

<p>return mel_db</p>

<p>def extract_from_stream(self, audio_chunk):
 â€œâ€â€
 Extract features from streaming audio</p>

<p>Optimized for low latency
 â€œâ€â€
 return self.extract(audio_chunk)</p>

<h1 id="usage">Usage</h1>
<p>extractor = KeywordSpottingFeatureExtractor(sample_rate=16000)</p>

<h1 id="extract-features-from-1-second-audio">Extract features from 1-second audio</h1>
<p>audio = np.random.randn(16000)
features = extractor.extract(audio)
print(fâ€Feature shape: {features.shape}â€) # (40, 101)
``</p>

<hr />

<h2 id="training-pipeline">Training Pipeline</h2>

<h3 id="data-preparation">Data Preparation</h3>

<p>``python
import torch
from torch.utils.data import Dataset, DataLoader
import librosa
import numpy as np</p>

<p>class KeywordSpottingDataset(Dataset):
 â€œâ€â€
 Dataset for keyword spotting training</p>

<p>Handles positive (keyword) and negative (non-keyword) examples
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, audio_files, labels, feature_extractor, 
 augment=True):
 self.audio_files = audio_files
 self.labels = labels
 self.feature_extractor = feature_extractor
 self.augment = augment</p>

<p>def <strong>len</strong>(self):
 return len(self.audio_files)</p>

<p>def <strong>getitem</strong>(self, idx):
 # Load audio
 audio, sr = librosa.load(
 self.audio_files[idx],
 sr=self.feature_extractor.sample_rate
 )</p>

<p># Pad or trim to 1 second
 target_length = self.feature_extractor.sample_rate
 if len(audio) &lt; target_length:
 audio = np.pad(audio, (0, target_length - len(audio)))
 else:
 audio = audio[:target_length]</p>

<p># Data augmentation
 if self.augment:
 audio = self._augment(audio)</p>

<p># Extract features
 features = self.feature_extractor.extract(audio)</p>

<p># Convert to tensor
 features = torch.FloatTensor(features).unsqueeze(0) # Add channel dim
 label = torch.LongTensor([self.labels[idx]])</p>

<p>return features, label</p>

<p>def _augment(self, audio):
 â€œâ€â€
 Data augmentation</p>

<ul>
  <li>Add noise</li>
  <li>Time shift</li>
  <li>Speed perturbation
 â€œâ€â€
 # Add background noise
 noise_level = np.random.uniform(0, 0.005)
 noise = np.random.randn(len(audio)) * noise_level
 audio = audio + noise</li>
</ul>

<p># Time shift
 shift = np.random.randint(-1600, 1600) # Â±100ms at 16kHz
 audio = np.roll(audio, shift)</p>

<p># Speed perturbation (simplified)
 speed_factor = np.random.uniform(0.9, 1.1)
 # In practice, use librosa.effects.time_stretch</p>

<p>return audio</p>

<h1 id="create-dataset">Create dataset</h1>
<p>dataset = KeywordSpottingDataset(
 audio_files=[â€˜audio1.wavâ€™, â€˜audio2.wavâ€™, â€¦],
 labels=[1, 0, â€¦], # 1=keyword, 0=no keyword
 feature_extractor=extractor,
 augment=True
)</p>

<p>dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
``</p>

<h3 id="training-loop">Training Loop</h3>

<p>``python
import torch
import torch.nn as nn</p>

<p>def train_keyword_spotting_model(model, train_loader, val_loader, 
 n_epochs=50, device=â€™cudaâ€™):
 â€œâ€â€
 Train keyword spotting model</p>

<p>Args:
 model: PyTorch model
 train_loader: Training data loader
 val_loader: Validation data loader
 n_epochs: Number of epochs
 device: Device to train on
 â€œâ€â€
 model = model.to(device)</p>

<p># Loss and optimizer
 criterion = nn.CrossEntropyLoss()
 optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
 scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
 optimizer, mode=â€™maxâ€™, patience=5
 )</p>

<p>best_val_acc = 0</p>

<p>for epoch in range(n_epochs):
 # Training
 model.train()
 train_loss = 0
 train_correct = 0
 train_total = 0</p>

<p>for features, labels in train_loader:
 features = features.to(device)
 labels = labels.squeeze().to(device)</p>

<p># Forward pass
 outputs = model(features)
 loss = criterion(outputs, labels)</p>

<p># Backward pass
 optimizer.zero_grad()
 loss.backward()
 optimizer.step()</p>

<p># Track metrics
 train_loss += loss.item()
 _, predicted = torch.max(outputs, 1)
 train_correct += (predicted == labels).sum().item()
 train_total += labels.size(0)</p>

<p>train_acc = train_correct / train_total</p>

<p># Validation
 model.eval()
 val_correct = 0
 val_total = 0</p>

<p>with torch.no_grad():
 for features, labels in val_loader:
 features = features.to(device)
 labels = labels.squeeze().to(device)</p>

<p>outputs = model(features)
 _, predicted = torch.max(outputs, 1)</p>

<p>val_correct += (predicted == labels).sum().item()
 val_total += labels.size(0)</p>

<p>val_acc = val_correct / val_total</p>

<p># Learning rate scheduling
 scheduler.step(val_acc)</p>

<p># Save best model
 if val_acc &gt; best_val_acc:
 best_val_acc = val_acc
 torch.save(model.state_dict(), â€˜best_kws_model.pthâ€™)</p>

<p>print(fâ€Epoch {epoch+1}/{n_epochs}: â€œ
 fâ€Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}â€)</p>

<p>return model</p>

<h1 id="train">Train</h1>
<p>model = KeywordSpottingCNN()
trained_model = train_keyword_spotting_model(
 model, train_loader, val_loader, n_epochs=50
)
``</p>

<hr />

<h2 id="deployment-optimization">Deployment Optimization</h2>

<h3 id="model-quantization">Model Quantization</h3>

<p>``python
def quantize_kws_model(model):
 â€œâ€â€
 Apply dynamic quantization to linear layers for edge deployment
 â€œâ€â€
 import torch
 import torch.nn as nn</p>

<p>model.eval()
 qmodel = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)
 return qmodel</p>

<h1 id="quantize">Quantize</h1>
<p>model_quantized = quantize_kws_model(model)</p>

<h1 id="compare-serialized-sizes-for-accurate-measurement">Compare serialized sizes for accurate measurement</h1>
<p>import io
import torch</p>

<p>def get_model_size_mb(m):
 buffer = io.BytesIO()
 torch.save(m.state_dict(), buffer)
 return len(buffer.getvalue()) / (1024 ** 2)</p>

<p>original_size = get_model_size_mb(model)
quantized_size = get_model_size_mb(model_quantized)</p>

<p>print(fâ€Original: {original_size:.2f} MBâ€)
print(fâ€Quantized: {quantized_size:.2f} MBâ€)
print(fâ€Compression: {original_size / max(quantized_size, 1e-6):.1f}xâ€)
``</p>

<h3 id="tensorflow-lite-conversion">TensorFlow Lite Conversion</h3>

<p>``python
def convert_to_tflite(model, sample_input):
 â€œâ€â€
 Convert PyTorch model to TensorFlow Lite</p>

<p>For deployment on mobile/edge devices
 â€œâ€â€
 import torch
 import onnx
 import tensorflow as tf
 from onnx_tf.backend import prepare</p>

<p># Step 1: PyTorch â†’ ONNX
 torch.onnx.export(
 model,
 sample_input,
 â€˜kws_model.onnxâ€™,
 input_names=[â€˜inputâ€™],
 output_names=[â€˜outputâ€™],
 dynamic_axes={â€˜inputâ€™: {0: â€˜batchâ€™}, â€˜outputâ€™: {0: â€˜batchâ€™}}
 )</p>

<p># Step 2: ONNX â†’ TensorFlow
 onnx_model = onnx.load(â€˜kws_model.onnxâ€™)
 tf_rep = prepare(onnx_model)
 tf_rep.export_graph(â€˜kws_model_tfâ€™)</p>

<p># Step 3: TensorFlow â†’ TFLite
 converter = tf.lite.TFLiteConverter.from_saved_model(â€˜kws_model_tfâ€™)</p>

<p># Optimization
 converter.optimizations = [tf.lite.Optimize.DEFAULT]
 converter.target_spec.supported_types = [tf.float16]</p>

<p>tflite_model = converter.convert()</p>

<p># Save
 with open(â€˜kws_model.tfliteâ€™, â€˜wbâ€™) as f:
 f.write(tflite_model)</p>

<p>print(fâ€TFLite model size: {len(tflite_model) / 1024:.1f} KBâ€)</p>

<h1 id="convert">Convert</h1>
<p>convert_to_tflite(model, sample_input)
``</p>

<hr />

<h2 id="real-time-inference">Real-time Inference</h2>

<h3 id="streaming-kws-system">Streaming KWS System</h3>

<p>``python
import sounddevice as sd
import numpy as np
import torch
from collections import deque</p>

<p>class StreamingKeywordSpotter:
 â€œâ€â€
 Real-time keyword spotting system</p>

<p>Continuously monitors audio and detects keywords
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model, feature_extractor, 
 threshold=0.8, cooldown_ms=1000):
 self.model = model
 self.model.eval()</p>

<p>self.feature_extractor = feature_extractor
 self.threshold = threshold
 self.cooldown_samples = int(cooldown_ms * 16000 / 1000)</p>

<p># Audio buffer (1 second)
 self.buffer_size = 16000
 self.audio_buffer = deque(maxlen=self.buffer_size)</p>

<p># Detection cooldown
 self.last_detection = -self.cooldown_samples
 self.sample_count = 0</p>

<p>def process_audio_chunk(self, audio_chunk):
 â€œâ€â€
 Process incoming audio chunk</p>

<p>Args:
 audio_chunk: Audio samples [n_samples]</p>

<p>Returns:
 (detected, confidence) tuple
 â€œâ€â€
 # Add to buffer
 self.audio_buffer.extend(audio_chunk)
 self.sample_count += len(audio_chunk)</p>

<p># Wait until buffer is full
 if len(self.audio_buffer) &lt; self.buffer_size:
 return False, 0.0</p>

<p># Check cooldown
 if self.sample_count - self.last_detection &lt; self.cooldown_samples:
 return False, 0.0</p>

<p># Extract features
 audio = np.array(self.audio_buffer)
 features = self.feature_extractor.extract(audio)</p>

<p># Add batch and channel dimensions
 features_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)</p>

<p># Run inference
 with torch.no_grad():
 output = self.model(features_tensor)
 probs = torch.softmax(output, dim=1)
 confidence = probs[0][1].item() # Probability of keyword</p>

<p># Check threshold
 if confidence &gt;= self.threshold:
 self.last_detection = self.sample_count
 return True, confidence</p>

<p>return False, confidence</p>

<p>def start_listening(self, callback=None):
 â€œâ€â€
 Start continuous listening</p>

<p>Args:
 callback: Function called when keyword detected
 â€œâ€â€
 print(â€œğŸ¤ Listening for keywordsâ€¦â€)</p>

<p>def audio_callback(indata, frames, time_info, status):
 â€œ"â€Process audio in callbackâ€â€â€
 if status:
 print(fâ€Audio status: {status}â€)</p>

<p># Process chunk
 detected, confidence = self.process_audio_chunk(indata[:, 0])</p>

<p>if detected:
 print(fâ€âœ“ Keyword detected! (confidence={confidence:.3f})â€)
 if callback:
 callback(confidence)</p>

<p># Start audio stream
 with sd.InputStream(
 samplerate=16000,
 channels=1,
 blocksize=1600, # 100ms chunks
 callback=audio_callback
 ):
 print(â€œPress Ctrl+C to stopâ€)
 sd.sleep(1000000) # Sleep indefinitely</p>

<h1 id="usage-1">Usage</h1>
<p>model = KeywordSpottingCNN()
model.load_state_dict(torch.load(â€˜best_kws_model.pthâ€™))</p>

<p>spotter = StreamingKeywordSpotter(
 model=model,
 feature_extractor=extractor,
 threshold=0.8,
 cooldown_ms=1000
)</p>

<p>def on_keyword_detected(confidence):
 â€œ"â€Callback when keyword detectedâ€â€â€
 print(fâ€ğŸ”” Activating voice assistantâ€¦ (conf={confidence:.2f})â€)
 # Trigger downstream processing</p>

<p>spotter.start_listening(callback=on_keyword_detected)
``</p>

<hr />

<h2 id="connection-to-binary-search-dsa">Connection to Binary Search (DSA)</h2>

<p>Keyword spotting uses binary search for threshold optimization:</p>

<p>``python
class KeywordThresholdOptimizer:
 â€œâ€â€
 Find optimal detection threshold using binary search</p>

<p>Balances false accepts vs false rejects
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model, feature_extractor):
 self.model = model
 self.feature_extractor = feature_extractor
 self.model.eval()</p>

<p>def find_optimal_threshold(self, positive_samples, negative_samples,
 target_far=0.01):
 â€œâ€â€
 Binary search for threshold that achieves target FAR</p>

<p>FAR = False Acceptance Rate</p>

<p>Args:
 positive_samples: List of keyword audio samples
 negative_samples: List of non-keyword audio samples
 target_far: Target false acceptance rate (e.g., 0.01 = 1%)</p>

<p>Returns:
 Optimal threshold
 â€œâ€â€
 # Get confidence scores for all samples
 pos_scores = self._get_scores(positive_samples)
 neg_scores = self._get_scores(negative_samples)</p>

<p># Binary search on threshold space [0, 1]
 left, right = 0.0, 1.0
 best_threshold = 0.5</p>

<p>for iteration in range(20): # 20 iterations for precision
 mid = (left + right) / 2</p>

<p># Calculate FAR at this threshold
 false_accepts = sum(1 for score in neg_scores if score &gt;= mid)
 far = false_accepts / len(neg_scores)</p>

<p># Calculate FRR at this threshold
 false_rejects = sum(1 for score in pos_scores if score &lt; mid)
 frr = false_rejects / len(pos_scores)</p>

<p>print(fâ€Iteration {iteration}: threshold={mid:.4f}, â€œ
 fâ€FAR={far:.4f}, FRR={frr:.4f}â€)</p>

<p># Adjust search space
 if far &gt; target_far:
 # Too many false accepts, increase threshold
 left = mid
 else:
 # FAR is good, try lowering threshold to reduce FRR
 right = mid
 best_threshold = mid</p>

<p>return best_threshold</p>

<p>def _get_scores(self, audio_samples):
 â€œ"â€Get confidence scores for audio samplesâ€â€â€
 scores = []</p>

<p>for audio in audio_samples:
 # Extract features
 features = self.feature_extractor.extract(audio)
 features_tensor = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0)</p>

<p># Inference
 with torch.no_grad():
 output = self.model(features_tensor)
 probs = torch.softmax(output, dim=1)
 confidence = probs[0][1].item()
 scores.append(confidence)</p>

<p>return scores</p>

<h1 id="usage-2">Usage</h1>
<p>optimizer = KeywordThresholdOptimizer(model, extractor)</p>

<p>optimal_threshold = optimizer.find_optimal_threshold(
 positive_samples=keyword_audios,
 negative_samples=background_audios,
 target_far=0.01 # 1% false accept rate
)</p>

<p>print(fâ€Optimal threshold: {optimal_threshold:.4f}â€)
``</p>

<hr />

<h2 id="advanced-model-architectures">Advanced Model Architectures</h2>

<h3 id="1-attention-based-kws">1. Attention-Based KWS</h3>

<p>``python
import torch
import torch.nn as nn</p>

<p>class AttentionKWS(nn.Module):
 â€œâ€â€
 Keyword spotting with attention mechanism</p>

<p>Learns to focus on important parts of audio
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, n_mels=40, hidden_dim=128, n_classes=2):
 super().<strong>init</strong>()</p>

<p># Bidirectional LSTM
 self.lstm = nn.LSTM(
 input_size=n_mels,
 hidden_size=hidden_dim,
 num_layers=2,
 batch_first=True,
 bidirectional=True
 )</p>

<p># Attention layer
 self.attention = nn.Sequential(
 nn.Linear(hidden_dim * 2, 64),
 nn.Tanh(),
 nn.Linear(64, 1)
 )</p>

<p># Classifier
 self.fc = nn.Linear(hidden_dim * 2, n_classes)</p>

<p>def forward(self, x):
 â€œâ€â€
 Args:
 x: [batch, time_steps, n_mels]</p>

<p>Returns:
 [batch, n_classes]
 â€œâ€â€
 # LSTM
 lstm_out, _ = self.lstm(x) # [batch, time, hidden*2]</p>

<p># Attention scores
 attention_scores = self.attention(lstm_out) # [batch, time, 1]
 attention_weights = torch.softmax(attention_scores, dim=1)</p>

<p># Weighted sum
 context = torch.sum(lstm_out * attention_weights, dim=1) # [batch, hidden*2]</p>

<p># Classify
 output = self.fc(context)</p>

<p>return output, attention_weights</p>

<h1 id="usage-3">Usage</h1>
<p>model = AttentionKWS(n_mels=40, hidden_dim=128)</p>

<h1 id="train-and-visualize-attention">Train and visualize attention</h1>
<p>x = torch.randn(1, 100, 40) # 1 sample
output, attention = model(x)</p>

<h1 id="visualize-which-parts-of-audio-model-focuses-on">Visualize which parts of audio model focuses on</h1>
<p>import matplotlib.pyplot as plt
plt.figure(figsize=(12, 4))
plt.plot(attention[0].detach().numpy())
plt.title(â€˜Attention Weights Over Timeâ€™)
plt.xlabel(â€˜Time Stepâ€™)
plt.ylabel(â€˜Attention Weightâ€™)
plt.savefig(â€˜attention_visualization.pngâ€™)
``</p>

<h3 id="2-res-net-based-kws">2. Res-Net Based KWS</h3>

<p>``python
import torch
import torch.nn as nn</p>

<p>class ResNetBlock(nn.Module):
 â€œ"â€Residual block for audioâ€â€â€</p>

<p>def <strong>init</strong>(self, channels):
 super().<strong>init</strong>()
 self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
 self.bn1 = nn.BatchNorm2d(channels)
 self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
 self.bn2 = nn.BatchNorm2d(channels)
 self.relu = nn.ReLU()</p>

<p>def forward(self, x):
 residual = x
 out = self.relu(self.bn1(self.conv1(x)))
 out = self.bn2(self.conv2(out))
 out += residual
 out = self.relu(out)
 return out</p>

<p>class ResNetKWS(nn.Module):
 â€œâ€â€
 ResNet-based keyword spotting</p>

<p>Deeper network for better accuracy
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, n_mels=40, n_classes=2):
 super().<strong>init</strong>()</p>

<p># Initial conv
 self.conv1 = nn.Sequential(
 nn.Conv2d(1, 32, kernel_size=3, padding=1),
 nn.BatchNorm2d(32),
 nn.ReLU()
 )</p>

<p># Residual blocks
 self.res_blocks = nn.Sequential(
 ResNetBlock(32),
 ResNetBlock(32),
 ResNetBlock(32)
 )</p>

<p># Pooling
 self.pool = nn.AdaptiveAvgPool2d((1, 1))</p>

<p># Classifier
 self.fc = nn.Linear(32, n_classes)</p>

<p>def forward(self, x):
 â€œâ€â€
 Args:
 x: [batch, 1, n_mels, time_steps]
 â€œâ€â€
 x = self.conv1(x)
 x = self.res_blocks(x)
 x = self.pool(x)
 x = x.view(x.size(0), -1)
 x = self.fc(x)
 return x</p>

<p>model_resnet = ResNetKWS(n_mels=40)
``</p>

<h3 id="3-transformer-based-kws">3. Transformer-Based KWS</h3>

<p>``python
import torch
import torch.nn as nn
import numpy as np</p>

<p>class TransformerKWS(nn.Module):
 â€œâ€â€
 Transformer for keyword spotting</p>

<p>State-of-the-art performance but larger model
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, n_mels=40, d_model=128, nhead=4, 
 num_layers=2, n_classes=2):
 super().<strong>init</strong>()</p>

<p># Input projection
 self.input_proj = nn.Linear(n_mels, d_model)</p>

<p># Positional encoding
 self.pos_encoder = PositionalEncoding(d_model)</p>

<p># Transformer encoder
 encoder_layer = nn.TransformerEncoderLayer(
 d_model=d_model,
 nhead=nhead,
 dim_feedforward=d_model * 4,
 dropout=0.1,
 batch_first=True
 )
 self.transformer = nn.TransformerEncoder(
 encoder_layer,
 num_layers=num_layers
 )</p>

<p># Classifier
 self.fc = nn.Linear(d_model, n_classes)</p>

<p>def forward(self, x):
 â€œâ€â€
 Args:
 x: [batch, time_steps, n_mels]
 â€œâ€â€
 # Project input
 x = self.input_proj(x)</p>

<p># Add positional encoding
 x = self.pos_encoder(x)</p>

<p># Transformer
 x = self.transformer(x)</p>

<p># Global average pooling
 x = x.mean(dim=1)</p>

<p># Classify
 x = self.fc(x)
 return x</p>

<p>class PositionalEncoding(nn.Module):
 â€œ"â€Positional encoding for transformerâ€â€â€</p>

<p>def <strong>init</strong>(self, d_model, max_len=5000):
 super().<strong>init</strong>()</p>

<p>pe = torch.zeros(max_len, d_model)
 position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
 div_term = torch.exp(
 torch.arange(0, d_model, 2).float() * 
 (-np.log(10000.0) / d_model)
 )</p>

<p>pe[:, 0::2] = torch.sin(position * div_term)
 pe[:, 1::2] = torch.cos(position * div_term)</p>

<p>self.register_buffer(â€˜peâ€™, pe.unsqueeze(0))</p>

<p>def forward(self, x):
 return x + self.pe[:, :x.size(1)]</p>

<p>model_transformer = TransformerKWS(n_mels=40)
``</p>

<hr />

<h2 id="data-augmentation-strategies">Data Augmentation Strategies</h2>

<h3 id="advanced-audio-augmentation">Advanced Audio Augmentation</h3>

<p>``python
import librosa
import numpy as np</p>

<p>class AudioAugmenter:
 â€œâ€â€
 Comprehensive audio augmentation for KWS training</p>

<p>Improves robustness to real-world conditions
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.sample_rate = 16000</p>

<p>def time_stretch(self, audio, rate=None):
 â€œâ€â€
 Stretch/compress audio in time</p>

<p>Args:
 audio: Audio samples
 rate: Stretch factor (0.8-1.2 typical)
 â€œâ€â€
 if rate is None:
 rate = np.random.uniform(0.9, 1.1)</p>

<p>return librosa.effects.time_stretch(audio, rate=rate)</p>

<p>def pitch_shift(self, audio, n_steps=None):
 â€œâ€â€
 Shift pitch without changing speed</p>

<p>Args:
 n_steps: Semitones to shift (-3 to +3 typical)
 â€œâ€â€
 if n_steps is None:
 n_steps = np.random.randint(-2, 3)</p>

<p>return librosa.effects.pitch_shift(
 audio,
 sr=self.sample_rate,
 n_steps=n_steps
 )</p>

<p>def add_background_noise(self, audio, noise_audio, snr_db=None):
 â€œâ€â€
 Add background noise at specified SNR</p>

<p>Args:
 noise_audio: Background noise samples
 snr_db: Signal-to-noise ratio in dB (10-30 typical)
 â€œâ€â€
 if snr_db is None:
 snr_db = np.random.uniform(10, 30)</p>

<p># Calculate noise scaling factor
 audio_power = np.mean(audio ** 2)
 noise_power = np.mean(noise_audio ** 2)</p>

<p>snr_linear = 10 ** (snr_db / 10)
 noise_scale = np.sqrt(audio_power / (snr_linear * noise_power))</p>

<p># Mix audio and noise
 return audio + noise_scale * noise_audio</p>

<p>def room_simulation(self, audio, room_size=â€™mediumâ€™):
 â€œâ€â€
 Simulate room acoustics (reverb)</p>

<p>Args:
 room_size: â€˜smallâ€™, â€˜mediumâ€™, or â€˜largeâ€™
 â€œâ€â€
 # Room impulse response parameters
 params = {
 â€˜smallâ€™: {â€˜delayâ€™: 0.05, â€˜decayâ€™: 0.3},
 â€˜mediumâ€™: {â€˜delayâ€™: 0.1, â€˜decayâ€™: 0.5},
 â€˜largeâ€™: {â€˜delayâ€™: 0.2, â€˜decayâ€™: 0.7}
 }</p>

<p>delay_samples = int(params[room_size][â€˜delayâ€™] * self.sample_rate)
 decay = params[room_size][â€˜decayâ€™]</p>

<p># Simple reverb simulation
 reverb = np.zeros_like(audio)
 reverb[delay_samples:] = audio[:-delay_samples] * decay</p>

<p>return audio + reverb</p>

<p>def apply_compression(self, audio, threshold_db=-20):
 â€œâ€â€
 Dynamic range compression</p>

<p>Makes quiet sounds louder, loud sounds quieter
 â€œâ€â€
 threshold = 10 ** (threshold_db / 20)
 compressed = np.copy(audio)</p>

<p># Compress samples above threshold
 mask = np.abs(audio) &gt; threshold
 compressed[mask] = threshold + (audio[mask] - threshold) * 0.5</p>

<p>return compressed</p>

<p>def augment(self, audio):
 â€œâ€â€
 Apply random augmentation pipeline</p>

<p>Returns augmented audio
 â€œâ€â€
 # Random selection of augmentations
 aug_functions = [
 lambda x: self.time_stretch(x),
 lambda x: self.pitch_shift(x),
 lambda x: self.apply_compression(x),
 ]</p>

<p># Apply 1-2 random augmentations
 n_augs = np.random.randint(1, 3)
 for _ in range(n_augs):
 aug_fn = np.random.choice(aug_functions)
 audio = aug_fn(audio)</p>

<p># Add background noise (always)
 noise = np.random.randn(len(audio)) * 0.005
 audio = self.add_background_noise(audio, noise)</p>

<p>return audio</p>

<h1 id="usage-in-training">Usage in training</h1>
<p>augmenter = AudioAugmenter()</p>

<h1 id="augment-training-data">Augment training data</h1>
<p>augmented_audio = augmenter.augment(original_audio)
``</p>

<hr />

<h2 id="production-deployment-patterns">Production Deployment Patterns</h2>

<h3 id="multi-stage-detection-pipeline">Multi-Stage Detection Pipeline</h3>

<p>``python
import torch</p>

<p>class MultiStageKWSPipeline:
 â€œâ€â€
 Multi-stage KWS for production</p>

<p>Stage 1: Lightweight detector (always-on)
 Stage 2: Accurate model (triggered by stage 1)</p>

<p>Optimizes power consumption vs accuracy
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, stage1_model, stage2_model, 
 stage1_threshold=0.7, stage2_threshold=0.9):
 self.stage1_model = stage1_model # Tiny model (~50KB)
 self.stage2_model = stage2_model # Accurate model (~500KB)</p>

<p>self.stage1_threshold = stage1_threshold
 self.stage2_threshold = stage2_threshold</p>

<p>self.stats = {
 â€˜stage1_triggersâ€™: 0,
 â€˜stage2_confirmsâ€™: 0,
 â€˜false_positivesâ€™: 0
 }
 self.total_chunks = 0</p>

<p>def detect(self, audio_chunk):
 â€œâ€â€
 Two-stage detection</p>

<p>Returns: (detected, confidence, stage)
 â€œâ€â€
 # Increment processed chunks counter
 self.total_chunks += 1</p>

<p># Stage 1: Lightweight screening
 stage1_conf = self._run_stage1(audio_chunk)</p>

<p>if stage1_conf &lt; self.stage1_threshold:
 # Not a keyword, skip stage 2
 return False, stage1_conf, 1</p>

<p>self.stats[â€˜stage1_triggersâ€™] += 1</p>

<p># Stage 2: Accurate verification
 stage2_conf = self._run_stage2(audio_chunk)</p>

<p>if stage2_conf &gt;= self.stage2_threshold:
 self.stats[â€˜stage2_confirmsâ€™] += 1
 return True, stage2_conf, 2
 else:
 self.stats[â€˜false_positivesâ€™] += 1
 return False, stage2_conf, 2</p>

<p>def _run_stage1(self, audio_chunk):
 â€œ"â€Run lightweight modelâ€â€â€
 features = extract_features_fast(audio_chunk)</p>

<p>with torch.no_grad():
 output = self.stage1_model(features)
 confidence = torch.softmax(output, dim=1)[0][1].item()</p>

<p>return confidence</p>

<p>def _run_stage2(self, audio_chunk):
 â€œ"â€Run accurate modelâ€â€â€
 features = extract_features_high_quality(audio_chunk)</p>

<p>with torch.no_grad():
 output = self.stage2_model(features)
 confidence = torch.softmax(output, dim=1)[0][1].item()</p>

<p>return confidence</p>

<p>def get_precision(self):
 â€œ"â€Calculate precision of two-stage systemâ€â€â€
 total_detections = self.stats[â€˜stage2_confirmsâ€™] + self.stats[â€˜false_positivesâ€™]
 if total_detections == 0:
 return 0.0</p>

<p>return self.stats[â€˜stage2_confirmsâ€™] / total_detections</p>

<p>def get_power_savings(self):
 â€œ"â€Estimate power savings from two-stage approachâ€â€â€
 # Stage 2 ~10x power of stage 1 (normalized units)
 stage2_invocations = self.stats[â€˜stage1_triggersâ€™]
 total = max(self.total_chunks, 1)
 cost_stage1 = 1.0
 cost_stage2 = 10.0</p>

<p>energy_two_stage = total * cost_stage1 + stage2_invocations * cost_stage2
 energy_single_stage = total * cost_stage2</p>

<p>savings = 1.0 - (energy_two_stage / energy_single_stage)
 return max(0.0, min(1.0, savings))</p>

<h1 id="usage-4">Usage</h1>
<p>pipeline = MultiStageKWSPipeline(
 stage1_model=lightweight_model,
 stage2_model=accurate_model
)</p>

<h1 id="continuous-monitoring">Continuous monitoring</h1>
<p>for chunk in audio_stream:
 detected, confidence, stage = pipeline.detect(chunk)</p>

<p>if detected:
 print(fâ€Keyword detected! (stage={stage}, conf={confidence:.3f})â€)</p>

<p>print(fâ€Precision: {pipeline.get_precision():.2%}â€)
print(fâ€Power savings: {pipeline.get_power_savings():.2%}â€)
``</p>

<h3 id="on-device-learning">On-Device Learning</h3>

<p>``python
class OnDeviceKWSLearner:
 â€œâ€â€
 Personalized KWS with on-device learning</p>

<p>Adapts to userâ€™s voice without sending data to cloud
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, base_model):
 self.base_model = base_model</p>

<p># Freeze base model
 for param in self.base_model.parameters():
 param.requires_grad = False</p>

<p># Add personalization layer
 self.personalization_layer = nn.Linear(
 self.base_model.output_dim,
 2
 )</p>

<p>self.optimizer = torch.optim.SGD(
 self.personalization_layer.parameters(),
 lr=0.01
 )</p>

<p>self.user_examples = []
 self.max_examples = 50 # Limited on-device storage</p>

<p>def collect_user_example(self, audio, label):
 â€œâ€â€
 Collect user-specific training example</p>

<p>Args:
 audio: Userâ€™s audio sample
 label: 1 for keyword, 0 for non-keyword
 â€œâ€â€
 features = extract_features(audio)</p>

<p>self.user_examples.append((features, label))</p>

<p># Keep only recent examples
 if len(self.user_examples) &gt; self.max_examples:
 self.user_examples.pop(0)</p>

<p>def personalize(self, n_epochs=10):
 â€œâ€â€
 Personalize model to user</p>

<p>Quick fine-tuning on device
 â€œâ€â€
 if len(self.user_examples) &lt; 5:
 print(â€œNot enough user examples yetâ€)
 return</p>

<p>print(fâ€Personalizing with {len(self.user_examples)} examplesâ€¦â€)</p>

<p>for epoch in range(n_epochs):
 total_loss = 0</p>

<p>for features, label in self.user_examples:
 # Extract base features
 with torch.no_grad():
 base_output = self.base_model(features)</p>

<p># Personalization layer
 output = self.personalization_layer(base_output)</p>

<p># Loss
 loss = nn.CrossEntropyLoss()(
 output.unsqueeze(0),
 torch.tensor([label])
 )</p>

<p># Update
 self.optimizer.zero_grad()
 loss.backward()
 self.optimizer.step()</p>

<p>total_loss += loss.item()</p>

<p>if epoch % 5 == 0:
 print(fâ€Epoch {epoch}: Loss = {total_loss / len(self.user_examples):.4f}â€)</p>

<p>print(â€œPersonalization complete!â€)</p>

<p>def predict(self, audio):
 â€œ"â€Predict with personalized modelâ€â€â€
 features = extract_features(audio)</p>

<p>with torch.no_grad():
 base_output = self.base_model(features)
 output = self.personalization_layer(base_output)
 confidence = torch.softmax(output, dim=1)[0][1].item()</p>

<p>return confidence</p>

<h1 id="usage-5">Usage</h1>
<p>learner = OnDeviceKWSLearner(base_model)</p>

<h1 id="user-trains-their-custom-wake-word">User trains their custom wake word</h1>
<p>print(â€œPlease say your wake word 5 timesâ€¦â€)
for i in range(5):
 audio = record_audio()
 learner.collect_user_example(audio, label=1)</p>

<p>print(â€œPlease say 5 non-wake-word phrasesâ€¦â€)
for i in range(5):
 audio = record_audio()
 learner.collect_user_example(audio, label=0)</p>

<h1 id="personalize-on-device">Personalize on-device</h1>
<p>learner.personalize(n_epochs=20)</p>

<h1 id="use-personalized-model">Use personalized model</h1>
<p>confidence = learner.predict(test_audio)
``</p>

<hr />

<h2 id="real-world-integration-examples">Real-World Integration Examples</h2>

<h3 id="smart-speaker-integration">Smart Speaker Integration</h3>

<p>``python
import time</p>

<p>class SmartSpeakerKWS:
 â€œâ€â€
 KWS integrated with smart speaker</p>

<p>Handles wake word â†’ command processing pipeline
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, wake_word_model, command_asr_model):
 self.wake_word_model = wake_word_model
 self.command_asr_model = command_asr_model</p>

<p>self.state = â€˜listeningâ€™ # â€˜listeningâ€™ or â€˜processingâ€™
 self.wake_word_detected = False
 self.command_timeout = 5.0 # seconds</p>

<p>async def process_audio_stream(self, audio_stream):
 â€œâ€â€
 Main processing loop</p>

<p>Always listening for wake word, then processes command
 â€œâ€â€
 wake_word_detector = StreamingKeywordSpotter(
 model=self.wake_word_model,
 feature_extractor=KeywordSpottingFeatureExtractor(sample_rate=16000)
 )</p>

<p>async for chunk in audio_stream:
 if self.state == â€˜listeningâ€™:
 # Check for wake word
 detected, confidence = wake_word_detector.process_audio_chunk(chunk)</p>

<p>if detected:
 print(â€œğŸ”Š Wake word detected!â€)
 await self.play_sound(â€˜ding.wavâ€™) # Audio feedback</p>

<p># Switch to command processing
 self.state = â€˜processingâ€™
 self.wake_word_detected = True</p>

<p># Start command capture
 command_audio = await self.capture_command(audio_stream)</p>

<p># Process command
 await self.process_command(command_audio)</p>

<p># Return to listening
 self.state = â€˜listeningâ€™</p>

<p>async def capture_command(self, audio_stream, timeout=5.0):
 â€œ"â€Capture user command after wake wordâ€â€â€
 command_chunks = []
 start_time = time.time()</p>

<p>async for chunk in audio_stream:
 command_chunks.append(chunk)</p>

<p># Check timeout
 if time.time() - start_time &gt; timeout:
 break</p>

<p># Check for silence (end of command)
 if self.is_silence(chunk):
 break</p>

<p>return np.concatenate(command_chunks)</p>

<p>async def process_command(self, command_audio):
 â€œ"â€Process voice commandâ€â€â€
 # Transcribe command
 transcription = self.command_asr_model.transcribe(command_audio)
 print(fâ€Command: {transcription}â€)</p>

<p># Execute command
 response = await self.execute_command(transcription)</p>

<p># Speak response
 await self.speak(response)</p>

<p>async def execute_command(self, command):
 â€œ"â€Execute voice commandâ€â€â€
 # Command routing
 if â€˜weatherâ€™ in command.lower():
 return await self.get_weather()
 elif â€˜musicâ€™ in command.lower():
 return await self.play_music()
 elif â€˜timerâ€™ in command.lower():
 return await self.set_timer(command)
 else:
 return â€œSorry, I didnâ€™t understand that.â€</p>

<h1 id="usage-6">Usage</h1>
<p>speaker = SmartSpeakerKWS(wake_word_model, command_asr_model)
await speaker.process_audio_stream(microphone_stream)
``</p>

<h3 id="mobile-app-integration">Mobile App Integration</h3>

<p>``python
class MobileKWSManager:
 â€œâ€â€
 KWS manager for mobile apps</p>

<p>Handles battery optimization and background processing
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model_path):
 self.model = self.load_optimized_model(model_path)
 self.is_active = False
 self.battery_saver_mode = False</p>

<p># Performance tracking
 self.battery_usage = 0
 self.detections = 0</p>

<p>def load_optimized_model(self, model_path):
 â€œ"â€Load quantized model for mobileâ€â€â€
 # Load TFLite model
 import tensorflow as tf
 interpreter = tf.lite.Interpreter(model_path=model_path)
 interpreter.allocate_tensors()</p>

<p>return interpreter</p>

<p>def start_listening(self, battery_level=100):
 â€œ"â€Start KWS with battery-aware modeâ€â€â€
 self.is_active = True</p>

<p># Enable battery saver if low battery
 if battery_level &lt; 20:
 self.enable_battery_saver()</p>

<p># Start audio capture thread
 self.audio_thread = threading.Thread(target=self._audio_processing_loop)
 self.audio_thread.start()</p>

<p>def enable_battery_saver(self):
 â€œ"â€Enable battery saving modeâ€â€â€
 self.battery_saver_mode = True</p>

<p># Reduce processing frequency
 self.chunk_duration_ms = 200 # Longer chunks = less processing</p>

<p># Lower threshold for stage 1
 self.stage1_threshold = 0.8 # Higher threshold = fewer stage 2 triggers</p>

<p>print(â€œâš¡ Battery saver mode enabledâ€)</p>

<p>def _audio_processing_loop(self):
 â€œ"â€Background audio processingâ€â€â€
 while self.is_active:
 # Capture audio
 audio_chunk = self.capture_audio_chunk()</p>

<p># Process
 detected, confidence = self.detect_keyword(audio_chunk)</p>

<p>if detected:
 self.detections += 1
 self.trigger_callback(confidence)</p>

<p># Track battery usage (simplified)
 self.battery_usage += 0.001 # mAh per iteration</p>

<p># Sleep to save battery
 if self.battery_saver_mode:
 time.sleep(0.1)</p>

<p>def detect_keyword(self, audio_chunk):
 â€œ"â€Run inference on mobileâ€â€â€
 # Extract features
 features = extract_features(audio_chunk)</p>

<p># TFLite inference
 input_details = self.model.get_input_details()
 output_details = self.model.get_output_details()</p>

<p>self.model.set_tensor(input_details[0][â€˜indexâ€™], features)
 self.model.invoke()</p>

<p>output = self.model.get_tensor(output_details[0][â€˜indexâ€™])
 confidence = output[0][1]</p>

<p>return confidence &gt; 0.8, confidence</p>

<p>def get_battery_impact(self):
 â€œ"â€Estimate battery impactâ€â€â€
 return {
 â€˜total_usage_mahâ€™: self.battery_usage,
 â€˜detectionsâ€™: self.detections,
 â€˜usage_per_hourâ€™: self.battery_usage * 3600 # Extrapolate
 }</p>

<h1 id="usage-in-mobile-app">Usage in mobile app</h1>
<p>kws_manager = MobileKWSManager(â€˜kws_model.tfliteâ€™)
kws_manager.start_listening(battery_level=get_battery_level())</p>

<h1 id="check-battery-impact">Check battery impact</h1>
<p>impact = kws_manager.get_battery_impact()
print(fâ€Battery usage: {impact[â€˜usage_per_hourâ€™]:.2f} mAh/hourâ€)
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>âœ… <strong>Ultra-lightweight models</strong> - &lt; 1MB for edge deployment 
âœ… <strong>Real-time processing</strong> - &lt; 50ms latency requirement 
âœ… <strong>Always-on capability</strong> - Low power consumption 
âœ… <strong>Noise robustness</strong> - Data augmentation and preprocessing critical 
âœ… <strong>Binary search optimization</strong> - Find optimal detection thresholds 
âœ… <strong>Model compression</strong> - Quantization, pruning for deployment 
âœ… <strong>Streaming architecture</strong> - Process continuous audio efficiently</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0009-keyword-spotting/">arunbaby.com/speech-tech/0009-keyword-spotting</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#always-on" class="page__taxonomy-item p-category" rel="tag">always-on</a><span class="sep">, </span>
    
      <a href="/tags/#edge-ai" class="page__taxonomy-item p-category" rel="tag">edge-ai</a><span class="sep">, </span>
    
      <a href="/tags/#keyword-spotting" class="page__taxonomy-item p-category" rel="tag">keyword-spotting</a><span class="sep">, </span>
    
      <a href="/tags/#real-time" class="page__taxonomy-item p-category" rel="tag">real-time</a><span class="sep">, </span>
    
      <a href="/tags/#wake-word-detection" class="page__taxonomy-item p-category" rel="tag">wake-word-detection</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0009-binary-search/" rel="permalink">Binary Search
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          28 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master binary search to understand logarithmic algorithms and efficient searching, foundational for optimization and search systems.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0009-online-learning-systems/" rel="permalink">Online Learning Systems
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Design systems that learn continuously from streaming data, adapting to changing patterns without full retraining.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0009-retrieval-augmented-generation/" rel="permalink">Retrieval-Augmented Generation (RAG)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">â€œGiving the Brain a Library: The Foundation of Knowledge-Intensive Agents.â€
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Real-time+Keyword+Spotting%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0009-keyword-spotting%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0009-keyword-spotting%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0009-keyword-spotting/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0008-streaming-speech-pipeline/" class="pagination--pager" title="Streaming Speech Processing Pipeline">Previous</a>
    
    
      <a href="/speech-tech/0010-voice-enhancement/" class="pagination--pager" title="Voice Enhancement &amp; Noise Reduction">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
