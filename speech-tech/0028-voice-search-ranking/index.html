<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Voice Search Ranking - Arun Baby</title>
<meta name="description" content="“Play Call Me Maybe”. Did you mean the song, the video, or the contact named ‘Maybe’?">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Voice Search Ranking">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0028-voice-search-ranking/">


  <meta property="og:description" content="“Play Call Me Maybe”. Did you mean the song, the video, or the contact named ‘Maybe’?">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Voice Search Ranking">
  <meta name="twitter:description" content="“Play Call Me Maybe”. Did you mean the song, the video, or the contact named ‘Maybe’?">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0028-voice-search-ranking/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-22T11:38:01+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0028-voice-search-ranking/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Voice Search Ranking">
    <meta itemprop="description" content="“Play Call Me Maybe”. Did you mean the song, the video, or the contact named ‘Maybe’?">
    <meta itemprop="datePublished" content="2025-12-22T11:38:01+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0028-voice-search-ranking/" itemprop="url">Voice Search Ranking
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-the-unique-challenge-of-voice-search">1. The Unique Challenge of Voice Search</a></li><li><a href="#2-high-level-architecture">2. High-Level Architecture</a></li><li><a href="#3-asr-n-best-lists-and-lattice-rescoring">3. ASR N-Best Lists and Lattice Rescoring</a></li><li><a href="#4-spoken-language-understanding-slu">4. Spoken Language Understanding (SLU)</a></li><li><a href="#5-federated-search--domain-ranking">5. Federated Search &amp; Domain Ranking</a></li><li><a href="#6-personalization-signals">6. Personalization Signals</a></li><li><a href="#7-evaluation-metrics">7. Evaluation Metrics</a></li><li><a href="#deep-dive-weighted-finite-state-transducers-wfst">Deep Dive: Weighted Finite State Transducers (WFST)</a></li><li><a href="#deep-dive-contextual-biasing-class-based-lms">Deep Dive: Contextual Biasing (Class-Based LMs)</a></li><li><a href="#deep-dive-hotword-detection-wake-word-in-depth">Deep Dive: Hotword Detection (Wake Word) in Depth</a></li><li><a href="#deep-dive-end-to-end-asr-architectures">Deep Dive: End-to-End ASR Architectures</a><ul><li><a href="#1-listen-attend-spell-las">1. Listen-Attend-Spell (LAS)</a></li><li><a href="#2-rnn-t-recurrent-neural-network-transducer">2. RNN-T (Recurrent Neural Network Transducer)</a></li><li><a href="#3-conformer-convolution--transformer">3. Conformer (Convolution + Transformer)</a></li></ul></li><li><a href="#deep-dive-language-model-fusion-strategies">Deep Dive: Language Model Fusion Strategies</a></li><li><a href="#deep-dive-speaker-diarization">Deep Dive: Speaker Diarization</a></li><li><a href="#deep-dive-voice-activity-detection-vad">Deep Dive: Voice Activity Detection (VAD)</a></li><li><a href="#deep-dive-text-to-speech-tts-for-response">Deep Dive: Text-to-Speech (TTS) for Response</a></li><li><a href="#deep-dive-inverse-text-normalization-itn">Deep Dive: Inverse Text Normalization (ITN)</a></li><li><a href="#deep-dive-confidence-calibration">Deep Dive: Confidence Calibration</a></li><li><a href="#deep-dive-voice-search-ux-guidelines">Deep Dive: Voice Search UX Guidelines</a></li><li><a href="#deep-dive-end-to-end-slu-audio---intent">Deep Dive: End-to-End SLU (Audio -&gt; Intent)</a></li><li><a href="#deep-dive-zero-shot-entity-recognition">Deep Dive: Zero-Shot Entity Recognition</a></li><li><a href="#deep-dive-multilingual-voice-search">Deep Dive: Multilingual Voice Search</a></li><li><a href="#deep-dive-privacy-and-federated-learning">Deep Dive: Privacy and Federated Learning</a></li><li><a href="#deep-dive-federated-learning-for-hotword">Deep Dive: Federated Learning for Hotword</a></li><li><a href="#deep-dive-audio-codecs-opus-vs-lyra">Deep Dive: Audio Codecs (Opus vs Lyra)</a></li><li><a href="#deep-dive-microphone-arrays--beamforming">Deep Dive: Microphone Arrays &amp; Beamforming</a></li><li><a href="#deep-dive-emotion-recognition-sentiment-analysis">Deep Dive: Emotion Recognition (Sentiment Analysis)</a></li><li><a href="#deep-dive-personalization-architecture">Deep Dive: Personalization Architecture</a></li><li><a href="#deep-dive-multi-device-arbitration">Deep Dive: Multi-Device Arbitration</a></li><li><a href="#deep-dive-privacy-preserving-asr-on-device">Deep Dive: Privacy-Preserving ASR (On-Device)</a></li><li><a href="#deep-dive-evaluation-frameworks-sxs">Deep Dive: Evaluation Frameworks (SxS)</a></li><li><a href="#deep-dive-latency-optimization">Deep Dive: Latency Optimization</a></li><li><a href="#deep-dive-internationalization-i18n">Deep Dive: Internationalization (i18n)</a></li><li><a href="#deep-dive-the-long-tail-problem">Deep Dive: The “Long Tail” Problem</a></li><li><a href="#deep-dive-spoken-language-understanding-slu">Deep Dive: Spoken Language Understanding (SLU)</a></li><li><a href="#deep-dive-federated-search-logic">Deep Dive: Federated Search Logic</a></li><li><a href="#deep-dive-neural-beam-search">Deep Dive: Neural Beam Search</a></li><li><a href="#deep-dive-handling-noise-and-far-field-audio">Deep Dive: Handling Noise and Far-Field Audio</a></li><li><a href="#deep-dive-context-carryover-conversational-ai">Deep Dive: Context Carryover (Conversational AI)</a></li><li><a href="#deep-dive-training-data-generation-tts-augmentation">Deep Dive: Training Data Generation (TTS Augmentation)</a></li><li><a href="#deep-dive-the-future-multimodal-llms">Deep Dive: The Future (Multimodal LLMs)</a></li><li><a href="#deep-dive-hardware-acceleration-tpunpu">Deep Dive: Hardware Acceleration (TPU/NPU)</a></li><li><a href="#evaluation-semantic-error-rate-semer">Evaluation: Semantic Error Rate (SemER)</a></li><li><a href="#deep-dive-common-failure-modes">Deep Dive: Common Failure Modes</a></li><li><a href="#top-interview-questions">Top Interview Questions</a></li><li><a href="#key-takeaways">Key Takeaways</a></li><li><a href="#8-summary">8. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Play Call Me Maybe”. Did you mean the song, the video, or the contact named ‘Maybe’?</strong></p>

<h2 id="1-the-unique-challenge-of-voice-search">1. The Unique Challenge of Voice Search</h2>

<p>Voice Search is harder than Text Search for three reasons:</p>
<ol>
  <li><strong>ASR Errors:</strong> The user said “Ice cream”, but the ASR heard “I scream”.</li>
  <li><strong>Ambiguity:</strong> “Play Frozen” could mean the movie, the soundtrack, or a playlist.</li>
  <li><strong>No UI:</strong> In a smart speaker, you can’t show 10 results. You must pick <strong>The One</strong> (Top-1 Accuracy is critical).</li>
</ol>

<h2 id="2-high-level-architecture">2. High-Level Architecture</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Audio]
   |
   v
[ASR Engine] -&gt; Generates N-Best List
   |            (1. "Play Call Me Maybe", 0.9)
   |            (2. "Play Call Me Baby", 0.4)
   v
[Query Understanding (NLU)] -&gt; Intent Classification &amp; Slot Filling
   |
   v
[Federated Search] -&gt; Search Music, Contacts, Videos, Web
   |
   v
[Cross-Domain Ranking] -&gt; Selects the best domain (Music vs. Video)
   |
   v
[Response Generation] -&gt; TTS ("Playing Call Me Maybe on Spotify")
</code></pre></div></div>

<h2 id="3-asr-n-best-lists-and-lattice-rescoring">3. ASR N-Best Lists and Lattice Rescoring</h2>

<p>We don’t just take the top ASR hypothesis. We take the <strong>N-Best List</strong> (e.g., top 10).
We re-score these hypotheses using a <strong>Personalized Language Model</strong>.</p>

<p><strong>Example:</strong></p>
<ul>
  <li>User says: “Call [Name]”</li>
  <li>ASR Top 1: “Call Al” (Generic LM probability is high).</li>
  <li>ASR Top 2: “Call Hal” (User has a contact named ‘Hal’).</li>
  <li><strong>Rescoring:</strong> Boost “Call Hal” because ‘Hal’ is in the user’s contact list (Personalization).</li>
</ul>

<p><strong>Lattice Rescoring:</strong>
Instead of a list, we can use the full <strong>Word Lattice</strong> (a graph of all possible paths).
We can search the lattice for entities (Contacts, Song Titles) using Finite State Transducers (FSTs).</p>

<h2 id="4-spoken-language-understanding-slu">4. Spoken Language Understanding (SLU)</h2>

<p>Once we have the text, we need <strong>Intent</strong> and <strong>Slots</strong>.</p>

<p><strong>Query:</strong> “Play Taylor Swift”</p>
<ul>
  <li><strong>Intent:</strong> <code class="language-plaintext highlighter-rouge">PlayMusic</code></li>
  <li><strong>Slot:</strong> <code class="language-plaintext highlighter-rouge">Artist = "Taylor Swift"</code></li>
</ul>

<p><strong>Model:</strong> BERT-based Joint Intent/Slot model.
<strong>Challenge:</strong> Spoken language is messy. “Umm, play that song by, you know, Taylor.”
<strong>Solution:</strong> Train on noisy, disfluent text.</p>

<h2 id="5-federated-search--domain-ranking">5. Federated Search &amp; Domain Ranking</h2>

<p>Voice Assistants connect to multiple backends (Domains).</p>
<ul>
  <li><strong>Music:</strong> Spotify, Apple Music.</li>
  <li><strong>Video:</strong> YouTube, Netflix.</li>
  <li><strong>Knowledge:</strong> Wikipedia.</li>
  <li><strong>IoT:</strong> Smart Lights.</li>
</ul>

<p><strong>The Domain Ranking Problem:</strong>
User says: “Frozen”.</p>
<ul>
  <li>Music Domain Confidence: 0.8 (Soundtrack).</li>
  <li>Video Domain Confidence: 0.9 (Movie).</li>
  <li><strong>Winner:</strong> Video.</li>
</ul>

<p><strong>Calibration:</strong>
Confidence scores from different domains are not comparable.
We train a <strong>Domain Selector Model</strong> (Classifier) that takes features from all domains and predicts the probability of user satisfaction.</p>

<h2 id="6-personalization-signals">6. Personalization Signals</h2>

<p>Personalization is the strongest signal in Voice Search.</p>
<ol>
  <li><strong>App Usage:</strong> If the user uses Spotify 90% of the time, “Play X” implies Spotify.</li>
  <li><strong>Location:</strong> “Where is the nearest Starbucks?” depends entirely on GPS.</li>
  <li><strong>Device State:</strong> “Turn on the lights” implies the lights in the <em>same room</em> as the speaker.</li>
</ol>

<h2 id="7-evaluation-metrics">7. Evaluation Metrics</h2>

<ul>
  <li><strong>WER (Word Error Rate):</strong> ASR metric.</li>
  <li><strong>SemER (Semantic Error Rate):</strong> Did we get the Intent/Slots right? (Even if ASR was wrong).</li>
  <li><strong>Task Completion Rate:</strong> Did the music actually start playing?</li>
  <li><strong>Latency:</strong> Voice users are impatient. Total budget &lt; 1 second.</li>
</ul>

<h2 id="deep-dive-weighted-finite-state-transducers-wfst">Deep Dive: Weighted Finite State Transducers (WFST)</h2>

<p>Traditional ASR decoding uses the <strong>HCLG</strong> graph.
[ H \circ C \circ L \circ G ]</p>
<ul>
  <li><strong>H (HMM):</strong> Acoustic states -&gt; Context-dependent phones.</li>
  <li><strong>C (Context):</strong> Context-dependent phones -&gt; Monophones.</li>
  <li><strong>L (Lexicon):</strong> Monophones -&gt; Words.</li>
  <li><strong>G (Grammar):</strong> Words -&gt; Sentences (N-gram LM).</li>
</ul>

<p><strong>Lattice Generation:</strong>
The decoder outputs a <strong>Lattice</strong> (a compact representation of many hypotheses).</p>
<ul>
  <li>Nodes: Time points.</li>
  <li>Arcs: Words with acoustic and LM scores.</li>
  <li><strong>Rescoring:</strong> We can take this lattice and intersect it with a larger, more complex LM (e.g., a Neural LM) to find a better path.</li>
</ul>

<h2 id="deep-dive-contextual-biasing-class-based-lms">Deep Dive: Contextual Biasing (Class-Based LMs)</h2>

<p>How do we recognize “Call <strong>Arun</strong>” if “Arun” is a rare name?
We use <strong>Contextual Biasing</strong>.</p>

<p><strong>Mechanism:</strong></p>
<ol>
  <li><strong>Class Tagging:</strong> In the LM, we replace names with a class tag: <code class="language-plaintext highlighter-rouge">Call @CONTACT_NAME</code>.</li>
  <li><strong>Runtime Injection:</strong> When the user speaks, we fetch their contact list <code class="language-plaintext highlighter-rouge">["Arun", "Bob", "Charlie"]</code>.</li>
  <li><strong>FST Composition:</strong> We dynamically build a small FST for the contact list and compose it with the main graph at the <code class="language-plaintext highlighter-rouge">@CONTACT_NAME</code> node.</li>
</ol>

<p><strong>Result:</strong>
The model effectively has a dynamic vocabulary that changes per user, per query.
This is critical for:</p>
<ul>
  <li>Contacts (“Call X”)</li>
  <li>App Names (“Open Y”)</li>
  <li>Song Titles (“Play Z”)</li>
</ul>

<h2 id="deep-dive-hotword-detection-wake-word-in-depth">Deep Dive: Hotword Detection (Wake Word) in Depth</h2>

<p>Before any ranking happens, the device must wake up.
<strong>Constraint:</strong> Must run on a DSP with &lt; 100KB RAM and &lt; 1mW power.</p>

<p><strong>Architectures:</strong></p>
<ol>
  <li><strong>DNN:</strong> Simple fully connected layers. (Old).</li>
  <li><strong>CNN:</strong> ResNet-15 or TC-ResNet. Good accuracy, but computationally heavy.</li>
  <li><strong>DS-CNN (Depthwise Separable CNN):</strong> Separates spatial and channel convolutions. 8x smaller and faster. Standard for mobile.</li>
</ol>

<p><strong>Metrics:</strong></p>
<ul>
  <li><strong>False Reject Rate (FRR):</strong> User says “Hey Google” and nothing happens. (Frustrating).</li>
  <li><strong>False Accept Rate (FAR):</strong> TV says “Hey Poodle” and device wakes up. (Creepy).</li>
  <li><strong>Trade-off:</strong> We tune the threshold to minimize FAR (0.1 per hour) while keeping FRR reasonable (&lt; 5%).</li>
</ul>

<h2 id="deep-dive-end-to-end-asr-architectures">Deep Dive: End-to-End ASR Architectures</h2>

<p>Ranking depends on the ASR N-best list. How is it generated?</p>

<h3 id="1-listen-attend-spell-las">1. Listen-Attend-Spell (LAS)</h3>
<ul>
  <li><strong>Encoder:</strong> Pyramidal LSTM.</li>
  <li><strong>Decoder:</strong> Attention-based LSTM.</li>
  <li><strong>Pros:</strong> High accuracy.</li>
  <li><strong>Cons:</strong> Not streaming. Must wait for end of utterance to start decoding.</li>
</ul>

<h3 id="2-rnn-t-recurrent-neural-network-transducer">2. RNN-T (Recurrent Neural Network Transducer)</h3>
<ul>
  <li><strong>Encoder:</strong> Audio -&gt; Features.</li>
  <li><strong>Prediction Network:</strong> Text -&gt; Features (Language Model).</li>
  <li><strong>Joint Network:</strong> Combines them.</li>
  <li><strong>Pros:</strong> Streaming. Low latency.</li>
  <li><strong>Cons:</strong> Hard to train (huge memory).</li>
</ul>

<h3 id="3-conformer-convolution--transformer">3. Conformer (Convolution + Transformer)</h3>
<ul>
  <li>Combines local convolution (for fine-grained audio details) with global self-attention (for long-range context).</li>
  <li><strong>Status:</strong> Current SOTA for streaming ASR.</li>
</ul>

<h2 id="deep-dive-language-model-fusion-strategies">Deep Dive: Language Model Fusion Strategies</h2>

<p>How do we combine the ASR model (AM) with the external Language Model (LM)?</p>

<ol>
  <li><strong>Shallow Fusion:</strong>
    <ul>
      <li>Linearly interpolate scores at inference time.</li>
      <li>Simple, flexible. Can swap LMs easily.</li>
    </ul>
  </li>
  <li><strong>Deep Fusion:</strong>
    <ul>
      <li>Concatenate hidden states of AM and LM and feed to a gating network.</li>
      <li>Requires retraining the AM-LM interface.</li>
    </ul>
  </li>
  <li><strong>Cold Fusion:</strong>
    <ul>
      <li>Train the AM <em>conditioned</em> on the LM.</li>
      <li>The AM learns to rely on the LM for difficult words.</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-speaker-diarization">Deep Dive: Speaker Diarization</h2>

<p>“Who spoke when?”
<strong>Pipeline:</strong></p>
<ol>
  <li><strong>Segmentation:</strong> Split audio into homogeneous segments.</li>
  <li><strong>Embedding:</strong> Extract d-vector for each segment.</li>
  <li><strong>Clustering:</strong>
    <ul>
      <li><strong>K-Means:</strong> If we know the number of speakers (N=2).</li>
      <li><strong>Spectral Clustering:</strong> If N is unknown.</li>
      <li><strong>UIS-RNN:</strong> Unbounded Interleaved-State RNN. Fully supervised online clustering.</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-voice-activity-detection-vad">Deep Dive: Voice Activity Detection (VAD)</h2>

<p>Ranking needs to know when the user <em>stopped</em> speaking to execute the query.
<strong>Approaches:</strong></p>
<ol>
  <li><strong>Energy-Based:</strong> If volume &lt; Threshold for 500ms -&gt; End. (Fails in noisy rooms).</li>
  <li><strong>Model-Based:</strong> Small GMM or DNN trained on Speech vs Noise.</li>
  <li><strong>Semantic VAD:</strong> “Play music” (Complete). “Play…” (Incomplete). Wait longer if incomplete.</li>
</ol>

<h2 id="deep-dive-text-to-speech-tts-for-response">Deep Dive: Text-to-Speech (TTS) for Response</h2>

<p>The ranking system chooses the <em>text</em> response. The TTS system generates the <em>audio</em>.
<strong>Ranking Influence:</strong></p>
<ul>
  <li>If the ranker is unsure (“Did you mean A or B?”), the TTS should sound <strong>inquisitive</strong> (rising pitch).</li>
  <li>If the ranker is confident, the TTS should sound <strong>affirmative</strong>.</li>
</ul>

<p><strong>TTS Architecture:</strong></p>
<ol>
  <li><strong>Text Analysis:</strong> Normalize text, predict prosody.</li>
  <li><strong>Acoustic Model (Tacotron 2):</strong> Text -&gt; Mel Spectrogram.</li>
  <li><strong>Vocoder (WaveNet / HiFi-GAN):</strong> Mel Spectrogram -&gt; Waveform.
    <ul>
      <li><strong>WaveNet:</strong> Autoregressive, slow.</li>
      <li><strong>HiFi-GAN:</strong> GAN-based, real-time, high fidelity.</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-inverse-text-normalization-itn">Deep Dive: Inverse Text Normalization (ITN)</h2>

<p>ASR outputs: “play song number two”.
The Music API expects: <code class="language-plaintext highlighter-rouge">song_id: 2</code>.
ITN converts spoken form to written form.</p>

<p><strong>Rules:</strong></p>
<ul>
  <li>“twenty twenty four” -&gt; “2024” (Year) or “2024” (Number). Context matters.</li>
  <li>“five dollars” -&gt; “$5”.</li>
  <li>“doctor smith” -&gt; “Dr. Smith”.</li>
</ul>

<p><strong>Technology:</strong></p>
<ul>
  <li><strong>FSTs (Finite State Transducers):</strong> Hand-written grammars (Kestrel).</li>
  <li><strong>Neural ITN:</strong> Seq2Seq models that translate “spoken” to “written”.</li>
</ul>

<h2 id="deep-dive-confidence-calibration">Deep Dive: Confidence Calibration</h2>

<p>The ASR model outputs a probability. “I am 90% sure this is ‘Call Mom’”.
But is it <em>really</em> 90% sure?
<strong>Calibration Error:</strong> When the model says 90%, it should be right 90% of the time.
Deep Neural Networks are notoriously <strong>Overconfident</strong>.</p>

<p><strong>Temperature Scaling:</strong>
[ P = \text{softmax}(logits / T) ]</p>
<ul>
  <li>If (T &gt; 1), the distribution flattens (less confident).</li>
  <li>We tune (T) on a validation set to minimize ECE (Expected Calibration Error).</li>
  <li><strong>Why it matters:</strong> If confidence is low, we should ask the user to repeat (“Sorry, I didn’t catch that”). If we are overconfident, we execute the wrong command.</li>
</ul>

<h2 id="deep-dive-voice-search-ux-guidelines">Deep Dive: Voice Search UX Guidelines</h2>

<p>Ranking for Voice is different from Web.
<strong>The “Eyes-Free” Constraint.</strong></p>

<ol>
  <li><strong>Brevity:</strong> Don’t read a Wikipedia article. Read the summary.</li>
  <li><strong>Confirmation:</strong>
    <ul>
      <li>High Confidence: Just do it. (“Turning on lights”).</li>
      <li>Medium Confidence: Implicit confirm. (“OK, playing Taylor Swift”).</li>
      <li>Low Confidence: Explicit confirm. (“Did you say Taylor Swift?”).</li>
    </ul>
  </li>
  <li><strong>Disambiguation:</strong>
    <ul>
      <li>Don’t list 10 options. List 2 or 3.</li>
      <li>“I found a few contacts. Did you mean Bob Smith or Bob Jones?”</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-end-to-end-slu-audio---intent">Deep Dive: End-to-End SLU (Audio -&gt; Intent)</h2>

<p>Why have ASR -&gt; Text -&gt; NLU?
Errors cascade.
<strong>E2E SLU:</strong></p>
<ul>
  <li>Input: Audio.</li>
  <li>Output: Intent/Slots directly.</li>
  <li><strong>Pros:</strong> The model can use prosody (tone of voice). “Yeah right” (Sarcasm) -&gt; Negative Sentiment. Text-only models miss this.</li>
  <li><strong>Cons:</strong> Requires massive Audio-Intent labeled data, which is rare.</li>
</ul>

<h2 id="deep-dive-zero-shot-entity-recognition">Deep Dive: Zero-Shot Entity Recognition</h2>

<p>How do we handle “Play [New Song that came out 5 minutes ago]”?
The ASR model hasn’t seen it. The NLU model hasn’t seen it.</p>

<p><strong>Solution: Copy Mechanisms (Pointer Networks).</strong></p>
<ul>
  <li>The NLU model can decide to “Copy” a span of text from the ASR output into the <code class="language-plaintext highlighter-rouge">Song</code> slot, even if it doesn’t recognize the entity.</li>
  <li>We rely on the <strong>Knowledge Graph</strong> (KG) to validate it.</li>
  <li><strong>KG Lookup:</strong> Fuzzy match the copied span against the daily updated KG index.</li>
</ul>

<h2 id="deep-dive-multilingual-voice-search">Deep Dive: Multilingual Voice Search</h2>

<p>“Play Despacito” (Spanish song, English user).
<strong>Code Switching</strong> is a nightmare for ASR.</p>

<p><strong>Approaches:</strong></p>
<ol>
  <li><strong>LID (Language ID):</strong> Run a classifier first. “Is this English or Spanish?”.
    <ul>
      <li><strong>Problem:</strong> Latency. And “Despacito” is a Spanish word in an English sentence.</li>
    </ul>
  </li>
  <li><strong>Bilingual Models:</strong> Train one model on English + Spanish data.
    <ul>
      <li><strong>Problem:</strong> The phone sets might conflict.</li>
    </ul>
  </li>
  <li><strong>Transliteration:</strong>
    <ul>
      <li>Map Spanish phonemes to English approximations.</li>
      <li>User says “Des-pa-see-to”.</li>
      <li>ASR outputs “Despacito”.</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-privacy-and-federated-learning">Deep Dive: Privacy and Federated Learning</h2>

<p>Voice data is sensitive. Users don’t want their bedroom conversations sent to the cloud.</p>

<p><strong>Federated Learning:</strong></p>
<ol>
  <li><strong>Local Training:</strong> The wake word model (“Hey Siri”) runs locally.</li>
  <li><strong>Gradient Updates:</strong> If the model fails (False Reject), the user manually activates it.</li>
  <li><strong>Aggregation:</strong> The phone computes the gradient update and sends <em>only the gradient</em> (encrypted) to the cloud.</li>
  <li><strong>Global Model:</strong> The cloud aggregates gradients from millions of phones and updates the global model.</li>
  <li><strong>No Audio Upload:</strong> The raw audio never leaves the device.</li>
</ol>

<h2 id="deep-dive-federated-learning-for-hotword">Deep Dive: Federated Learning for Hotword</h2>

<p>We want to improve “Hey Google” detection without uploading false accepts (privacy).
<strong>Protocol:</strong></p>
<ol>
  <li><strong>Local Cache:</strong> Device stores the last 10 “Hey Google” triggers that the user <em>cancelled</em> (False Accepts).</li>
  <li><strong>Training:</strong> When charging + on WiFi, the device fine-tunes the Hotword model on these negative examples.</li>
  <li><strong>Aggregation:</strong> Device sends weight updates to the cloud.</li>
  <li><strong>Result:</strong> The global model learns that “Hey Poodle” is NOT a wake word, without ever hearing the user’s voice in the cloud.</li>
</ol>

<h2 id="deep-dive-audio-codecs-opus-vs-lyra">Deep Dive: Audio Codecs (Opus vs Lyra)</h2>

<p>Streaming raw audio (PCM) is heavy (16kHz * 16bit = 256kbps).
We need compression.</p>
<ol>
  <li><strong>Opus:</strong> Standard for VoIP. Good quality at 24kbps.</li>
  <li><strong>Lyra (Google):</strong> Neural Audio Codec.
    <ul>
      <li>Uses a Generative Model (WaveRNN) to reconstruct speech.</li>
      <li><strong>Bitrate:</strong> 3kbps (Very low bandwidth).</li>
      <li><strong>Quality:</strong> Comparable to Opus at 3kbps.</li>
      <li><strong>Use Case:</strong> Voice Search on 2G networks.</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-microphone-arrays--beamforming">Deep Dive: Microphone Arrays &amp; Beamforming</h2>

<p>How do we hear a user across the room?
<strong>Hardware:</strong> 2-7 microphones (Linear on TV, Circular on Speaker).
<strong>Math (Delay-and-Sum Beamforming):</strong></p>
<ul>
  <li>Sound arrives at Mic 1 at time (t).</li>
  <li>Sound arrives at Mic 2 at time (t + \Delta t).</li>
  <li>We shift Mic 2’s signal by (-\Delta t) and sum them.</li>
  <li><strong>Constructive Interference:</strong> Signals from the target direction add up.</li>
  <li><strong>Destructive Interference:</strong> Noise from other directions cancels out.</li>
  <li><strong>MVDR (Minimum Variance Distortionless Response):</strong> Adaptive beamforming that minimizes noise power while keeping the target signal.</li>
</ul>

<h2 id="deep-dive-emotion-recognition-sentiment-analysis">Deep Dive: Emotion Recognition (Sentiment Analysis)</h2>

<p>Voice contains signals that text doesn’t: <strong>Prosody</strong> (Pitch, Volume, Speed).
<strong>Use Case:</strong></p>
<ul>
  <li>User shouts “Representative!” (Anger).</li>
  <li>System detects High Pitch + High Energy.</li>
  <li><strong>Action:</strong> Route to a senior agent immediately. Don’t ask “Did you mean…”.</li>
</ul>

<p><strong>Model:</strong></p>
<ul>
  <li><strong>Input:</strong> Mel Spectrogram.</li>
  <li><strong>Backbone:</strong> CNN (ResNet).</li>
  <li><strong>Head:</strong> Classification (Neutral, Happy, Sad, Angry).</li>
  <li><strong>Fusion:</strong> Combine with Text Sentiment (BERT).</li>
</ul>

<h2 id="deep-dive-personalization-architecture">Deep Dive: Personalization Architecture</h2>

<p>Personalization is the “Secret Sauce” of Voice Search.
If I say “Play my workout playlist”, the system needs to know:</p>
<ol>
  <li>Who am I? (Speaker ID).</li>
  <li>What is “my workout playlist”? (Entity Resolution).</li>
</ol>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>User Graph Service:</strong> A low-latency KV store (e.g., Bigtable/Cassandra) storing user entities.
    <ul>
      <li>Key: <code class="language-plaintext highlighter-rouge">UserID</code></li>
      <li>Value: <code class="language-plaintext highlighter-rouge">{Contacts: [...], Playlists: [...], SmartDevices: [...]}</code></li>
    </ul>
  </li>
  <li><strong>Biasing Context:</strong>
    <ul>
      <li>When a request comes in, we fetch the User Profile.</li>
      <li>We extract relevant entities (e.g., “Workout Mix”).</li>
      <li>We inject these into the ASR (Contextual Biasing) and the Ranker (Personalization Features).</li>
    </ul>
  </li>
</ul>

<p><strong>Latency Challenge:</strong>
Fetching 10,000 contacts takes time.
<strong>Optimization:</strong></p>
<ul>
  <li><strong>Prefetching:</strong> Fetch profile as soon as “Hey Google” is detected.</li>
  <li><strong>Caching:</strong> Cache active user profiles on the Edge (near the ASR server).</li>
</ul>

<h2 id="deep-dive-multi-device-arbitration">Deep Dive: Multi-Device Arbitration</h2>

<p>You are in the living room. You have a Phone, a Smart Watch, and a Smart Speaker.
You say “Hey Google”. All three wake up.
Who answers?</p>

<p><strong>The Arbitration Protocol:</strong></p>
<ol>
  <li><strong>Wake Word Detection:</strong> All devices detect the wake word.</li>
  <li><strong>Energy Estimation:</strong> Each device calculates the volume (energy) of the speech.</li>
  <li><strong>Gossip:</strong> Devices broadcast their energy scores over the local network (WiFi/BLE).
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Speaker: Energy=90</code></li>
      <li><code class="language-plaintext highlighter-rouge">Phone: Energy=60</code></li>
      <li><code class="language-plaintext highlighter-rouge">Watch: Energy=40</code></li>
    </ul>
  </li>
  <li><strong>Decision:</strong> The device with the highest energy “wins” and lights up. The others go back to sleep.</li>
  <li><strong>Cloud Arbitration:</strong> If devices are not on the same WiFi, the Cloud decides based on timestamp and account ID.</li>
</ol>

<h2 id="deep-dive-privacy-preserving-asr-on-device">Deep Dive: Privacy-Preserving ASR (On-Device)</h2>

<p>Sending audio to the cloud is a privacy risk.
Modern devices (Pixel, iPhone) run ASR <strong>completely on-device</strong>.</p>

<p><strong>Technical Challenges:</strong></p>
<ol>
  <li><strong>Model Size:</strong> Cloud models are 10GB+. Mobile models must be &lt; 100MB.
    <ul>
      <li><strong>Solution:</strong> Quantization (Int8), Pruning, Knowledge Distillation.</li>
    </ul>
  </li>
  <li><strong>Battery:</strong> Continuous listening drains battery.
    <ul>
      <li><strong>Solution:</strong> Low-power DSP for Wake Word. Main CPU only wakes up for the query.</li>
    </ul>
  </li>
  <li><strong>Updates:</strong> How to update the model?
    <ul>
      <li><strong>Solution:</strong> Federated Learning (train on device, send gradients).</li>
    </ul>
  </li>
</ol>

<p><strong>The Hybrid Approach:</strong></p>
<ul>
  <li>Run On-Device ASR for speed and privacy.</li>
  <li>Run Cloud ASR for accuracy (if network is available).</li>
  <li><strong>Ranker:</strong> Choose the result with higher confidence.</li>
</ul>

<h2 id="deep-dive-evaluation-frameworks-sxs">Deep Dive: Evaluation Frameworks (SxS)</h2>

<p>How do we measure “Quality”? WER is not enough.
We use <strong>Side-by-Side (SxS)</strong> evaluation.</p>

<p><strong>Process:</strong></p>
<ol>
  <li>Take a sample of 1000 queries.</li>
  <li>Run them through <strong>System A</strong> (Production) and <strong>System B</strong> (Experiment).</li>
  <li>Show the results to human raters.</li>
  <li><strong>Question:</strong> “Which result is better?”
    <ul>
      <li>A is much better.</li>
      <li>A is slightly better.</li>
      <li>Neutral.</li>
      <li>B is slightly better.</li>
      <li>B is much better.</li>
    </ul>
  </li>
</ol>

<p><strong>Metrics:</strong></p>
<ul>
  <li><strong>Wins/Losses:</strong> “System B won 10% more queries”.</li>
  <li><strong>Satisifaction Score:</strong> Average rating (1-5 stars).</li>
</ul>

<h2 id="deep-dive-latency-optimization">Deep Dive: Latency Optimization</h2>

<p>Latency is the #1 killer of Voice UX.
<strong>Budget:</strong> 200ms ASR + 200ms NLU + 200ms Search + 200ms TTS = 800ms.</p>

<p><strong>Techniques:</strong></p>
<ol>
  <li><strong>Streaming RPCs (gRPC):</strong> Don’t wait for the full audio. Stream chunks.</li>
  <li><strong>Speculative Execution:</strong>
    <ul>
      <li>ASR says “Play Tay…”.</li>
      <li>Search starts searching for “Taylor Swift”, “Taylor Lautner”.</li>
      <li>ASR finishes “…lor Swift”.</li>
      <li>Search is already done.</li>
    </ul>
  </li>
  <li><strong>Early Media:</strong> Start playing the music <em>before</em> the TTS says “Okay, playing…”.</li>
</ol>

<h2 id="deep-dive-internationalization-i18n">Deep Dive: Internationalization (i18n)</h2>

<p>Voice Search must work in 50+ languages.
<strong>Challenges:</strong></p>
<ol>
  <li><strong>Date/Time Formats:</strong> “Set alarm for half past five”.
    <ul>
      <li>US: 5:30.</li>
      <li>Germany: 5:30 (halb sechs).</li>
    </ul>
  </li>
  <li><strong>Addresses:</strong>
    <ul>
      <li>US: Number, Street, City.</li>
      <li>Japan: Prefecture, City, Ward, Block, Number.</li>
    </ul>
  </li>
  <li><strong>Code Switching:</strong>
    <ul>
      <li>India (Hinglish): “Play <em>Kal Ho Naa Ho</em> song”.</li>
      <li>The model must support mixed-language input.</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-the-long-tail-problem">Deep Dive: The “Long Tail” Problem</h2>

<p>Top 1000 queries (Head) are easy (“Weather”, “Timer”, “Music”).
The “Long Tail” (Rare queries) is hard.
“Who was the second cousin of Napoleon?”</p>

<p><strong>Solution:</strong></p>
<ul>
  <li><strong>Knowledge Graph:</strong> Structured data helps answer factual queries.</li>
  <li><strong>LLM Integration:</strong> Use Large Language Models (Gemini/GPT) to generate answers for long-tail queries where structured data fails.</li>
  <li><strong>RAG (Retrieval Augmented Generation):</strong> Retrieve documents -&gt; LLM summarizes -&gt; TTS reads summary.</li>
</ul>

<p>“Call Mom”.</p>
<ul>
  <li>If I say it, call <em>my</em> mom.</li>
  <li>If my wife says it, call <em>her</em> mom.</li>
</ul>

<p><strong>Speaker Diarization / Identification:</strong></p>
<ul>
  <li>We extract a <strong>d-vector</strong> (Speaker Embedding) from the audio.</li>
  <li>We compare it to the enrolled voice profiles on the device.</li>
  <li><strong>Fusion:</strong>
    <ul>
      <li>Input: <code class="language-plaintext highlighter-rouge">[Audio Embedding, User Profile Embedding]</code></li>
      <li>The NLU uses this to resolve “Mom”.</li>
    </ul>
  </li>
</ul>

<h2 id="deep-dive-spoken-language-understanding-slu">Deep Dive: Spoken Language Understanding (SLU)</h2>

<p>ASR gives text. SLU gives meaning.
<strong>Task:</strong> Slot Filling.
<strong>Input:</strong> “Play the new song by Taylor Swift”
<strong>Output:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Intent: PlayMusic</code></li>
  <li><code class="language-plaintext highlighter-rouge">SortOrder: Newest</code></li>
  <li><code class="language-plaintext highlighter-rouge">Artist: Taylor Swift</code></li>
</ul>

<p><strong>Architecture:</strong>
<strong>BERT + CRF (Conditional Random Field).</strong></p>
<ol>
  <li><strong>BERT:</strong> Generates contextual embeddings for each token.</li>
  <li><strong>Linear Layer:</strong> Predicts the intent (<code class="language-plaintext highlighter-rouge">[CLS]</code> token).</li>
  <li><strong>CRF Layer:</strong> Predicts the slot tags (BIO format) for each token.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Play (O)</code></li>
      <li><code class="language-plaintext highlighter-rouge">the (O)</code></li>
      <li><code class="language-plaintext highlighter-rouge">new (B-Sort)</code></li>
      <li><code class="language-plaintext highlighter-rouge">song (O)</code></li>
      <li><code class="language-plaintext highlighter-rouge">by (O)</code></li>
      <li><code class="language-plaintext highlighter-rouge">Taylor (B-Artist)</code></li>
      <li><code class="language-plaintext highlighter-rouge">Swift (I-Artist)</code></li>
    </ul>
  </li>
</ol>

<p><strong>CRF Importance:</strong>
The CRF ensures valid transitions. It prevents predicting <code class="language-plaintext highlighter-rouge">I-Artist</code> without a preceding <code class="language-plaintext highlighter-rouge">B-Artist</code>.</p>

<h2 id="deep-dive-federated-search-logic">Deep Dive: Federated Search Logic</h2>

<p>The “Federator” is a meta-ranker.
It sends the query to N domains and decides which one wins.</p>

<p><strong>Signals for Federation:</strong></p>
<ol>
  <li><strong>Explicit Trigger:</strong> “Ask <strong>Spotify</strong> to play…” -&gt; Force Music Domain.</li>
  <li><strong>Entity Type:</strong> “Play <strong>Frozen</strong>”. ‘Frozen’ is in the Video Knowledge Graph and Music Knowledge Graph.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>**Historical P(Domain</td>
          <td>Query):** “Frozen” usually means the movie (80%), not the soundtrack (20%).</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<p><strong>Arbitration:</strong>
If confidence scores are close (Music: 0.85, Video: 0.84), the system might:</p>
<ul>
  <li><strong>Disambiguate:</strong> Ask the user “Did you mean the movie or the soundtrack?”</li>
  <li><strong>Multimodal Response:</strong> Show the movie on the screen but play the song (if on a Smart Display).</li>
</ul>

<h2 id="deep-dive-neural-beam-search">Deep Dive: Neural Beam Search</h2>

<p>Standard Beam Search uses a simple N-gram LM.
Neural Beam Search uses a Transformer LM (GPT-style).</p>

<p><strong>Challenge:</strong> Neural LMs are slow.
<strong>Solution:</strong></p>
<ol>
  <li><strong>First Pass:</strong> Use a small N-gram LM to generate a lattice.</li>
  <li><strong>Second Pass (Rescoring):</strong> Use the Neural LM to rescore the paths in the lattice.</li>
  <li><strong>Shallow Fusion:</strong>
[ Score = \log P_{ASR}(y|x) + \lambda \log P_{NeuralLM}(y) ]
Compute (P_{NeuralLM}) only for the top K tokens in the beam.</li>
</ol>

<h2 id="deep-dive-handling-noise-and-far-field-audio">Deep Dive: Handling Noise and Far-Field Audio</h2>

<p>Smart Speakers are often 5 meters away, with TV playing in the background.
<strong>Signal Processing Pipeline (Front-End):</strong></p>
<ol>
  <li><strong>AEC (Acoustic Echo Cancellation):</strong> Subtract the music the speaker itself is playing from the microphone input.</li>
  <li><strong>Beamforming:</strong> Use the microphone array (7 mics) to focus on the direction of the user’s voice.</li>
  <li><strong>Dereverberation:</strong> Remove the room echo.</li>
  <li><strong>NS (Noise Suppression):</strong> Remove steady-state noise (AC, Fan).</li>
</ol>

<p><strong>Impact:</strong> Without this, WER increases from 5% to 50%.</p>

<h2 id="deep-dive-context-carryover-conversational-ai">Deep Dive: Context Carryover (Conversational AI)</h2>

<p>User: “Who is the President of France?”
System: “Emmanuel Macron.”
User: “How old is <strong>he</strong>?”</p>

<p><strong>Coreference Resolution:</strong>
The system must resolve “<strong>he</strong>” to “<strong>Emmanuel Macron</strong>”.
<strong>Architecture:</strong></p>
<ul>
  <li><strong>Context Encoder:</strong> Encodes the previous turn (<code class="language-plaintext highlighter-rouge">Query_t-1</code>, <code class="language-plaintext highlighter-rouge">Response_t-1</code>).</li>
  <li><strong>Current Encoder:</strong> Encodes <code class="language-plaintext highlighter-rouge">Query_t</code>.</li>
  <li><strong>Fusion:</strong> Concatenates the embeddings.</li>
  <li><strong>NLU:</strong> Predicts <code class="language-plaintext highlighter-rouge">Intent: GetAge</code>, <code class="language-plaintext highlighter-rouge">Entity: Emmanuel Macron</code>.</li>
</ul>

<h2 id="deep-dive-training-data-generation-tts-augmentation">Deep Dive: Training Data Generation (TTS Augmentation)</h2>

<p>We need audio data for “Play [New Song]”.
We don’t have recordings of users saying it yet.
<strong>Solution:</strong> Use TTS.</p>
<ol>
  <li><strong>Text Generation:</strong> Generate millions of sentences: “Play X”, “Listen to X”.</li>
  <li><strong>TTS Synthesis:</strong> Use a high-quality TTS engine to generate audio.</li>
  <li><strong>Audio Augmentation:</strong> Add background noise (Cafe, Street) and Room Impulse Response (Reverb).</li>
  <li><strong>Training:</strong> Train the ASR model on this synthetic data.
<strong>Result:</strong> The model learns to recognize the new entity before a single user has spoken it.</li>
</ol>

<h2 id="deep-dive-the-future-multimodal-llms">Deep Dive: The Future (Multimodal LLMs)</h2>

<p>Traditional: ASR -&gt; Text -&gt; LLM -&gt; Text -&gt; TTS.
<strong>Latency:</strong> High (Cascading delays).
<strong>Loss:</strong> Prosody is lost. (Emotion, Sarcasm).</p>

<p><strong>Future: Audio-In, Audio-Out (GPT-4o, Gemini Live).</strong></p>
<ul>
  <li>The model takes raw audio tokens as input.</li>
  <li>It generates raw audio tokens as output.</li>
  <li><strong>Benefits:</strong>
    <ul>
      <li><strong>End-to-End Latency:</strong> &lt; 500ms.</li>
      <li><strong>Emotion:</strong> Can laugh, whisper, and sing.</li>
      <li><strong>Interruption:</strong> Can handle “Barge-in” naturally.</li>
    </ul>
  </li>
</ul>

<h2 id="deep-dive-hardware-acceleration-tpunpu">Deep Dive: Hardware Acceleration (TPU/NPU)</h2>

<p>Running a Conformer model (100M params) on a phone CPU is too slow.
We use dedicated <strong>Neural Processing Units (NPUs)</strong> or <strong>Edge TPUs</strong>.</p>

<p><strong>Optimization Pipeline:</strong></p>
<ol>
  <li><strong>Quantization:</strong> Convert Float32 weights to Int8.
    <ul>
      <li><strong>Post-Training Quantization (PTQ):</strong> Easy, slight accuracy drop.</li>
      <li><strong>Quantization Aware Training (QAT):</strong> Train with simulated quantization noise. Recovers accuracy.</li>
    </ul>
  </li>
  <li><strong>Operator Fusion:</strong> Merge <code class="language-plaintext highlighter-rouge">Conv2D + BatchNorm + ReLU</code> into a single kernel call. Reduces memory bandwidth.</li>
  <li><strong>Systolic Arrays:</strong>
    <ul>
      <li>TPUs use a grid of Multiplier-Accumulators (MACs).</li>
      <li>Data flows through the array like a pulse (systole).</li>
      <li>Maximizes reuse of loaded weights.</li>
    </ul>
  </li>
</ol>

<p><strong>Result:</strong></p>
<ul>
  <li><strong>CPU:</strong> 200ms latency, 1W power.</li>
  <li><strong>Edge TPU:</strong> 10ms latency, 0.1W power.</li>
</ul>

<h2 id="evaluation-semantic-error-rate-semer">Evaluation: Semantic Error Rate (SemER)</h2>

<p>WER is not enough.</p>
<ul>
  <li>Ref: “Play Taylor Swift”</li>
  <li>Hyp: “Play Tailor Swift”</li>
  <li><strong>WER:</strong> 1/3 (33% error).</li>
  <li><strong>SemER:</strong> 0% (The NLU correctly maps ‘Tailor’ to the artist entity ‘Taylor Swift’).</li>
</ul>

<p><strong>Calculation:</strong>
[ \text{SemER} = \frac{D + I + S}{C + D + S} ]
Where D, I, S are Deletion, Insertion, Substitution of <strong>Slots</strong>.
If we miss the <code class="language-plaintext highlighter-rouge">Artist</code> slot, that’s an error. If we get the artist right but misspell the word ‘Play’, it’s not a Semantic Error.</p>

<h2 id="deep-dive-common-failure-modes">Deep Dive: Common Failure Modes</h2>

<p>Even with SOTA models, things go wrong.</p>
<ol>
  <li><strong>Barge-in Failure:</strong>
    <ul>
      <li>User: “Hey Google, play music.”</li>
      <li>System: “Okay, playing…” (Music starts loud).</li>
      <li>User: “Hey Google, STOP!”</li>
      <li><strong>Failure:</strong> The AEC cannot cancel the loud music fast enough. The system doesn’t hear “Stop”.</li>
    </ul>
  </li>
  <li><strong>Side-Speech:</strong>
    <ul>
      <li>User A: “Hey Google, set a timer.”</li>
      <li>User B (to User A): “No, we need to leave now.”</li>
      <li><strong>Failure:</strong> System hears “Set a timer no we need to leave”. Intent classification fails.</li>
    </ul>
  </li>
  <li><strong>Trigger-Happy:</strong>
    <ul>
      <li>TV Commercial: “OK, Google.”</li>
      <li><strong>Failure:</strong> Millions of devices wake up.</li>
      <li><strong>Fix:</strong> Audio Fingerprinting on the commercial audio to blacklist it in real-time.</li>
    </ul>
  </li>
</ol>

<h2 id="top-interview-questions">Top Interview Questions</h2>

<p><strong>Q1: How do you handle “Homophones” in Voice Search?</strong>
<em>Answer:</em>
“Call <strong>Al</strong>” vs “Call <strong>Hal</strong>”.</p>
<ol>
  <li><strong>Personalization:</strong> Check contacts. If ‘Hal’ exists, boost it.</li>
  <li><strong>Entity Linking:</strong> Check the Knowledge Graph.</li>
  <li><strong>Query Rewriting:</strong> If ASR consistently outputs “Call Al”, but users usually mean “Hal”, add a rewrite rule <code class="language-plaintext highlighter-rouge">Al -&gt; Hal</code> based on click logs.</li>
</ol>

<p><strong>Q2: Why is Latency so critical in Voice?</strong>
<em>Answer:</em>
In text search, results stream in. In voice, the system is silent while thinking.
Silence &gt; 1 second feels “broken”.
<strong>Techniques:</strong></p>
<ul>
  <li><strong>Streaming ASR:</strong> Transcribe while the user is speaking.</li>
  <li><strong>Speculative Execution:</strong> Start searching “Taylor Swift” before the user finishes saying “Play…”.</li>
</ul>

<p><strong>Q3: What is “Endpointing” and why is it hard?</strong>
<em>Answer:</em>
Endpointing is deciding when the user has finished speaking.</p>
<ul>
  <li><strong>Too fast:</strong> Cut off the user mid-sentence (“Play Call Me…”).</li>
  <li><strong>Too slow:</strong> The system feels laggy.</li>
  <li><strong>Solution:</strong> Hybrid approach. Use VAD (Voice Activity Detection) + Semantic Completeness (NLU says “Intent is complete”).</li>
</ul>

<h2 id="key-takeaways">Key Takeaways</h2>
<h2 id="8-summary">8. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Component</th>
      <th style="text-align: left">Role</th>
      <th style="text-align: left">Key Technology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>ASR</strong></td>
      <td style="text-align: left">Audio -&gt; Text Candidates</td>
      <td style="text-align: left">Conformer / RNN-T</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Rescoring</strong></td>
      <td style="text-align: left">Fix ASR errors using Context</td>
      <td style="text-align: left">Biasing / FSTs</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>NLU</strong></td>
      <td style="text-align: left">Text -&gt; Intent/Slots</td>
      <td style="text-align: left">BERT / LLMs</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Ranker</strong></td>
      <td style="text-align: left">Select Best Domain/Action</td>
      <td style="text-align: left">GBDT / Neural Ranker</td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0028-voice-search-ranking/">arunbaby.com/speech-tech/0028-voice-search-ranking</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#nlu" class="page__taxonomy-item p-category" rel="tag">nlu</a><span class="sep">, </span>
    
      <a href="/tags/#ranking" class="page__taxonomy-item p-category" rel="tag">ranking</a><span class="sep">, </span>
    
      <a href="/tags/#voice-search" class="page__taxonomy-item p-category" rel="tag">voice_search</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech_tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0028-kth-smallest-in-bst/" rel="permalink">Kth Smallest Element in a BST
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Finding the median or the 99th percentile is easy in a sorted array. Can we do it in a tree?
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0028-ranking-systems-at-scale/" rel="permalink">Ranking Systems at Scale
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How does Google search 50 billion pages in 0.1 seconds? The answer is the “Ranking Funnel”.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0028-database-interaction-agents/" rel="permalink">Database Interaction Agents: Mastering Text-to-SQL &amp; Beyond
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Democratizing data access through natural language.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Voice+Search+Ranking%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0028-voice-search-ranking%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0028-voice-search-ranking%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0028-voice-search-ranking/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0027-end-to-end-speech-model-design/" class="pagination--pager" title="End-to-End Speech Model Design">Previous</a>
    
    
      <a href="/speech-tech/0029-hierarchical-speech-classification/" class="pagination--pager" title="Hierarchical Speech Classification">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
