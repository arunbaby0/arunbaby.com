<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>End-to-End Text-to-Speech (TTS) - Arun Baby</title>
<meta name="description" content="“Giving machines a voice.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="End-to-End Text-to-Speech (TTS)">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0039-end-to-end-tts/">


  <meta property="og:description" content="“Giving machines a voice.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="End-to-End Text-to-Speech (TTS)">
  <meta name="twitter:description" content="“Giving machines a voice.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0039-end-to-end-tts/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-16T00:28:52+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0039-end-to-end-tts/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="End-to-End Text-to-Speech (TTS)">
    <meta itemprop="description" content="“Giving machines a voice.”">
    <meta itemprop="datePublished" content="2025-12-16T00:28:52+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0039-end-to-end-tts/" itemprop="url">End-to-End Text-to-Speech (TTS)
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          14 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-the-evolution-of-tts">1. The Evolution of TTS</a><ul><li><a href="#1-concatenative-synthesis-1990s---2010s">1. Concatenative Synthesis (1990s - 2010s)</a></li><li><a href="#2-statistical-parametric-synthesis-hmms-2000s---2015">2. Statistical Parametric Synthesis (HMMs) (2000s - 2015)</a></li><li><a href="#3-neural--end-to-end-tts-2016---present">3. Neural / End-to-End TTS (2016 - Present)</a></li></ul></li><li><a href="#2-anatomy-of-a-modern-tts-system">2. Anatomy of a Modern TTS System</a></li><li><a href="#3-deep-dive-tacotron-2-architecture">3. Deep Dive: Tacotron 2 Architecture</a><ul><li><a href="#1-encoder">1. Encoder</a></li><li><a href="#2-attention-mechanism">2. Attention Mechanism</a></li><li><a href="#3-decoder">3. Decoder</a></li><li><a href="#4-post-net">4. Post-Net</a></li></ul></li><li><a href="#4-deep-dive-neural-vocoders">4. Deep Dive: Neural Vocoders</a><ul><li><a href="#1-griffin-lim-algorithm">1. Griffin-Lim (Algorithm)</a></li><li><a href="#2-wavenet-autoregressive">2. WaveNet (Autoregressive)</a></li><li><a href="#3-waveglow-flow-based">3. WaveGlow (Flow-based)</a></li><li><a href="#4-hifi-gan-gan-based">4. HiFi-GAN (GAN-based)</a></li></ul></li><li><a href="#5-fastspeech-2-non-autoregressive-tts">5. FastSpeech 2: Non-Autoregressive TTS</a></li><li><a href="#6-system-design-building-a-tts-api">6. System Design: Building a TTS API</a></li><li><a href="#7-evaluation-metrics">7. Evaluation Metrics</a></li><li><a href="#8-advanced-voice-cloning-zero-shot-tts">8. Advanced: Voice Cloning (Zero-Shot TTS)</a></li><li><a href="#9-common-challenges">9. Common Challenges</a></li><li><a href="#10-ethical-considerations">10. Ethical Considerations</a></li><li><a href="#11-deep-dive-tacotron-2-attention-mechanism">11. Deep Dive: Tacotron 2 Attention Mechanism</a></li><li><a href="#12-deep-dive-wavenet-dilated-convolutions">12. Deep Dive: WaveNet Dilated Convolutions</a></li><li><a href="#13-deep-dive-hifi-gan-architecture">13. Deep Dive: HiFi-GAN Architecture</a></li><li><a href="#14-system-design-streaming-tts-architecture">14. System Design: Streaming TTS Architecture</a></li><li><a href="#15-advanced-style-transfer-and-emotion-control">15. Advanced: Style Transfer and Emotion Control</a></li><li><a href="#16-case-study-voice-cloning-for-accessibility">16. Case Study: Voice Cloning for Accessibility</a></li><li><a href="#17-deep-dive-vits-conditional-variational-autoencoder-with-adversarial-learning">17. Deep Dive: VITS (Conditional Variational Autoencoder with Adversarial Learning)</a></li><li><a href="#18-deep-dive-prosody-modeling-pitch-energy-duration">18. Deep Dive: Prosody Modeling (Pitch, Energy, Duration)</a></li><li><a href="#19-deep-dive-multi-speaker-and-multi-lingual-tts">19. Deep Dive: Multi-Speaker and Multi-Lingual TTS</a></li><li><a href="#20-deep-dive-audio-codecs-for-generative-audio">20. Deep Dive: Audio Codecs for Generative Audio</a></li><li><a href="#21-system-design-on-device-tts-optimization">21. System Design: On-Device TTS Optimization</a></li><li><a href="#22-evaluation-mushra-tests">22. Evaluation: MUSHRA Tests</a></li><li><a href="#23-interview-questions">23. Interview Questions</a></li><li><a href="#24-further-reading">24. Further Reading</a></li><li><a href="#25-conclusion">25. Conclusion</a></li><li><a href="#26-summary">26. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Giving machines a voice.”</strong></p>

<h2 id="1-the-evolution-of-tts">1. The Evolution of TTS</h2>

<h3 id="1-concatenative-synthesis-1990s---2010s">1. Concatenative Synthesis (1990s - 2010s)</h3>
<ul>
  <li><strong>Method:</strong> Record a voice actor reading thousands of sentences. Chop them into phonemes/diphones. Glue them together at runtime.</li>
  <li><strong>Pros:</strong> Very natural sound for recorded segments.</li>
  <li><strong>Cons:</strong> “Glitchy” at boundaries. Cannot change emotion or style. Requires massive database (GBs).</li>
</ul>

<h3 id="2-statistical-parametric-synthesis-hmms-2000s---2015">2. Statistical Parametric Synthesis (HMMs) (2000s - 2015)</h3>
<ul>
  <li><strong>Method:</strong> Generate acoustic features (F0, spectral envelope) from text using HMMs. Use a vocoder to convert features to audio.</li>
  <li><strong>Pros:</strong> Flexible, small footprint.</li>
  <li><strong>Cons:</strong> “Muffled” or “robotic” sound (due to averaging in HMMs).</li>
</ul>

<h3 id="3-neural--end-to-end-tts-2016---present">3. Neural / End-to-End TTS (2016 - Present)</h3>
<ul>
  <li><strong>Method:</strong> Deep Neural Networks map Text $\to$ Spectrogram $\to$ Waveform.</li>
  <li><strong>Pros:</strong> Human-level naturalness. Controllable style/emotion.</li>
  <li><strong>Cons:</strong> Computationally expensive.</li>
</ul>

<h2 id="2-anatomy-of-a-modern-tts-system">2. Anatomy of a Modern TTS System</h2>

<p>A typical Neural TTS system has two stages:</p>

<ol>
  <li><strong>Acoustic Model (Text $\to$ Mel-Spectrogram):</strong>
    <ul>
      <li>Converts character/phoneme sequence into a time-frequency representation (Mel-spectrogram).</li>
      <li>Example: <strong>Tacotron 2</strong>, FastSpeech 2, VITS.</li>
    </ul>
  </li>
  <li><strong>Vocoder (Mel-Spectrogram $\to$ Waveform):</strong>
    <ul>
      <li>Inverts the spectrogram back to time-domain audio.</li>
      <li>Example: <strong>WaveNet</strong>, WaveGlow, HiFi-GAN.</li>
    </ul>
  </li>
</ol>

<h2 id="3-deep-dive-tacotron-2-architecture">3. Deep Dive: Tacotron 2 Architecture</h2>

<p>Tacotron 2 (Google, 2017) is the gold standard for high-quality TTS.</p>

<h3 id="1-encoder">1. Encoder</h3>
<ul>
  <li><strong>Input:</strong> Character sequence.</li>
  <li><strong>Layers:</strong> 3 Convolutional layers (context) + Bi-directional LSTM.</li>
  <li><strong>Output:</strong> Encoded text features.</li>
</ul>

<h3 id="2-attention-mechanism">2. Attention Mechanism</h3>
<ul>
  <li><strong>Location-Sensitive Attention:</strong> Crucial for TTS.</li>
  <li>Unlike translation (where reordering happens), speech is monotonic.</li>
  <li>The attention weights must move forward linearly.</li>
  <li>Uses previous attention weights as input to calculate current attention.</li>
</ul>

<h3 id="3-decoder">3. Decoder</h3>
<ul>
  <li><strong>Type:</strong> Autoregressive LSTM.</li>
  <li><strong>Input:</strong> Previous Mel-frame.</li>
  <li><strong>Output:</strong> Current Mel-frame.</li>
  <li><strong>Stop Token:</strong> Predicts when to stop generating.</li>
</ul>

<h3 id="4-post-net">4. Post-Net</h3>
<ul>
  <li><strong>Purpose:</strong> Refine the Mel-spectrogram.</li>
  <li><strong>Layers:</strong> 5 Convolutional layers.</li>
  <li><strong>Residual Connection:</strong> Adds detail to the decoder output.</li>
</ul>

<p><strong>Loss Function:</strong> MSE between predicted and ground-truth Mel-spectrograms.</p>

<h2 id="4-deep-dive-neural-vocoders">4. Deep Dive: Neural Vocoders</h2>

<p>The Mel-spectrogram is lossy (phase information is discarded). The Vocoder must “hallucinate” the phase to generate high-fidelity audio.</p>

<h3 id="1-griffin-lim-algorithm">1. Griffin-Lim (Algorithm)</h3>
<ul>
  <li><strong>Method:</strong> Iterative algorithm to estimate phase.</li>
  <li><strong>Pros:</strong> Fast, no training.</li>
  <li><strong>Cons:</strong> Robotic, metallic artifacts.</li>
</ul>

<h3 id="2-wavenet-autoregressive">2. WaveNet (Autoregressive)</h3>
<ul>
  <li><strong>Method:</strong> Predicts sample $x_t$ based on $x_{t-1}, x_{t-2}, …$</li>
  <li><strong>Architecture:</strong> Dilated Causal Convolutions.</li>
  <li><strong>Pros:</strong> State-of-the-art quality.</li>
  <li><strong>Cons:</strong> Extremely slow (sequential generation). 1 second of audio = 24,000 steps.</li>
</ul>

<h3 id="3-waveglow-flow-based">3. WaveGlow (Flow-based)</h3>
<ul>
  <li><strong>Method:</strong> Normalizing Flows. Maps Gaussian noise to audio.</li>
  <li><strong>Pros:</strong> Parallel inference (fast). High quality.</li>
  <li><strong>Cons:</strong> Huge model (hundreds of millions of parameters).</li>
</ul>

<h3 id="4-hifi-gan-gan-based">4. HiFi-GAN (GAN-based)</h3>
<ul>
  <li><strong>Method:</strong> Generator produces audio, Discriminator distinguishes real vs fake.</li>
  <li><strong>Pros:</strong> Very fast (real-time on CPU), high quality.</li>
  <li><strong>Cons:</strong> Training instability (GANs).</li>
  <li><strong>Current Standard:</strong> HiFi-GAN is the default for most systems today.</li>
</ul>

<h2 id="5-fastspeech-2-non-autoregressive-tts">5. FastSpeech 2: Non-Autoregressive TTS</h2>

<p><strong>Problem with Tacotron:</strong></p>
<ul>
  <li>Autoregressive generation is slow ($O(N)$).</li>
  <li>Attention failures (skipping or repeating words).</li>
</ul>

<p><strong>FastSpeech 2 Solution:</strong></p>
<ul>
  <li><strong>Non-Autoregressive:</strong> Generate all frames in parallel ($O(1)$).</li>
  <li><strong>Duration Predictor:</strong> Explicitly predict how many frames each phoneme lasts.</li>
  <li><strong>Pitch/Energy Predictors:</strong> Explicitly model prosody.</li>
</ul>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Encoder (Transformer) $\to$ Variance Adaptor (Duration/Pitch/Energy) $\to$ Decoder (Transformer).</li>
</ul>

<p><strong>Pros:</strong></p>
<ul>
  <li>Extremely fast training and inference.</li>
  <li>Robust (no skipping/repeating).</li>
  <li>Controllable (can manually adjust speed/pitch).</li>
</ul>

<h2 id="6-system-design-building-a-tts-api">6. System Design: Building a TTS API</h2>

<p><strong>Scenario:</strong> Build a scalable TTS service like Amazon Polly.</p>

<p><strong>Requirements:</strong></p>
<ul>
  <li><strong>Latency:</strong> &lt; 200ms time-to-first-byte (streaming).</li>
  <li><strong>Throughput:</strong> 1000 concurrent streams.</li>
  <li><strong>Voices:</strong> Support multiple speakers/languages.</li>
</ul>

<p><strong>Architecture:</strong></p>

<ol>
  <li><strong>Frontend (Text Normalization):</strong>
    <ul>
      <li>“Dr. Smith lives on St. John St.” $\to$ “Doctor Smith lives on Saint John Street”.</li>
      <li>“12:30” $\to$ “twelve thirty”.</li>
      <li><strong>G2P (Grapheme-to-Phoneme):</strong> Convert text to phonemes (using CMU Dict or Model).</li>
    </ul>
  </li>
  <li><strong>Synthesis Engine:</strong>
    <ul>
      <li><strong>Model:</strong> FastSpeech 2 (for speed) + HiFi-GAN.</li>
      <li><strong>Optimization:</strong> ONNX Runtime / TensorRT.</li>
      <li><strong>Streaming:</strong> Chunk text into sentences. Synthesize sentence 1 while user listens.</li>
    </ul>
  </li>
  <li><strong>Caching:</strong>
    <ul>
      <li>Cache common phrases (“Your ride has arrived”).</li>
      <li>Hit rate for TTS is surprisingly high for navigational/assistant apps.</li>
    </ul>
  </li>
  <li><strong>Scaling:</strong>
    <ul>
      <li>GPU inference is preferred (T4/A10G).</li>
      <li>Autoscaling based on queue depth.</li>
    </ul>
  </li>
</ol>

<h2 id="7-evaluation-metrics">7. Evaluation Metrics</h2>

<p><strong>1. MOS (Mean Opinion Score):</strong></p>
<ul>
  <li>Human raters listen and rate from 1 (Bad) to 5 (Excellent).</li>
  <li><strong>Ground Truth:</strong> ~4.5.</li>
  <li><strong>Tacotron 2:</strong> ~4.3.</li>
  <li><strong>Parametric:</strong> ~3.5.</li>
</ul>

<p><strong>2. Intelligibility (Word Error Rate):</strong></p>
<ul>
  <li>Feed generated audio into an ASR system.</li>
  <li>Check if the ASR transcribes it correctly.</li>
</ul>

<p><strong>3. Latency (RTF - Real Time Factor):</strong></p>
<ul>
  <li>Time to generate / Duration of audio.</li>
  <li>RTF &lt; 1.0 means faster than real-time.</li>
</ul>

<h2 id="8-advanced-voice-cloning-zero-shot-tts">8. Advanced: Voice Cloning (Zero-Shot TTS)</h2>

<p><strong>Goal:</strong> Generate speech in a target speaker’s voice given only a 3-second reference clip.</p>

<p><strong>Architecture (e.g., Vall-E, XTTS):</strong></p>
<ol>
  <li><strong>Speaker Encoder:</strong> Compresses reference audio into a fixed-size vector (d-vector).</li>
  <li><strong>Conditioning:</strong> Feed d-vector into the TTS model (AdaIN or Concatenation).</li>
  <li><strong>Language Modeling:</strong> Treat TTS as a language modeling task (Audio Tokens).</li>
</ol>

<p><strong>Vall-E (Microsoft):</strong></p>
<ul>
  <li>Uses EnCodec (Audio Codec) to discretize audio.</li>
  <li>Trains a GPT-style model to predict audio tokens from text + acoustic prompt.</li>
</ul>

<h2 id="9-common-challenges">9. Common Challenges</h2>

<p><strong>1. Text Normalization:</strong></p>
<ul>
  <li>“$19.99” -&gt; “nineteen dollars and ninety-nine cents”.</li>
  <li>Context dependency: “I read the book” (past) vs “I will read” (future).</li>
</ul>

<p><strong>2. Prosody and Emotion:</strong></p>
<ul>
  <li>Default TTS is “neutral/newsreader” style.</li>
  <li>Generating “angry” or “whispering” speech requires labeled data or style transfer.</li>
</ul>

<p><strong>3. Long-Form Synthesis:</strong></p>
<ul>
  <li>Attention mechanisms can drift over long paragraphs.</li>
  <li><strong>Fix:</strong> Windowed attention or sentence-level splitting.</li>
</ul>

<h2 id="10-ethical-considerations">10. Ethical Considerations</h2>

<p><strong>1. Deepfakes:</strong></p>
<ul>
  <li>Voice cloning can break biometric auth (banks).</li>
  <li>Used for scams (“Grandma, I’m in jail, send money”).</li>
  <li><strong>Mitigation:</strong> Watermarking audio (inaudible noise).</li>
</ul>

<p><strong>2. Copyright:</strong></p>
<ul>
  <li>Training on audiobooks without consent.</li>
  <li><strong>Impact:</strong> Voice actors losing jobs.</li>
</ul>

<h2 id="11-deep-dive-tacotron-2-attention-mechanism">11. Deep Dive: Tacotron 2 Attention Mechanism</h2>

<p>Why does standard Attention fail for TTS?</p>
<ul>
  <li>In Machine Translation, alignment is soft and can jump (e.g., “red house” -&gt; “maison rouge”).</li>
  <li>In TTS, alignment is <strong>monotonic</strong> and <strong>continuous</strong>. You never read the end of the sentence before the beginning.</li>
</ul>

<p><strong>Location-Sensitive Attention:</strong>
Standard Bahdanau Attention uses query $s_{i-1}$ and values $h_j$.
\(e_{i,j} = v^T \tanh(W s_{i-1} + V h_j + b)\)</p>

<p>Location-Sensitive Attention adds the <strong>previous alignment</strong> $\alpha_{i-1}$ as a feature.
\(f_i = F * \alpha_{i-1}\)
\(e_{i,j} = v^T \tanh(W s_{i-1} + V h_j + U f_{i,j} + b)\)</p>

<p><strong>Effect:</strong></p>
<ul>
  <li>The model “knows” where it attended last time.</li>
  <li>It learns to simply shift the attention window forward.</li>
  <li>Prevents “babbling” (repeating the same word forever) or skipping words.</li>
</ul>

<h2 id="12-deep-dive-wavenet-dilated-convolutions">12. Deep Dive: WaveNet Dilated Convolutions</h2>

<p>WaveNet generates raw audio sample-by-sample (16,000 samples/sec).
To generate sample $x_t$, it needs context from a long history (e.g., 1 second).</p>

<p><strong>Problem:</strong> Standard convolution with size 3 needs thousands of layers to reach a receptive field of 16,000.</p>

<p><strong>Solution: Dilated Convolutions:</strong></p>
<ul>
  <li>Skip input values with a step size (dilation).</li>
  <li>Layer 1: Dilation 1 (Look at $t, t-1$)</li>
  <li>Layer 2: Dilation 2 (Look at outputs of Layer 1 at $t, t-2$)</li>
  <li>Layer 3: Dilation 4 (Look at outputs of Layer 2 at $t, t-4$)</li>
  <li>…</li>
  <li>Layer 10: Dilation 512.</li>
</ul>

<p><strong>Receptive Field:</strong>
Exponential growth: $2^L$. With 10 layers, we cover 1024 samples. Stack multiple blocks to reach 16,000.</p>

<p><strong>Conditioning:</strong>
WaveNet is conditioned on the Mel-spectrogram $c$.
\(P(x_t | x_{&lt;t}, c) = \text{softmax}(W \cdot \tanh(W_f x + V_f c) \cdot \sigma(W_g x + V_g c))\)
(Gated Activation Unit).</p>

<h2 id="13-deep-dive-hifi-gan-architecture">13. Deep Dive: HiFi-GAN Architecture</h2>

<p>HiFi-GAN (High Fidelity GAN) is the current state-of-the-art vocoder because it’s fast and high quality.</p>

<p><strong>Generator:</strong></p>
<ul>
  <li>Input: Mel-spectrogram.</li>
  <li><strong>Multi-Receptive Field Fusion (MRF):</strong>
    <ul>
      <li>Instead of one ResNet block, it runs multiple ResNet blocks with different kernel sizes and dilation rates in parallel.</li>
      <li>Sums their outputs.</li>
      <li>Allows capturing both fine-grained details (high frequency) and long-term dependencies (low frequency).</li>
    </ul>
  </li>
</ul>

<p><strong>Discriminators:</strong></p>
<ol>
  <li><strong>Multi-Period Discriminator (MPD):</strong>
    <ul>
      <li>Reshapes 1D audio into 2D matrices of height $p$ (periods 2, 3, 5, 7, 11).</li>
      <li>Applies 2D convolution.</li>
      <li>Detects periodic artifacts (metallic sounds).</li>
    </ul>
  </li>
  <li><strong>Multi-Scale Discriminator (MSD):</strong>
    <ul>
      <li>Operates on raw audio, 2x downsampled, 4x downsampled audio.</li>
      <li>Ensures structure is correct at different time scales.</li>
    </ul>
  </li>
</ol>

<p><strong>Loss:</strong></p>
<ul>
  <li>GAN Loss (Adversarial).</li>
  <li>Feature Matching Loss (Match intermediate layers of discriminator).</li>
  <li>Mel-Spectrogram Loss (L1 distance).</li>
</ul>

<h2 id="14-system-design-streaming-tts-architecture">14. System Design: Streaming TTS Architecture</h2>

<p><strong>Challenge:</strong> User shouldn’t wait 5 seconds for a long paragraph to be synthesized.</p>

<p><strong>Architecture:</strong></p>

<ol>
  <li><strong>Text Chunking:</strong>
    <ul>
      <li>Split text by punctuation (., !, ?).</li>
      <li>“Hello world! How are you?” -&gt; [“Hello world!”, “How are you?”].</li>
    </ul>
  </li>
  <li><strong>Incremental Synthesis:</strong>
    <ul>
      <li>Send Chunk 1 to TTS Engine.</li>
      <li>While Chunk 1 is playing, synthesize Chunk 2.</li>
    </ul>
  </li>
  <li><strong>Buffer Management:</strong>
    <ul>
      <li>Client maintains a jitter buffer (e.g., 200ms).</li>
      <li>If synthesis is faster than playback (RTF &lt; 1.0), buffer fills up.</li>
      <li>If synthesis is slower, buffer underruns (stuttering).</li>
    </ul>
  </li>
  <li><strong>Protocol:</strong>
    <ul>
      <li><strong>WebSocket / gRPC:</strong> Bi-directional streaming.</li>
      <li>Server sends binary audio chunks (PCM or Opus encoded).</li>
    </ul>
  </li>
</ol>

<p><strong>Stateful Context:</strong></p>
<ul>
  <li>Simply splitting by sentence breaks prosody (pitch resets at start of sentence).</li>
  <li><strong>Contextual TTS:</strong> Pass the <em>embedding</em> of the previous sentence’s end state as the initial state for the next sentence.</li>
</ul>

<h2 id="15-advanced-style-transfer-and-emotion-control">15. Advanced: Style Transfer and Emotion Control</h2>

<p><strong>Global Style Tokens (GST):</strong></p>
<ul>
  <li>Learn a bank of “style embeddings” (tokens) during training in an unsupervised way.</li>
  <li>At inference, we can choose a token (e.g., Token 3 might capture “fast/angry”, Token 5 “slow/sad”).</li>
  <li>We can mix styles: $0.5 \times \text{Happy} + 0.5 \times \text{Whisper}$.</li>
</ul>

<p><strong>Reference Audio:</strong></p>
<ul>
  <li>Feed a 3-second clip of <em>expressive</em> speech.</li>
  <li>Reference Encoder extracts style vector.</li>
  <li>TTS synthesizes new text with that style.</li>
</ul>

<h2 id="16-case-study-voice-cloning-for-accessibility">16. Case Study: Voice Cloning for Accessibility</h2>

<p><strong>Scenario:</strong> A patient with ALS (Lou Gehrig’s disease) is losing their voice. They want to “bank” their voice to use with a TTS system later.</p>

<p><strong>Process:</strong></p>
<ol>
  <li><strong>Recording:</strong> Patient records 30-60 minutes of reading scripts while they can still speak.</li>
  <li><strong>Fine-Tuning:</strong>
    <ul>
      <li>Take a pre-trained multi-speaker model (e.g., trained on LibriTTS).</li>
      <li>Freeze the encoder/decoder layers.</li>
      <li>Fine-tune the <strong>Speaker Embedding</strong> and last few decoder layers on the patient’s data.</li>
    </ul>
  </li>
  <li><strong>Deployment:</strong> Run the model on an iPad (using CoreML/TensorFlow Lite).</li>
</ol>

<p><strong>Challenges:</strong></p>
<ul>
  <li><strong>Fatigue:</strong> Patient cannot record for hours. Need data-efficient adaptation (Few-Shot Learning).</li>
  <li>
    <p><strong>Dysarthria:</strong> If speech is already slurred, the model will learn the slur. Need “Voice Repair” (mapping slurred speech to healthy speech space).</p>
  </li>
  <li><strong>Dysarthria:</strong> If speech is already slurred, the model will learn the slur. Need “Voice Repair” (mapping slurred speech to healthy speech space).</li>
</ul>

<h2 id="17-deep-dive-vits-conditional-variational-autoencoder-with-adversarial-learning">17. Deep Dive: VITS (Conditional Variational Autoencoder with Adversarial Learning)</h2>

<p>VITS (2021) is the current state-of-the-art “all-in-one” model. It combines Acoustic Model and Vocoder into a single end-to-end network.</p>

<p><strong>Key Idea:</strong></p>
<ul>
  <li><strong>Training:</strong> It’s a VAE.
    <ul>
      <li>Encoder: Takes <strong>Audio</strong> $\to$ Latent $z$.</li>
      <li>Decoder: Takes Latent $z$ $\to$ Audio (HiFi-GAN generator).</li>
      <li><strong>Prior:</strong> The latent $z$ is forced to follow a distribution predicted from <strong>Text</strong>.</li>
    </ul>
  </li>
  <li><strong>Inference:</strong>
    <ul>
      <li>Text Encoder predicts the distribution of $z$.</li>
      <li>Sample $z$.</li>
      <li>Decoder generates audio.</li>
    </ul>
  </li>
</ul>

<p><strong>Flow-based Prior:</strong></p>
<ul>
  <li>To make the text-to-latent prediction expressive, it uses Normalizing Flows.</li>
</ul>

<p><strong>Monotonic Alignment Search (MAS):</strong></p>
<ul>
  <li>VITS learns the alignment between text and audio <em>unsupervised</em> during training using Dynamic Programming (MAS). No external aligner needed.</li>
</ul>

<p><strong>Pros:</strong></p>
<ul>
  <li>Higher quality than Tacotron+WaveGlow.</li>
  <li>Faster than autoregressive models.</li>
  <li>No mismatch between acoustic model and vocoder.</li>
</ul>

<h2 id="18-deep-dive-prosody-modeling-pitch-energy-duration">18. Deep Dive: Prosody Modeling (Pitch, Energy, Duration)</h2>

<p>To make speech sound human, we need to control <em>how</em> it’s said.</p>

<p><strong>1. Duration:</strong></p>
<ul>
  <li>How long is each phoneme?</li>
  <li><strong>Model:</strong> Predict log-duration for each phoneme.</li>
  <li><strong>Control:</strong> Multiply predicted durations by 1.2x to speak slower.</li>
</ul>

<p><strong>2. Pitch (F0):</strong></p>
<ul>
  <li>Fundamental frequency contour.</li>
  <li><strong>Model:</strong> Predict continuous F0 curve.</li>
  <li><strong>Control:</strong> Shift F0 mean to make voice higher/lower. Scale variance to make it more expressive/monotone.</li>
</ul>

<p><strong>3. Energy:</strong></p>
<ul>
  <li>Loudness (L2 norm of frame).</li>
  <li><strong>Model:</strong> Predict energy per frame.</li>
</ul>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Add these predictors after the Text Encoder.</li>
  <li>Add the predicted embeddings to the content embedding before the Decoder.</li>
</ul>

<h2 id="19-deep-dive-multi-speaker-and-multi-lingual-tts">19. Deep Dive: Multi-Speaker and Multi-Lingual TTS</h2>

<p><strong>1. Speaker Embeddings (d-vectors):</strong></p>
<ul>
  <li>Train a speaker verification model (e.g., GE2E loss).</li>
  <li>Extract the embedding from the last layer.</li>
  <li>Condition the TTS model on this vector (Concatenate or AdaIN).</li>
</ul>

<p><strong>2. Code-Switching:</strong></p>
<ul>
  <li>“I want to eat <em>Sushi</em> today.” (English sentence, Japanese word).</li>
  <li><strong>Challenge:</strong> English TTS doesn’t know Japanese phonemes.</li>
  <li><strong>Solution:</strong> Shared Phoneme Set (IPA).</li>
  <li><strong>Model:</strong> Train on mixed data. Use a Language ID embedding.</li>
</ul>

<h2 id="20-deep-dive-audio-codecs-for-generative-audio">20. Deep Dive: Audio Codecs for Generative Audio</h2>

<p>With models like <strong>Vall-E</strong> and <strong>AudioLM</strong>, we treat audio generation as language modeling. But audio is continuous.</p>

<p><strong>Neural Audio Codecs (EnCodec / DAC):</strong></p>
<ul>
  <li><strong>Encoder:</strong> Compresses audio to low-framerate latent.</li>
  <li><strong>Quantizer (RVQ - Residual Vector Quantization):</strong>
    <ul>
      <li>Discretizes latent into “codebook indices” (tokens).</li>
      <li>Hierarchical: Codebook 1 captures coarse structure, Codebook 2 captures residual error, etc.</li>
    </ul>
  </li>
  <li><strong>Decoder:</strong> Reconstructs audio from tokens.</li>
</ul>

<p><strong>Result:</strong></p>
<ul>
  <li>1 second of audio $\to$ 75 tokens.</li>
  <li>Now we can use GPT-4 on these tokens!</li>
</ul>

<h2 id="21-system-design-on-device-tts-optimization">21. System Design: On-Device TTS Optimization</h2>

<p><strong>Scenario:</strong> Siri/Google Assistant running on a phone without internet.</p>

<p><strong>Constraints:</strong></p>
<ul>
  <li><strong>Size:</strong> Model &lt; 50MB.</li>
  <li><strong>Compute:</strong> &lt; 10% CPU usage.</li>
</ul>

<p><strong>Techniques:</strong></p>
<ol>
  <li><strong>Quantization:</strong> Float32 $\to$ Int8. (4x smaller).</li>
  <li><strong>Pruning:</strong> Remove 50% of weights that are near zero.</li>
  <li><strong>Knowledge Distillation:</strong> Train a tiny student model to mimic the large teacher.</li>
  <li><strong>Streaming Vocoder:</strong> Use <strong>LPCNet</strong> (combines DSP with small RNN) or <strong>Multi-Band MelGAN</strong> (generates 4 frequency bands in parallel).</li>
</ol>

<h2 id="22-evaluation-mushra-tests">22. Evaluation: MUSHRA Tests</h2>

<p>MOS is simple but subjective. <strong>MUSHRA (Multiple Stimuli with Hidden Reference and Anchor)</strong> is more rigorous.</p>

<p><strong>Setup:</strong></p>
<ul>
  <li>Listener hears:
    <ul>
      <li><strong>Reference:</strong> Original recording (Ground Truth).</li>
      <li><strong>Anchor:</strong> Low-pass filtered version (Bad quality baseline).</li>
      <li><strong>Samples:</strong> Model A, Model B, Model C (blinded).</li>
    </ul>
  </li>
  <li>Task: Rate all of them from 0-100 relative to Reference.</li>
</ul>

<p><strong>Why Anchor?</strong></p>
<ul>
  <li>Calibrates the scale. If someone rates the Anchor as 80, their data is discarded.</li>
</ul>

<h2 id="23-interview-questions">23. Interview Questions</h2>

<p><strong>Q1: Why use Mel-spectrograms instead of linear spectrograms?</strong>
<em>Answer:</em> Mel-scale matches human hearing (logarithmic perception of pitch). It compresses the data dimension (e.g., 1024 linear $\to$ 80 Mel), making the model easier to train.</p>

<p><strong>Q2: Autoregressive vs Non-Autoregressive TTS?</strong>
<em>Answer:</em></p>
<ul>
  <li><strong>AR (Tacotron):</strong> Higher quality, better prosody, slow, robustness issues.</li>
  <li><strong>Non-AR (FastSpeech):</strong> Fast, robust, controllable, slightly lower prosody quality (averaged).</li>
</ul>

<p><strong>Q3: How to handle OOV words?</strong>
<em>Answer:</em> Use a G2P (Grapheme-to-Phoneme) model that predicts pronunciation from spelling, rather than a dictionary lookup.</p>

<h2 id="24-further-reading">24. Further Reading</h2>

<ol>
  <li><strong>“Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions” (Shen et al., 2017):</strong> Tacotron 2 paper.</li>
  <li><strong>“FastSpeech 2: Fast and High-Quality End-to-End TTS” (Ren et al., 2020).</strong></li>
  <li><strong>“HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis” (Kong et al., 2020).</strong></li>
</ol>

<h2 id="25-conclusion">25. Conclusion</h2>

<p>End-to-End TTS has crossed the “Uncanny Valley”. With models like Tacotron 2 and HiFi-GAN, synthesized speech is often indistinguishable from human speech. The focus has now shifted from “quality” to “control” (emotion, style), “efficiency” (on-device TTS), and “adaptation” (zero-shot cloning). As generative audio models (like Vall-E) merge with LLMs, we are entering an era of conversational AI that sounds as human as it thinks.</p>

<h2 id="26-summary">26. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Component</th>
      <th style="text-align: left">Role</th>
      <th style="text-align: left">Examples</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Frontend</strong></td>
      <td style="text-align: left">Text $\to$ Phonemes</td>
      <td style="text-align: left">G2P, Normalization</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Acoustic Model</strong></td>
      <td style="text-align: left">Phonemes $\to$ Mel-Spec</td>
      <td style="text-align: left">Tacotron 2, FastSpeech 2</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Vocoder</strong></td>
      <td style="text-align: left">Mel-Spec $\to$ Audio</td>
      <td style="text-align: left">WaveNet, HiFi-GAN</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Speaker Encoder</strong></td>
      <td style="text-align: left">Voice Cloning</td>
      <td style="text-align: left">d-vector, x-vector</td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0039-end-to-end-tts/">arunbaby.com/speech-tech/0039-end-to-end-tts</a></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#deep-learning" class="page__taxonomy-item p-category" rel="tag">deep-learning</a><span class="sep">, </span>
    
      <a href="/tags/#tacotron" class="page__taxonomy-item p-category" rel="tag">tacotron</a><span class="sep">, </span>
    
      <a href="/tags/#tts" class="page__taxonomy-item p-category" rel="tag">tts</a><span class="sep">, </span>
    
      <a href="/tags/#vocoder" class="page__taxonomy-item p-category" rel="tag">vocoder</a><span class="sep">, </span>
    
      <a href="/tags/#wavenet" class="page__taxonomy-item p-category" rel="tag">wavenet</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech_tech</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=End-to-End+Text-to-Speech+%28TTS%29%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0039-end-to-end-tts%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0039-end-to-end-tts%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0039-end-to-end-tts/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0038-speech-hyperparameter-tuning/" class="pagination--pager" title="Speech Hyperparameter Tuning">Previous</a>
    
    
      <a href="/speech-tech/0040-wake-word-detection/" class="pagination--pager" title="Wake Word Detection">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
