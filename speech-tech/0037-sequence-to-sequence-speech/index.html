<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Sequence-to-Sequence Speech Models - Arun Baby</title>
<meta name="description" content="“From waveforms to words, and back again.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Sequence-to-Sequence Speech Models">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0037-sequence-to-sequence-speech/">


  <meta property="og:description" content="“From waveforms to words, and back again.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Sequence-to-Sequence Speech Models">
  <meta name="twitter:description" content="“From waveforms to words, and back again.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0037-sequence-to-sequence-speech/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T16:05:30+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0037-sequence-to-sequence-speech/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Sequence-to-Sequence Speech Models">
    <meta itemprop="description" content="“From waveforms to words, and back again.”">
    <meta itemprop="datePublished" content="2025-12-29T16:05:30+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0037-sequence-to-sequence-speech/" itemprop="url">Sequence-to-Sequence Speech Models
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          21 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-the-seq2seq-paradigm">1. The Seq2Seq Paradigm</a></li><li><a href="#2-asr-listen-attend-and-spell-las">2. ASR: Listen, Attend, and Spell (LAS)</a></li><li><a href="#3-tts-tacotron-2">3. TTS: Tacotron 2</a></li><li><a href="#4-speech-translation-direct-s2st">4. Speech Translation: Direct S2ST</a></li><li><a href="#5-attention-mechanisms">5. Attention Mechanisms</a><ul><li><a href="#1-content-based-attention">1. Content-Based Attention</a></li><li><a href="#2-location-aware-attention">2. Location-Aware Attention</a></li><li><a href="#3-monotonic-attention">3. Monotonic Attention</a></li></ul></li><li><a href="#6-challenges">6. Challenges</a><ul><li><a href="#1-exposure-bias">1. Exposure Bias</a></li><li><a href="#2-alignment-failures">2. Alignment Failures</a></li><li><a href="#3-long-sequences">3. Long Sequences</a></li></ul></li><li><a href="#7-modern-approach-transformer-based">7. Modern Approach: Transformer-Based</a></li><li><a href="#8-summary">8. Summary</a></li><li><a href="#9-deep-dive-attention-alignment-visualization">9. Deep Dive: Attention Alignment Visualization</a></li><li><a href="#10-deep-dive-streaming-asr-with-monotonic-attention">10. Deep Dive: Streaming ASR with Monotonic Attention</a></li><li><a href="#11-deep-dive-tacotron-2-architecture-details">11. Deep Dive: Tacotron 2 Architecture Details</a></li><li><a href="#12-deep-dive-vocoder-evolution">12. Deep Dive: Vocoder Evolution</a></li><li><a href="#13-system-design-low-latency-tts">13. System Design: Low-Latency TTS</a></li><li><a href="#14-deep-dive-translatotron-direct-s2st">14. Deep Dive: Translatotron (Direct S2ST)</a></li><li><a href="#15-code-simple-seq2seq-asr">15. Code: Simple Seq2Seq ASR</a></li><li><a href="#16-production-considerations">16. Production Considerations</a></li><li><a href="#17-deep-dive-the-evolution-of-seq2seq-in-speech">17. Deep Dive: The Evolution of Seq2Seq in Speech</a></li><li><a href="#18-deep-dive-connectionist-temporal-classification-ctc">18. Deep Dive: Connectionist Temporal Classification (CTC)</a></li><li><a href="#19-deep-dive-rnn-transducer-rnn-t">19. Deep Dive: RNN-Transducer (RNN-T)</a></li><li><a href="#20-deep-dive-transformer-vs-conformer">20. Deep Dive: Transformer vs Conformer</a></li><li><a href="#21-deep-dive-fastspeech-2-non-autoregressive-tts">21. Deep Dive: FastSpeech 2 (Non-Autoregressive TTS)</a></li><li><a href="#22-deep-dive-vits-conditional-variational-autoencoder">22. Deep Dive: VITS (Conditional Variational Autoencoder)</a></li><li><a href="#23-system-design-real-time-speech-translation-system">23. System Design: Real-Time Speech Translation System</a></li><li><a href="#24-case-study-openai-whisper">24. Case Study: OpenAI Whisper</a></li><li><a href="#25-case-study-google-usm-universal-speech-model">25. Case Study: Google USM (Universal Speech Model)</a></li><li><a href="#26-deep-dive-evaluation-metrics">26. Deep Dive: Evaluation Metrics</a></li><li><a href="#27-code-implementing-beam-search-for-seq2seq">27. Code: Implementing Beam Search for Seq2Seq</a></li><li><a href="#28-production-serving-speech-models-with-triton">28. Production: Serving Speech Models with Triton</a></li><li><a href="#29-deep-dive-end-to-end-speech-to-speech-translation-s2st">29. Deep Dive: End-to-End Speech-to-Speech Translation (S2ST)</a></li><li><a href="#30-summary">30. Summary</a></li><li><a href="#31-deep-dive-self-supervised-learning-ssl-in-speech">31. Deep Dive: Self-Supervised Learning (SSL) in Speech</a></li><li><a href="#32-deep-dive-multilingual-seq2seq">32. Deep Dive: Multilingual Seq2Seq</a></li><li><a href="#33-deep-dive-end-to-end-slu-spoken-language-understanding">33. Deep Dive: End-to-End SLU (Spoken Language Understanding)</a></li><li><a href="#34-deep-dive-attention-visualization-code">34. Deep Dive: Attention Visualization Code</a></li><li><a href="#35-deep-dive-handling-long-form-audio">35. Deep Dive: Handling Long-Form Audio</a></li><li><a href="#36-future-trends-audio-language-models">36. Future Trends: Audio-Language Models</a></li><li><a href="#37-ethical-considerations-voice-cloning--deepfakes">37. Ethical Considerations: Voice Cloning &amp; Deepfakes</a></li><li><a href="#38-deep-dive-the-mathematics-of-transformers-in-speech">38. Deep Dive: The Mathematics of Transformers in Speech</a></li><li><a href="#39-deep-dive-troubleshooting-seq2seq-models">39. Deep Dive: Troubleshooting Seq2Seq Models</a></li><li><a href="#40-deep-dive-the-future---audio-language-models">40. Deep Dive: The Future - Audio-Language Models</a></li><li><a href="#41-ethical-considerations-in-seq2seq-speech">41. Ethical Considerations in Seq2Seq Speech</a></li><li><a href="#42-further-reading">42. Further Reading</a></li><li><a href="#43-summary">43. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“From waveforms to words, and back again.”</strong></p>

<h2 id="1-the-seq2seq-paradigm">1. The Seq2Seq Paradigm</h2>

<p>Traditional speech systems were pipelines:</p>
<ul>
  <li><strong>ASR:</strong> Acoustic Model + Language Model + Decoder.</li>
  <li><strong>TTS:</strong> Text Analysis + Acoustic Model + Vocoder.</li>
</ul>

<p><strong>Seq2Seq</strong> unifies this into a single neural network:</p>
<ul>
  <li><strong>Input:</strong> Sequence (audio features or text).</li>
  <li><strong>Output:</strong> Sequence (text or audio features).</li>
  <li><strong>No hand-crafted features or rules.</strong></li>
</ul>

<h2 id="2-asr-listen-attend-and-spell-las">2. ASR: Listen, Attend, and Spell (LAS)</h2>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Listener (Encoder):</strong> Pyramid LSTM processes audio.
    <ul>
      <li>Reduces time resolution (e.g., 100 frames → 25 frames).</li>
    </ul>
  </li>
  <li><strong>Attender:</strong> Attention mechanism focuses on relevant audio frames.</li>
  <li><strong>Speller (Decoder):</strong> LSTM generates characters/words.</li>
</ol>

<p><strong>Attention:</strong>
<code class="language-plaintext highlighter-rouge">\alpha_t = \text{softmax}(e_t)</code>
<code class="language-plaintext highlighter-rouge">e_{t,i} = \text{score}(s_{t-1}, h_i)</code>
<code class="language-plaintext highlighter-rouge">c_t = \sum_i \alpha_{t,i} h_i</code></p>

<p>Where:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">s_{t-1}</code>: Previous decoder state.</li>
  <li><code class="language-plaintext highlighter-rouge">h_i</code>: Encoder hidden state at time <code class="language-plaintext highlighter-rouge">i</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">c_t</code>: Context vector (weighted sum of encoder states).</li>
</ul>

<h2 id="3-tts-tacotron-2">3. TTS: Tacotron 2</h2>

<p><strong>Goal:</strong> Text → Mel-Spectrogram → Waveform.</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Encoder:</strong> Character embeddings → LSTM.</li>
  <li><strong>Attention:</strong> Decoder attends to encoder states.</li>
  <li><strong>Decoder:</strong> Predicts mel-spectrogram frames.</li>
  <li><strong>Vocoder (WaveNet/HiFi-GAN):</strong> Mel → Waveform.</li>
</ol>

<p><strong>Key Innovation:</strong> Predict multiple frames per step (faster).</p>

<h2 id="4-speech-translation-direct-s2st">4. Speech Translation: Direct S2ST</h2>

<p><strong>Traditional:</strong> Audio → ASR → Text → MT → Text → TTS → Audio.
<strong>Direct:</strong> Audio → Audio (no text intermediate).</p>

<p><strong>Advantages:</strong></p>
<ul>
  <li>Preserves prosody (emotion, emphasis).</li>
  <li>Works for unwritten languages.</li>
</ul>

<p><strong>Model:</strong> Translatotron (Google).</p>
<ul>
  <li>Encoder: Audio (Spanish).</li>
  <li>Decoder: Spectrogram (English).</li>
</ul>

<h2 id="5-attention-mechanisms">5. Attention Mechanisms</h2>

<h3 id="1-content-based-attention">1. Content-Based Attention</h3>
<ul>
  <li>Decoder decides where to attend based on content.</li>
  <li><strong>Problem:</strong> Can attend to same location twice (repetition).</li>
</ul>

<h3 id="2-location-aware-attention">2. Location-Aware Attention</h3>
<ul>
  <li>Uses previous attention weights to guide current attention.</li>
  <li>Encourages monotonic progression (left-to-right).</li>
</ul>

<h3 id="3-monotonic-attention">3. Monotonic Attention</h3>
<ul>
  <li>Enforces strict left-to-right alignment.</li>
  <li>Good for streaming ASR (can’t look ahead).</li>
</ul>

<h2 id="6-challenges">6. Challenges</h2>

<h3 id="1-exposure-bias">1. Exposure Bias</h3>
<ul>
  <li>During training, decoder sees ground truth.</li>
  <li>During inference, decoder sees its own (possibly wrong) predictions.</li>
  <li><strong>Fix:</strong> Scheduled sampling.</li>
</ul>

<h3 id="2-alignment-failures">2. Alignment Failures</h3>
<ul>
  <li>Attention might skip words or repeat.</li>
  <li><strong>Fix:</strong> Guided attention (force diagonal alignment early in training).</li>
</ul>

<h3 id="3-long-sequences">3. Long Sequences</h3>
<ul>
  <li>Attention is O(N^2) in memory.</li>
  <li><strong>Fix:</strong> Chunked attention, or use CTC for ASR.</li>
</ul>

<h2 id="7-modern-approach-transformer-based">7. Modern Approach: Transformer-Based</h2>

<p><strong>Whisper (OpenAI):</strong></p>
<ul>
  <li>Encoder-Decoder Transformer.</li>
  <li>Trained on 680k hours.</li>
  <li>Handles ASR, Translation, Language ID in one model.</li>
</ul>

<p><strong>Advantages:</strong></p>
<ul>
  <li>Parallel training (unlike RNN).</li>
  <li>Better long-range dependencies.</li>
</ul>

<h2 id="8-summary">8. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Task</th>
      <th style="text-align: left">Model</th>
      <th style="text-align: left">Input</th>
      <th style="text-align: left">Output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>ASR</strong></td>
      <td style="text-align: left">LAS, Whisper</td>
      <td style="text-align: left">Audio</td>
      <td style="text-align: left">Text</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>TTS</strong></td>
      <td style="text-align: left">Tacotron 2</td>
      <td style="text-align: left">Text</td>
      <td style="text-align: left">Audio</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>ST</strong></td>
      <td style="text-align: left">Translatotron</td>
      <td style="text-align: left">Audio (L1)</td>
      <td style="text-align: left">Audio (L2)</td>
    </tr>
  </tbody>
</table>

<h2 id="9-deep-dive-attention-alignment-visualization">9. Deep Dive: Attention Alignment Visualization</h2>

<p>In ASR, attention should be <strong>monotonic</strong> (left-to-right).</p>

<p><strong>Good Alignment:</strong>
<code class="language-plaintext highlighter-rouge">
Audio frames: [a1][a2][a3][a4][a5]
Text: [ h ][ e ][ l ][ l ][ o ]
Attention: ████
 ░░░░████
 ░░░░████
 ░░░░████
 ░░░░████
</code></p>

<p><strong>Bad Alignment (Skipping):</strong>
<code class="language-plaintext highlighter-rouge">
Audio frames: [a1][a2][a3][a4][a5]
Text: [ h ][ e ][ l ][ l ][ o ]
Attention: ████
 ░░░░████
 ░░░░████ ← Skipped a3!
</code></p>

<p><strong>Fix:</strong> Guided attention loss forces diagonal alignment early in training.</p>

<h2 id="10-deep-dive-streaming-asr-with-monotonic-attention">10. Deep Dive: Streaming ASR with Monotonic Attention</h2>

<p><strong>Problem:</strong> Standard attention looks at the entire input. Can’t stream.</p>

<p><strong>Monotonic Chunkwise Attention (MoChA):</strong></p>
<ol>
  <li>At each decoder step, decide: “Should I move to the next audio chunk?”</li>
  <li>Use a sigmoid gate: <code class="language-plaintext highlighter-rouge">p_{\text{move}} = \sigma(g(h_t, s_{t-1}))</code></li>
  <li>If yes, attend to next chunk. If no, stay.</li>
</ol>

<p><strong>Advantage:</strong> Can process audio in real-time (with bounded latency).</p>

<h2 id="11-deep-dive-tacotron-2-architecture-details">11. Deep Dive: Tacotron 2 Architecture Details</h2>

<p><strong>Encoder:</strong></p>
<ul>
  <li>Character embeddings → 3 Conv layers → Bidirectional LSTM.</li>
  <li>Output: Encoded representation of text.</li>
</ul>

<p><strong>Decoder:</strong></p>
<ul>
  <li><strong>Prenet:</strong> 2 FC layers with dropout (helps with generalization).</li>
  <li><strong>Attention RNN:</strong> 1-layer LSTM.</li>
  <li><strong>Decoder RNN:</strong> 2-layer LSTM.</li>
  <li><strong>Output:</strong> Predicts 80-dim mel-spectrogram frame.</li>
</ul>

<p><strong>Postnet:</strong></p>
<ul>
  <li>5 Conv layers.</li>
  <li>Refines the mel-spectrogram (adds high-frequency details).</li>
</ul>

<p><strong>Stop Token:</strong></p>
<ul>
  <li>Decoder also predicts “Should I stop?” at each step.</li>
  <li>Training: Sigmoid BCE loss.</li>
</ul>

<h2 id="12-deep-dive-vocoder-evolution">12. Deep Dive: Vocoder Evolution</h2>

<p><strong>Goal:</strong> Mel-Spectrogram → Waveform.</p>

<p><strong>WaveNet (2016):</strong></p>
<ul>
  <li>Autoregressive CNN.</li>
  <li>Generates one sample at a time (16kHz = 16,000 samples/sec).</li>
  <li><strong>Slow:</strong> 1 second of audio takes 10 seconds to generate.</li>
</ul>

<p><strong>WaveGlow (2018):</strong></p>
<ul>
  <li>Flow-based model.</li>
  <li>Parallel generation (all samples at once).</li>
  <li><strong>Fast:</strong> Real-time on GPU.</li>
</ul>

<p><strong>HiFi-GAN (2020):</strong></p>
<ul>
  <li>GAN-based.</li>
  <li><strong>Fastest:</strong> 167x faster than real-time on V100.</li>
  <li><strong>Quality:</strong> Indistinguishable from ground truth.</li>
</ul>

<h2 id="13-system-design-low-latency-tts">13. System Design: Low-Latency TTS</h2>

<p><strong>Scenario:</strong> Voice assistant (Alexa, Siri).</p>

<p><strong>Requirements:</strong></p>
<ul>
  <li><strong>Latency:</strong> &lt; 300ms from text to first audio chunk.</li>
  <li><strong>Quality:</strong> Natural, expressive.</li>
</ul>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Text Normalization:</strong> “Dr.” → “Doctor”, “$100” → “one hundred dollars”.</li>
  <li><strong>Grapheme-to-Phoneme (G2P):</strong> “read” → /riːd/ or /rɛd/ (context-dependent).</li>
  <li><strong>Prosody Prediction:</strong> Predict pitch, duration, energy.</li>
  <li><strong>Acoustic Model:</strong> FastSpeech 2 (non-autoregressive, parallel).</li>
  <li><strong>Vocoder:</strong> HiFi-GAN.</li>
</ol>

<p><strong>Streaming:</strong></p>
<ul>
  <li>Generate mel-spectrogram in chunks (e.g., 50ms).</li>
  <li>Vocoder processes each chunk independently.</li>
</ul>

<h2 id="14-deep-dive-translatotron-direct-s2st">14. Deep Dive: Translatotron (Direct S2ST)</h2>

<p><strong>Challenge:</strong> No parallel S2ST data (Audio_Spanish → Audio_English).</p>

<p><strong>Solution:</strong> Weak supervision.</p>
<ol>
  <li>Use ASR to get Spanish text.</li>
  <li>Use MT to get English text.</li>
  <li>Use TTS to get English audio.</li>
  <li>Train Translatotron on (Audio_Spanish, Audio_English) pairs.</li>
</ol>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Encoder:</strong> Processes Spanish audio.</li>
  <li><strong>Decoder:</strong> Generates English mel-spectrogram.</li>
  <li><strong>Speaker Encoder:</strong> Preserves source speaker’s voice.</li>
</ul>

<h2 id="15-code-simple-seq2seq-asr">15. Code: Simple Seq2Seq ASR</h2>

<p>``python
import torch
import torch.nn as nn</p>

<p>class Seq2SeqASR(nn.Module):
 def <strong>init</strong>(self, input_dim, hidden_dim, vocab_size):
 super().<strong>init</strong>()</p>

<p># Encoder: Bi-LSTM
 self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers=3, 
 batch_first=True, bidirectional=True)</p>

<p># Attention
 self.attention = nn.Linear(hidden_dim * 2, 1)</p>

<p># Decoder: LSTM
 self.decoder = nn.LSTM(vocab_size + hidden_dim * 2, hidden_dim, 
 num_layers=2, batch_first=True)</p>

<p># Output projection
 self.fc = nn.Linear(hidden_dim, vocab_size)</p>

<p>def forward(self, audio_features, text_input):
 # Encode audio
 encoder_out, _ = self.encoder(audio_features) # (B, T, 2*H)</p>

<p># Decode
 outputs = []
 hidden = None</p>

<p>for t in range(text_input.size(1)):
 # Compute attention
 attn_scores = self.attention(encoder_out).squeeze(-1) # (B, T)
 attn_weights = torch.softmax(attn_scores, dim=1) # (B, T)
 context = torch.bmm(attn_weights.unsqueeze(1), encoder_out) # (B, 1, 2*H)</p>

<p># Decoder input: previous char + context
 decoder_input = torch.cat([text_input[:, t:t+1], context], dim=-1)</p>

<p># Decoder step
 decoder_out, hidden = self.decoder(decoder_input, hidden)</p>

<p># Predict next char
 logits = self.fc(decoder_out)
 outputs.append(logits)</p>

<p>return torch.cat(outputs, dim=1)
``</p>

<h2 id="16-production-considerations">16. Production Considerations</h2>

<ol>
  <li><strong>Model Size:</strong> Quantize to INT8 for mobile deployment.</li>
  <li><strong>Latency:</strong> Use non-autoregressive models (FastSpeech, Conformer-CTC).</li>
  <li><strong>Personalization:</strong> Fine-tune on user’s voice (few-shot learning).</li>
  <li><strong>Multilingual:</strong> Train on 100+ languages (mSLAM, Whisper).</li>
</ol>

<h2 id="17-deep-dive-the-evolution-of-seq2seq-in-speech">17. Deep Dive: The Evolution of Seq2Seq in Speech</h2>

<p>To understand where we are, we must understand the journey.</p>

<p><strong>1. HMM-GMM (1980s-2010):</strong></p>
<ul>
  <li><strong>Acoustic Model:</strong> Gaussian Mixture Models (GMM) modeled the probability of audio features given a phoneme state.</li>
  <li><strong>Sequence Model:</strong> Hidden Markov Models (HMM) modeled the transition between states.</li>
  <li><strong>Pros:</strong> Mathematically rigorous, worked on small data.</li>
  <li><strong>Cons:</strong> Assumed independence of frames (false), complex training pipeline.</li>
</ul>

<p><strong>2. DNN-HMM (2010-2015):</strong></p>
<ul>
  <li>Replaced GMM with Deep Neural Networks (DNN).</li>
  <li><strong>Impact:</strong> Massive drop in WER (30% relative).</li>
  <li><strong>Cons:</strong> Still relied on HMM for alignment.</li>
</ul>

<p><strong>3. CTC (2006, popularized 2015):</strong></p>
<ul>
  <li><strong>Connectionist Temporal Classification.</strong></li>
  <li>First “End-to-End” loss.</li>
  <li>Allowed predicting sequences shorter than input without explicit alignment.</li>
  <li><strong>Cons:</strong> Conditional independence assumption (output at time <code class="language-plaintext highlighter-rouge">t</code> depends only on input, not previous outputs).</li>
</ul>

<p><strong>4. LAS (2016):</strong></p>
<ul>
  <li><strong>Listen-Attend-Spell.</strong></li>
  <li>Introduced Attention to speech.</li>
  <li><strong>Pros:</strong> No independence assumption.</li>
  <li><strong>Cons:</strong> Not streaming (needs full audio to attend).</li>
</ul>

<p><strong>5. RNN-T (2012, popularized 2018):</strong></p>
<ul>
  <li><strong>RNN Transducer.</strong></li>
  <li>Combined CTC (streaming) with LAS (label dependency).</li>
  <li><strong>Result:</strong> The standard for streaming ASR (Pixel, Siri, Alexa).</li>
</ul>

<p><strong>6. Transformer &amp; Conformer (2017-Present):</strong></p>
<ul>
  <li>Self-attention replaces RNNs.</li>
  <li><strong>Conformer:</strong> Combines CNN (local patterns) with Transformer (global context).</li>
</ul>

<h2 id="18-deep-dive-connectionist-temporal-classification-ctc">18. Deep Dive: Connectionist Temporal Classification (CTC)</h2>

<p><strong>Problem:</strong> Audio has <code class="language-plaintext highlighter-rouge">T</code> frames (e.g., 1000). Text has <code class="language-plaintext highlighter-rouge">L</code> characters (e.g., 50). <code class="language-plaintext highlighter-rouge">T \gg L</code>. How do we map them?</p>

<p><strong>CTC Solution:</strong></p>
<ul>
  <li>Introduce a <strong>blank token</strong> <code class="language-plaintext highlighter-rouge">\epsilon</code>.</li>
  <li>Output sequence length is <code class="language-plaintext highlighter-rouge">T</code>.</li>
  <li><strong>Collapse:</strong> Remove repeats and blanks.</li>
  <li><code class="language-plaintext highlighter-rouge">h h e e l l l l o</code> → <code class="language-plaintext highlighter-rouge">hello</code></li>
  <li><code class="language-plaintext highlighter-rouge">h h \epsilon e e l l \epsilon l l o</code> → <code class="language-plaintext highlighter-rouge">hello</code></li>
</ul>

<p><strong>Loss Function:</strong>
<code class="language-plaintext highlighter-rouge">P(Y|X) = \sum_{A \in \mathcal{B}^{-1}(Y)} P(A|X)</code></p>
<ul>
  <li>Sum over all valid alignments <code class="language-plaintext highlighter-rouge">A</code> that collapse to <code class="language-plaintext highlighter-rouge">Y</code>.</li>
  <li>Computed efficiently using <strong>Dynamic Programming</strong> (Forward-Backward algorithm).</li>
</ul>

<p><strong>Pros:</strong></p>
<ul>
  <li>Fast inference (O(1) per step).</li>
  <li>Streaming friendly.</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>Can’t model language (e.g., “pair” vs “pear” sounds same). Needs external Language Model.</li>
</ul>

<h2 id="19-deep-dive-rnn-transducer-rnn-t">19. Deep Dive: RNN-Transducer (RNN-T)</h2>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Encoder (Audio):</strong> Processes audio frames <code class="language-plaintext highlighter-rouge">x_t</code>. Produces <code class="language-plaintext highlighter-rouge">h_t^{enc}</code>.</li>
  <li><strong>Prediction Network (Text):</strong> Processes previous non-blank output <code class="language-plaintext highlighter-rouge">y_{u-1}</code>. Produces <code class="language-plaintext highlighter-rouge">h_u^{pred}</code>. (Like an LM).</li>
  <li><strong>Joint Network:</strong> Combines them.
 <code class="language-plaintext highlighter-rouge">z_{t,u} = \text{ReLU}(W_{enc} h_t^{enc} + W_{pred} h_u^{pred})</code>
 <code class="language-plaintext highlighter-rouge">P(y|t,u) = \text{softmax}(W_{out} z_{t,u})</code></li>
</ol>

<p><strong>Inference (Greedy):</strong></p>
<ul>
  <li>If output is non-blank (<code class="language-plaintext highlighter-rouge">y</code>), feed to Prediction Network, increment <code class="language-plaintext highlighter-rouge">u</code>. Stay at audio frame <code class="language-plaintext highlighter-rouge">t</code>.</li>
  <li>If output is blank (<code class="language-plaintext highlighter-rouge">\epsilon</code>), move to next audio frame <code class="language-plaintext highlighter-rouge">t+1</code>.</li>
</ul>

<p><strong>Why it wins:</strong></p>
<ul>
  <li><strong>Streaming:</strong> Encoder is causal.</li>
  <li><strong>Accuracy:</strong> Prediction network models label dependencies (unlike CTC).</li>
  <li><strong>Latency:</strong> Controllable.</li>
</ul>

<h2 id="20-deep-dive-transformer-vs-conformer">20. Deep Dive: Transformer vs Conformer</h2>

<p><strong>Transformer:</strong></p>
<ul>
  <li>Great at global context (Self-Attention).</li>
  <li>Bad at local fine-grained details (needs deep layers to see local patterns).</li>
  <li>Positional encodings are brittle for varying audio lengths.</li>
</ul>

<p><strong>Conformer (Convolution-augmented Transformer):</strong></p>
<ul>
  <li><strong>Macaron Style:</strong> Feed-Forward -&gt; Multi-Head Attention -&gt; Conv Module -&gt; Feed-Forward.</li>
  <li><strong>Conv Module:</strong> Pointwise Conv -&gt; Gated Linear Unit (GLU) -&gt; Depthwise Conv -&gt; BatchNorm -&gt; Swish -&gt; Pointwise Conv.</li>
  <li><strong>Why:</strong></li>
  <li><strong>CNNs</strong> capture local features (formants, transitions).</li>
  <li><strong>Transformers</strong> capture global semantics.</li>
  <li><strong>Result:</strong> SOTA on LibriSpeech.</li>
</ul>

<h2 id="21-deep-dive-fastspeech-2-non-autoregressive-tts">21. Deep Dive: FastSpeech 2 (Non-Autoregressive TTS)</h2>

<p><strong>Problem with Tacotron 2:</strong></p>
<ul>
  <li>Autoregressive (slow).</li>
  <li>Unstable attention (skipping/repeating).</li>
  <li>Hard to control prosody (speed, pitch).</li>
</ul>

<p><strong>FastSpeech 2 Architecture:</strong></p>
<ol>
  <li><strong>Encoder:</strong> Feed-Forward Transformer.</li>
  <li><strong>Variance Adaptor:</strong>
    <ul>
      <li><strong>Duration Predictor:</strong> How many frames does this phoneme last? (Trained on forced alignment).</li>
      <li><strong>Pitch Predictor:</strong> Predicts F0 contour.</li>
      <li><strong>Energy Predictor:</strong> Predicts volume.</li>
      <li>Adds embeddings of these predictions to the encoder output.</li>
    </ul>
  </li>
  <li><strong>Length Regulator:</strong> Expands hidden states based on duration (e.g., “a” lasts 5 frames -&gt; repeat vector 5 times).</li>
  <li><strong>Decoder:</strong> Feed-Forward Transformer.</li>
</ol>

<p><strong>Pros:</strong></p>
<ul>
  <li><strong>Fast:</strong> Parallel generation.</li>
  <li><strong>Robust:</strong> No attention failures.</li>
  <li><strong>Controllable:</strong> Can explicitly set “speak 1.2x faster” or “raise pitch”.</li>
</ul>

<h2 id="22-deep-dive-vits-conditional-variational-autoencoder">22. Deep Dive: VITS (Conditional Variational Autoencoder)</h2>

<p><strong>VITS (Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech)</strong> is the current SOTA for E2E TTS.</p>

<p><strong>Key Idea:</strong> Combine Acoustic Model and Vocoder into one flow-based model.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Posterior Encoder:</strong> Encodes linear spectrogram into latent <code class="language-plaintext highlighter-rouge">z</code>.</li>
  <li><strong>Prior Encoder:</strong> Predicts distribution of <code class="language-plaintext highlighter-rouge">z</code> from text (conditioned on alignment).</li>
  <li><strong>Decoder (Generator):</strong> Transforms <code class="language-plaintext highlighter-rouge">z</code> into waveform (HiFi-GAN style).</li>
  <li><strong>Stochastic Duration Predictor:</strong> Models duration uncertainty.</li>
</ul>

<p><strong>Training:</strong></p>
<ul>
  <li><strong>Reconstruction Loss:</strong> Mel-spectrogram L1 loss.</li>
  <li><strong>KL Divergence:</strong> Between Posterior and Prior.</li>
  <li><strong>Adversarial Loss:</strong> Discriminator tries to distinguish real vs generated audio.</li>
</ul>

<p><strong>Result:</strong> extremely natural, high-fidelity speech.</p>

<h2 id="23-system-design-real-time-speech-translation-system">23. System Design: Real-Time Speech Translation System</h2>

<p><strong>Scenario:</strong> “Universal Translator” device. English Audio -&gt; Spanish Audio.</p>

<p><strong>Constraints:</strong></p>
<ul>
  <li><strong>Latency:</strong> &lt; 2 seconds lag.</li>
  <li><strong>Compute:</strong> Edge device (limited).</li>
</ul>

<p><strong>Architecture Choices:</strong></p>

<p><strong>Option A: Cascade (ASR -&gt; MT -&gt; TTS)</strong></p>
<ul>
  <li><strong>Pros:</strong> Modular. Can use SOTA for each.</li>
  <li><strong>Cons:</strong> Error propagation. Latency adds up. Loss of paralinguistics (tone).</li>
</ul>

<p><strong>Option B: Direct S2ST (Audio -&gt; Audio)</strong></p>
<ul>
  <li><strong>Pros:</strong> Fast. Preserves tone.</li>
  <li><strong>Cons:</strong> Data scarcity. Hard to train.</li>
</ul>

<p><strong>Hybrid Design (Production):</strong></p>
<ol>
  <li><strong>Streaming ASR (RNN-T):</strong> Generates English text stream.</li>
  <li><strong>Streaming MT:</strong> Translates partial sentences. “Hello” -&gt; “Hola”.</li>
  <li><strong>Incremental TTS:</strong> Generates audio for “Hola” immediately.</li>
</ol>

<p><strong>Wait-k Policy:</strong></p>
<ul>
  <li>MT model waits for <code class="language-plaintext highlighter-rouge">k</code> words before translating.</li>
  <li>Balances context (accuracy) vs latency.</li>
</ul>

<h2 id="24-case-study-openai-whisper">24. Case Study: OpenAI Whisper</h2>

<p><strong>Goal:</strong> Robust ASR that works on “in the wild” audio.</p>

<p><strong>Data:</strong></p>
<ul>
  <li>680,000 hours of web audio.</li>
  <li><strong>Weak Supervision:</strong> Transcripts from ASR systems, subtitles (noisy).</li>
  <li><strong>Multitask:</strong></li>
  <li>English Transcription.</li>
  <li>Any-to-English Translation.</li>
  <li>Language Identification.</li>
  <li>Voice Activity Detection.</li>
</ul>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Standard Encoder-Decoder Transformer.</li>
  <li><strong>Input:</strong> Log-Mel Spectrogram (30 seconds).</li>
  <li><strong>Output:</strong> Text tokens.</li>
</ul>

<p><strong>Key Features:</strong></p>
<ul>
  <li><strong>Task Tokens:</strong> <code class="language-plaintext highlighter-rouge">&lt;|startoftranscript|&gt; &lt;|en|&gt; &lt;|transcribe|&gt; &lt;|notimestamps|&gt;</code>.</li>
  <li><strong>Long-form:</strong> Processes 30s chunks. Uses previous chunk’s text as prompt (context).</li>
</ul>

<p><strong>Impact:</strong></p>
<ul>
  <li>Zero-shot performance on many datasets matches supervised models.</li>
  <li>Proved that <strong>Data Scale &gt; Model Architecture</strong>.</li>
</ul>

<h2 id="25-case-study-google-usm-universal-speech-model">25. Case Study: Google USM (Universal Speech Model)</h2>

<p><strong>Goal:</strong> One model for 300+ languages.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Conformer</strong> (2 Billion parameters).</li>
  <li><strong>MOST (Multi-Objective Supervised Training):</strong></li>
  <li><strong>BEST-RQ:</strong> Self-supervised loss (BERT-style on audio).</li>
  <li><strong>Text-Injection:</strong> Train on text-only data (using shared encoder layers).</li>
  <li><strong>ASR:</strong> Supervised loss on labeled audio.</li>
</ul>

<p><strong>Result:</strong></p>
<ul>
  <li>SOTA on 73 languages.</li>
  <li>Enables ASR for languages with &lt; 10 hours of data.</li>
</ul>

<h2 id="26-deep-dive-evaluation-metrics">26. Deep Dive: Evaluation Metrics</h2>

<p><strong>ASR:</strong></p>
<ul>
  <li><strong>WER (Word Error Rate):</strong> <code class="language-plaintext highlighter-rouge">\frac{S + D + I}{N}</code></li>
  <li>S: Substitutions, D: Deletions, I: Insertions, N: Total words.</li>
  <li><strong>CER (Character Error Rate):</strong> For languages without spaces (Chinese).</li>
  <li><strong>RTF (Real-Time Factor):</strong> <code class="language-plaintext highlighter-rouge">\frac{\text{Processing Time}}{\text{Audio Duration}}</code>. RTF &lt; 1 means real-time.</li>
</ul>

<p><strong>TTS:</strong></p>
<ul>
  <li><strong>MOS (Mean Opinion Score):</strong> Humans rate naturalness 1-5.</li>
  <li><strong>MCD (Mel Cepstral Distortion):</strong> Objective distance between generated and ground truth spectrograms.</li>
</ul>

<p><strong>ST (Speech Translation):</strong></p>
<ul>
  <li><strong>BLEU:</strong> Standard MT metric.</li>
</ul>

<h2 id="27-code-implementing-beam-search-for-seq2seq">27. Code: Implementing Beam Search for Seq2Seq</h2>

<p>Greedy decoding (pick max prob) is suboptimal. Beam search explores <code class="language-plaintext highlighter-rouge">k</code> paths.</p>

<p>``python
def beam_search_decoder(model, encoder_out, beam_width=3, max_len=50):
 # Start with [SOS] token
 # Beam: list of (sequence, score, hidden_state)
 start_token = model.vocab[‘<sos>']
 beam = [([start_token], 0.0, None)]</sos></p>

<p>for _ in range(max_len):
 candidates = []</p>

<p>for seq, score, hidden in beam:
 if seq[-1] == model.vocab[‘<eos>']:
 candidates.append((seq, score, hidden))
 continue</eos></p>

<p># Predict next token
 last_token = torch.tensor([[seq[-1]]])
 output, new_hidden = model.decoder(last_token, hidden, encoder_out)
 log_probs = torch.log_softmax(output, dim=-1)</p>

<p># Get top k
 topk_probs, topk_ids = log_probs.topk(beam_width)</p>

<p>for i in range(beam_width):
 token = topk_ids[0][i].item()
 prob = topk_probs[0][i].item()
 candidates.append((seq + [token], score + prob, new_hidden))</p>

<p># Select top k candidates
 beam = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]</p>

<p># Check if all finished
 if all(c[0][-1] == model.vocab[‘<eos>'] for c in beam):
 break</eos></p>

<p>return beam[0][0] # Return best sequence
``</p>

<h2 id="28-production-serving-speech-models-with-triton">28. Production: Serving Speech Models with Triton</h2>

<p><strong>NVIDIA Triton Inference Server</strong> is standard for deploying speech models.</p>

<p><strong>Pipeline:</strong></p>
<ol>
  <li><strong>Feature Extractor (Python Backend):</strong> Audio -&gt; Mel-Spec.</li>
  <li><strong>Encoder (TensorRT):</strong> Mel-Spec -&gt; Hidden States.</li>
  <li><strong>Decoder (TensorRT):</strong> Hidden States -&gt; Text (Beam Search).</li>
</ol>

<p><strong>Dynamic Batching:</strong></p>
<ul>
  <li>Triton groups requests arriving within 5ms into a batch.</li>
  <li>Increases GPU utilization significantly.</li>
</ul>

<p><strong>Ensemble Model:</strong></p>
<ul>
  <li>Define a DAG of models.</li>
  <li>Client sends audio, Triton handles the flow.</li>
</ul>

<h2 id="29-deep-dive-end-to-end-speech-to-speech-translation-s2st">29. Deep Dive: End-to-End Speech-to-Speech Translation (S2ST)</h2>

<p><strong>Unit-Based S2ST:</strong></p>
<ul>
  <li>Instead of spectrograms, predict <strong>discrete acoustic units</strong> (from HuBERT or k-means).</li>
  <li><strong>Advantage:</strong> Discrete tokens allow using standard Transformer (like NLP).</li>
  <li><strong>Vocoder:</strong> Unit HiFi-GAN converts discrete units to waveform.</li>
</ul>

<p><strong>SpeechMatrix:</strong></p>
<ul>
  <li>Mining parallel speech from 100k hours of multilingual audio.</li>
  <li>Uses LASER embeddings to find matching sentences in different languages.</li>
</ul>

<h2 id="30-summary">30. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Task</th>
      <th style="text-align: left">Model</th>
      <th style="text-align: left">Input</th>
      <th style="text-align: left">Output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>ASR</strong></td>
      <td style="text-align: left">LAS, Whisper</td>
      <td style="text-align: left">Audio</td>
      <td style="text-align: left">Text</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>TTS</strong></td>
      <td style="text-align: left">Tacotron 2</td>
      <td style="text-align: left">Text</td>
      <td style="text-align: left">Audio</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>ST</strong></td>
      <td style="text-align: left">Translatotron</td>
      <td style="text-align: left">Audio (L1)</td>
      <td style="text-align: left">Audio (L2)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Vocoder</strong></td>
      <td style="text-align: left">HiFi-GAN</td>
      <td style="text-align: left">Mel-Spec</td>
      <td style="text-align: left">Waveform</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Streaming</strong></td>
      <td style="text-align: left">RNN-T</td>
      <td style="text-align: left">Audio Stream</td>
      <td style="text-align: left">Text Stream</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Fast TTS</strong></td>
      <td style="text-align: left">FastSpeech 2</td>
      <td style="text-align: left">Text</td>
      <td style="text-align: left">Mel-Spec</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td><strong>Streaming</strong></td>
      <td>RNN-T</td>
      <td>Audio Stream</td>
      <td>Text Stream</td>
    </tr>
    <tr>
      <td><strong>Fast TTS</strong></td>
      <td>FastSpeech 2</td>
      <td>Text</td>
      <td>Mel-Spec</td>
    </tr>
  </tbody>
</table>

<h2 id="31-deep-dive-self-supervised-learning-ssl-in-speech">31. Deep Dive: Self-Supervised Learning (SSL) in Speech</h2>

<p><strong>Problem:</strong> Labeled data (audio + text) is expensive. Unlabeled audio is free.</p>

<p><strong>Wav2Vec 2.0 (Meta):</strong></p>
<ul>
  <li><strong>Idea:</strong> Mask parts of the audio latent space and predict the quantized representation of the masked part.</li>
  <li><strong>Contrastive Loss:</strong> Identify the correct quantized vector among distractors.</li>
  <li><strong>Result:</strong> Can reach SOTA with only 10 minutes of labeled data (after pre-training on 53k hours).</li>
</ul>

<p><strong>HuBERT (Hidden Unit BERT):</strong></p>
<ul>
  <li><strong>Idea:</strong> Offline clustering (k-means) of MFCCs to generate pseudo-labels.</li>
  <li><strong>Masked Prediction:</strong> BERT-style objective. Predict the cluster ID of masked frames.</li>
  <li><strong>Result:</strong> More robust than Wav2Vec 2.0.</li>
</ul>

<p><strong>Impact on Seq2Seq:</strong></p>
<ul>
  <li>Initialize the Encoder with Wav2Vec 2.0 / HuBERT.</li>
  <li>Add a random Decoder.</li>
  <li>Fine-tune on ASR task.</li>
  <li><strong>Benefit:</strong> Drastic reduction in required labeled data.</li>
</ul>

<h2 id="32-deep-dive-multilingual-seq2seq">32. Deep Dive: Multilingual Seq2Seq</h2>

<p><strong>Goal:</strong> One model for 100 languages.</p>

<p><strong>Challenges:</strong></p>
<ul>
  <li><strong>Data Imbalance:</strong> English has 1M hours, Swahili has 100.</li>
  <li><strong>Script Diversity:</strong> Latin, Cyrillic, Chinese, Arabic.</li>
  <li><strong>Phonetic Overlap:</strong> “P” in English <code class="language-plaintext highlighter-rouge">\neq</code> “P” in French.</li>
</ul>

<p><strong>Solutions:</strong></p>
<ol>
  <li><strong>Language ID Token:</strong> <code class="language-plaintext highlighter-rouge">&lt;|en|&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;|fr|&gt;</code>.</li>
  <li><strong>Shared Vocabulary:</strong> SentencePiece (BPE) trained on all languages.</li>
  <li><strong>Balancing Sampling:</strong> Up-sample low-resource languages during training.
 <code class="language-plaintext highlighter-rouge">p_l \propto (\frac{N_l}{N_{total}})^\alpha</code>
 where <code class="language-plaintext highlighter-rouge">\alpha &lt; 1</code> (e.g., 0.3) flattens the distribution.</li>
  <li><strong>Adapter Modules:</strong> Small, language-specific layers inserted into the frozen giant model.</li>
</ol>

<h2 id="33-deep-dive-end-to-end-slu-spoken-language-understanding">33. Deep Dive: End-to-End SLU (Spoken Language Understanding)</h2>

<p><strong>Traditional:</strong> Audio → ASR → Text → NLP → Intent/Slots.
<strong>E2E SLU:</strong> Audio → Intent/Slots.</p>

<p><strong>Why E2E?</strong></p>
<ul>
  <li><strong>Error Robustness:</strong> ASR might transcribe “play jazz” as “play jas”. NLP fails. E2E model learns acoustic features of “jazz”.</li>
  <li><strong>Paralinguistics:</strong> “Yeah right” (sarcastic) -&gt; Negative Sentiment. Text-only NLP misses this.</li>
</ul>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Encoder:</strong> Pre-trained Wav2Vec 2.0.</li>
  <li><strong>Decoder:</strong> Predicts semantic frame directly.</li>
  <li><code class="language-plaintext highlighter-rouge">[INTENT: PlayMusic] [ARTIST: The Beatles] [GENRE: Rock]</code></li>
</ul>

<p><strong>Challenges:</strong></p>
<ul>
  <li>Lack of labeled Audio-to-Semantics data.</li>
  <li><strong>Solution:</strong> Transfer learning from ASR models.</li>
</ul>

<h2 id="34-deep-dive-attention-visualization-code">34. Deep Dive: Attention Visualization Code</h2>

<p>Visualizing attention maps is the best way to debug Seq2Seq models.</p>

<p>``python
import matplotlib.pyplot as plt
import seaborn as sns</p>

<p>def plot_attention(attention_matrix, input_text, output_text):
 “””
 attention_matrix: (Output_Len, Input_Len) numpy array
 “””
 plt.figure(figsize=(10, 8))
 sns.heatmap(attention_matrix, cmap=’viridis’, 
 xticklabels=input_text, yticklabels=output_text)
 plt.xlabel(‘Encoder Input’)
 plt.ylabel(‘Decoder Output’)
 plt.show()</p>

<h1 id="example-usage">Example usage</h1>
<h1 id="attn--modelget_attention_weights">attn = model.get_attention_weights(…)</h1>
<h1 id="plot_attentionattn-audio_frames-predicted_words">plot_attention(attn, audio_frames, predicted_words)</h1>
<p>``</p>

<p><strong>What to look for:</strong></p>
<ul>
  <li><strong>Diagonal:</strong> Good alignment.</li>
  <li><strong>Vertical Line:</strong> Decoder is stuck on one frame (repeating output).</li>
  <li><strong>Horizontal Line:</strong> Encoder frame is ignored.</li>
  <li><strong>Fuzzy:</strong> Weak attention (low confidence).</li>
</ul>

<h2 id="35-deep-dive-handling-long-form-audio">35. Deep Dive: Handling Long-Form Audio</h2>

<p>Standard Transformers have O(N^2) attention complexity. 30s audio = 1500 frames. 1 hour = 180,000 frames.</p>

<p><strong>Strategies:</strong></p>
<ol>
  <li><strong>Chunking (Whisper):</strong>
    <ul>
      <li>Slice audio into 30s segments.</li>
      <li>Transcribe independently.</li>
      <li><strong>Problem:</strong> Context cut off at boundaries.</li>
      <li><strong>Fix:</strong> Pass previous segment’s text as prompt.</li>
    </ul>
  </li>
  <li><strong>Streaming (RNN-T):</strong>
    <ul>
      <li>Process frame-by-frame. Infinite length.</li>
      <li><strong>Problem:</strong> No future context.</li>
    </ul>
  </li>
  <li><strong>Sparse Attention (BigBird / Longformer):</strong>
    <ul>
      <li>Attend only to local window + global tokens.</li>
      <li>O(N) complexity.</li>
    </ul>
  </li>
  <li><strong>Block-Processing (Emformer):</strong>
    <ul>
      <li>Block-wise processing with memory bank for history.</li>
    </ul>
  </li>
</ol>

<h2 id="36-future-trends-audio-language-models">36. Future Trends: Audio-Language Models</h2>

<p><strong>SpeechGPT / AudioLM:</strong></p>
<ul>
  <li>Treat audio tokens and text tokens as the same thing.</li>
  <li><strong>Tokenizer:</strong> SoundStream / EnCodec (Neural Audio Codec).</li>
  <li><strong>Model:</strong> Decoder-only Transformer (GPT).</li>
  <li><strong>Training:</strong></li>
  <li>Text-only data (Web).</li>
  <li>Audio-only data (Radio).</li>
  <li>Paired data (ASR).</li>
</ul>

<p><strong>Capabilities:</strong></p>
<ul>
  <li><strong>Speech-to-Speech Translation:</strong> “Translate this to French” (Audio input) -&gt; Audio output.</li>
  <li><strong>Voice Continuation:</strong> Continue speaking in the user’s voice.</li>
  <li><strong>Zero-shot TTS:</strong> “Say ‘Hello’ in this voice: [Audio Prompt]”.</li>
</ul>

<h2 id="37-ethical-considerations-voice-cloning--deepfakes">37. Ethical Considerations: Voice Cloning &amp; Deepfakes</h2>

<p>Seq2Seq TTS (Vall-E) can clone a voice with 3 seconds of audio.</p>

<p><strong>Risks:</strong></p>
<ul>
  <li><strong>Fraud:</strong> Impersonating CEOs or relatives.</li>
  <li><strong>Disinformation:</strong> Fake speeches by politicians.</li>
  <li><strong>Harassment:</strong> Fake audio of individuals.</li>
</ul>

<p><strong>Mitigation:</strong></p>
<ul>
  <li><strong>Watermarking:</strong> Embed inaudible signals in generated audio.</li>
  <li><strong>Detection:</strong> Train classifiers to detect artifacts of synthesis.</li>
  <li>
    <p><strong>Regulation:</strong> “Know Your Customer” for TTS APIs.</p>
  </li>
  <li><strong>Regulation:</strong> “Know Your Customer” for TTS APIs.</li>
</ul>

<h2 id="38-deep-dive-the-mathematics-of-transformers-in-speech">38. Deep Dive: The Mathematics of Transformers in Speech</h2>

<p>Why did Transformers replace RNNs?</p>

<p><strong>1. Self-Attention Complexity:</strong></p>
<ul>
  <li><strong>RNN:</strong> O(N) sequential operations. Cannot parallelize.</li>
  <li><strong>Transformer:</strong> O(1) sequential operations (parallelizable). O(N^2) memory.</li>
  <li><strong>Benefit:</strong> We can train on 1000 GPUs efficiently.</li>
</ul>

<p><strong>2. Positional Encodings in Speech:</strong></p>
<ul>
  <li>Text uses absolute sinusoidal encodings.</li>
  <li><strong>Speech Problem:</strong> Audio length varies wildly. 10s vs 10m.</li>
  <li><strong>Solution:</strong> <strong>Relative Positional Encoding.</strong></li>
  <li>Instead of adding <code class="language-plaintext highlighter-rouge">P_i</code> to input <code class="language-plaintext highlighter-rouge">X_i</code>, add a bias <code class="language-plaintext highlighter-rouge">b_{i-j}</code> to the attention score <code class="language-plaintext highlighter-rouge">A_{ij}</code>.</li>
  <li>Allows the model to generalize to audio lengths unseen during training.</li>
</ul>

<p><strong>3. Subsampling:</strong></p>
<ul>
  <li>Audio is high-frequency (100 frames/sec). Text is low-frequency (3 chars/sec).</li>
  <li><strong>Conv Subsampling:</strong> First 2 layers of Encoder are strided Convolutions (stride 2x2 = 4x reduction).</li>
  <li>Reduces sequence length <code class="language-plaintext highlighter-rouge">N \to N/4</code>. Reduces attention cost <code class="language-plaintext highlighter-rouge">N^2 \to (N/4)^2 = N^2/16</code>.</li>
</ul>

<h2 id="39-deep-dive-troubleshooting-seq2seq-models">39. Deep Dive: Troubleshooting Seq2Seq Models</h2>

<p><strong>1. The “Hallucination” Problem:</strong></p>
<ul>
  <li><strong>Symptom:</strong> Model outputs “Thank you Thank you Thank you” during silence.</li>
  <li><strong>Cause:</strong> Decoder language model is too strong; it predicts likely text even without acoustic evidence.</li>
  <li><strong>Fix:</strong></li>
  <li><strong>Voice Activity Detection (VAD):</strong> Filter out silence.</li>
  <li><strong>Coverage Penalty:</strong> Penalize attending to the same frames repeatedly.</li>
</ul>

<p><strong>2. The “NaN” Loss:</strong></p>
<ul>
  <li><strong>Symptom:</strong> Training crashes.</li>
  <li><strong>Cause:</strong> Exploding gradients in LSTM or division by zero in BatchNorm.</li>
  <li><strong>Fix:</strong></li>
  <li>Gradient Clipping (norm 1.0).</li>
  <li>Warmup learning rate.</li>
  <li>Check for empty transcripts in data.</li>
</ul>

<p><strong>3. The “Babble” Problem:</strong></p>
<ul>
  <li><strong>Symptom:</strong> Output is gibberish.</li>
  <li><strong>Cause:</strong> CTC alignment failed or Attention didn’t converge.</li>
  <li><strong>Fix:</strong></li>
  <li><strong>Curriculum Learning:</strong> Train on short utterances first, then long.</li>
  <li>
    <p><strong>Guided Attention Loss:</strong> Force diagonal alignment for first few epochs.</p>
  </li>
  <li><strong>Guided Attention Loss:</strong> Force diagonal alignment for first few epochs.</li>
</ul>

<h2 id="40-deep-dive-the-future---audio-language-models">40. Deep Dive: The Future - Audio-Language Models</h2>

<p>The boundary between Speech and NLP is blurring.</p>

<p><strong>AudioLM (Google):</strong></p>
<ul>
  <li>Treats audio as a sequence of discrete tokens (using SoundStream codec).</li>
  <li>Uses a GPT-style decoder to generate audio tokens.</li>
  <li><strong>Capabilities:</strong></li>
  <li><strong>Speech Continuation:</strong> Given 3s of speech, continue speaking in the same voice and style.</li>
  <li><strong>Zero-Shot TTS:</strong> Generate speech from text in a target voice without fine-tuning.</li>
</ul>

<p><strong>SpeechGPT (Fudan University):</strong></p>
<ul>
  <li>Fine-tunes LLaMA on paired speech-text data.</li>
  <li><strong>Modality Adaptation:</strong> Teaches the LLM to understand discrete audio tokens.</li>
  <li><strong>Result:</strong> A chatbot you can talk to, which talks back with emotion and nuance.</li>
</ul>

<p><strong>Implication:</strong></p>
<ul>
  <li>Seq2Seq models (Encoder-Decoder) might be replaced by Decoder-only LLMs that handle all modalities (Text, Audio, Image) in a unified token space.</li>
</ul>

<h2 id="41-ethical-considerations-in-seq2seq-speech">41. Ethical Considerations in Seq2Seq Speech</h2>

<p><strong>1. Deepfakes &amp; Voice Cloning:</strong></p>
<ul>
  <li>Models like Vall-E can clone a voice from 3 seconds of audio.</li>
  <li><strong>Risk:</strong> Fraud (fake CEO calls), harassment, disinformation.</li>
  <li><strong>Mitigation:</strong></li>
  <li><strong>Watermarking:</strong> Embed inaudible signals in generated audio.</li>
  <li><strong>Detection:</strong> Train classifiers to detect synthesis artifacts.</li>
</ul>

<p><strong>2. Bias in ASR:</strong></p>
<ul>
  <li>Models trained on LibriSpeech (audiobooks) fail on AAVE (African American Vernacular English) or Indian accents.</li>
  <li><strong>Fix:</strong> Diverse training data. “Fairness-aware” loss functions that penalize disparity between groups.</li>
</ul>

<p><strong>3. Privacy:</strong></p>
<ul>
  <li>Smart speakers listen constantly.</li>
  <li><strong>Fix:</strong> <strong>On-Device Processing.</strong> Run the Seq2Seq model on the phone’s NPU, never sending audio to the cloud.</li>
</ul>

<h2 id="42-further-reading">42. Further Reading</h2>

<p>To master Seq2Seq speech models, these papers are essential:</p>

<ol>
  <li><strong>“Listen, Attend and Spell” (Chan et al., 2016):</strong> The paper that introduced Attention to ASR.</li>
  <li><strong>“Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions” (Shen et al., 2018):</strong> The Tacotron 2 paper.</li>
  <li><strong>“Attention Is All You Need” (Vaswani et al., 2017):</strong> The Transformer paper (foundation of modern speech).</li>
  <li><strong>“Conformer: Convolution-augmented Transformer for Speech Recognition” (Gulati et al., 2020):</strong> The current SOTA architecture for ASR.</li>
  <li><strong>“Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech Representations” (Baevski et al., 2020):</strong> The SSL revolution.</li>
  <li><strong>“Whisper: Robust Speech Recognition via Large-Scale Weak Supervision” (Radford et al., 2022):</strong> How scale beats architecture.</li>
</ol>

<h2 id="43-summary">43. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Task</th>
      <th style="text-align: left">Model</th>
      <th style="text-align: left">Input</th>
      <th style="text-align: left">Output</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>ASR</strong></td>
      <td style="text-align: left">LAS, Whisper</td>
      <td style="text-align: left">Audio</td>
      <td style="text-align: left">Text</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>TTS</strong></td>
      <td style="text-align: left">Tacotron 2</td>
      <td style="text-align: left">Text</td>
      <td style="text-align: left">Audio</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>ST</strong></td>
      <td style="text-align: left">Translatotron</td>
      <td style="text-align: left">Audio (L1)</td>
      <td style="text-align: left">Audio (L2)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Vocoder</strong></td>
      <td style="text-align: left">HiFi-GAN</td>
      <td style="text-align: left">Mel-Spec</td>
      <td style="text-align: left">Waveform</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Streaming</strong></td>
      <td style="text-align: left">RNN-T</td>
      <td style="text-align: left">Audio Stream</td>
      <td style="text-align: left">Text Stream</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Fast TTS</strong></td>
      <td style="text-align: left">FastSpeech 2</td>
      <td style="text-align: left">Text</td>
      <td style="text-align: left">Mel-Spec</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>SSL</strong></td>
      <td style="text-align: left">Wav2Vec 2.0</td>
      <td style="text-align: left">Masked Audio</td>
      <td style="text-align: left">Quantized Vector</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>E2E SLU</strong></td>
      <td style="text-align: left">SLU-BERT</td>
      <td style="text-align: left">Audio</td>
      <td style="text-align: left">Intent/Slots</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Troubleshooting</strong></td>
      <td style="text-align: left">VAD, Gradient Clipping</td>
      <td style="text-align: left">-</td>
      <td style="text-align: left">-</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Future</strong></td>
      <td style="text-align: left">AudioLM</td>
      <td style="text-align: left">Discrete Tokens</td>
      <td style="text-align: left">Discrete Tokens</td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0037-sequence-to-sequence-speech/">arunbaby.com/speech-tech/0037-sequence-to-sequence-speech</a></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#attention" class="page__taxonomy-item p-category" rel="tag">attention</a><span class="sep">, </span>
    
      <a href="/tags/#seq2seq" class="page__taxonomy-item p-category" rel="tag">seq2seq</a><span class="sep">, </span>
    
      <a href="/tags/#speech-translation" class="page__taxonomy-item p-category" rel="tag">speech-translation</a><span class="sep">, </span>
    
      <a href="/tags/#tts" class="page__taxonomy-item p-category" rel="tag">tts</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech_tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0037-longest-increasing-subsequence/" rel="permalink">Longest Increasing Subsequence (LIS)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Finding the longest upward trend in chaos.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0037-sequence-modeling/" rel="permalink">Sequence Modeling in ML
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Predicting the next word, the next stock price, the next frame.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0037-code-execution-agents/" rel="permalink">Code Execution Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Let agents run code safely: sandbox execution, cap damage, and verify outputs like a production system.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Sequence-to-Sequence+Speech+Models%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0037-sequence-to-sequence-speech%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0037-sequence-to-sequence-speech%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0037-sequence-to-sequence-speech/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0036-multi-task-speech-learning/" class="pagination--pager" title="Multi-task Speech Learning">Previous</a>
    
    
      <a href="/speech-tech/0038-speech-hyperparameter-tuning/" class="pagination--pager" title="Speech Hyperparameter Tuning">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
