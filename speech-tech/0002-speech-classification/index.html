<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Speech Command Classification - Arun Baby</title>
<meta name="description" content="How voice assistants recognize “turn on the lights” from raw audio in under 100ms without full ASR transcription.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Speech Command Classification">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0002-speech-classification/">


  <meta property="og:description" content="How voice assistants recognize “turn on the lights” from raw audio in under 100ms without full ASR transcription.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Speech Command Classification">
  <meta name="twitter:description" content="How voice assistants recognize “turn on the lights” from raw audio in under 100ms without full ASR transcription.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0002-speech-classification/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0002-speech-classification/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Speech Command Classification">
    <meta itemprop="description" content="How voice assistants recognize “turn on the lights” from raw audio in under 100ms without full ASR transcription.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0002-speech-classification/" itemprop="url">Speech Command Classification
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          29 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#problem-definition">Problem Definition</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li></ul></li><li><a href="#why-not-asr--nlu">Why Not ASR + NLU?</a><ul><li><a href="#traditional-pipeline">Traditional Pipeline</a></li><li><a href="#direct-classification">Direct Classification</a></li></ul></li><li><a href="#architecture">Architecture</a></li><li><a href="#component-1-audio-preprocessing">Component 1: Audio Preprocessing</a><ul><li><a href="#fixed-length-input">Fixed-Length Input</a></li><li><a href="#normalization">Normalization</a></li></ul></li><li><a href="#component-2-feature-extraction">Component 2: Feature Extraction</a><ul><li><a href="#option-1-mfccs-mel-frequency-cepstral-coefficients">Option 1: MFCCs (Mel-Frequency Cepstral Coefficients)</a></li><li><a href="#option-2-mel-spectrogram">Option 2: Mel-Spectrogram</a></li></ul></li><li><a href="#component-3-model-architectures">Component 3: Model Architectures</a><ul><li><a href="#architecture-1-cnn-fastest-for-on-device">Architecture 1: CNN (Fastest for On-Device)</a></li><li><a href="#architecture-2-rnn-better-temporal-modeling">Architecture 2: RNN (Better Temporal Modeling)</a></li><li><a href="#architecture-3-attention-based-best-accuracy">Architecture 3: Attention-Based (Best Accuracy)</a></li><li><a href="#model-comparison">Model Comparison</a></li></ul></li><li><a href="#training-strategy">Training Strategy</a><ul><li><a href="#data-collection">Data Collection</a></li><li><a href="#data-augmentation">Data Augmentation</a></li><li><a href="#training-loop">Training Loop</a></li></ul></li><li><a href="#component-4-handling-unknown-commands">Component 4: Handling Unknown Commands</a><ul><li><a href="#strategy-1-add-unknown-class">Strategy 1: Add “Unknown” Class</a></li><li><a href="#strategy-2-confidence-thresholding">Strategy 2: Confidence Thresholding</a></li><li><a href="#strategy-3-out-of-distribution-ood-detection">Strategy 3: Out-of-Distribution (OOD) Detection</a></li></ul></li><li><a href="#model-optimization-for-edge-deployment">Model Optimization for Edge Deployment</a><ul><li><a href="#quantization">Quantization</a></li><li><a href="#pruning">Pruning</a></li><li><a href="#knowledge-distillation">Knowledge Distillation</a></li></ul></li><li><a href="#on-device-deployment">On-Device Deployment</a><ul><li><a href="#export-to-mobile-formats">Export to Mobile Formats</a></li><li><a href="#mobile-inference-code">Mobile Inference Code</a></li></ul></li><li><a href="#monitoring--evaluation">Monitoring &amp; Evaluation</a><ul><li><a href="#metrics-dashboard">Metrics Dashboard</a></li><li><a href="#online-monitoring">Online Monitoring</a></li></ul></li><li><a href="#multi-language-support">Multi-Language Support</a><ul><li><a href="#approach-1-separate-models-per-language">Approach 1: Separate Models per Language</a></li><li><a href="#approach-2-multilingual-shared-model">Approach 2: Multilingual Shared Model</a></li></ul></li><li><a href="#failure-cases--mitigation">Failure Cases &amp; Mitigation</a><ul><li><a href="#common-failure-modes">Common Failure Modes</a><ul><li><a href="#1-background-speechtv">1. Background Speech/TV</a></li><li><a href="#2-accented-speech">2. Accented Speech</a></li><li><a href="#3-noisy-environments">3. Noisy Environments</a></li><li><a href="#4-similar-sounding-commands">4. Similar Sounding Commands</a></li></ul></li></ul></li><li><a href="#production-deployment-architecture">Production Deployment Architecture</a><ul><li><a href="#edge-deployment-smart-speaker">Edge Deployment (Smart Speaker)</a></li><li><a href="#hybrid-edge-cloud-architecture">Hybrid Edge-Cloud Architecture</a></li></ul></li><li><a href="#ab-testing--gradual-rollout">A/B Testing &amp; Gradual Rollout</a><ul><li><a href="#experiment-framework">Experiment Framework</a></li><li><a href="#metrics-to-track">Metrics to Track</a></li></ul></li><li><a href="#real-world-examples">Real-World Examples</a><ul><li><a href="#google-assistant">Google Assistant</a></li><li><a href="#amazon-alexa">Amazon Alexa</a></li><li><a href="#apple-siri">Apple Siri</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li><li><a href="#further-reading">Further Reading</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>How voice assistants recognize “turn on the lights” from raw audio in under 100ms without full ASR transcription.</strong></p>

<h2 id="introduction">Introduction</h2>

<p>When you say “Alexa, turn off the lights” or “Hey Google, set a timer,” your voice assistant doesn’t actually transcribe your speech to text first. Instead, it uses a <strong>direct audio-to-intent classification</strong> system that’s:</p>

<ul>
  <li><strong>Faster</strong> than ASR + NLU (50-100ms vs 200-500ms)</li>
  <li><strong>Smaller</strong> models (&lt; 10MB vs 100MB+)</li>
  <li><strong>Works offline</strong> (on-device inference)</li>
  <li><strong>More privacy-preserving</strong> (no text sent to cloud)</li>
</ul>

<p>This approach is perfect for a <strong>limited vocabulary of commands</strong> (30-100 commands) where you care more about speed and privacy than open-ended understanding.</p>

<p><strong>What you’ll learn:</strong></p>
<ul>
  <li>Why direct audio→intent beats ASR→NLU for commands</li>
  <li>Audio feature extraction (MFCCs, mel-spectrograms)</li>
  <li>Model architectures (CNN, RNN, Attention)</li>
  <li>Training strategies and data augmentation</li>
  <li>On-device deployment and optimization</li>
  <li>Unknown command handling (OOD detection)</li>
  <li>Real-world examples from Google, Amazon, Apple</li>
</ul>

<hr />

<h2 id="problem-definition">Problem Definition</h2>

<p>Design a speech command classification system for a voice assistant that:</p>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Multi-class Classification</strong>
    <ul>
      <li>30-50 predefined commands</li>
      <li>Examples: “lights on”, “volume up”, “play music”, “stop timer”</li>
      <li>Support synonyms and variations</li>
    </ul>
  </li>
  <li><strong>Unknown Detection</strong>
    <ul>
      <li>Detect and reject out-of-vocabulary audio</li>
      <li>Handle background conversation</li>
      <li>Distinguish commands from non-commands</li>
    </ul>
  </li>
  <li><strong>Multi-language Support</strong>
    <ul>
      <li>5+ languages initially</li>
      <li>Shared model or separate models per language</li>
    </ul>
  </li>
  <li><strong>Context Awareness</strong>
    <ul>
      <li>Optional: Use device state as context</li>
      <li>Example: “turn it off” depends on what’s currently on</li>
    </ul>
  </li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Latency</strong>
    <ul>
      <li>End-to-end &lt; 100ms</li>
      <li>Includes audio buffering, processing, inference</li>
    </ul>
  </li>
  <li><strong>Model Constraints</strong>
    <ul>
      <li>Model size &lt; 10MB (on-device)</li>
      <li>RAM usage &lt; 50MB during inference</li>
      <li>CPU-only (no GPU on most devices)</li>
    </ul>
  </li>
  <li><strong>Accuracy</strong>
    <ul>
      <li>
        <blockquote>
          <p>95% on target commands (clean audio)</p>
        </blockquote>
      </li>
      <li>
        <blockquote>
          <p>90% on noisy audio</p>
        </blockquote>
      </li>
      <li>&lt; 5% false positive rate</li>
    </ul>
  </li>
  <li><strong>Throughput</strong>
    <ul>
      <li>1000 QPS per server (cloud)</li>
      <li>Single inference on device</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="why-not-asr--nlu">Why Not ASR + NLU?</h2>

<h3 id="traditional-pipeline">Traditional Pipeline</h3>

<p><code class="language-plaintext highlighter-rouge">
Audio → ASR → Text → NLU → Intent
"lights on" → ASR (200ms) → "lights on" → NLU (50ms) → {action: "lights", state: "on"}
Total latency: 250ms
</code></p>

<h3 id="direct-classification">Direct Classification</h3>

<p><code class="language-plaintext highlighter-rouge">
Audio → Audio Features → CNN → Intent
"lights on" → Mel-spec (5ms) → CNN (40ms) → {action: "lights", state: "on"}
Total latency: 45ms
</code></p>

<p><strong>Advantages:</strong></p>
<ul>
  <li>✅ 5x faster (45ms vs 250ms)</li>
  <li>✅ 10x smaller model (5MB vs 50MB)</li>
  <li>✅ Works offline</li>
  <li>✅ More private (no text)</li>
  <li>✅ Fewer points of failure</li>
</ul>

<p><strong>Disadvantages:</strong></p>
<ul>
  <li>❌ Limited vocabulary (30-50 commands vs unlimited)</li>
  <li>❌ Less flexible (new commands need retraining)</li>
  <li>❌ Can’t handle complex queries (“turn on the lights in the living room at 8pm”)</li>
</ul>

<p><strong>When to use each:</strong></p>
<ul>
  <li><strong>Direct classification:</strong> Simple commands, latency-critical, on-device</li>
  <li><strong>ASR + NLU:</strong> Complex queries, unlimited vocabulary, cloud-based</li>
</ul>

<hr />

<h2 id="architecture">Architecture</h2>

<p><code class="language-plaintext highlighter-rouge">
Audio Input (1-2 seconds @ 16kHz)
 ↓
Audio Preprocessing
 ├─ Resampling (if needed)
 ├─ Padding/Trimming to fixed length
 └─ Normalization
 ↓
Feature Extraction
 ├─ MFCCs (40 coefficients)
 or
 ├─ Mel-Spectrogram (40 bins)
 ↓
Neural Network
 ├─ CNN (fastest, on-device)
 or
 ├─ RNN (better temporal modeling)
 or
 ├─ Attention (best accuracy, slower)
 ↓
Softmax Layer (31 classes)
 ├─ 30 command classes
 └─ 1 unknown class
 ↓
Post-processing
 ├─ Confidence thresholding
 ├─ Unknown detection
 └─ Output filtering
 ↓
Prediction: {command: "lights_on", confidence: 0.94}
</code></p>

<hr />

<h2 id="component-1-audio-preprocessing">Component 1: Audio Preprocessing</h2>

<h3 id="fixed-length-input">Fixed-Length Input</h3>

<p><strong>Problem:</strong> Audio clips have variable duration (0.5s - 3s)</p>

<p><strong>Solution:</strong> Standardize to fixed length (e.g., 1 second)</p>

<p>``python
def preprocess_audio(audio: np.ndarray, sr=16000, target_duration=1.0):
 “””
 Ensure all audio clips are same length</p>

<p>Args:
 audio: Audio waveform
 sr: Sample rate
 target_duration: Target duration in seconds</p>

<p>Returns:
 Processed audio of length sr * target_duration
 “””
 target_length = int(sr * target_duration)</p>

<p># Pad if too short
 if len(audio) &lt; target_length:
 pad_length = target_length - len(audio)
 audio = np.pad(audio, (0, pad_length), mode=’constant’)</p>

<p># Trim if too long
 elif len(audio) &gt; target_length:
 # Take central portion
 start = (len(audio) - target_length) // 2
 audio = audio[start:start + target_length]</p>

<p>return audio
``</p>

<p><strong>Why fixed length?</strong></p>
<ul>
  <li>Neural networks expect fixed-size inputs</li>
  <li>Enables batching during training</li>
  <li>Simplifies model architecture</li>
</ul>

<p><strong>Alternative: Variable-length with padding</strong>
``python
def pad_sequence(audios: list, sr=16000):
 “””
 Pad multiple audio clips to longest length
 Used during batched inference
 “””
 max_length = max(len(a) for a in audios)</p>

<p>padded = []
 masks = []</p>

<p>for audio in audios:
 pad_length = max_length - len(audio)
 padded_audio = np.pad(audio, (0, pad_length))
 mask = np.ones(len(audio)).tolist() + [0] * pad_length</p>

<p>padded.append(padded_audio)
 masks.append(mask)</p>

<p>return np.array(padded), np.array(masks)
``</p>

<h3 id="normalization">Normalization</h3>

<p>``python
def normalize_audio(audio: np.ndarray) -&gt; np.ndarray:
 “””
 Normalize audio to [-1, 1] range</p>

<p>Improves model convergence and generalization
 “””
 # Peak normalization
 max_val = np.max(np.abs(audio))
 if max_val &gt; 0:
 audio = audio / max_val</p>

<p>return audio</p>

<p>def normalize_rms(audio: np.ndarray, target_rms=0.1) -&gt; np.ndarray:
 “””
 Normalize by RMS (root mean square) energy</p>

<p>Better for handling volume variations
 “””
 current_rms = np.sqrt(np.mean(audio ** 2))
 if current_rms &gt; 0:
 audio = audio * (target_rms / current_rms)</p>

<p>return audio
``</p>

<hr />

<h2 id="component-2-feature-extraction">Component 2: Feature Extraction</h2>

<h3 id="option-1-mfccs-mel-frequency-cepstral-coefficients">Option 1: MFCCs (Mel-Frequency Cepstral Coefficients)</h3>

<p><strong>MFCCs</strong> capture the spectral envelope of speech, which is important for phonetic content.</p>

<p>``python
import librosa</p>

<p>def extract_mfcc(audio, sr=16000, n_mfcc=40, n_fft=512, hop_length=160):
 “””
 Extract MFCC features</p>

<p>Args:
 audio: Waveform
 sr: Sample rate (Hz)
 n_mfcc: Number of MFCC coefficients
 n_fft: FFT window size
 hop_length: Hop length between frames (10ms at 16kHz)</p>

<p>Returns:
 MFCCs: (n_mfcc, time_steps)
 “””
 # Compute MFCCs
 mfccs = librosa.feature.mfcc(
 y=audio,
 sr=sr,
 n_mfcc=n_mfcc,
 n_fft=n_fft,
 hop_length=hop_length,
 n_mels=40, # Number of mel bands
 fmin=20, # Minimum frequency
 fmax=sr//2 # Maximum frequency (Nyquist)
 )</p>

<p># Add delta (velocity) and delta-delta (acceleration)
 delta = librosa.feature.delta(mfccs)
 delta2 = librosa.feature.delta(mfccs, order=2)</p>

<p># Stack all features
 features = np.vstack([mfccs, delta, delta2]) # (120, time)</p>

<p>return features.T # (time, 120)
``</p>

<p><strong>Why delta features?</strong></p>
<ul>
  <li><strong>MFCCs:</strong> Spectral shape (what phonemes)</li>
  <li><strong>Delta:</strong> How spectral shape is changing (dynamics)</li>
  <li><strong>Delta-delta:</strong> Rate of change (acceleration)</li>
</ul>

<p>Together they capture both static and dynamic characteristics of speech.</p>

<h3 id="option-2-mel-spectrogram">Option 2: Mel-Spectrogram</h3>

<p><strong>Mel-spectrograms</strong> preserve more temporal resolution than MFCCs.</p>

<p>``python
def extract_mel_spectrogram(audio, sr=16000, n_mels=40, n_fft=512, hop_length=160):
 “””
 Extract log mel-spectrogram</p>

<p>Returns:
 Log mel-spectrogram: (time, n_mels)
 “””
 # Compute mel spectrogram
 mel_spec = librosa.feature.melspectrogram(
 y=audio,
 sr=sr,
 n_fft=n_fft,
 hop_length=hop_length,
 n_mels=n_mels,
 fmin=20,
 fmax=sr//2
 )</p>

<p># Convert to log scale (dB)
 log_mel = librosa.power_to_db(mel_spec, ref=np.max)</p>

<p>return log_mel.T # (time, n_mels)
``</p>

<p><strong>MFCCs vs Mel-Spectrogram:</strong></p>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>MFCCs</th>
      <th>Mel-Spectrogram</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Size</td>
      <td>(time, 13-40)</td>
      <td>(time, 40-80)</td>
    </tr>
    <tr>
      <td>Information</td>
      <td>Spectral envelope</td>
      <td>Full spectrum</td>
    </tr>
    <tr>
      <td>Works better with</td>
      <td>Small models</td>
      <td>CNNs (image-like)</td>
    </tr>
    <tr>
      <td>Training time</td>
      <td>Faster</td>
      <td>Slower</td>
    </tr>
    <tr>
      <td>Accuracy</td>
      <td>Slightly lower</td>
      <td>Slightly higher</td>
    </tr>
  </tbody>
</table>

<p><strong>Recommendation:</strong> Use <strong>mel-spectrograms with CNNs</strong> for best accuracy.</p>

<hr />

<h2 id="component-3-model-architectures">Component 3: Model Architectures</h2>

<h3 id="architecture-1-cnn-fastest-for-on-device">Architecture 1: CNN (Fastest for On-Device)</h3>

<p>``python
import torch
import torch.nn as nn</p>

<p>class CommandCNN(nn.Module):
 “””
 CNN for audio command classification</p>

<p>Treats mel-spectrogram as 2D image
 “””
 def <strong>init</strong>(self, num_classes=31, input_channels=1):
 super().<strong>init</strong>()</p>

<p># Convolutional layers
 self.conv1 = nn.Sequential(
 nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),
 nn.BatchNorm2d(32),
 nn.ReLU(),
 nn.MaxPool2d(2, 2)
 )</p>

<p>self.conv2 = nn.Sequential(
 nn.Conv2d(32, 64, kernel_size=3, padding=1),
 nn.BatchNorm2d(64),
 nn.ReLU(),
 nn.MaxPool2d(2, 2)
 )</p>

<p>self.conv3 = nn.Sequential(
 nn.Conv2d(64, 128, kernel_size=3, padding=1),
 nn.BatchNorm2d(128),
 nn.ReLU(),
 nn.MaxPool2d(2, 2)
 )</p>

<p># Global average pooling (instead of fully-connected)
 self.gap = nn.AdaptiveAvgPool2d((1, 1))</p>

<p># Classification head
 self.classifier = nn.Sequential(
 nn.Dropout(0.3),
 nn.Linear(128, num_classes)
 )</p>

<p>def forward(self, x):
 # x: (batch, 1, time, freq)</p>

<p>x = self.conv1(x) # → (batch, 32, time/2, freq/2)
 x = self.conv2(x) # → (batch, 64, time/4, freq/4)
 x = self.conv3(x) # → (batch, 128, time/8, freq/8)</p>

<p>x = self.gap(x) # → (batch, 128, 1, 1)
 x = x.view(x.size(0), -1) # → (batch, 128)</p>

<p>x = self.classifier(x) # → (batch, num_classes)</p>

<p>return x</p>

<h1 id="model-size-2mb">Model size: ~2MB</h1>
<h1 id="inference-time-cpu-15ms">Inference time (CPU): 15ms</h1>
<h1 id="accuracy-93">Accuracy: ~93%</h1>
<p>``</p>

<p><strong>Why CNNs work for audio:</strong></p>
<ul>
  <li><strong>Local patterns:</strong> Phonemes have localized frequency patterns</li>
  <li><strong>Translation invariance:</strong> Command can start at different times</li>
  <li><strong>Parameter sharing:</strong> Same filters across time/frequency</li>
  <li><strong>Efficient:</strong> Mostly matrix operations, highly optimized</li>
</ul>

<h3 id="architecture-2-rnn-better-temporal-modeling">Architecture 2: RNN (Better Temporal Modeling)</h3>

<p>``python
class CommandRNN(nn.Module):
 “””
 RNN for command classification</p>

<p>Better at capturing temporal dependencies
 “””
 def <strong>init</strong>(self, input_dim=40, hidden_dim=128, num_layers=2, num_classes=31):
 super().<strong>init</strong>()</p>

<p># LSTM layers
 self.lstm = nn.LSTM(
 input_size=input_dim,
 hidden_size=hidden_dim,
 num_layers=num_layers,
 batch_first=True,
 bidirectional=True,
 dropout=0.2
 )</p>

<p># Attention mechanism (optional)
 self.attention = nn.Linear(hidden_dim * 2, 1)</p>

<p># Classification head
 self.classifier = nn.Linear(hidden_dim * 2, num_classes)</p>

<p>def forward(self, x):
 # x: (batch, time, features)</p>

<p># LSTM
 lstm_out, _ = self.lstm(x) # → (batch, time, hidden*2)</p>

<p># Attention pooling (instead of taking last time step)
 attention_weights = torch.softmax(
 self.attention(lstm_out), # → (batch, time, 1)
 dim=1
 )</p>

<p># Weighted sum
 context = torch.sum(attention_weights * lstm_out, dim=1) # → (batch, hidden*2)</p>

<p># Classify
 logits = self.classifier(context) # → (batch, num_classes)</p>

<p>return logits</p>

<h1 id="model-size-5mb">Model size: ~5MB</h1>
<h1 id="inference-time-cpu-30ms">Inference time (CPU): 30ms</h1>
<h1 id="accuracy-95">Accuracy: ~95%</h1>
<p>``</p>

<h3 id="architecture-3-attention-based-best-accuracy">Architecture 3: Attention-Based (Best Accuracy)</h3>

<p>``python
class CommandTransformer(nn.Module):
 “””
 Transformer for command classification</p>

<p>Best accuracy but slower inference
 “””
 def <strong>init</strong>(self, input_dim=40, d_model=128, nhead=4, num_layers=2, num_classes=31):
 super().<strong>init</strong>()</p>

<p># Input projection
 self.embedding = nn.Linear(input_dim, d_model)</p>

<p># Positional encoding
 self.pos_encoder = PositionalEncoding(d_model)</p>

<p># Transformer encoder
 encoder_layer = nn.TransformerEncoderLayer(
 d_model=d_model,
 nhead=nhead,
 dim_feedforward=d_model * 4,
 dropout=0.1
 )
 self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)</p>

<p># Classification head
 self.classifier = nn.Linear(d_model, num_classes)</p>

<p>def forward(self, x):
 # x: (batch, time, features)</p>

<p># Project to d_model
 x = self.embedding(x) # → (batch, time, d_model)</p>

<p># Add positional encoding
 x = self.pos_encoder(x)</p>

<p># Transformer expects (time, batch, d_model)
 x = x.transpose(0, 1)
 x = self.transformer(x)
 x = x.transpose(0, 1)</p>

<p># Average pool over time
 x = x.mean(dim=1) # → (batch, d_model)</p>

<p># Classify
 logits = self.classifier(x) # → (batch, num_classes)</p>

<p>return logits</p>

<p>class PositionalEncoding(nn.Module):
 def <strong>init</strong>(self, d_model, max_len=5000):
 super().<strong>init</strong>()</p>

<p>pe = torch.zeros(max_len, d_model)
 position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
 div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))</p>

<p>pe[:, 0::2] = torch.sin(position * div_term)
 pe[:, 1::2] = torch.cos(position * div_term)</p>

<p>self.register_buffer(‘pe’, pe.unsqueeze(0))</p>

<p>def forward(self, x):
 return x + self.pe[:, :x.size(1), :]</p>

<h1 id="model-size-8mb">Model size: ~8MB</h1>
<h1 id="inference-time-cpu-50ms">Inference time (CPU): 50ms</h1>
<h1 id="accuracy-97">Accuracy: ~97%</h1>
<p>``</p>

<h3 id="model-comparison">Model Comparison</h3>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Params</th>
      <th>Size</th>
      <th>CPU Latency</th>
      <th>GPU Latency</th>
      <th>Accuracy</th>
      <th>Best For</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CNN</td>
      <td>500K</td>
      <td>2MB</td>
      <td>15ms</td>
      <td>3ms</td>
      <td>93%</td>
      <td>Mobile devices</td>
    </tr>
    <tr>
      <td>RNN</td>
      <td>1.2M</td>
      <td>5MB</td>
      <td>30ms</td>
      <td>5ms</td>
      <td>95%</td>
      <td>Balanced</td>
    </tr>
    <tr>
      <td>Transformer</td>
      <td>2M</td>
      <td>8MB</td>
      <td>50ms</td>
      <td>8ms</td>
      <td>97%</td>
      <td>Cloud/high-end</td>
    </tr>
  </tbody>
</table>

<p><strong>Production choice:</strong> CNN for on-device, RNN for cloud</p>

<hr />

<h2 id="training-strategy">Training Strategy</h2>

<h3 id="data-collection">Data Collection</h3>

<p><strong>Per command, need:</strong></p>
<ul>
  <li>1000-5000 examples</li>
  <li>100+ speakers (diversity)</li>
  <li>Both genders, various ages</li>
  <li>Different accents</li>
  <li>Background noise variations</li>
  <li>Different recording devices</li>
</ul>

<p><strong>Example dataset structure:</strong>
<code class="language-plaintext highlighter-rouge">
data/
├── lights_on/
│ ├── speaker001_01.wav
│ ├── speaker001_02.wav
│ ├── speaker002_01.wav
│ └── ...
├── lights_off/
│ └── ...
├── volume_up/
│ └── ...
└── unknown/
 ├── random_speech/
 ├── music/
 ├── noise/
 └── silence/
</code></p>

<h3 id="data-augmentation">Data Augmentation</h3>

<p><strong>Critical for robustness!</strong> Augment during training:</p>

<p>``python
import random</p>

<p>def augment_audio(audio, sr=16000):
 “””
 Apply random augmentation</p>

<p>Each training example augmented differently
 “””
 augmentations = [
 add_noise,
 time_shift,
 time_stretch,
 pitch_shift,
 add_reverb
 ]</p>

<p># Apply 1-3 random augmentations
 num_augs = random.randint(1, 3)
 selected = random.sample(augmentations, num_augs)</p>

<p>for aug_fn in selected:
 audio = aug_fn(audio, sr)</p>

<p>return audio</p>

<p>def add_noise(audio, sr, snr_db=random.uniform(5, 20)):
 “"”Add background noise at specific SNR”””
 # Load random noise sample
 noise = load_random_noise_sample(len(audio))</p>

<p># Calculate noise power for target SNR
 audio_power = np.mean(audio ** 2)
 noise_power = audio_power / (10 ** (snr_db / 10))
 noise_scaled = noise * np.sqrt(noise_power / np.mean(noise ** 2))</p>

<p>return audio + noise_scaled</p>

<p>def time_shift(audio, sr, shift_max=0.1):
 “"”Shift audio in time (simulates different reaction times)”””
 shift = int(sr * shift_max * (random.random() - 0.5))
 return np.roll(audio, shift)</p>

<p>def time_stretch(audio, sr, rate=random.uniform(0.9, 1.1)):
 “"”Change speed without changing pitch”””
 return librosa.effects.time_stretch(audio, rate=rate)</p>

<p>def pitch_shift(audio, sr, n_steps=random.randint(-2, 2)):
 “"”Shift pitch (simulates different speakers)”””
 return librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)</p>

<p>def add_reverb(audio, sr):
 “"”Add room reverb (simulates different environments)”””
 # Simple reverb using convolution with impulse response
 impulse_response = generate_simple_reverb(sr)
 return np.convolve(audio, impulse_response, mode=’same’)
``</p>

<p><strong>Impact:</strong> 2-3x effective dataset size, 10-20% accuracy improvement</p>

<h3 id="training-loop">Training Loop</h3>

<p>``python
def train_command_classifier(
 model, 
 train_loader, 
 val_loader, 
 epochs=100, 
 lr=0.001
):
 “””
 Train speech command classifier
 “””
 criterion = nn.CrossEntropyLoss()
 optimizer = torch.optim.Adam(model.parameters(), lr=lr)
 scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
 optimizer,
 mode=’max’,
 factor=0.5,
 patience=5,
 verbose=True
 )</p>

<p>best_val_acc = 0.0</p>

<p>for epoch in range(epochs):
 # Training
 model.train()
 train_loss = 0
 train_correct = 0
 train_total = 0</p>

<p>for batch_idx, (audio, labels) in enumerate(train_loader):
 # Extract features
 features = extract_features_batch(audio, sr=16000)
 features = torch.tensor(features, dtype=torch.float32)</p>

<p># Add channel dimension for CNN
 if len(features.shape) == 3:
 features = features.unsqueeze(1) # (batch, 1, time, freq)</p>

<p>labels = torch.tensor(labels, dtype=torch.long)</p>

<p># Forward
 outputs = model(features)
 loss = criterion(outputs, labels)</p>

<p># Backward
 optimizer.zero_grad()
 loss.backward()
 optimizer.step()</p>

<p># Track accuracy
 _, predicted = torch.max(outputs, 1)
 train_correct += (predicted == labels).sum().item()
 train_total += labels.size(0)
 train_loss += loss.item()</p>

<p>train_acc = train_correct / train_total
 avg_loss = train_loss / len(train_loader)</p>

<p># Validation
 val_acc = validate(model, val_loader)</p>

<p># Learning rate scheduling
 scheduler.step(val_acc)</p>

<p># Save best model
 if val_acc &gt; best_val_acc:
 best_val_acc = val_acc
 torch.save(model.state_dict(), ‘best_model.pth’)
 print(f”✓ New best model: {val_acc:.4f}”)</p>

<p>print(f”Epoch {epoch+1}/{epochs}: “
 f”Loss={avg_loss:.4f}, “
 f”Train Acc={train_acc:.4f}, “
 f”Val Acc={val_acc:.4f}”)</p>

<p>return model</p>

<p>def validate(model, val_loader):
 “"”Evaluate on validation set”””
 model.eval()
 correct = 0
 total = 0</p>

<p>with torch.no_grad():
 for audio, labels in val_loader:
 features = extract_features_batch(audio)
 features = torch.tensor(features).unsqueeze(1)
 labels = torch.tensor(labels)</p>

<p>outputs = model(features)
 _, predicted = torch.max(outputs, 1)</p>

<p>correct += (predicted == labels).sum().item()
 total += labels.size(0)</p>

<p>return correct / total
``</p>

<hr />

<h2 id="component-4-handling-unknown-commands">Component 4: Handling Unknown Commands</h2>

<h3 id="strategy-1-add-unknown-class">Strategy 1: Add “Unknown” Class</h3>

<p>``python</p>
<h1 id="training-data">Training data</h1>
<p>command_classes = [
 “lights_on”, “lights_off”, “volume_up”, “volume_down”,
 “play_music”, “stop”, “pause”, “next”, “previous”,
 # … 30 total commands
]</p>

<h1 id="collect-negative-examples">Collect negative examples</h1>
<p>unknown_class = [
 “random_speech”, # Conversations
 “music”, # Background music
 “noise”, # Environmental sounds
 “silence” # No speech
]</p>

<h1 id="labels-0-29-for-commands-30-for-unknown">Labels: 0-29 for commands, 30 for unknown</h1>
<p>all_classes = command_classes + [“unknown”]
``</p>

<p><strong>Collecting unknown data:</strong>
``python</p>
<h1 id="record-actual-user-interactions">Record actual user interactions</h1>
<h1 id="label-anything-thats-not-a-command-as-unknown">Label anything that’s NOT a command as “unknown”</h1>

<p>unknown_samples = []</p>

<p>for audio in production_audio_stream:
 if not is_valid_command(audio):
 unknown_samples.append(audio)</p>

<p>if len(unknown_samples) &gt;= 10000:
 # Add to training set
 augment_and_save(unknown_samples, label=”unknown”)
``</p>

<h3 id="strategy-2-confidence-thresholding">Strategy 2: Confidence Thresholding</h3>

<p>``python
def predict_with_threshold(model, audio, threshold=0.7):
 “””
 Reject low-confidence predictions as unknown
 “””
 # Extract features
 features = extract_mel_spectrogram(audio)
 features = torch.tensor(features).unsqueeze(0).unsqueeze(0)</p>

<p># Predict
 with torch.no_grad():
 logits = model(features)
 probs = torch.softmax(logits, dim=1)[0]</p>

<p># Get top prediction
 max_prob, predicted_class = torch.max(probs, 0)</p>

<p># Threshold check
 if max_prob &lt; threshold:
 return “unknown”, float(max_prob)</p>

<p>return command_classes[predicted_class], float(max_prob)
``</p>

<h3 id="strategy-3-out-of-distribution-ood-detection">Strategy 3: Out-of-Distribution (OOD) Detection</h3>

<p>``python
def detect_ood_with_entropy(probs):
 “””
 High entropy = model is uncertain = likely OOD
 “””
 entropy = -torch.sum(probs * torch.log(probs + 1e-10))</p>

<p># Calibrate threshold on validation set
 # In-distribution: entropy ~0.5
 # Out-of-distribution: entropy &gt; 2.0</p>

<p>if entropy &gt; 2.0:
 return True # OOD
 return False</p>

<p>def detect_ood_with_mahalanobis(features, class_means, class_covariances):
 “””
 Mahalanobis distance to class centroids</p>

<p>Far from all classes = likely OOD
 “””
 min_distance = float(‘inf’)</p>

<p>for class_idx in range(len(class_means)):
 mean = class_means[class_idx]
 cov = class_covariances[class_idx]</p>

<p># Mahalanobis distance
 diff = features - mean
 distance = np.sqrt(diff.T @ np.linalg.inv(cov) @ diff)</p>

<p>min_distance = min(min_distance, distance)</p>

<p># Threshold: 3-sigma rule
 if min_distance &gt; 3.0:
 return True # OOD
 return False
``</p>

<hr />

<h2 id="model-optimization-for-edge-deployment">Model Optimization for Edge Deployment</h2>

<h3 id="quantization">Quantization</h3>

<p>``python</p>
<h1 id="post-training-quantization-dynamic-quantization-targets-linear-conv2d-not-supported">Post-training quantization (dynamic quantization targets Linear; Conv2d not supported)</h1>
<p>model_fp32 = CommandCNN(num_classes=31)
model_fp32.load_state_dict(torch.load(‘model.pth’))
model_fp32.eval()</p>

<h1 id="dynamic-quantization-linear-layers">Dynamic quantization (Linear layers)</h1>
<p>model_int8 = torch.quantization.quantize_dynamic(
 model_fp32,
 {torch.nn.Linear},
 dtype=torch.qint8
)</p>

<h1 id="save">Save</h1>
<p>torch.save(model_int8.state_dict(), ‘model_int8.pth’)</p>

<h1 id="results-typical-on-cpu-with-cnn-head-including-linear">Results (typical on CPU with CNN head including Linear):</h1>
<h1 id="--model-size-2mb--12mb-16x-smaller">- Model size: 2MB → ~1.2MB (1.6x smaller)</h1>
<h1 id="--inference-15ms--10-12ms-13-15x-faster">- Inference: 15ms → ~10-12ms (1.3-1.5x faster)</h1>
<h1 id="--accuracy-932--930-02-drop">- Accuracy: ~93.2% → ~93.0% (≤0.2% drop)</h1>
<p>``</p>

<h3 id="pruning">Pruning</h3>

<p>``python
import torch.nn.utils.prune as prune</p>

<p>def prune_model(model, amount=0.3):
 “””
 Remove 30% of weights with lowest magnitude
 “””
 for name, module in model.named_modules():
 if isinstance(module, (nn.Conv2d, nn.Linear)):
 prune.l1_unstructured(module, name=’weight’, amount=amount)</p>

<p>return model</p>

<h1 id="results-with-30-pruning">Results with 30% pruning:</h1>
<h1 id="--model-size-2mb--14mb">- Model size: 2MB → 1.4MB</h1>
<h1 id="--inference-15ms--12ms">- Inference: 15ms → 12ms</h1>
<h1 id="--accuracy-932--927">- Accuracy: 93.2% → 92.7%</h1>
<p>``</p>

<h3 id="knowledge-distillation">Knowledge Distillation</h3>

<p>``python
def distillation_loss(student_logits, teacher_logits, labels, temperature=3.0, alpha=0.7):
 “””
 Train small student to mimic large teacher</p>

<p>Args:
 temperature: Soften probability distributions
 alpha: Weight between soft and hard targets
 “””
 # Soft targets from teacher
 soft_targets = torch.softmax(teacher_logits / temperature, dim=1)
 soft_prob = torch.log_softmax(student_logits / temperature, dim=1)
 soft_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0]
 soft_loss = soft_loss * (temperature ** 2)</p>

<p># Hard targets (ground truth)
 hard_loss = nn.CrossEntropyLoss()(student_logits, labels)</p>

<p># Combine
 return alpha * soft_loss + (1 - alpha) * hard_loss</p>

<h1 id="train-student">Train student</h1>
<p>teacher = CommandTransformer(num_classes=31) # 8MB, 97% accuracy
student = CommandCNN(num_classes=31) # 2MB, 93% accuracy</p>

<p>for audio, labels in train_loader:
 # Teacher predictions (frozen)
 with torch.no_grad():
 teacher_logits = teacher(audio)</p>

<p># Student predictions
 student_logits = student(audio)</p>

<p># Distillation loss
 loss = distillation_loss(student_logits, teacher_logits, labels)</p>

<p># Optimize student
 loss.backward()
 optimizer.step()</p>

<h1 id="result-student-achieves-95-vs-93-without-distillation">Result: Student achieves 95% (vs 93% without distillation)</h1>
<p>``</p>

<hr />

<h2 id="on-device-deployment">On-Device Deployment</h2>

<h3 id="export-to-mobile-formats">Export to Mobile Formats</h3>

<p><strong>TensorFlow Lite (Android):</strong></p>

<p>``python
import tensorflow as tf</p>

<h1 id="convert-pytorch-to-tensorflow-via-onnx">Convert PyTorch to TensorFlow (via ONNX)</h1>
<h1 id="1-export-pytorch-to-onnx">1. Export PyTorch to ONNX</h1>
<p>torch.onnx.export(
 model,
 dummy_input,
 “model.onnx”,
 input_names=[‘input’],
 output_names=[‘output’]
)</p>

<h1 id="2-convert-onnx-to-tf">2. Convert ONNX to TF</h1>
<p>import onnx
from onnx_tf.backend import prepare</p>

<p>onnx_model = onnx.load(“model.onnx”)
tf_model = prepare(onnx_model)
tf_model.export_graph(“model_tf”)</p>

<h1 id="3-convert-tf-to-tflite">3. Convert TF to TFLite</h1>
<p>converter = tf.lite.TFLiteConverter.from_saved_model(“model_tf”)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()</p>

<p>with open(‘command_classifier.tflite’, ‘wb’) as f:
 f.write(tflite_model)
``</p>

<p><strong>Core ML (iOS):</strong></p>

<p>``python
import coremltools as ct</p>

<h1 id="trace-pytorch-model">Trace PyTorch model</h1>
<p>example_input = torch.randn(1, 1, 100, 40)
traced_model = torch.jit.trace(model, example_input)</p>

<h1 id="convert-to-core-ml">Convert to Core ML</h1>
<p>coreml_model = ct.convert(
 traced_model,
 inputs=[ct.TensorType(name=”audio”, shape=(1, 1, 100, 40))],
 outputs=[ct.TensorType(name=”logits”)]
)</p>

<h1 id="add-metadata">Add metadata</h1>
<p>coreml_model.author = “Arun Baby”
coreml_model.short_description = “Speech command classifier”
coreml_model.version = “1.0”</p>

<h1 id="save-1">Save</h1>
<p>coreml_model.save(“CommandClassifier.mlmodel”)
``</p>

<h3 id="mobile-inference-code">Mobile Inference Code</h3>

<p><strong>Android (Kotlin):</strong></p>

<p>``kotlin
import org.tensorflow.lite.Interpreter
import java.nio.ByteBuffer</p>

<p>class CommandClassifier(private val context: Context) {
 private lateinit var interpreter: Interpreter</p>

<p>init {
 // Load model
 val model = loadModelFile(“command_classifier.tflite”)
 interpreter = Interpreter(model)
 }</p>

<p>fun classify(audio: FloatArray): Pair&lt;String, Float&gt; {
 // Extract features
 val features = extractMelSpectrogram(audio)</p>

<p>// Prepare input
 val inputBuffer = ByteBuffer.allocateDirect(4 * features.size)
 inputBuffer.order(ByteOrder.nativeOrder())
 features.forEach { inputBuffer.putFloat(it) }</p>

<p>// Prepare output
 val output = Array(1) { FloatArray(31) }</p>

<p>// Run inference
 interpreter.run(inputBuffer, output)</p>

<p>// Get top prediction
 val probabilities = output[0]
 val maxIndex = probabilities.indices.maxByOrNull { probabilities[it] } ?: 0
 val confidence = probabilities[maxIndex]</p>

<p>return Pair(commandNames[maxIndex], confidence)
 }
}
``</p>

<p><strong>iOS (Swift):</strong></p>

<p>``swift
import CoreML</p>

<p>class CommandClassifier {
 private var model: CommandClassifierModel!</p>

<p>init() {
 model = try! CommandClassifierModel(configuration: MLModelConfiguration())
 }</p>

<p>func classify(audio: [Float]) -&gt; (command: String, confidence: Double) {
 // Extract features
 let features = extractMelSpectrogram(audio)</p>

<p>// Create MLMultiArray
 let input = try! MLMultiArray(shape: [1, 1, 100, 40], dataType: .float32)
 for i in 0..&lt;features.count {
 input[i] = NSNumber(value: features[i])
 }</p>

<p>// Run inference
 let output = try! model.prediction(audio: input)</p>

<p>// Get top prediction
 let probabilities = output.logits
 let maxIndex = probabilities.argmax()
 let confidence = probabilities[maxIndex]</p>

<p>return (commandNames[maxIndex], Double(confidence))
 }
}
``</p>

<hr />

<h2 id="monitoring--evaluation">Monitoring &amp; Evaluation</h2>

<h3 id="metrics-dashboard">Metrics Dashboard</h3>

<p>``python
from dataclasses import dataclass
from typing import List</p>

<p>@dataclass
class ClassificationMetrics:
 “"”Per-class metrics”””
 precision: float
 recall: float
 f1_score: float
 support: int # Number of samples</p>

<p>def compute_metrics(y_true: List[int], y_pred: List[int], num_classes: int):
 “””
 Compute detailed metrics per class
 “””
 from sklearn.metrics import classification_report, confusion_matrix</p>

<p># Per-class metrics
 report = classification_report(y_true, y_pred, output_dict=True)</p>

<p># Confusion matrix
 cm = confusion_matrix(y_true, y_pred)</p>

<p># Identify problematic classes
 for i in range(num_classes):
 if report[str(i)][‘f1-score’] &lt; 0.85:
 print(f”⚠️ Class {i} ({command_names[i]}) has low F1: {report[str(i)][‘f1-score’]:.3f}”)</p>

<p># Find most confused class
 confused_with = cm[i].argmax()
 if confused_with != i:
 print(f” Most confused with class {confused_with} ({command_names[confused_with]})”)</p>

<p>return report, cm
``</p>

<h3 id="online-monitoring">Online Monitoring</h3>

<p>``python
class OnlineMetricsTracker:
 “””
 Track metrics in production
 “””
 def <strong>init</strong>(self):
 self.predictions = []
 self.confidences = []
 self.latencies = []</p>

<p>def record(self, prediction: int, confidence: float, latency_ms: float):
 “"”Record single prediction”””
 self.predictions.append(prediction)
 self.confidences.append(confidence)
 self.latencies.append(latency_ms)</p>

<p>def get_stats(self, last_n=1000):
 “"”Get recent statistics”””
 recent_preds = self.predictions[-last_n:]
 recent_confs = self.confidences[-last_n:]
 recent_lats = self.latencies[-last_n:]</p>

<p># Class distribution
 from collections import Counter
 class_dist = Counter(recent_preds)</p>

<p>return {
 ‘total_predictions’: len(recent_preds),
 ‘class_distribution’: dict(class_dist),
 ‘avg_confidence’: np.mean(recent_confs),
 ‘low_confidence_rate’: sum(c &lt; 0.7 for c in recent_confs) / len(recent_confs),
 ‘p50_latency’: np.percentile(recent_lats, 50),
 ‘p95_latency’: np.percentile(recent_lats, 95),
 ‘p99_latency’: np.percentile(recent_lats, 99)
 }
``</p>

<hr />

<h2 id="multi-language-support">Multi-Language Support</h2>

<h3 id="approach-1-separate-models-per-language">Approach 1: Separate Models per Language</h3>

<p><strong>Pros:</strong></p>
<ul>
  <li>Best accuracy per language</li>
  <li>Language-specific optimizations</li>
  <li>Easier to add new languages</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>Multiple models to maintain</li>
  <li>Higher storage footprint</li>
  <li>Language detection needed first</li>
</ul>

<p>``python
class MultilingualClassifier:
 “””
 Separate model per language
 “””
 def <strong>init</strong>(self):
 self.models = {
 ‘en’: load_model(‘command_en.pth’),
 ‘es’: load_model(‘command_es.pth’),
 ‘fr’: load_model(‘command_fr.pth’),
 ‘de’: load_model(‘command_de.pth’),
 ‘ja’: load_model(‘command_ja.pth’)
 }
 self.language_detector = load_model(‘lang_detect.pth’)</p>

<p>def predict(self, audio):
 # Detect language first
 language = self.language_detector.predict(audio)</p>

<p># Use language-specific model
 model = self.models[language]
 prediction = model.predict(audio)</p>

<p>return prediction, language
``</p>

<p><strong>Storage requirement:</strong> 5 languages × 2MB = 10MB</p>

<h3 id="approach-2-multilingual-shared-model">Approach 2: Multilingual Shared Model</h3>

<p><strong>Training strategy:</strong></p>

<p>``python
def train_multilingual_model():
 “””
 Single model trained on all languages</p>

<p>Add language ID as auxiliary input
 “””
 model = MultilingualCommandCNN(
 num_classes=30,
 num_languages=5
 )</p>

<p># Training data from all languages
 for audio, command_label, lang_id in train_loader:
 features = extract_features(audio)</p>

<p># Forward pass with language embedding
 command_pred = model(features, lang_id)</p>

<p># Loss
 loss = criterion(command_pred, command_label)</p>

<p>loss.backward()
 optimizer.step()</p>

<p>return model
``</p>

<p><strong>Model architecture:</strong></p>

<p>``python
class MultilingualCommandCNN(nn.Module):
 “””
 Shared model with language embeddings
 “””
 def <strong>init</strong>(self, num_classes=30, num_languages=5, embedding_dim=16):
 super().<strong>init</strong>()</p>

<p># Language embedding
 self.lang_embedding = nn.Embedding(num_languages, embedding_dim)</p>

<p># Shared CNN backbone
 self.cnn = CommandCNN(num_classes=128) # Feature extractor</p>

<p># Language-conditioned classifier
 self.classifier = nn.Linear(128 + embedding_dim, num_classes)</p>

<p>def forward(self, audio_features, language_id):
 # CNN features
 cnn_features = self.cnn(audio_features) # (batch, 128)</p>

<p># Language embedding
 lang_emb = self.lang_embedding(language_id) # (batch, 16)</p>

<p># Concatenate
 combined = torch.cat([cnn_features, lang_emb], dim=1) # (batch, 144)</p>

<p># Classify
 logits = self.classifier(combined) # (batch, num_classes)</p>

<p>return logits
``</p>

<p><strong>Pros:</strong></p>
<ul>
  <li>Single model (2-3MB)</li>
  <li>Shared representations across languages</li>
  <li>Transfer learning for low-resource languages</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>Slightly lower accuracy per language</li>
  <li>All languages must use same command set</li>
</ul>

<hr />

<h2 id="failure-cases--mitigation">Failure Cases &amp; Mitigation</h2>

<h3 id="common-failure-modes">Common Failure Modes</h3>

<h4 id="1-background-speechtv">1. <strong>Background Speech/TV</strong></h4>

<p><strong>Problem:</strong> Model activates on TV dialogue or background conversation</p>

<p><strong>Mitigation:</strong></p>

<p>``python
def detect_background_speech(audio, sr=16000):
 “””
 Detect if audio is from TV/background vs direct user speech</p>

<p>Features:</p>
<ul>
  <li>Energy envelope variation (TV more consistent)</li>
  <li>Reverb characteristics (TV more reverberant)</li>
  <li>Spectral rolloff (TV often compressed)
 “””
 # Energy variation
 frame_energy = librosa.feature.rms(y=audio)[0]
 energy_std = np.std(frame_energy)</li>
</ul>

<p># TV has lower energy variation
 if energy_std &lt; 0.01:
 return True # Likely background</p>

<p># Spectral centroid (TV often band-limited)
 spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
 avg_centroid = np.mean(spectral_centroid)</p>

<p>if avg_centroid &lt; 1000: # Hz
 return True # Likely background</p>

<p>return False
``</p>

<p><strong>Additional strategy:</strong> Use speaker verification to check if it’s the registered user</p>

<h4 id="2-accented-speech">2. <strong>Accented Speech</strong></h4>

<p><strong>Problem:</strong> Model trained on standard accent performs poorly on regional accents</p>

<p><strong>Mitigation:</strong></p>

<p>``python</p>
<h1 id="data-collection-strategy">Data collection strategy</h1>
<p>accent_distribution = {
 ‘general_american’: 0.3,
 ‘british’: 0.15,
 ‘australian’: 0.1,
 ‘indian’: 0.15,
 ‘southern_us’: 0.1,
 ‘canadian’: 0.1,
 ‘other’: 0.1
}</p>

<h1 id="ensure-balanced-training-data">Ensure balanced training data</h1>
<p>for accent, proportion in accent_distribution.items():
 required_samples = total_samples * proportion
 collect_samples(accent, required_samples)</p>

<h1 id="use-accent-aware-data-augmentation">Use accent-aware data augmentation</h1>
<p>def accent_aware_augmentation(audio, accent_type):
 “"”Apply accent-specific augmentations”””
 if accent_type == ‘indian’:
 # Indian English: Stronger pitch variation
 audio = pitch_shift(audio, n_steps=random.randint(-3, 3))
 elif accent_type == ‘southern_us’:
 # Southern US: Slower speech
 audio = time_stretch(audio, rate=random.uniform(0.85, 1.0))</p>

<p>return audio
``</p>

<h4 id="3-noisy-environments">3. <strong>Noisy Environments</strong></h4>

<p><strong>Problem:</strong> Model degrades in cafes, cars, streets</p>

<p><strong>Mitigation:</strong></p>

<p>``python
def enhance_audio_for_inference(audio, sr=16000):
 “””
 Lightweight denoising for inference</p>

<p>Must be &lt; 5ms to maintain latency budget
 “””
 # Spectral gating (simple but effective)
 stft = librosa.stft(audio)
 magnitude = np.abs(stft)</p>

<p># Estimate noise floor (first 100ms)
 noise_frames = magnitude[:, :10]
 noise_threshold = np.mean(noise_frames, axis=1, keepdims=True) * 1.5</p>

<p># Gate
 mask = magnitude &gt; noise_threshold
 stft_denoised = stft * mask</p>

<p># Inverse STFT
 audio_denoised = librosa.istft(stft_denoised)</p>

<p>return audio_denoised
``</p>

<p><strong>Better approach:</strong> Train with noisy data</p>

<p>``python</p>
<h1 id="use-diverse-noise-types-during-training">Use diverse noise types during training</h1>
<p>noise_types = [
 ‘cafe_ambiance’,
 ‘car_interior’,
 ‘street_traffic’,
 ‘office_chatter’,
 ‘home_appliances’,
 ‘rain’,
 ‘wind’
]</p>

<p>for audio, label in train_loader:
 # Add random noise
 noise_type = random.choice(noise_types)
 noisy_audio = add_noise(audio, noise_type, snr_db=random.uniform(5, 20))
``</p>

<h4 id="4-similar-sounding-commands">4. <strong>Similar Sounding Commands</strong></h4>

<p><strong>Problem:</strong> “lights on” vs “lights off”, “volume up” vs “volume down”</p>

<p><strong>Mitigation:</strong></p>

<p>``python</p>
<h1 id="use-contrastive-learning-during-training">Use contrastive learning during training</h1>
<p>def contrastive_loss(anchor, positive, negative, margin=1.0):
 “””
 Pull together similar commands, push apart confusable ones
 “””
 pos_distance = torch.norm(anchor - positive, dim=1)
 neg_distance = torch.norm(anchor - negative, dim=1)</p>

<p>loss = torch.relu(pos_distance - neg_distance + margin)</p>

<p>return loss.mean()</p>

<h1 id="identify-confusable-pairs">Identify confusable pairs</h1>
<p>confusable_pairs = [
 (‘lights_on’, ‘lights_off’),
 (‘volume_up’, ‘volume_down’),
 (‘next’, ‘previous’),
 (‘play’, ‘pause’)
]</p>

<h1 id="during-training">During training</h1>
<p>for audio, label in train_loader:
 features = model.extract_features(audio)</p>

<p># For confusable commands, add contrastive loss
 if label in confusable_commands:
 opposite_label = get_opposite_command(label)
 opposite_audio = sample_from_class(opposite_label)
 opposite_features = model.extract_features(opposite_audio)</p>

<p>total_loss = classification_loss + 0.2 * contrastive_loss(
 features, 
 features, # Anchor to itself
 opposite_features
 )
``</p>

<hr />

<h2 id="production-deployment-architecture">Production Deployment Architecture</h2>

<h3 id="edge-deployment-smart-speaker">Edge Deployment (Smart Speaker)</h3>

<p>``
┌─────────────────────────────────────────┐
│ Smart Speaker Device │
├─────────────────────────────────────────┤
│ │
│ Microphone Array │
│ ↓ │
│ Beamforming (5ms) │
│ ↓ │
│ Wake Word Detection (10ms) │
│ ↓ │
│ [If wake word detected] │
│ ↓ │
│ Audio Buffer (1 second) │
│ ↓ │
│ Feature Extraction (5ms) │
│ ↓ │
│ Command CNN Inference (15ms) │
│ ↓ │
│ ┌──────────────┐ │
│ │ Confidence │ │
│ │ &gt; 0.85? │ │
│ └──────┬───────┘ │
│ │ │
│ Yes │ No │
│ ↓ │
│ Execute Command Send to Cloud ASR │
│ │
└─────────────────────────────────────────┘</p>

<p>Total latency (on-device): &lt; 40ms
Power consumption: &lt; 100mW during inference
``</p>

<h3 id="hybrid-edge-cloud-architecture">Hybrid Edge-Cloud Architecture</h3>

<p>``python
class HybridCommandClassifier:
 “””
 Intelligent routing between edge and cloud
 “””
 def <strong>init</strong>(self):
 self.edge_model = load_edge_model() # Small CNN
 self.cloud_client = CloudASRClient()</p>

<p># Common commands handled on-device
 self.edge_commands = {
 ‘lights_on’, ‘lights_off’, 
 ‘volume_up’, ‘volume_down’,
 ‘play’, ‘pause’, ‘stop’,
 ‘next’, ‘previous’
 }</p>

<p>async def classify(self, audio):
 # Try edge first
 edge_pred, edge_conf = self.edge_model.predict(audio)</p>

<p># High confidence + known command → use edge
 if edge_conf &gt; 0.85 and edge_pred in self.edge_commands:
 return {
 ‘command’: edge_pred,
 ‘confidence’: edge_conf,
 ‘source’: ‘edge’,
 ‘latency_ms’: 35
 }</p>

<p># Otherwise → cloud ASR
 cloud_result = await self.cloud_client.recognize(audio)</p>

<p>return {
 ‘command’: cloud_result[‘text’],
 ‘confidence’: cloud_result[‘confidence’],
 ‘source’: ‘cloud’,
 ‘latency_ms’: 250
 }
``</p>

<p><strong>Benefits:</strong></p>
<ul>
  <li>✅ 90% of commands handled on-device (&lt; 50ms)</li>
  <li>✅ 10% fall back to cloud for complex queries</li>
  <li>✅ Privacy for common commands</li>
  <li>✅ Graceful degradation if network unavailable</li>
</ul>

<hr />

<h2 id="ab-testing--gradual-rollout">A/B Testing &amp; Gradual Rollout</h2>

<h3 id="experiment-framework">Experiment Framework</h3>

<p>``python
class ModelExperiment:
 “””
 A/B test new model versions
 “””
 def <strong>init</strong>(self, control_model, treatment_model, treatment_percentage=10):
 self.control = control_model
 self.treatment = treatment_model
 self.treatment_pct = treatment_percentage</p>

<p>def predict(self, audio, user_id):
 # Deterministic assignment based on user_id
 bucket = hash(user_id) % 100</p>

<p>if bucket &lt; self.treatment_pct:
 # Treatment group
 pred, conf = self.treatment.predict(audio)
 variant = ‘treatment’
 else:
 # Control group
 pred, conf = self.control.predict(audio)
 variant = ‘control’</p>

<p># Log for analysis
 self.log_prediction(user_id, variant, pred, conf)</p>

<p>return pred, conf</p>

<p>def log_prediction(self, user_id, variant, prediction, confidence):
 “"”Log to analytics system”””
 event = {
 ‘user_id’: user_id,
 ‘timestamp’: time.time(),
 ‘variant’: variant,
 ‘prediction’: prediction,
 ‘confidence’: confidence
 }</p>

<p>analytics_logger.log(event)
``</p>

<h3 id="metrics-to-track">Metrics to Track</h3>

<p>``python
def compute_experiment_metrics(control_group, treatment_group):
 “””
 Compare model versions
 “””
 metrics = {}</p>

<p># Accuracy (if ground truth available)
 if has_ground_truth:
 metrics[‘accuracy_control’] = compute_accuracy(control_group)
 metrics[‘accuracy_treatment’] = compute_accuracy(treatment_group)</p>

<p># Confidence distribution
 metrics[‘avg_confidence_control’] = np.mean([x[‘confidence’] for x in control_group])
 metrics[‘avg_confidence_treatment’] = np.mean([x[‘confidence’] for x in treatment_group])</p>

<p># Latency
 metrics[‘p95_latency_control’] = np.percentile([x[‘latency’] for x in control_group], 95)
 metrics[‘p95_latency_treatment’] = np.percentile([x[‘latency’] for x in treatment_group], 95)</p>

<p># User engagement (proxy for accuracy)
 metrics[‘retry_rate_control’] = compute_retry_rate(control_group)
 metrics[‘retry_rate_treatment’] = compute_retry_rate(treatment_group)</p>

<p># Statistical significance
 from scipy.stats import ttest_ind</p>

<p>control_success = [x[‘success’] for x in control_group]
 treatment_success = [x[‘success’] for x in treatment_group]</p>

<p>t_stat, p_value = ttest_ind(control_success, treatment_success)
 metrics[‘p_value’] = p_value
 metrics[‘is_significant’] = p_value &lt; 0.05</p>

<p>return metrics
``</p>

<hr />

<h2 id="real-world-examples">Real-World Examples</h2>

<h3 id="google-assistant">Google Assistant</h3>

<p><strong>“Hey Google” Wake Word:</strong></p>
<ul>
  <li>Always-on detection using tiny model (&lt; 1MB)</li>
  <li>Runs on low-power co-processor (DSP)</li>
  <li>&lt; 10ms latency, ~0.5mW power</li>
  <li>~ 99.5% accuracy on target phrase</li>
  <li>Personalized over time with on-device learning</li>
</ul>

<p><strong>Command Classification:</strong></p>
<ul>
  <li>Separate model for common commands (~30 commands)</li>
  <li>Fallback to full ASR for complex queries</li>
  <li>On-device for privacy (no audio sent to cloud)</li>
  <li>Multi-language support (40+ languages)</li>
</ul>

<p><strong>Architecture:</strong>
<code class="language-plaintext highlighter-rouge">
Microphone → Beamformer → Wake Word → Command CNN → Execute
 ↓
 (if low conf)
 ↓
 Cloud ASR
</code></p>

<h3 id="amazon-alexa">Amazon Alexa</h3>

<p><strong>“Alexa” Wake Word:</strong></p>
<ul>
  <li>Multi-stage cascade:</li>
  <li>Stage 1: Energy detector (&lt; 1ms, filters silence)</li>
  <li>Stage 2: Keyword spotter (&lt; 10ms, CNN)</li>
  <li>Stage 3: Full verification (&lt; 50ms, larger model)</li>
  <li>Reduces false positives by 10x</li>
  <li>Power-efficient (only stage 3 uses main CPU)</li>
</ul>

<p><strong>Custom Skills:</strong></p>
<ul>
  <li>Slot-filling approach for structured commands</li>
  <li>Template: “play {song} by {artist}”</li>
  <li>Combined classification + entity extraction</li>
  <li>~100K custom skills available</li>
</ul>

<p><strong>Deployment:</strong></p>
<ul>
  <li>Edge: Wake word + simple commands</li>
  <li>Cloud: Everything else (200ms latency acceptable)</li>
</ul>

<h3 id="apple-siri">Apple Siri</h3>

<p><strong>“Hey Siri” Detection:</strong></p>
<ul>
  <li>Neural network on Neural Engine (dedicated ML chip)</li>
  <li>Personalized to user’s voice during setup</li>
  <li>Continuously adapts to voice changes</li>
  <li>&lt; 50ms latency</li>
  <li>Works offline (completely on-device)</li>
  <li>Power: &lt; 1mW in always-listening mode</li>
</ul>

<p><strong>Privacy Design:</strong></p>
<ul>
  <li>Audio never sent to cloud without explicit activation</li>
  <li>Voice profile stored locally (encrypted)</li>
  <li>Random identifier (not tied to Apple ID)</li>
</ul>

<p><strong>Technical Details:</strong></p>
<ul>
  <li>Uses LSTM for temporal modeling</li>
  <li>Trained on millions of “Hey Siri” variations</li>
  <li>Negative examples: TV shows, movies, other voices</li>
</ul>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Direct audio→intent</strong> faster than ASR→NLU for limited commands 
✅ <strong>CNNs on mel-spectrograms</strong> work excellently for on-device 
✅ <strong>Data augmentation</strong> critical for robustness (noise, time shift, pitch) 
✅ <strong>Unknown class handling</strong> prevents false activations 
✅ <strong>Quantization</strong> achieves 4x compression with &lt; 1% accuracy loss 
✅ <strong>Threshold tuning</strong> balances precision/recall for business needs</p>

<hr />

<h2 id="further-reading">Further Reading</h2>

<p><strong>Papers:</strong></p>
<ul>
  <li><a href="https://arxiv.org/abs/1804.03209">Speech Commands Dataset (Google)</a></li>
  <li><a href="https://arxiv.org/abs/1711.07128">Efficient Keyword Spotting</a></li>
  <li><a href="https://arxiv.org/abs/1811.07684">Hey Snips</a></li>
</ul>

<p><strong>Datasets:</strong></p>
<ul>
  <li><a href="https://www.tensorflow.org/datasets/catalog/speech_commands">Google Speech Commands v2</a></li>
  <li><a href="https://commonvoice.mozilla.org/">Mozilla Common Voice</a></li>
</ul>

<p><strong>Tools:</strong></p>
<ul>
  <li><a href="https://www.tensorflow.org/lite">TensorFlow Lite</a></li>
  <li><a href="https://developer.apple.com/documentation/coreml">Core ML</a></li>
  <li><a href="https://librosa.org/">Librosa</a> - Audio processing</li>
</ul>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0002-speech-classification/">arunbaby.com/speech-tech/0002-speech-classification</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#classification" class="page__taxonomy-item p-category" rel="tag">classification</a><span class="sep">, </span>
    
      <a href="/tags/#intent-recognition" class="page__taxonomy-item p-category" rel="tag">intent-recognition</a><span class="sep">, </span>
    
      <a href="/tags/#voice-commands" class="page__taxonomy-item p-category" rel="tag">voice-commands</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0002-valid-parentheses/" rel="permalink">Valid Parentheses
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Why a simple stack solves bracket matching, expression parsing, and even neural network depth management in one elegant pattern.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0002-classification-pipeline/" rel="permalink">Classification Pipeline Design
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">From raw data to production predictions: building a classification pipeline that handles millions of requests with 99.9% uptime.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0002-llm-capabilities-for-agents/" rel="permalink">LLM Capabilities for Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The Engine of Autonomy: Understanding the Agentic ‘Brain’.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Speech+Command+Classification%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0002-speech-classification%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0002-speech-classification%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0002-speech-classification/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0001-streaming-asr/" class="pagination--pager" title="Streaming ASR Architecture">Previous</a>
    
    
      <a href="/speech-tech/0003-audio-feature-extraction/" class="pagination--pager" title="Audio Feature Extraction for Speech ML">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
