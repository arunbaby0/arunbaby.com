<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Voice Activity Detection (VAD) - Arun Baby</title>
<meta name="description" content="How voice assistants and video conferencing apps detect when you’re speaking vs silence, the critical first step in every speech pipeline.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Voice Activity Detection (VAD)">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0004-voice-activity-detection/">


  <meta property="og:description" content="How voice assistants and video conferencing apps detect when you’re speaking vs silence, the critical first step in every speech pipeline.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Voice Activity Detection (VAD)">
  <meta name="twitter:description" content="How voice assistants and video conferencing apps detect when you’re speaking vs silence, the critical first step in every speech pipeline.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0004-voice-activity-detection/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T10:08:45+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0004-voice-activity-detection/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Voice Activity Detection (VAD)">
    <meta itemprop="description" content="How voice assistants and video conferencing apps detect when you’re speaking vs silence, the critical first step in every speech pipeline.">
    <meta itemprop="datePublished" content="2025-12-31T10:08:45+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0004-voice-activity-detection/" itemprop="url">Voice Activity Detection (VAD)
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#problem-definition">Problem Definition</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li></ul></li><li><a href="#approach-1-energy-based-vad">Approach 1: Energy-Based VAD</a><ul><li><a href="#implementation">Implementation</a></li><li><a href="#adaptive-thresholding">Adaptive Thresholding</a></li></ul></li><li><a href="#approach-2-zero-crossing-rate--energy">Approach 2: Zero-Crossing Rate + Energy</a><ul><li><a href="#implementation-1">Implementation</a></li></ul></li><li><a href="#approach-3-webrtc-vad">Approach 3: WebRTC VAD</a><ul><li><a href="#using-webrtc-vad">Using WebRTC VAD</a></li></ul></li><li><a href="#approach-4-ml-based-vad">Approach 4: ML-Based VAD</a><ul><li><a href="#cnn-based-vad">CNN-based VAD</a></li><li><a href="#training-ml-vad">Training ML VAD</a></li></ul></li><li><a href="#real-time-streaming-vad">Real-Time Streaming VAD</a><ul><li><a href="#streaming-implementation">Streaming Implementation</a></li></ul></li><li><a href="#production-considerations">Production Considerations</a><ul><li><a href="#hangover-and-padding">Hangover and Padding</a></li><li><a href="#performance-optimization">Performance Optimization</a></li></ul></li><li><a href="#integration-with-asr-pipeline">Integration with ASR Pipeline</a><ul><li><a href="#end-to-end-pipeline">End-to-End Pipeline</a></li><li><a href="#double-pass-vad-for-higher-accuracy">Double-Pass VAD for Higher Accuracy</a></li></ul></li><li><a href="#comparison-of-vad-methods">Comparison of VAD Methods</a></li><li><a href="#production-deployment">Production Deployment</a><ul><li><a href="#latency-budgets">Latency Budgets</a></li><li><a href="#resource-usage">Resource Usage</a></li><li><a href="#mobileedge-deployment">Mobile/Edge Deployment</a></li></ul></li><li><a href="#monitoring--debugging">Monitoring &amp; Debugging</a><ul><li><a href="#vad-quality-metrics">VAD Quality Metrics</a></li><li><a href="#debugging-common-issues">Debugging Common Issues</a></li></ul></li><li><a href="#advanced-techniques">Advanced Techniques</a><ul><li><a href="#noise-robust-vad">Noise-Robust VAD</a></li><li><a href="#multi-condition-training-data">Multi-Condition Training Data</a></li></ul></li><li><a href="#real-world-deployment-examples">Real-World Deployment Examples</a><ul><li><a href="#zoomvideo-conferencing">Zoom/Video Conferencing</a></li><li><a href="#smart-speakers-alexa-google-home">Smart Speakers (Alexa, Google Home)</a></li><li><a href="#call-centers">Call Centers</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>How voice assistants and video conferencing apps detect when you’re speaking vs silence, the critical first step in every speech pipeline.</strong></p>

<h2 id="introduction">Introduction</h2>

<p><strong>Voice Activity Detection (VAD)</strong> is the task of determining which parts of an audio stream contain speech vs non-speech (silence, background noise, music).</p>

<p>VAD is the <strong>gatekeeper</strong> of speech systems:</p>
<ul>
  <li>Triggers when to start listening (wake word detection)</li>
  <li>Determines when utterance ends (endpoint detection)</li>
  <li>Saves compute by only processing speech frames</li>
  <li>Improves bandwidth by only transmitting speech</li>
</ul>

<p><strong>Why it matters:</strong></p>
<ul>
  <li><strong>Power efficiency:</strong> Voice assistants sleep until speech detected</li>
  <li><strong>Latency:</strong> Know when user finished speaking → respond faster</li>
  <li><strong>Bandwidth:</strong> Transmit only speech frames in VoIP</li>
  <li><strong>Accuracy:</strong> Reduce false alarms in ASR systems</li>
</ul>

<p><strong>What you’ll learn:</strong></p>
<ul>
  <li>Energy-based VAD (simple, fast)</li>
  <li>WebRTC VAD (production standard)</li>
  <li>ML-based VAD (state-of-the-art)</li>
  <li>Real-time streaming implementation</li>
  <li>Production deployment considerations</li>
</ul>

<hr />

<h2 id="problem-definition">Problem Definition</h2>

<p>Design a real-time voice activity detection system.</p>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Detection</strong>
    <ul>
      <li>Classify each audio frame as speech or non-speech</li>
      <li>Handle noisy environments</li>
      <li>Detect speech from multiple speakers</li>
    </ul>
  </li>
  <li><strong>Endpoint Detection</strong>
    <ul>
      <li>Determine start of speech</li>
      <li>Determine end of speech</li>
      <li>Handle pauses within utterances</li>
    </ul>
  </li>
  <li><strong>Real-time Processing</strong>
    <ul>
      <li>Process audio frames as they arrive</li>
      <li>Minimal buffering</li>
      <li>Low latency</li>
    </ul>
  </li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Latency</strong>
    <ul>
      <li>Frame-level detection: &lt; 5ms</li>
      <li>Endpoint detection: &lt; 100ms after speech ends</li>
    </ul>
  </li>
  <li><strong>Accuracy</strong>
    <ul>
      <li>True positive rate &gt; 95% (detect speech)</li>
      <li>False positive rate &lt; 5% (mistake noise for speech)</li>
    </ul>
  </li>
  <li><strong>Robustness</strong>
    <ul>
      <li>Work in SNR (Signal-to-Noise Ratio) down to 0 dB</li>
      <li>Handle various noise types (music, traffic, crowds)</li>
      <li>Adapt to different speakers</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="approach-1-energy-based-vad">Approach 1: Energy-Based VAD</h2>

<p>Simplest approach: Speech has higher energy than silence.</p>

<h3 id="implementation">Implementation</h3>

<p>``python
import numpy as np
import librosa</p>

<p>class EnergyVAD:
 “””
 Energy-based Voice Activity Detection</p>

<p>Pros: Simple, fast, no training required
 Cons: Sensitive to noise, poor in low SNR
 “””</p>

<p>def <strong>init</strong>(
 self,
 sr=16000,
 frame_length_ms=20,
 hop_length_ms=10,
 energy_threshold=0.01
 ):
 self.sr = sr
 self.frame_length = int(sr * frame_length_ms / 1000)
 self.hop_length = int(sr * hop_length_ms / 1000)
 self.energy_threshold = energy_threshold</p>

<p>def compute_energy(self, frame):
 “””
 Compute frame energy (RMS)</p>

<p>Energy = sqrt(mean(x^2))
 “””
 return np.sqrt(np.mean(frame ** 2))</p>

<p>def detect(self, audio):
 “””
 Detect speech frames</p>

<p>Args:
 audio: Audio signal</p>

<p>Returns:
 List of booleans (True = speech, False = non-speech)
 “””
 # Frame the audio
 frames = librosa.util.frame(
 audio,
 frame_length=self.frame_length,
 hop_length=self.hop_length
 )</p>

<p># Compute energy per frame
 energies = np.array([self.compute_energy(frame) for frame in frames.T])</p>

<p># Threshold
 is_speech = energies &gt; self.energy_threshold</p>

<p>return is_speech</p>

<p>def get_speech_segments(self, audio):
 “””
 Get speech segments (start, end) in seconds</p>

<p>Returns:
 List of (start_time, end_time) tuples
 “””
 is_speech = self.detect(audio)</p>

<p>segments = []
 in_speech = False
 start_frame = 0</p>

<p>for i, speech in enumerate(is_speech):
 if speech and not in_speech:
 # Speech started
 start_frame = i
 in_speech = True
 elif not speech and in_speech:
 # Speech ended
 end_frame = i
 in_speech = False</p>

<p># Convert frames to time
 start_time = start_frame * self.hop_length / self.sr
 end_time = end_frame * self.hop_length / self.sr</p>

<p>segments.append((start_time, end_time))</p>

<p># Handle case where audio ends during speech
 if in_speech:
 end_time = len(is_speech) * self.hop_length / self.sr
 start_time = start_frame * self.hop_length / self.sr
 segments.append((start_time, end_time))</p>

<p>return segments</p>

<h1 id="usage">Usage</h1>
<p>vad = EnergyVAD(energy_threshold=0.01)</p>

<h1 id="load-audio">Load audio</h1>
<p>audio, sr = librosa.load(‘speech_with_silence.wav’, sr=16000)</p>

<h1 id="detect-speech">Detect speech</h1>
<p>is_speech = vad.detect(audio)
print(f”Speech frames: {is_speech.sum()} / {len(is_speech)}”)</p>

<h1 id="get-segments">Get segments</h1>
<p>segments = vad.get_speech_segments(audio)
for start, end in segments:
 print(f”Speech from {start:.2f}s to {end:.2f}s”)
``</p>

<h3 id="adaptive-thresholding">Adaptive Thresholding</h3>

<p>Fixed thresholds fail in varying noise conditions. Use adaptive thresholds.</p>

<p>``python
class AdaptiveEnergyVAD(EnergyVAD):
 “””
 Energy VAD with adaptive threshold</p>

<p>Threshold adapts to background noise level
 “””</p>

<p>def <strong>init</strong>(self, sr=16000, frame_length_ms=20, hop_length_ms=10):
 super().<strong>init</strong>(sr, frame_length_ms, hop_length_ms)
 self.noise_energy = 0.001 # Initial estimate
 self.alpha = 0.95 # Smoothing factor</p>

<p>def detect(self, audio):
 “"”Detect with adaptive threshold”””
 frames = librosa.util.frame(
 audio,
 frame_length=self.frame_length,
 hop_length=self.hop_length
 )</p>

<p>is_speech = []</p>

<p>for frame in frames.T:
 energy = self.compute_energy(frame)</p>

<p># Adaptive threshold: 3x noise energy
 threshold = 3.0 * self.noise_energy</p>

<p>if energy &gt; threshold:
 # Likely speech
 is_speech.append(True)
 else:
 # Likely noise/silence
 is_speech.append(False)</p>

<p># Update noise estimate (during silence only)
 self.noise_energy = self.alpha * self.noise_energy + (1 - self.alpha) * energy</p>

<p>return np.array(is_speech)
``</p>

<hr />

<h2 id="approach-2-zero-crossing-rate--energy">Approach 2: Zero-Crossing Rate + Energy</h2>

<p>Combine energy with zero-crossing rate for better accuracy.</p>

<h3 id="implementation-1">Implementation</h3>

<p>``python
class ZCR_Energy_VAD:
 “””
 VAD using Energy + Zero-Crossing Rate</p>

<p>Intuition:</p>
<ul>
  <li>Speech: Low ZCR (voiced sounds), moderate to high energy</li>
  <li>Noise: High ZCR (unvoiced), varying energy</li>
  <li>Silence: Low energy
 “””</li>
</ul>

<p>def <strong>init</strong>(
 self,
 sr=16000,
 frame_length_ms=20,
 hop_length_ms=10,
 energy_threshold=0.01,
 zcr_threshold=0.1
 ):
 self.sr = sr
 self.frame_length = int(sr * frame_length_ms / 1000)
 self.hop_length = int(sr * hop_length_ms / 1000)
 self.energy_threshold = energy_threshold
 self.zcr_threshold = zcr_threshold</p>

<p>def compute_zcr(self, frame):
 “””
 Compute zero-crossing rate</p>

<p>ZCR = # of times signal crosses zero / # samples
 “””
 signs = np.sign(frame)
 zcr = np.mean(np.abs(np.diff(signs))) / 2
 return zcr</p>

<p>def detect(self, audio):
 “””
 Detect using both energy and ZCR
 “””
 frames = librosa.util.frame(
 audio,
 frame_length=self.frame_length,
 hop_length=self.hop_length
 )</p>

<p>is_speech = []</p>

<p>for frame in frames.T:
 energy = np.sqrt(np.mean(frame ** 2))
 zcr = self.compute_zcr(frame)</p>

<p># Decision logic
 if energy &gt; self.energy_threshold:
 # High energy: could be speech or noise
 if zcr &lt; self.zcr_threshold:
 # Low ZCR → likely speech (voiced)
 is_speech.append(True)
 else:
 # High ZCR → likely noise
 is_speech.append(False)
 else:
 # Low energy → silence
 is_speech.append(False)</p>

<p>return np.array(is_speech)
``</p>

<hr />

<h2 id="approach-3-webrtc-vad">Approach 3: WebRTC VAD</h2>

<p>Industry-standard VAD used in Chrome, Skype, etc.</p>

<h3 id="using-webrtc-vad">Using WebRTC VAD</h3>

<p>``python</p>
<h1 id="webrtc-vad-requires-pip-install-webrtcvad">WebRTC VAD requires: pip install webrtcvad</h1>
<p>import webrtcvad
import struct</p>

<p>class WebRTCVAD:
 “””
 WebRTC Voice Activity Detector</p>

<p>Pros:</p>
<ul>
  <li>Production-tested (billions of users)</li>
  <li>Fast, CPU-efficient</li>
  <li>Robust to noise</li>
</ul>

<p>Cons:</p>
<ul>
  <li>Only works with specific sample rates (8/16/32/48 kHz)</li>
  <li>Fixed frame sizes (10/20/30 ms)
 “””</li>
</ul>

<p>def <strong>init</strong>(self, sr=16000, frame_duration_ms=30, aggressiveness=3):
 “””
 Args:
 sr: Sample rate (must be 8000, 16000, 32000, or 48000)
 frame_duration_ms: Frame duration (10, 20, or 30 ms)
 aggressiveness: 0-3 (0=least aggressive, 3=most aggressive)</p>
<ul>
  <li>Higher = more likely to classify as non-speech</li>
  <li>Use 3 for noisy environments
 “””
 if sr not in [8000, 16000, 32000, 48000]:
 raise ValueError(“Sample rate must be 8000, 16000, 32000, or 48000”)</li>
</ul>

<p>if frame_duration_ms not in [10, 20, 30]:
 raise ValueError(“Frame duration must be 10, 20, or 30 ms”)</p>

<p>self.sr = sr
 self.frame_duration_ms = frame_duration_ms
 self.frame_length = int(sr * frame_duration_ms / 1000)</p>

<p># Create VAD instance
 self.vad = webrtcvad.Vad(aggressiveness)</p>

<p>def detect(self, audio):
 “””
 Detect speech in audio</p>

<p>Args:
 audio: numpy array of int16 samples</p>

<p>Returns:
 List of booleans (True = speech)
 “””
 # Convert float to int16 if needed (clip to avoid overflow)
 if audio.dtype == np.float32 or audio.dtype == np.float64:
 audio = np.clip(audio, -1.0, 1.0)
 audio = (audio * 32767).astype(np.int16)</p>

<p># Frame audio
 num_frames = len(audio) // self.frame_length
 is_speech = []</p>

<p>for i in range(num_frames):
 start = i * self.frame_length
 end = start + self.frame_length
 frame = audio[start:end]</p>

<p># Convert to bytes
 frame_bytes = struct.pack(‘%dh’ % len(frame), *frame)</p>

<p># Detect
 speech = self.vad.is_speech(frame_bytes, self.sr)
 is_speech.append(speech)</p>

<p>return np.array(is_speech)</p>

<p>def get_speech_timestamps(self, audio):
 “””
 Get speech timestamps</p>

<p>Returns:
 List of (start_time, end_time) in seconds
 “””
 is_speech = self.detect(audio)</p>

<p>segments = []
 in_speech = False
 start_frame = 0</p>

<p>for i, speech in enumerate(is_speech):
 if speech and not in_speech:
 start_frame = i
 in_speech = True
 elif not speech and in_speech:
 in_speech = False
 start_time = start_frame * self.frame_length / self.sr
 end_time = i * self.frame_length / self.sr
 segments.append((start_time, end_time))</p>

<p>if in_speech:
 start_time = start_frame * self.frame_length / self.sr
 end_time = len(is_speech) * self.frame_length / self.sr
 segments.append((start_time, end_time))</p>

<p>return segments</p>

<h1 id="usage-1">Usage</h1>
<p>vad = WebRTCVAD(sr=16000, frame_duration_ms=30, aggressiveness=3)</p>

<p>audio, sr = librosa.load(‘audio.wav’, sr=16000)
segments = vad.get_speech_timestamps(audio)</p>

<p>print(“Speech segments:”)
for start, end in segments:
 print(f” {start:.2f}s - {end:.2f}s”)
``</p>

<hr />

<h2 id="approach-4-ml-based-vad">Approach 4: ML-Based VAD</h2>

<p>Use neural networks for state-of-the-art performance.</p>

<h3 id="cnn-based-vad">CNN-based VAD</h3>

<p>``python
import torch
import torch.nn as nn</p>

<p>class CNNVAD(nn.Module):
 “””
 CNN-based Voice Activity Detector</p>

<p>Input: Mel-spectrogram (time, freq)
 Output: Speech probability per frame
 “””</p>

<p>def <strong>init</strong>(self, n_mels=40):
 super().<strong>init</strong>()</p>

<p># CNN layers
 self.conv1 = nn.Sequential(
 nn.Conv2d(1, 32, kernel_size=3, padding=1),
 nn.BatchNorm2d(32),
 nn.ReLU(),
 nn.MaxPool2d(2, 2)
 )</p>

<p>self.conv2 = nn.Sequential(
 nn.Conv2d(32, 64, kernel_size=3, padding=1),
 nn.BatchNorm2d(64),
 nn.ReLU(),
 nn.MaxPool2d(2, 2)
 )</p>

<p># LSTM for temporal modeling
 self.lstm = nn.LSTM(
 input_size=64 * (n_mels // 4),
 hidden_size=128,
 num_layers=2,
 batch_first=True,
 bidirectional=True
 )</p>

<p># Classification head
 self.fc = nn.Linear(256, 1) # Binary classification
 self.sigmoid = nn.Sigmoid()</p>

<p>def forward(self, x):
 “””
 Forward pass</p>

<p>Args:
 x: (batch, 1, time, n_mels)</p>

<p>Returns:
 Speech probabilities: (batch, time)
 “””
 # CNN
 x = self.conv1(x) # (batch, 32, time/2, n_mels/2)
 x = self.conv2(x) # (batch, 64, time/4, n_mels/4)</p>

<p># Reshape for LSTM
 batch, channels, time, freq = x.size()
 x = x.permute(0, 2, 1, 3) # (batch, time, channels, freq)
 x = x.reshape(batch, time, channels * freq)</p>

<p># LSTM
 x, _ = self.lstm(x) # (batch, time, 256)</p>

<p># Classification
 x = self.fc(x) # (batch, time, 1)
 x = self.sigmoid(x) # (batch, time, 1)</p>

<p>return x.squeeze(-1) # (batch, time)</p>

<h1 id="usage-2">Usage</h1>
<p>model = CNNVAD(n_mels=40)</p>

<h1 id="example-input-mel-spectrogram">Example input: mel-spectrogram</h1>
<p>mel_spec = torch.randn(1, 1, 100, 40) # (batch=1, channels=1, time=100, mels=40)</p>

<h1 id="predict">Predict</h1>
<p>speech_prob = model(mel_spec) # (1, 100) - probability per frame
is_speech = speech_prob &gt; 0.5 # Threshold at 0.5</p>

<p>print(f”Speech probability shape: {speech_prob.shape}”)
print(f”Detected speech in {is_speech.sum().item()} / {is_speech.size(1)} frames”)
``</p>

<h3 id="training-ml-vad">Training ML VAD</h3>

<p>``python
class VADTrainer:
 “””
 Train VAD model
 “””</p>

<p>def <strong>init</strong>(self, model, device=’cuda’):
 self.model = model.to(device)
 self.device = device
 self.criterion = nn.BCELoss()
 self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001)</p>

<p>def train_epoch(self, train_loader):
 “"”Train for one epoch”””
 self.model.train()
 total_loss = 0</p>

<p>for mel_specs, labels in train_loader:
 mel_specs = mel_specs.to(self.device)
 labels = labels.to(self.device)</p>

<p># Forward
 predictions = self.model(mel_specs)
 loss = self.criterion(predictions, labels)</p>

<p># Backward
 self.optimizer.zero_grad()
 loss.backward()
 self.optimizer.step()</p>

<p>total_loss += loss.item()</p>

<p>return total_loss / len(train_loader)</p>

<p>def evaluate(self, val_loader):
 “"”Evaluate model”””
 self.model.eval()
 correct = 0
 total = 0</p>

<p>with torch.no_grad():
 for mel_specs, labels in val_loader:
 mel_specs = mel_specs.to(self.device)
 labels = labels.to(self.device)</p>

<p>predictions = self.model(mel_specs)
 predicted = (predictions &gt; 0.5).float()</p>

<p>correct += (predicted == labels).sum().item()
 total += labels.numel()</p>

<p>accuracy = correct / total
 return accuracy
``</p>

<hr />

<h2 id="real-time-streaming-vad">Real-Time Streaming VAD</h2>

<p>Process audio as it arrives (streaming).</p>

<h3 id="streaming-implementation">Streaming Implementation</h3>

<p>``python
from collections import deque
import numpy as np
import struct</p>

<p>class StreamingVAD:
 “””
 Real-time VAD for streaming audio</p>

<p>Use case: Voice assistants, VoIP, live transcription
 “””</p>

<p>def <strong>init</strong>(
 self,
 sr=16000,
 frame_duration_ms=30,
 aggressiveness=3,
 speech_pad_ms=300
 ):
 self.sr = sr
 self.frame_duration_ms = frame_duration_ms
 self.frame_length = int(sr * frame_duration_ms / 1000)
 self.speech_pad_ms = speech_pad_ms
 self.speech_pad_frames = int(speech_pad_ms / frame_duration_ms)</p>

<p># WebRTC VAD
 self.vad = webrtcvad.Vad(aggressiveness)</p>

<p># State
 self.buffer = deque(maxlen=10000) # Audio buffer
 self.speech_frames = 0 # Consecutive speech frames
 self.silence_frames = 0 # Consecutive silence frames
 self.in_speech = False</p>

<p># Store speech segments
 self.current_speech = []</p>

<p>def add_audio(self, audio_chunk):
 “””
 Add audio chunk to buffer</p>

<p>Args:
 audio_chunk: New audio samples (int16)
 “””
 self.buffer.extend(audio_chunk)</p>

<p>def process_frame(self):
 “””
 Process one frame from buffer</p>

<p>Returns:
 (is_speech, speech_ended, speech_audio)
 “””
 if len(self.buffer) &lt; self.frame_length:
 return None, False, None</p>

<p># Extract frame
 frame = np.array([self.buffer.popleft() for _ in range(self.frame_length)])</p>

<p># Convert to bytes
 frame_bytes = struct.pack(‘%dh’ % len(frame), *frame)</p>

<p># Detect
 is_speech = self.vad.is_speech(frame_bytes, self.sr)</p>

<p># Update state
 if is_speech:
 self.speech_frames += 1
 self.silence_frames = 0</p>

<p>if not self.in_speech:
 # Speech just started
 self.in_speech = True
 self.current_speech = []</p>

<p># Add to current speech
 self.current_speech.extend(frame)</p>

<p>else:
 self.silence_frames += 1
 self.speech_frames = 0</p>

<p>if self.in_speech:
 # Add padding
 self.current_speech.extend(frame)</p>

<p># Check if speech ended
 if self.silence_frames &gt;= self.speech_pad_frames:
 # Speech ended
 self.in_speech = False
 speech_audio = np.array(self.current_speech)
 self.current_speech = []</p>

<p>return False, True, speech_audio</p>

<p>return is_speech, False, None</p>

<p>def process_stream(self):
 “””
 Process all buffered audio</p>

<p>Yields speech segments as they complete
 “””
 while len(self.buffer) &gt;= self.frame_length:
 is_speech, speech_ended, speech_audio = self.process_frame()</p>

<p>if speech_ended:
 yield speech_audio</p>

<h1 id="usage-3">Usage</h1>
<p>streaming_vad = StreamingVAD(sr=16000, frame_duration_ms=30)</p>

<h1 id="simulate-streaming-process-chunks-as-they-arrive">Simulate streaming (process chunks as they arrive)</h1>
<p>chunk_size = 480 # 30ms at 16kHz</p>

<p>for chunk_start in range(0, len(audio), chunk_size):
 chunk = audio[chunk_start:chunk_start + chunk_size]</p>

<p># Add to buffer
 streaming_vad.add_audio(chunk.astype(np.int16))</p>

<p># Process
 for speech_segment in streaming_vad.process_stream():
 print(f”Speech segment detected: {len(speech_segment)} samples”)
 # Send to ASR, save, etc.
``</p>

<hr />

<h2 id="production-considerations">Production Considerations</h2>

<h3 id="hangover-and-padding">Hangover and Padding</h3>

<p>Add padding before/after speech to avoid cutting off words.</p>

<p>``python
class VADWithPadding:
 “””
 VAD with pre/post padding
 “””</p>

<p>def <strong>init</strong>(
 self,
 vad,
 pre_pad_ms=200,
 post_pad_ms=500,
 sr=16000
 ):
 self.vad = vad
 self.pre_pad_frames = int(pre_pad_ms / 30) # Assuming 30ms frames
 self.post_pad_frames = int(post_pad_ms / 30)
 self.sr = sr</p>

<p>def detect_with_padding(self, audio):
 “””
 Detect speech with padding
 “””
 is_speech = self.vad.detect(audio)</p>

<p># Add pre-padding
 padded = np.copy(is_speech)
 for i in range(len(is_speech)):
 if is_speech[i]:
 # Mark previous frames as speech
 start = max(0, i - self.pre_pad_frames)
 padded[start:i] = True</p>

<p># Add post-padding
 for i in range(len(is_speech)):
 if is_speech[i]:
 # Mark following frames as speech
 end = min(len(is_speech), i + self.post_pad_frames)
 padded[i:end] = True</p>

<p>return padded
``</p>

<h3 id="performance-optimization">Performance Optimization</h3>

<p>``python
import time</p>

<p>class OptimizedVAD:
 “””
 Optimized VAD for production
 “””</p>

<p>def <strong>init</strong>(self, vad_impl):
 self.vad = vad_impl
 self.stats = {
 ‘total_frames’: 0,
 ‘speech_frames’: 0,
 ‘processing_time’: 0
 }</p>

<p>def detect_with_stats(self, audio):
 “"”Detect with performance tracking”””
 start = time.perf_counter()</p>

<p>is_speech = self.vad.detect(audio)</p>

<p>end = time.perf_counter()</p>

<p># Update stats
 self.stats[‘total_frames’] += len(is_speech)
 self.stats[‘speech_frames’] += is_speech.sum()
 self.stats[‘processing_time’] += (end - start)</p>

<p>return is_speech</p>

<p>def get_stats(self):
 “"”Get performance statistics”””
 if self.stats[‘total_frames’] == 0:
 return None</p>

<p>speech_ratio = self.stats[‘speech_frames’] / self.stats[‘total_frames’]
 avg_time_per_frame = self.stats[‘processing_time’] / self.stats[‘total_frames’]</p>

<p>return {
 ‘speech_ratio’: speech_ratio,
 ‘avg_latency_ms’: avg_time_per_frame * 1000,
 ‘total_frames’: self.stats[‘total_frames’],
 ‘speech_frames’: self.stats[‘speech_frames’]
 }
``</p>

<hr />

<h2 id="integration-with-asr-pipeline">Integration with ASR Pipeline</h2>

<p>VAD as the first stage in speech recognition systems.</p>

<h3 id="end-to-end-pipeline">End-to-End Pipeline</h3>

<p>``python
class SpeechPipeline:
 “””
 Complete speech recognition pipeline with VAD</p>

<p>Pipeline: Audio → VAD → ASR → Text
 “””</p>

<p>def <strong>init</strong>(self):
 # VAD
 self.vad = WebRTCVAD(sr=16000, frame_duration_ms=30, aggressiveness=3)</p>

<p># Placeholder for ASR model
 self.asr_model = None # Would be actual ASR model</p>

<p># Buffering
 self.min_speech_duration = 0.5 # seconds
 self.max_speech_duration = 10.0 # seconds</p>

<p>def process_audio_file(self, audio_path):
 “””
 Process audio file end-to-end</p>

<p>Returns:
 List of transcriptions
 “””
 # Load audio
 import librosa
 audio, sr = librosa.load(audio_path, sr=16000)</p>

<p># Run VAD
 speech_segments = self.vad.get_speech_timestamps(audio)</p>

<p># Filter by duration
 valid_segments = [
 (start, end) for start, end in speech_segments
 if (end - start) &gt;= self.min_speech_duration and
 (end - start) &lt;= self.max_speech_duration
 ]</p>

<p>transcriptions = []</p>

<p>for start, end in valid_segments:
 # Extract speech segment
 start_sample = int(start * sr)
 end_sample = int(end * sr)
 speech_audio = audio[start_sample:end_sample]</p>

<p># Run ASR (placeholder)
 # transcript = self.asr_model.transcribe(speech_audio)
 transcript = f”[Speech from {start:.2f}s to {end:.2f}s]”</p>

<p>transcriptions.append({
 ‘start’: start,
 ‘end’: end,
 ‘duration’: end - start,
 ‘text’: transcript
 })</p>

<p>return transcriptions</p>

<p>def process_streaming(self, audio_stream):
 “””
 Process streaming audio</p>

<p>Yields transcriptions as speech segments complete
 “””
 streaming_vad = StreamingVAD(sr=16000, frame_duration_ms=30)</p>

<p>for chunk in audio_stream:
 streaming_vad.add_audio(chunk)</p>

<p>for speech_segment in streaming_vad.process_stream():
 # Run ASR on completed segment
 # transcript = self.asr_model.transcribe(speech_segment)
 transcript = “[Speech detected]”</p>

<p>yield {
 ‘audio’: speech_segment,
 ‘text’: transcript,
 ‘timestamp’: time.time()
 }</p>

<h1 id="usage-4">Usage</h1>
<p>pipeline = SpeechPipeline()</p>

<h1 id="process-file">Process file</h1>
<p>transcriptions = pipeline.process_audio_file(‘conversation.wav’)
for t in transcriptions:
 print(f”{t[‘start’]:.2f}s - {t[‘end’]:.2f}s: {t[‘text’]}”)
``</p>

<h3 id="double-pass-vad-for-higher-accuracy">Double-Pass VAD for Higher Accuracy</h3>

<p>Use aggressive VAD first, then refine with ML model.</p>

<p>``python
class TwoPassVAD:
 “””
 Two-pass VAD for improved accuracy</p>

<p>Pass 1: Fast WebRTC VAD (aggressive) → candidate segments
 Pass 2: ML VAD (accurate) → final segments
 “””</p>

<p>def <strong>init</strong>(self):
 # Fast pass: WebRTC VAD (aggressive)
 self.fast_vad = WebRTCVAD(sr=16000, frame_duration_ms=30, aggressiveness=3)</p>

<p># Accurate pass: ML VAD
 self.ml_vad = CNNVAD(n_mels=40)
 self.ml_vad.eval()</p>

<p>def detect(self, audio):
 “””
 Two-pass detection</p>

<p>Returns:
 Refined speech segments
 “””
 # Pass 1: Fast VAD to get candidate regions
 candidate_segments = self.fast_vad.get_speech_timestamps(audio)</p>

<p># Pass 2: ML VAD to refine each candidate
 refined_segments = []</p>

<p>for start, end in candidate_segments:
 # Extract segment
 start_sample = int(start * 16000)
 end_sample = int(end * 16000)
 segment_audio = audio[start_sample:end_sample]</p>

<p># Run ML VAD on segment
 # Convert to mel-spectrogram
 import librosa
 mel_spec = librosa.feature.melspectrogram(
 y=segment_audio,
 sr=16000,
 n_mels=40
 )</p>

<p># ML model prediction
 # mel_tensor = torch.from_numpy(mel_spec).unsqueeze(0).unsqueeze(0)
 # with torch.no_grad():
 # predictions = self.ml_vad(mel_tensor)
 # is_speech_frames = predictions &gt; 0.5</p>

<p># For now, accept if fast VAD said speech
 refined_segments.append((start, end))</p>

<p>return refined_segments
``</p>

<hr />

<h2 id="comparison-of-vad-methods">Comparison of VAD Methods</h2>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Pros</th>
      <th>Cons</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Energy-based</strong></td>
      <td>Simple, fast, no training</td>
      <td>Poor in noise</td>
      <td>Quiet environments</td>
    </tr>
    <tr>
      <td><strong>ZCR + Energy</strong></td>
      <td>Better than energy alone</td>
      <td>Still noise-sensitive</td>
      <td>Moderate noise</td>
    </tr>
    <tr>
      <td><strong>WebRTC VAD</strong></td>
      <td>Fast, robust, production-tested</td>
      <td>Fixed aggressiveness</td>
      <td>Real-time apps, VoIP</td>
    </tr>
    <tr>
      <td><strong>ML-based (CNN)</strong></td>
      <td>Best accuracy, adaptable</td>
      <td>Requires training, slower</td>
      <td>High-noise, accuracy-critical</td>
    </tr>
    <tr>
      <td><strong>ML-based (RNN)</strong></td>
      <td>Temporal modeling</td>
      <td>Higher latency</td>
      <td>Offline processing</td>
    </tr>
    <tr>
      <td><strong>Hybrid (2-pass)</strong></td>
      <td>Balance speed/accuracy</td>
      <td>More complex</td>
      <td>Production ASR</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="production-deployment">Production Deployment</h2>

<h3 id="latency-budgets">Latency Budgets</h3>

<p>For real-time applications:</p>

<p>``
Voice Assistant Latency Budget:
┌─────────────────────────────────────┐
│ VAD Detection: 5-10ms │
│ Endpoint Detection: 100-200ms │
│ ASR Processing: 500-1000ms │
│ NLU + Dialog: 100-200ms │
│ TTS Generation: 200-500ms │
├─────────────────────────────────────┤
│ Total: ~1-2 seconds│
└─────────────────────────────────────┘</p>

<p>VAD must be fast to keep overall latency low!
``</p>

<h3 id="resource-usage">Resource Usage</h3>

<p>``python
import psutil
import time</p>

<p>class VADProfiler:
 “””
 Profile VAD performance
 “””</p>

<p>def <strong>init</strong>(self, vad):
 self.vad = vad</p>

<p>def profile(self, audio, num_runs=100):
 “””
 Benchmark VAD</p>

<p>Returns:
 Performance metrics
 “””
 latencies = []</p>

<p># Warm-up
 for _ in range(10):
 self.vad.detect(audio)</p>

<p># Measure
 process = psutil.Process()</p>

<p>cpu_percent_before = process.cpu_percent()
 memory_before = process.memory_info().rss / 1024 / 1024 # MB</p>

<p>for _ in range(num_runs):
 start = time.perf_counter()
 result = self.vad.detect(audio)
 end = time.perf_counter()</p>

<p>latencies.append((end - start) * 1000) # ms</p>

<p>cpu_percent_after = process.cpu_percent()
 memory_after = process.memory_info().rss / 1024 / 1024 # MB</p>

<p>return {
 ‘mean_latency_ms’: np.mean(latencies),
 ‘p50_latency_ms’: np.percentile(latencies, 50),
 ‘p95_latency_ms’: np.percentile(latencies, 95),
 ‘p99_latency_ms’: np.percentile(latencies, 99),
 ‘throughput_fps’: 1000 / np.mean(latencies),
 ‘cpu_usage_pct’: cpu_percent_after - cpu_percent_before,
 ‘memory_mb’: memory_after - memory_before
 }</p>

<h1 id="usage-5">Usage</h1>
<p>profiler = VADProfiler(WebRTCVAD())</p>

<p>audio, sr = librosa.load(‘test.wav’, sr=16000, duration=10.0)
metrics = profiler.profile(audio)</p>

<p>print(f”Mean latency: {metrics[‘mean_latency_ms’]:.2f}ms”)
print(f”P95 latency: {metrics[‘p95_latency_ms’]:.2f}ms”)
print(f”Throughput: {metrics[‘throughput_fps’]:.0f} frames/sec”)
print(f”CPU usage: {metrics[‘cpu_usage_pct’]:.1f}%”)
print(f”Memory: {metrics[‘memory_mb’]:.1f} MB”)
``</p>

<h3 id="mobileedge-deployment">Mobile/Edge Deployment</h3>

<p>Optimize VAD for on-device deployment.</p>

<p>``python
class MobileOptimizedVAD:
 “””
 VAD optimized for mobile devices</p>

<p>Quantized model, reduced precision, smaller memory footprint
 “””</p>

<p>def <strong>init</strong>(self):
 # Use int8 quantization for mobile
 import torch</p>

<p>self.model = CNNVAD(n_mels=40)</p>

<p># Quantize model
 # Dynamic quantization applies to Linear/LSTM; Conv2d not supported
 self.model = torch.quantization.quantize_dynamic(
 self.model,
 {torch.nn.Linear},
 dtype=torch.qint8
 )</p>

<p>self.model.eval()</p>

<p>def detect_efficient(self, audio):
 “””
 Efficient detection with reduced memory</p>

<p>Process in chunks to reduce peak memory
 “””
 chunk_size = 16000 # 1 second chunks
 results = []</p>

<p>for i in range(0, len(audio), chunk_size):
 chunk = audio[i:i+chunk_size]</p>

<p># Process chunk
 # result = self.process_chunk(chunk)
 # results.extend(result)
 pass</p>

<p>return results
``</p>

<hr />

<h2 id="monitoring--debugging">Monitoring &amp; Debugging</h2>

<h3 id="vad-quality-metrics">VAD Quality Metrics</h3>

<p>``python
class VADEvaluator:
 “””
 Evaluate VAD performance</p>

<p>Metrics:</p>
<ul>
  <li>Precision: % of detected speech that is actual speech</li>
  <li>Recall: % of actual speech that was detected</li>
  <li>F1 score</li>
  <li>False alarm rate</li>
  <li>Miss rate
 “””</li>
</ul>

<p>def <strong>init</strong>(self):
 pass</p>

<p>def evaluate(
 self,
 predictions: np.ndarray,
 ground_truth: np.ndarray
 ) -&gt; dict:
 “””
 Compute VAD metrics</p>

<p>Args:
 predictions: Binary array (1=speech, 0=non-speech)
 ground_truth: Ground truth labels</p>

<p>Returns:
 Dictionary of metrics
 “””
 # True positives, false positives, etc.
 tp = np.sum((predictions == 1) &amp; (ground_truth == 1))
 fp = np.sum((predictions == 1) &amp; (ground_truth == 0))
 tn = np.sum((predictions == 0) &amp; (ground_truth == 0))
 fn = np.sum((predictions == 0) &amp; (ground_truth == 1))</p>

<p># Metrics
 precision = tp / (tp + fp) if (tp + fp) &gt; 0 else 0
 recall = tp / (tp + fn) if (tp + fn) &gt; 0 else 0
 f1 = 2 * precision * recall / (precision + recall) if (precision + recall) &gt; 0 else 0</p>

<p>accuracy = (tp + tn) / (tp + tn + fp + fn)</p>

<p>false_alarm_rate = fp / (fp + tn) if (fp + tn) &gt; 0 else 0
 miss_rate = fn / (fn + tp) if (fn + tp) &gt; 0 else 0</p>

<p>return {
 ‘precision’: precision,
 ‘recall’: recall,
 ‘f1_score’: f1,
 ‘accuracy’: accuracy,
 ‘false_alarm_rate’: false_alarm_rate,
 ‘miss_rate’: miss_rate,
 ‘tp’: int(tp),
 ‘fp’: int(fp),
 ‘tn’: int(tn),
 ‘fn’: int(fn)
 }</p>

<h1 id="usage-6">Usage</h1>
<p>evaluator = VADEvaluator()</p>

<h1 id="load-ground-truth">Load ground truth</h1>
<h1 id="ground_truth--load_annotationstest_audiotxt">ground_truth = load_annotations(‘test_audio.txt’)</h1>

<h1 id="run-vad">Run VAD</h1>
<p>vad = WebRTCVAD()</p>
<h1 id="predictions--vaddetectaudio">predictions = vad.detect(audio)</h1>

<h1 id="evaluate">Evaluate</h1>
<h1 id="metrics--evaluatorevaluatepredictions-ground_truth">metrics = evaluator.evaluate(predictions, ground_truth)</h1>

<h1 id="printfprecision-metricsprecision3f">print(f”Precision: {metrics[‘precision’]:.3f}”)</h1>
<h1 id="printfrecall-metricsrecall3f">print(f”Recall: {metrics[‘recall’]:.3f}”)</h1>
<h1 id="printff1-score-metricsf1_score3f">print(f”F1 Score: {metrics[‘f1_score’]:.3f}”)</h1>
<h1 id="printffalse-alarm-rate-metricsfalse_alarm_rate3f">print(f”False Alarm Rate: {metrics[‘false_alarm_rate’]:.3f}”)</h1>
<p>``</p>

<h3 id="debugging-common-issues">Debugging Common Issues</h3>

<p><strong>Issue 1: Clipping Speech Beginnings</strong></p>

<p>``python</p>
<h1 id="solution-increase-pre-padding">Solution: Increase pre-padding</h1>
<p>vad_with_padding = VADWithPadding(
 vad=WebRTCVAD(),
 pre_pad_ms=300, # Increase from 200ms
 post_pad_ms=500
)
``</p>

<p><strong>Issue 2: False Positives from Music</strong></p>

<p>``python</p>
<h1 id="solution-use-ml-vad-or-add-music-classifier">Solution: Use ML VAD or add music classifier</h1>
<p>class MusicFilteredVAD:
 “””
 VAD with music filtering
 “””</p>

<p>def <strong>init</strong>(self, vad, music_classifier):
 self.vad = vad
 self.music_classifier = music_classifier</p>

<p>def detect(self, audio):
 “"”Detect speech, filtering out music”””
 # Run VAD
 speech_frames = self.vad.detect(audio)</p>

<p># Filter music
 is_music = self.music_classifier.predict(audio)</p>

<p># Combine
 is_speech = speech_frames &amp; (~is_music)</p>

<p>return is_speech
``</p>

<p><strong>Issue 3: High CPU Usage</strong></p>

<p>``python</p>
<h1 id="solution-downsample-audio-or-use-simpler-vad">Solution: Downsample audio or use simpler VAD</h1>
<p>class DownsampledVAD:
 “””
 VAD with audio downsampling for efficiency
 “””</p>

<p>def <strong>init</strong>(self, target_sr=8000):
 self.target_sr = target_sr
 self.vad = WebRTCVAD(sr=8000) # 8kHz instead of 16kHz</p>

<p>def detect(self, audio, original_sr=16000):
 “"”Detect with downsampling”””
 # Downsample
 import librosa
 audio_downsampled = librosa.resample(
 audio,
 orig_sr=original_sr,
 target_sr=self.target_sr
 )</p>

<p># Run VAD on downsampled audio
 return self.vad.detect(audio_downsampled)
``</p>

<hr />

<h2 id="advanced-techniques">Advanced Techniques</h2>

<h3 id="noise-robust-vad">Noise-Robust VAD</h3>

<p>Use spectral subtraction for noise reduction before VAD.</p>

<p>``python
class NoiseRobustVAD:
 “””
 VAD with noise reduction preprocessing
 “””</p>

<p>def <strong>init</strong>(self, vad):
 self.vad = vad</p>

<p>def spectral_subtraction(self, audio, noise_profile):
 “””
 Simple spectral subtraction</p>

<p>Args:
 audio: Input audio
 noise_profile: Estimated noise spectrum</p>

<p>Returns:
 Denoised audio
 “””
 import librosa</p>

<p># STFT
 D = librosa.stft(audio)
 magnitude = np.abs(D)
 phase = np.angle(D)</p>

<p># Subtract noise
 magnitude_clean = np.maximum(magnitude - noise_profile, 0)</p>

<p># Reconstruct
 D_clean = magnitude_clean * np.exp(1j * phase)
 audio_clean = librosa.istft(D_clean)</p>

<p>return audio_clean</p>

<p>def detect_with_denoising(self, audio):
 “"”Detect speech after denoising”””
 # Estimate noise from first 0.5 seconds
 noise_segment = audio[:8000] # 0.5s at 16kHz</p>

<p>import librosa
 noise_spectrum = np.abs(librosa.stft(noise_segment))
 noise_profile = np.median(noise_spectrum, axis=1, keepdims=True)</p>

<p># Denoise
 audio_clean = self.spectral_subtraction(audio, noise_profile)</p>

<p># Run VAD on clean audio
 return self.vad.detect(audio_clean)
``</p>

<h3 id="multi-condition-training-data">Multi-Condition Training Data</h3>

<p>For ML-based VAD, train on diverse conditions.</p>

<p>``python
class DataAugmentationForVAD:
 “””
 Augment training data for robust VAD
 “””</p>

<p>def augment(self, clean_speech):
 “””
 Create augmented samples</p>

<p>Augmentations:</p>
<ul>
  <li>Add various noise types</li>
  <li>Vary SNR levels</li>
  <li>Apply room reverberation</li>
  <li>Change speaker characteristics
 “””
 augmented = []</li>
</ul>

<p># 1. Add white noise
 noise = np.random.randn(len(clean_speech)) * 0.01
 augmented.append(clean_speech + noise)</p>

<p># 2. Add babble noise (simulated)
 # babble = load_babble_noise()
 # augmented.append(clean_speech + babble)</p>

<p># 3. Apply reverberation
 # reverb = apply_reverb(clean_speech)
 # augmented.append(reverb)</p>

<p>return augmented
``</p>

<hr />

<h2 id="real-world-deployment-examples">Real-World Deployment Examples</h2>

<h3 id="zoomvideo-conferencing">Zoom/Video Conferencing</h3>

<p><strong>Requirements:</strong></p>
<ul>
  <li>Ultra-low latency (&lt; 10ms)</li>
  <li>Adaptive to varying network conditions</li>
  <li>Handle overlapping speech (multiple speakers)</li>
</ul>

<p><strong>Solution:</strong></p>
<ul>
  <li>WebRTC VAD for speed</li>
  <li>Adaptive aggressiveness based on network bandwidth</li>
  <li>Per-speaker VAD in multi-party calls</li>
</ul>

<h3 id="smart-speakers-alexa-google-home">Smart Speakers (Alexa, Google Home)</h3>

<p><strong>Requirements:</strong></p>
<ul>
  <li>Always-on (low power)</li>
  <li>Far-field audio (echoes, reverberation)</li>
  <li>Wake word detection + VAD</li>
</ul>

<p><strong>Solution:</strong></p>
<ul>
  <li>Two-stage: Wake word detector → VAD → ASR</li>
  <li>On-device VAD (WebRTC or lightweight ML)</li>
  <li>Cloud-based refinement for difficult cases</li>
</ul>

<h3 id="call-centers">Call Centers</h3>

<p><strong>Requirements:</strong></p>
<ul>
  <li>High accuracy (for analytics)</li>
  <li>Speaker diarization integration</li>
  <li>Post-processing acceptable</li>
</ul>

<p><strong>Solution:</strong></p>
<ul>
  <li>ML-based VAD with large models</li>
  <li>Two-pass processing</li>
  <li>Combined with speaker diarization</li>
</ul>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Energy + ZCR</strong> provides simple baseline VAD 
✅ <strong>WebRTC VAD</strong> is production-standard, fast, robust, widely deployed 
✅ <strong>ML-based VAD</strong> achieves best accuracy in noisy conditions 
✅ <strong>Two-pass VAD</strong> balances speed and accuracy for production 
✅ <strong>Streaming processing</strong> enables real-time applications 
✅ <strong>Padding is critical</strong> to avoid cutting off speech (200-500ms) 
✅ <strong>Adaptive thresholds</strong> handle varying noise levels 
✅ <strong>Frame size tradeoff:</strong> Smaller = lower latency, larger = better accuracy 
✅ <strong>Quantization &amp; optimization</strong> essential for mobile/edge deployment 
✅ <strong>Monitor precision/recall</strong> in production to catch degradation 
✅ <strong>Integration with ASR</strong> requires careful endpoint detection logic 
✅ <strong>Noise robustness</strong> via preprocessing or multi-condition training</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0004-voice-activity-detection/">arunbaby.com/speech-tech/0004-voice-activity-detection</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#audio-processing" class="page__taxonomy-item p-category" rel="tag">audio-processing</a><span class="sep">, </span>
    
      <a href="/tags/#real-time" class="page__taxonomy-item p-category" rel="tag">real-time</a><span class="sep">, </span>
    
      <a href="/tags/#vad" class="page__taxonomy-item p-category" rel="tag">vad</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0004-best-time-buy-sell-stock/" rel="permalink">Best Time to Buy and Sell Stock
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The single-pass pattern that powers streaming analytics, online algorithms, and real-time decision making in production systems.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0004-ab-testing-systems/" rel="permalink">A/B Testing Systems for ML
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          28 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How to design experimentation platforms that enable rapid iteration while maintaining statistical rigor at scale.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0004-tool-calling-fundamentals/" rel="permalink">Tool Calling Fundamentals
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Giving the Brain Hands to Act: The Interface Between Intelligence and Infrastructure.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Voice+Activity+Detection+%28VAD%29%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0004-voice-activity-detection%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0004-voice-activity-detection%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0004-voice-activity-detection/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0003-audio-feature-extraction/" class="pagination--pager" title="Audio Feature Extraction for Speech ML">Previous</a>
    
    
      <a href="/speech-tech/0005-speaker-recognition/" class="pagination--pager" title="Speaker Recognition &amp; Verification">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
