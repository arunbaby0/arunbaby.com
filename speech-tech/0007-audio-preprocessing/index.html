<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Audio Preprocessing &amp; Signal Processing - Arun Baby</title>
<meta name="description" content="Clean audio is the foundation of robust speech systems, master preprocessing pipelines that handle real-world noise and variability.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Audio Preprocessing &amp; Signal Processing">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0007-audio-preprocessing/">


  <meta property="og:description" content="Clean audio is the foundation of robust speech systems, master preprocessing pipelines that handle real-world noise and variability.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Audio Preprocessing &amp; Signal Processing">
  <meta name="twitter:description" content="Clean audio is the foundation of robust speech systems, master preprocessing pipelines that handle real-world noise and variability.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0007-audio-preprocessing/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0007-audio-preprocessing/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Audio Preprocessing &amp; Signal Processing">
    <meta itemprop="description" content="Clean audio is the foundation of robust speech systems, master preprocessing pipelines that handle real-world noise and variability.">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0007-audio-preprocessing/" itemprop="url">Audio Preprocessing &amp; Signal Processing
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#audio-fundamentals">Audio Fundamentals</a><ul><li><a href="#digital-audio-representation">Digital Audio Representation</a></li><li><a href="#nyquist-shannon-sampling-theorem">Nyquist-Shannon Sampling Theorem</a></li></ul></li><li><a href="#loading--format-conversion">Loading &amp; Format Conversion</a><ul><li><a href="#loading-audio">Loading Audio</a></li><li><a href="#format-conversion">Format Conversion</a></li><li><a href="#monostereo-conversion">Mono/Stereo Conversion</a></li></ul></li><li><a href="#resampling">Resampling</a><ul><li><a href="#high-quality-resampling">High-Quality Resampling</a></li></ul></li><li><a href="#normalization">Normalization</a><ul><li><a href="#amplitude-normalization">Amplitude Normalization</a></li><li><a href="#peak-normalization">Peak Normalization</a></li><li><a href="#dc-offset-removal">DC Offset Removal</a></li></ul></li><li><a href="#noise-reduction">Noise Reduction</a><ul><li><a href="#1-spectral-subtraction">1. Spectral Subtraction</a></li><li><a href="#2-wiener-filtering">2. Wiener Filtering</a></li><li><a href="#3-high-pass-filter-remove-low-frequency-noise">3. High-Pass Filter (Remove Low-Frequency Noise)</a></li></ul></li><li><a href="#voice-activity-detection-vad">Voice Activity Detection (VAD)</a></li><li><a href="#segmentation">Segmentation</a><ul><li><a href="#fixed-length-segmentation">Fixed-Length Segmentation</a></li><li><a href="#adaptive-segmentation-based-on-pauses">Adaptive Segmentation (Based on Pauses)</a></li></ul></li><li><a href="#data-augmentation">Data Augmentation</a><ul><li><a href="#1-time-stretching">1. Time Stretching</a></li><li><a href="#2-pitch-shifting">2. Pitch Shifting</a></li><li><a href="#3-adding-noise">3. Adding Noise</a></li><li><a href="#4-background-noise-mixing">4. Background Noise Mixing</a></li><li><a href="#5-specaugment-for-spectrograms">5. SpecAugment (For Spectrograms)</a></li></ul></li><li><a href="#connection-to-feature-engineering">Connection to Feature Engineering</a></li><li><a href="#production-pipeline">Production Pipeline</a></li><li><a href="#real-world-challenges--solutions">Real-World Challenges &amp; Solutions</a><ul><li><a href="#challenge-1-codec-artifacts">Challenge 1: Codec Artifacts</a></li><li><a href="#challenge-2-variable-sample-rates">Challenge 2: Variable Sample Rates</a></li><li><a href="#challenge-3-clipping--distortion">Challenge 3: Clipping &amp; Distortion</a></li><li><a href="#challenge-4-background-babble-noise">Challenge 4: Background Babble Noise</a></li></ul></li><li><a href="#audio-quality-metrics">Audio Quality Metrics</a><ul><li><a href="#signal-to-noise-ratio-snr">Signal-to-Noise Ratio (SNR)</a></li><li><a href="#perceptual-evaluation-of-speech-quality-pesq">Perceptual Evaluation of Speech Quality (PESQ)</a></li></ul></li><li><a href="#advanced-augmentation-strategies">Advanced Augmentation Strategies</a><ul><li><a href="#room-impulse-response-rir-convolution">Room Impulse Response (RIR) Convolution</a></li><li><a href="#codec-simulation">Codec Simulation</a></li><li><a href="#dynamic-range-compression">Dynamic Range Compression</a></li></ul></li><li><a href="#end-to-end-preprocessing-pipeline">End-to-End Preprocessing Pipeline</a></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Clean audio is the foundation of robust speech systems, master preprocessing pipelines that handle real-world noise and variability.</strong></p>

<h2 id="introduction">Introduction</h2>

<p><strong>Audio preprocessing</strong> transforms raw audio into clean, standardized representations suitable for ML models.</p>

<p><strong>Why it matters:</strong></p>
<ul>
  <li><strong>Garbage in, garbage out:</strong> Poor audio quality destroys model performance</li>
  <li><strong>Real-world audio is messy:</strong> Background noise, varying volumes, different devices</li>
  <li><strong>Standardization:</strong> Models expect consistent input formats</li>
  <li><strong>Data augmentation:</strong> Increase training data diversity</li>
</ul>

<p><strong>Pipeline overview:</strong></p>

<p><code class="language-plaintext highlighter-rouge">
Raw Audio (microphone)
 ↓
[Loading &amp; Format Conversion]
 ↓
[Resampling]
 ↓
[Normalization]
 ↓
[Noise Reduction]
 ↓
[Voice Activity Detection]
 ↓
[Segmentation]
 ↓
[Feature Extraction]
 ↓
Clean Features → Model
</code></p>

<hr />

<h2 id="audio-fundamentals">Audio Fundamentals</h2>

<h3 id="digital-audio-representation">Digital Audio Representation</h3>

<p>``
Analog Sound Wave:
 ∿∿∿∿∿∿∿∿∿∿∿</p>

<p>Sampling (digitization):
 ●─●─●─●─●─●─●─● (sample points)</p>

<p>Key parameters:</p>
<ul>
  <li>Sample Rate: Samples per second (Hz)</li>
  <li>CD quality: 44,100 Hz</li>
  <li>Speech: 16,000 Hz or 22,050 Hz</li>
  <li>
    <p>Telephone: 8,000 Hz</p>
  </li>
  <li>Bit Depth: Bits per sample</li>
  <li>16-bit: 65,536 possible values</li>
  <li>24-bit: 16,777,216 values</li>
  <li>
    <p>32-bit float: Highest precision</p>
  </li>
  <li>Channels:</li>
  <li>Mono: 1 channel</li>
  <li>Stereo: 2 channels (left, right)
``</li>
</ul>

<h3 id="nyquist-shannon-sampling-theorem">Nyquist-Shannon Sampling Theorem</h3>

<p><strong>Rule:</strong> To capture frequency <code class="language-plaintext highlighter-rouge">f</code>, sample rate must be ≥ 2f</p>

<p>``
Human hearing: 20 Hz - 20 kHz
→ Need ≥40 kHz sample rate
→ CD uses 44.1 kHz (margin above 40 kHz)</p>

<p>Speech frequencies: ~300 Hz - 8 kHz
→ 16 kHz sample rate is sufficient
``</p>

<hr />

<h2 id="loading--format-conversion">Loading &amp; Format Conversion</h2>

<h3 id="loading-audio">Loading Audio</h3>

<p>``python
import librosa
import soundfile as sf
import numpy as np</p>

<p>def load_audio(file_path, sr=None):
 “””
 Load audio file</p>

<p>Args:
 file_path: Path to audio file
 sr: Target sample rate (None = keep original)</p>

<p>Returns:
 audio: np.array of samples
 sr: Sample rate
 “””
 # Librosa (resamples automatically)
 audio, sample_rate = librosa.load(file_path, sr=sr)</p>

<p>return audio, sample_rate</p>

<h1 id="example">Example</h1>
<p>audio, sr = load_audio(‘speech.wav’, sr=16000)
print(f”Shape: {audio.shape}, Sample Rate: {sr} Hz”)
print(f”Duration: {len(audio) / sr:.2f} seconds”)
``</p>

<h3 id="format-conversion">Format Conversion</h3>

<p>``python
from pydub import AudioSegment</p>

<p>def convert_audio_format(input_path, output_path, output_format=’wav’):
 “””
 Convert between audio formats</p>

<p>Supports: mp3, wav, ogg, flac, m4a, etc.
 “””
 audio = AudioSegment.from_file(input_path)</p>

<p># Export in new format
 audio.export(output_path, format=output_format)</p>

<h1 id="convert-mp3-to-wav">Convert MP3 to WAV</h1>
<p>convert_audio_format(‘input.mp3’, ‘output.wav’, ‘wav’)
``</p>

<h3 id="monostereo-conversion">Mono/Stereo Conversion</h3>

<p>``python
def stereo_to_mono(audio_stereo):
 “””
 Convert stereo to mono by averaging channels</p>

<p>Args:
 audio_stereo: Shape (2, n_samples) or (n_samples, 2)</p>

<p>Returns:
 audio_mono: Shape (n_samples,)
 “””
 if audio_stereo.ndim == 1:
 # Already mono
 return audio_stereo</p>

<p># Average across channels
 if audio_stereo.shape[0] == 2:
 # Shape: (2, n_samples)
 return np.mean(audio_stereo, axis=0)
 else:
 # Shape: (n_samples, 2)
 return np.mean(audio_stereo, axis=1)</p>

<h1 id="example-1">Example</h1>
<p>audio_stereo, sr = librosa.load(‘stereo.wav’, sr=None, mono=False)
audio_mono = stereo_to_mono(audio_stereo)
``</p>

<hr />

<h2 id="resampling">Resampling</h2>

<p><strong>Purpose:</strong> Convert sample rate to match model requirements</p>

<h3 id="high-quality-resampling">High-Quality Resampling</h3>

<p>``python
import librosa</p>

<p>def resample_audio(audio, orig_sr, target_sr):
 “””
 Resample audio using high-quality algorithm</p>

<p>Args:
 audio: Audio samples
 orig_sr: Original sample rate
 target_sr: Target sample rate</p>

<p>Returns:
 resampled_audio
 “””
 if orig_sr == target_sr:
 return audio</p>

<p># Librosa uses high-quality resampling (Kaiser window)
 resampled = librosa.resample(
 audio,
 orig_sr=orig_sr,
 target_sr=target_sr,
 res_type=’kaiser_best’ # Highest quality
 )</p>

<p>return resampled</p>

<h1 id="example-downsample-441-khz-to-16-khz">Example: Downsample 44.1 kHz to 16 kHz</h1>
<p>audio_44k, _ = librosa.load(‘audio.wav’, sr=44100)
audio_16k = resample_audio(audio_44k, orig_sr=44100, target_sr=16000)</p>

<p>print(f”Original length: {len(audio_44k)}”)
print(f”Resampled length: {len(audio_16k)}”)
print(f”Ratio: {len(audio_44k) / len(audio_16k):.2f}”) # ~2.76
``</p>

<p><strong>Resampling visualization:</strong></p>

<p>``
Original (44.1 kHz):
●●●●●●●●●●●●●●●●●●●●●●●●●●●● (44,100 samples/second)</p>

<p>Downsampled (16 kHz):
●───●───●───●───●───●───●───● (16,000 samples/second)</p>

<p>Algorithm interpolates to avoid aliasing
``</p>

<hr />

<h2 id="normalization">Normalization</h2>

<h3 id="amplitude-normalization">Amplitude Normalization</h3>

<p>``python
def normalize_audio(audio, target_level=-20.0):
 “””
 Normalize audio to target level (dB)</p>

<p>Args:
 audio: Audio samples
 target_level: Target RMS level in dB</p>

<p>Returns:
 normalized_audio
 “””
 # Calculate current RMS
 rms = np.sqrt(np.mean(audio ** 2))</p>

<p>if rms == 0:
 return audio</p>

<p># Convert target level from dB to linear
 target_rms = 10 ** (target_level / 20.0)</p>

<p># Scale audio
 scaling_factor = target_rms / rms
 normalized = audio * scaling_factor</p>

<p># Clip to prevent overflow
 normalized = np.clip(normalized, -1.0, 1.0)</p>

<p>return normalized</p>

<h1 id="example-2">Example</h1>
<p>audio, sr = librosa.load(‘speech.wav’, sr=16000)
normalized_audio = normalize_audio(audio, target_level=-20.0)
``</p>

<h3 id="peak-normalization">Peak Normalization</h3>

<p>``python
def peak_normalize(audio):
 “””
 Normalize to peak amplitude = 1.0</p>

<p>Simple but can be problematic if audio has spikes
 “””
 peak = np.max(np.abs(audio))</p>

<p>if peak == 0:
 return audio</p>

<p>return audio / peak
``</p>

<h3 id="dc-offset-removal">DC Offset Removal</h3>

<p>``python
def remove_dc_offset(audio):
 “””
 Remove DC bias (mean offset)</p>

<p>DC offset can cause clicking sounds
 “””
 return audio - np.mean(audio)</p>

<h1 id="example-3">Example</h1>
<p>audio_clean = remove_dc_offset(audio)
``</p>

<hr />

<h2 id="noise-reduction">Noise Reduction</h2>

<h3 id="1-spectral-subtraction">1. Spectral Subtraction</h3>

<p>``python
import scipy.signal as signal</p>

<p>def spectral_subtraction(audio, sr, noise_duration=0.5):
 “””
 Reduce noise using spectral subtraction</p>

<p>Assumes first noise_duration seconds are noise only</p>

<p>Args:
 audio: Audio signal
 sr: Sample rate
 noise_duration: Duration of noise-only segment (seconds)</p>

<p>Returns:
 denoised_audio
 “””
 # Extract noise profile from beginning
 noise_samples = int(noise_duration * sr)
 noise_segment = audio[:noise_samples]</p>

<p># Compute noise spectrum
 noise_fft = np.fft.rfft(noise_segment)
 noise_power = np.abs(noise_fft) ** 2
 noise_power_avg = np.mean(noise_power)</p>

<p># STFT of full audio
 f, t, Zxx = signal.stft(audio, fs=sr, nperseg=1024)</p>

<p># Subtract noise spectrum
 magnitude = np.abs(Zxx)
 phase = np.angle(Zxx)</p>

<p># Spectral subtraction
 noise_estimate = np.sqrt(noise_power_avg)
 magnitude_denoised = np.maximum(magnitude - noise_estimate, 0.0)</p>

<p># Reconstruct
 Zxx_denoised = magnitude_denoised * np.exp(1j * phase)
 _, audio_denoised = signal.istft(Zxx_denoised, fs=sr)</p>

<p>return audio_denoised[:len(audio)]</p>

<h1 id="example-4">Example</h1>
<p>audio, sr = librosa.load(‘noisy_speech.wav’, sr=16000)
denoised = spectral_subtraction(audio, sr, noise_duration=0.5)
``</p>

<h3 id="2-wiener-filtering">2. Wiener Filtering</h3>

<p>``python
def wiener_filter(audio, sr, noise_reduction_factor=0.5):
 “””
 Apply Wiener filter for noise reduction</p>

<p>More sophisticated than spectral subtraction
 “””
 from scipy.signal import wiener</p>

<p># Apply Wiener filter
 filtered = wiener(audio, mysize=5, noise=noise_reduction_factor)</p>

<p>return filtered
``</p>

<h3 id="3-high-pass-filter-remove-low-frequency-noise">3. High-Pass Filter (Remove Low-Frequency Noise)</h3>

<p>``python
def high_pass_filter(audio, sr, cutoff_freq=80):
 “””
 Remove low-frequency noise (e.g., rumble, hum)</p>

<p>Args:
 audio: Audio signal
 sr: Sample rate
 cutoff_freq: Cutoff frequency in Hz</p>

<p>Returns:
 filtered_audio
 “””
 from scipy.signal import butter, filtfilt</p>

<p># Design high-pass filter
 nyquist = sr / 2
 normalized_cutoff = cutoff_freq / nyquist
 b, a = butter(N=5, Wn=normalized_cutoff, btype=’high’)</p>

<p># Apply filter (zero-phase filtering)
 filtered = filtfilt(b, a, audio)</p>

<p>return filtered</p>

<h1 id="example-remove-rumble-below-80-hz">Example: Remove rumble below 80 Hz</h1>
<p>audio_filtered = high_pass_filter(audio, sr=16000, cutoff_freq=80)
``</p>

<hr />

<h2 id="voice-activity-detection-vad">Voice Activity Detection (VAD)</h2>

<p><strong>Purpose:</strong> Identify speech segments, remove silence</p>

<p>``python
import librosa</p>

<p>def voice_activity_detection(audio, sr, frame_length=2048, hop_length=512, energy_threshold=0.02):
 “””
 Simple energy-based VAD</p>

<p>Args:
 audio: Audio signal
 sr: Sample rate
 energy_threshold: Threshold for voice activity</p>

<p>Returns:
 speech_segments: List of (start_sample, end_sample) tuples
 “””
 # Compute frame energy
 energy = librosa.feature.rms(
 y=audio,
 frame_length=frame_length,
 hop_length=hop_length
 )[0]</p>

<p># Normalize energy
 energy_normalized = energy / (np.max(energy) + 1e-8)</p>

<p># Threshold to get voice activity
 voice_activity = energy_normalized &gt; energy_threshold</p>

<p># Convert to sample indices
 def frame_to_sample(frame_idx):
 start = frame_idx * hop_length
 end = min(start + frame_length, len(audio))
 return start, end</p>

<p># Find continuous speech segments
 segments = []
 in_speech = False
 start_frame = 0</p>

<p>for i, is_voice in enumerate(voice_activity):
 if is_voice and not in_speech:
 # Start of speech
 start_frame = i
 in_speech = True
 elif not is_voice and in_speech:
 # End of speech
 end_frame = i
 start_sample, _ = frame_to_sample(start_frame)
 end_sample, _ = frame_to_sample(end_frame)
 segments.append((start_sample, end_sample))
 in_speech = False</p>

<p># Handle case where speech goes to end
 if in_speech:
 start_sample, _ = frame_to_sample(start_frame)
 end_sample = len(audio)
 segments.append((start_sample, end_sample))</p>

<p>return segments</p>

<h1 id="example-5">Example</h1>
<p>audio, sr = librosa.load(‘speech_with_pauses.wav’, sr=16000)
segments = voice_activity_detection(audio, sr)</p>

<p>print(f”Found {len(segments)} speech segments:”)
for i, (start, end) in enumerate(segments):
 duration = (end - start) / sr
 print(f” Segment {i+1}: {start/sr:.2f}s - {end/sr:.2f}s ({duration:.2f}s)”)
``</p>

<p><strong>VAD visualization:</strong></p>

<p>``
Audio waveform:
 <strong>_ ___ ___
 / \ / \ / <br />
__<em>/ _</em></strong><strong>/ _</strong>/ ___</p>

<p>Energy:
 ████ ████ ████
 ████ ████ ████
────████──────────████──────████──── ← threshold</p>

<p>VAD output:
 SSSS SSSS SSSS
 (S = Speech, spaces = Silence)
``</p>

<hr />

<h2 id="segmentation">Segmentation</h2>

<h3 id="fixed-length-segmentation">Fixed-Length Segmentation</h3>

<p>``python
def segment_audio_fixed_length(audio, sr, segment_duration=3.0, hop_duration=1.0):
 “””
 Segment audio into fixed-length chunks with overlap</p>

<p>Args:
 audio: Audio signal
 sr: Sample rate
 segment_duration: Segment length in seconds
 hop_duration: Hop between segments in seconds</p>

<p>Returns:
 segments: List of audio segments
 “””
 segment_samples = int(segment_duration * sr)
 hop_samples = int(hop_duration * sr)</p>

<p>segments = []
 start = 0</p>

<p>while start + segment_samples &lt;= len(audio):
 segment = audio[start:start + segment_samples]
 segments.append(segment)
 start += hop_samples</p>

<p>return segments</p>

<h1 id="example-3-second-segments-with-1-second-hop-2-second-overlap">Example: 3-second segments with 1-second hop (2-second overlap)</h1>
<p>segments = segment_audio_fixed_length(audio, sr=16000, segment_duration=3.0, hop_duration=1.0)
print(f”Created {len(segments)} segments”)
``</p>

<h3 id="adaptive-segmentation-based-on-pauses">Adaptive Segmentation (Based on Pauses)</h3>

<p>``python
def segment_by_pauses(audio, sr, min_silence_duration=0.3, silence_threshold=0.02):
 “””
 Segment audio at silence/pause points</p>

<p>Better than fixed-length for natural speech
 “””
 # Detect voice activity
 speech_segments = voice_activity_detection(
 audio, sr,
 energy_threshold=silence_threshold
 )</p>

<p># Filter out very short segments
 min_segment_samples = int(min_silence_duration * sr)
 filtered_segments = [
 (start, end) for start, end in speech_segments
 if end - start &gt;= min_segment_samples
 ]</p>

<p># Extract audio segments
 audio_segments = []
 for start, end in filtered_segments:
 segment = audio[start:end]
 audio_segments.append(segment)</p>

<p>return audio_segments, filtered_segments</p>

<h1 id="example-6">Example</h1>
<p>audio_segments, timestamps = segment_by_pauses(audio, sr=16000)
``</p>

<hr />

<h2 id="data-augmentation">Data Augmentation</h2>

<p><strong>Purpose:</strong> Increase training data diversity, improve model robustness</p>

<h3 id="1-time-stretching">1. Time Stretching</h3>

<p>``python
def time_stretch(audio, rate=1.0):
 “””
 Speed up or slow down audio without changing pitch</p>

<p>Args:
 audio: Audio signal
 rate: Stretch factor</p>
<blockquote>
  <p>1.0: speed up
 &lt; 1.0: slow down</p>
</blockquote>

<p>Returns:
 stretched_audio
 “””
 return librosa.effects.time_stretch(audio, rate=rate)</p>

<h1 id="example-speed-up-by-20">Example: Speed up by 20%</h1>
<p>audio_fast = time_stretch(audio, rate=1.2)</p>

<h1 id="slow-down-by-20">Slow down by 20%</h1>
<p>audio_slow = time_stretch(audio, rate=0.8)
``</p>

<h3 id="2-pitch-shifting">2. Pitch Shifting</h3>

<p>``python
def pitch_shift(audio, sr, n_steps=2):
 “””
 Shift pitch without changing speed</p>

<p>Args:
 audio: Audio signal
 sr: Sample rate
 n_steps: Semitones to shift (positive = higher, negative = lower)</p>

<p>Returns:
 pitch_shifted_audio
 “””
 return librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)</p>

<h1 id="example-shift-up-2-semitones">Example: Shift up 2 semitones</h1>
<p>audio_high = pitch_shift(audio, sr=16000, n_steps=2)</p>

<h1 id="shift-down-2-semitones">Shift down 2 semitones</h1>
<p>audio_low = pitch_shift(audio, sr=16000, n_steps=-2)
``</p>

<h3 id="3-adding-noise">3. Adding Noise</h3>

<p>``python
def add_noise(audio, noise_factor=0.005):
 “””
 Add random Gaussian noise</p>

<p>Args:
 audio: Audio signal
 noise_factor: Standard deviation of noise</p>

<p>Returns:
 noisy_audio
 “””
 noise = np.random.randn(len(audio)) * noise_factor
 return audio + noise</p>

<h1 id="example-7">Example</h1>
<p>audio_noisy = add_noise(audio, noise_factor=0.01)
``</p>

<h3 id="4-background-noise-mixing">4. Background Noise Mixing</h3>

<p>``python
def mix_background_noise(speech_audio, noise_audio, snr_db=10):
 “””
 Mix speech with background noise at specified SNR</p>

<p>Args:
 speech_audio: Clean speech
 noise_audio: Background noise
 snr_db: Signal-to-noise ratio in dB</p>

<p>Returns:
 mixed_audio
 “””
 # Match lengths
 if len(noise_audio) &lt; len(speech_audio):
 # Repeat noise to match speech length
 repeats = int(np.ceil(len(speech_audio) / len(noise_audio)))
 noise_audio = np.tile(noise_audio, repeats)[:len(speech_audio)]
 else:
 # Trim noise
 noise_audio = noise_audio[:len(speech_audio)]</p>

<p># Calculate signal and noise power
 speech_power = np.mean(speech_audio ** 2)
 noise_power = np.mean(noise_audio ** 2)</p>

<p># Calculate scaling factor for noise
 snr_linear = 10 ** (snr_db / 10)
 noise_scaling = np.sqrt(speech_power / (snr_linear * noise_power))</p>

<p># Mix
 mixed = speech_audio + noise_scaling * noise_audio</p>

<p># Normalize to prevent clipping
 mixed = mixed / (np.max(np.abs(mixed)) + 1e-8)</p>

<p>return mixed</p>

<h1 id="example-mix-with-café-noise-at-snr15db">Example: Mix with café noise at SNR=15dB</h1>
<p>cafe_noise, _ = librosa.load(‘cafe_background.wav’, sr=16000)
noisy_speech = mix_background_noise(audio, cafe_noise, snr_db=15)
``</p>

<h3 id="5-specaugment-for-spectrograms">5. SpecAugment (For Spectrograms)</h3>

<p>``python
def spec_augment(mel_spectrogram, num_mask=2, freq_mask_param=20, time_mask_param=30):
 “””
 SpecAugment: mask random time-frequency patches</p>

<p>Popular augmentation for speech recognition</p>

<p>Args:
 mel_spectrogram: Shape (n_mels, time)
 num_mask: Number of masks to apply
 freq_mask_param: Max width of frequency mask
 time_mask_param: Max width of time mask</p>

<p>Returns:
 augmented_spectrogram
 “””
 aug_spec = mel_spectrogram.copy()
 n_mels, n_frames = aug_spec.shape</p>

<p># Frequency masking
 for _ in range(num_mask):
 f = np.random.randint(0, freq_mask_param)
 f0 = np.random.randint(0, n_mels - f)
 aug_spec[f0:f0+f, :] = 0</p>

<p># Time masking
 for _ in range(num_mask):
 t = np.random.randint(0, time_mask_param)
 t0 = np.random.randint(0, n_frames - t)
 aug_spec[:, t0:t0+t] = 0</p>

<p>return aug_spec</p>

<h1 id="example-8">Example</h1>
<p>mel_spec = librosa.feature.melspectrogram(y=audio, sr=16000)
aug_mel_spec = spec_augment(mel_spec, num_mask=2)
``</p>

<hr />

<h2 id="connection-to-feature-engineering">Connection to Feature Engineering</h2>

<p>Audio preprocessing is feature engineering for speech:</p>

<p>``python
class AudioFeatureEngineeringPipeline:
 “””
 Complete pipeline: raw audio → features</p>

<p>Similar to general ML feature engineering
 “””</p>

<p>def <strong>init</strong>(self, sr=16000):
 self.sr = sr</p>

<p>def process(self, audio_path):
 “””
 Full preprocessing pipeline</p>

<p>Analogous to feature engineering pipeline in ML
 “””
 # 1. Load (like data loading)
 audio, sr = librosa.load(audio_path, sr=self.sr)</p>

<p># 2. Normalize (like feature scaling)
 audio = normalize_audio(audio)</p>

<p># 3. Noise reduction (like outlier removal)
 audio = high_pass_filter(audio, sr)</p>

<p># 4. VAD (like removing null values)
 segments = voice_activity_detection(audio, sr)</p>

<p># 5. Feature extraction (like creating derived features)
 features = self.extract_features(audio, sr)</p>

<p>return features</p>

<p>def extract_features(self, audio, sr):
 “””
 Extract multiple feature types</p>

<p>Like creating feature crosses and aggregations
 “””
 features = {}</p>

<p># Spectral features (numerical features)
 features[‘mfcc’] = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
 features[‘spectral_centroid’] = librosa.feature.spectral_centroid(y=audio, sr=sr)
 features[‘zero_crossing_rate’] = librosa.feature.zero_crossing_rate(audio)</p>

<p># Temporal features (time-based features)
 features[‘rms_energy’] = librosa.feature.rms(y=audio)</p>

<p># Aggregations (like SQL GROUP BY)
 features[‘mfcc_mean’] = np.mean(features[‘mfcc’], axis=1)
 features[‘mfcc_std’] = np.std(features[‘mfcc’], axis=1)</p>

<p>return features
``</p>

<hr />

<h2 id="production-pipeline">Production Pipeline</h2>

<p>``python
class ProductionAudioPreprocessor:
 “””
 Production-ready audio preprocessing</p>

<p>Handles errors, logging, monitoring
 “””</p>

<p>def <strong>init</strong>(self, config):
 self.sr = config.get(‘sample_rate’, 16000)
 self.normalize_level = config.get(‘normalize_level’, -20.0)
 self.enable_vad = config.get(‘enable_vad’, True)</p>

<p>def preprocess(self, audio_bytes):
 “””
 Preprocess audio from bytes</p>

<p>Returns: (processed_audio, metadata, success)
 “””
 metadata = {}</p>

<p>try:
 # Load from bytes
 audio = self._load_from_bytes(audio_bytes)
 metadata[‘original_length’] = len(audio)</p>

<p># Resample
 if self.sr != 16000: # Assuming input is 16kHz
 audio = resample_audio(audio, 16000, self.sr)</p>

<p># Normalize
 audio = normalize_audio(audio, self.normalize_level)
 metadata[‘normalized’] = True</p>

<p># VAD
 if self.enable_vad:
 segments = voice_activity_detection(audio, self.sr)
 if segments:
 # Keep only speech
 speech_audio = np.concatenate([
 audio[start:end] for start, end in segments
 ])
 audio = speech_audio
 metadata[‘vad_segments’] = len(segments)</p>

<p>metadata[‘final_length’] = len(audio)
 metadata[‘duration_seconds’] = len(audio) / self.sr</p>

<p>return audio, metadata, True</p>

<p>except Exception as e:
 return None, {‘error’: str(e)}, False</p>

<p>def _load_from_bytes(self, audio_bytes):
 “"”Load audio from bytes”””
 import io
 audio, _ = librosa.load(io.BytesIO(audio_bytes), sr=self.sr)
 return audio
``</p>

<hr />

<h2 id="real-world-challenges--solutions">Real-World Challenges &amp; Solutions</h2>

<h3 id="challenge-1-codec-artifacts">Challenge 1: Codec Artifacts</h3>

<p><strong>Problem:</strong> Different audio codecs introduce artifacts</p>

<p>``python
def detect_codec_artifacts(audio, sr):
 “””
 Detect codec artifacts (e.g., from MP3 compression)</p>

<p>Returns: artifact_score (higher = more artifacts)
 “””
 import scipy.signal as signal</p>

<p># Compute spectrogram
 f, t, Sxx = signal.spectrogram(audio, fs=sr)</p>

<p># MP3 artifacts often appear as:
 # 1. High-frequency cutoff (lossy codecs)
 cutoff_freq = 16000 # Hz
 high_freq_mask = f &gt; cutoff_freq
 high_freq_energy = np.mean(Sxx[high_freq_mask, :])</p>

<p># 2. Pre-echo artifacts
 # Sudden changes in energy
 energy = np.sum(Sxx, axis=0)
 energy_diff = np.diff(energy)
 pre_echo_score = np.std(energy_diff)</p>

<p>artifact_score = {
 ‘high_freq_loss’: high_freq_energy,
 ‘pre_echo’: pre_echo_score,
 ‘overall’: 1.0 - high_freq_energy + pre_echo_score
 }</p>

<p>return artifact_score</p>

<h1 id="example-9">Example</h1>
<p>audio_mp3, sr = librosa.load(‘compressed.mp3’, sr=16000)
audio_wav, sr = librosa.load(‘lossless.wav’, sr=16000)</p>

<p>artifacts_mp3 = detect_codec_artifacts(audio_mp3, sr)
artifacts_wav = detect_codec_artifacts(audio_wav, sr)</p>

<p>print(f”MP3 artifacts: {artifacts_mp3[‘overall’]:.3f}”)
print(f”WAV artifacts: {artifacts_wav[‘overall’]:.3f}”)
``</p>

<h3 id="challenge-2-variable-sample-rates">Challenge 2: Variable Sample Rates</h3>

<p>``python
class AdaptiveResampler:
 “””
 Handle audio from various sources with different sample rates</p>

<p>Production systems receive audio from:</p>
<ul>
  <li>Phone calls: 8 kHz</li>
  <li>Bluetooth: 16 kHz</li>
  <li>Studio mics: 44.1 kHz / 48 kHz
 “””</li>
</ul>

<p>def <strong>init</strong>(self, target_sr=16000):
 self.target_sr = target_sr
 self.cache = {} # Cache resampling filters</p>

<p>def resample(self, audio, orig_sr):
 “””
 Efficiently resample with caching
 “””
 if orig_sr == self.target_sr:
 return audio</p>

<p># Check cache
 cache_key = (orig_sr, self.target_sr)
 if cache_key not in self.cache:
 # Compute resampling filter once
 self.cache[cache_key] = self._compute_filter(orig_sr, self.target_sr)</p>

<p># Apply cached filter
 return librosa.resample(
 audio,
 orig_sr=orig_sr,
 target_sr=self.target_sr,
 res_type=’kaiser_fast’ # Good balance of quality/speed
 )</p>

<p>def _compute_filter(self, orig_sr, target_sr):
 “"”Compute and cache resampling filter”””
 # In real implementation, would compute filter coefficients
 return None</p>

<h1 id="usage">Usage</h1>
<p>resampler = AdaptiveResampler(target_sr=16000)</p>

<h1 id="handle-various-sources">Handle various sources</h1>
<p>phone_audio = resampler.resample(phone_audio, orig_sr=8000)
bluetooth_audio = resampler.resample(bluetooth_audio, orig_sr=16000)
studio_audio = resampler.resample(studio_audio, orig_sr=48000)
``</p>

<h3 id="challenge-3-clipping--distortion">Challenge 3: Clipping &amp; Distortion</h3>

<p>``python
def detect_and_fix_clipping(audio, threshold=0.99):
 “””
 Detect clipped samples and attempt interpolation</p>

<p>Args:
 audio: Audio signal
 threshold: Clipping threshold (absolute value)</p>

<p>Returns:
 fixed_audio, was_clipped
 “””
 # Detect clipping
 clipped_mask = np.abs(audio) &gt;= threshold
 num_clipped = np.sum(clipped_mask)</p>

<p>if num_clipped == 0:
 return audio, False</p>

<p>print(f”⚠️ Detected {num_clipped} clipped samples ({100*num_clipped/len(audio):.2f}%)”)</p>

<p># Simple interpolation for clipped regions
 fixed_audio = audio.copy()</p>

<p># Find clipped regions
 clipped_indices = np.where(clipped_mask)[0]</p>

<p>for idx in clipped_indices:
 # Skip edges
 if idx == 0 or idx == len(audio) - 1:
 continue</p>

<p># Interpolate from neighbors
 if not clipped_mask[idx-1] and not clipped_mask[idx+1]:
 fixed_audio[idx] = (audio[idx-1] + audio[idx+1]) / 2</p>

<p>return fixed_audio, True</p>

<h1 id="example-10">Example</h1>
<p>audio_with_clipping, sr = librosa.load(‘clipped_audio.wav’, sr=16000)
fixed_audio, was_clipped = detect_and_fix_clipping(audio_with_clipping)</p>

<p>if was_clipped:
 print(“Applied clipping repair”)
``</p>

<h3 id="challenge-4-background-babble-noise">Challenge 4: Background Babble Noise</h3>

<p>``python
def reduce_babble_noise(audio, sr, noise_profile_duration=1.0):
 “””
 Reduce background babble (multiple speakers)</p>

<p>More challenging than stationary noise
 “””
 import noisereduce as nr</p>

<p># Estimate noise profile from segments with lowest energy
 frame_length = int(0.1 * sr) # 100ms frames
 hop_length = frame_length // 2</p>

<p># Compute frame energy
 energy = librosa.feature.rms(
 y=audio,
 frame_length=frame_length,
 hop_length=hop_length
 )[0]</p>

<p># Select low-energy frames as noise
 noise_threshold = np.percentile(energy, 20)
 noise_frames = np.where(energy &lt; noise_threshold)[0]</p>

<p># Extract noise samples
 noise_samples = []
 for frame_idx in noise_frames:
 start = frame_idx * hop_length
 end = start + frame_length
 if end &lt;= len(audio):
 noise_samples.extend(audio[start:end])</p>

<p>noise_profile = np.array(noise_samples)</p>

<p># Apply noise reduction
 if len(noise_profile) &gt; sr * noise_profile_duration:
 reduced_noise = nr.reduce_noise(
 y=audio,
 y_noise=noise_profile[:int(sr * noise_profile_duration)],
 sr=sr,
 stationary=False, # Non-stationary noise
 prop_decrease=0.8
 )
 return reduced_noise
 else:
 print(“⚠️ Insufficient noise profile, returning original”)
 return audio</p>

<h1 id="example-11">Example</h1>
<p>audio_with_babble, sr = librosa.load(‘meeting_audio.wav’, sr=16000)
clean_audio = reduce_babble_noise(audio_with_babble, sr)
``</p>

<hr />

<h2 id="audio-quality-metrics">Audio Quality Metrics</h2>

<h3 id="signal-to-noise-ratio-snr">Signal-to-Noise Ratio (SNR)</h3>

<p>``python
def calculate_snr(clean_signal, noisy_signal):
 “””
 Calculate SNR in dB</p>

<p>Args:
 clean_signal: Ground truth clean signal
 noisy_signal: Signal with noise</p>

<p>Returns:
 SNR in dB
 “””
 # Ensure same length
 min_len = min(len(clean_signal), len(noisy_signal))
 clean = clean_signal[:min_len]
 noisy = noisy_signal[:min_len]</p>

<p># Compute noise
 noise = noisy - clean</p>

<p># Power
 signal_power = np.mean(clean ** 2)
 noise_power = np.mean(noise ** 2)</p>

<p># SNR in dB
 if noise_power == 0:
 return float(‘inf’)</p>

<p>snr = 10 * np.log10(signal_power / noise_power)</p>

<p>return snr</p>

<h1 id="example-12">Example</h1>
<p>clean, sr = librosa.load(‘clean_speech.wav’, sr=16000)
noisy, sr = librosa.load(‘noisy_speech.wav’, sr=16000)</p>

<p>snr = calculate_snr(clean, noisy)
print(f”SNR: {snr:.2f} dB”)</p>

<h1 id="typical-snrs">Typical SNRs:</h1>
<h1 id="-40-db-excellent">&gt; 40 dB: Excellent</h1>
<h1 id="25-40-db-good">25-40 dB: Good</h1>
<h1 id="10-25-db-fair">10-25 dB: Fair</h1>
<h1 id="-10-db-poor">&lt; 10 dB: Poor</h1>
<p>``</p>

<h3 id="perceptual-evaluation-of-speech-quality-pesq">Perceptual Evaluation of Speech Quality (PESQ)</h3>

<p>``python</p>
<h1 id="pesq-is-a-standard-metric-for-speech-quality">PESQ is a standard metric for speech quality</h1>
<h1 id="requires-pesq-library-pip-install-pesq">Requires pesq library: pip install pesq</h1>

<p>from pesq import pesq</p>

<p>def evaluate_speech_quality(reference_audio, degraded_audio, sr=16000):
 “””
 Evaluate speech quality using PESQ</p>

<p>Args:
 reference_audio: Clean reference
 degraded_audio: Processed/degraded audio
 sr: Sample rate (8000 or 16000)</p>

<p>Returns:
 PESQ score (1.0 to 4.5, higher is better)
 “””
 # PESQ requires 8kHz or 16kHz
 if sr not in [8000, 16000]:
 raise ValueError(“PESQ requires sr=8000 or sr=16000”)</p>

<p># Ensure same length
 min_len = min(len(reference_audio), degraded_audio)
 ref = reference_audio[:min_len]
 deg = degraded_audio[:min_len]</p>

<p># Compute PESQ
 if sr == 8000:
 mode = ‘nb’ # Narrowband
 else:
 mode = ‘wb’ # Wideband</p>

<p>score = pesq(sr, ref, deg, mode)</p>

<p>return score</p>

<h1 id="example-13">Example</h1>
<p>reference, sr = librosa.load(‘clean.wav’, sr=16000)
processed, sr = librosa.load(‘processed.wav’, sr=16000)</p>

<p>quality_score = evaluate_speech_quality(reference, processed, sr)
print(f”PESQ Score: {quality_score:.2f}”)</p>

<h1 id="pesq-interpretation">PESQ interpretation:</h1>
<h1 id="40-excellent">4.0+: Excellent</h1>
<h1 id="30-40-good">3.0-4.0: Good</h1>
<h1 id="20-30-fair">2.0-3.0: Fair</h1>
<h1 id="-20-poor">&lt; 2.0: Poor</h1>
<p>``</p>

<hr />

<h2 id="advanced-augmentation-strategies">Advanced Augmentation Strategies</h2>

<h3 id="room-impulse-response-rir-convolution">Room Impulse Response (RIR) Convolution</h3>

<p>``python
def apply_room_impulse_response(speech, rir):
 “””
 Simulate room acoustics using RIR</p>

<p>Makes model robust to reverberation</p>

<p>Args:
 speech: Clean speech signal
 rir: Room impulse response</p>

<p>Returns:
 Reverberant speech
 “””
 from scipy.signal import fftconvolve</p>

<p># Convolve speech with RIR
 reverb_speech = fftconvolve(speech, rir, mode=’same’)</p>

<p># Normalize
 reverb_speech = reverb_speech / (np.max(np.abs(reverb_speech)) + 1e-8)</p>

<p>return reverb_speech</p>

<h1 id="example-generate-synthetic-rir">Example: Generate synthetic RIR</h1>
<p>def generate_synthetic_rir(sr=16000, room_size=’medium’, rt60=0.5):
 “””
 Generate synthetic room impulse response</p>

<p>Args:
 sr: Sample rate
 room_size: ‘small’, ‘medium’, ‘large’
 rt60: Reverberation time (seconds)</p>

<p>Returns:
 RIR signal
 “””
 # Duration based on RT60
 duration = int(rt60 * sr)</p>

<p># Exponential decay
 t = np.arange(duration) / sr
 decay = np.exp(-6.91 * t / rt60) # -60 dB decay</p>

<p># Add random reflections
 rir = decay * np.random.randn(duration)</p>

<p># Initial spike (direct path)
 rir[0] = 1.0</p>

<p># Normalize
 rir = rir / np.max(np.abs(rir))</p>

<p>return rir</p>

<h1 id="usage-1">Usage</h1>
<p>clean_speech, sr = librosa.load(‘speech.wav’, sr=16000)</p>

<h1 id="simulate-different-rooms">Simulate different rooms</h1>
<p>small_room_rir = generate_synthetic_rir(sr, ‘small’, rt60=0.3)
large_room_rir = generate_synthetic_rir(sr, ‘large’, rt60=1.2)</p>

<p>speech_small_room = apply_room_impulse_response(clean_speech, small_room_rir)
speech_large_room = apply_room_impulse_response(clean_speech, large_room_rir)
``</p>

<h3 id="codec-simulation">Codec Simulation</h3>

<p>``python
def simulate_codec(audio, sr, codec=’mp3’, bitrate=32):
 “””
 Simulate lossy codec compression</p>

<p>Makes model robust to codec artifacts</p>

<p>Args:
 audio: Clean audio
 sr: Sample rate
 codec: ‘mp3’, ‘aac’, ‘opus’
 bitrate: Bitrate in kbps</p>

<p>Returns:
 Codec-compressed audio
 “””
 import subprocess
 import tempfile
 import os
 import soundfile as sf</p>

<p># Save to temp file
 with tempfile.NamedTemporaryFile(suffix=’.wav’, delete=False) as tmp_in:
 sf.write(tmp_in.name, audio, sr)
 input_path = tmp_in.name</p>

<p>with tempfile.NamedTemporaryFile(suffix=’.wav’, delete=False) as tmp_out:
 output_path = tmp_out.name</p>

<p>try:
 # Compress with ffmpeg
 if codec == ‘mp3’:
 subprocess.run([
 ‘ffmpeg’, ‘-i’, input_path,
 ‘-codec:a’, ‘libmp3lame’,
 ‘-b:a’, f’{bitrate}k’,
 ‘-y’, output_path
 ], capture_output=True, check=True)
 elif codec == ‘opus’:
 subprocess.run([
 ‘ffmpeg’, ‘-i’, input_path,
 ‘-codec:a’, ‘libopus’,
 ‘-b:a’, f’{bitrate}k’,
 ‘-y’, output_path
 ], capture_output=True, check=True)</p>

<p># Load compressed audio
 compressed_audio, _ = librosa.load(output_path, sr=sr)</p>

<p>return compressed_audio</p>

<p>finally:
 # Cleanup
 os.unlink(input_path)
 if os.path.exists(output_path):
 os.unlink(output_path)</p>

<h1 id="usage-2">Usage</h1>
<p>audio, sr = librosa.load(‘clean.wav’, sr=16000)</p>

<h1 id="simulate-low-bitrate-compression">Simulate low-bitrate compression</h1>
<p>audio_32kbps = simulate_codec(audio, sr, codec=’mp3’, bitrate=32)
audio_64kbps = simulate_codec(audio, sr, codec=’mp3’, bitrate=64)
``</p>

<h3 id="dynamic-range-compression">Dynamic Range Compression</h3>

<p>``python
def dynamic_range_compression(audio, threshold=-20, ratio=4, attack=0.005, release=0.1, sr=16000):
 “””
 Apply dynamic range compression (like audio compressors)</p>

<p>Reduces loudness variation, simulating broadcast audio</p>

<p>Args:
 audio: Input audio
 threshold: Threshold in dB
 ratio: Compression ratio (4:1 means 4dB input → 1dB output above threshold)
 attack: Attack time in seconds
 release: Release time in seconds
 sr: Sample rate</p>

<p>Returns:
 Compressed audio
 “””
 # Convert to dB
 audio_db = 20 * np.log10(np.abs(audio) + 1e-8)</p>

<p># Compute gain reduction
 gain_db = np.zeros_like(audio_db)</p>

<p>for i in range(len(audio_db)):
 if audio_db[i] &gt; threshold:
 # Above threshold: apply compression
 excess_db = audio_db[i] - threshold
 gain_db[i] = -excess_db * (1 - 1/ratio)
 else:
 gain_db[i] = 0</p>

<p># Smooth gain reduction (attack/release)
 attack_samples = int(attack * sr)
 release_samples = int(release * sr)</p>

<p>smoothed_gain = np.zeros_like(gain_db)
 for i in range(1, len(gain_db)):
 if gain_db[i] &lt; smoothed_gain[i-1]:
 # Attack
 alpha = 1 - np.exp(-1 / attack_samples)
 else:
 # Release
 alpha = 1 - np.exp(-1 / release_samples)</p>

<p>smoothed_gain[i] = alpha * gain_db[i] + (1 - alpha) * smoothed_gain[i-1]</p>

<p># Apply gain
 gain_linear = 10 ** (smoothed_gain / 20)
 compressed = audio * gain_linear</p>

<p>return compressed</p>

<h1 id="example-14">Example</h1>
<p>audio, sr = librosa.load(‘speech.wav’, sr=16000)
compressed = dynamic_range_compression(audio, threshold=-20, ratio=4, sr=sr)
``</p>

<hr />

<h2 id="end-to-end-preprocessing-pipeline">End-to-End Preprocessing Pipeline</h2>

<p>``python
class ProductionAudioPipeline:
 “””
 Complete production-ready preprocessing pipeline</p>

<p>Handles all edge cases and monitors quality
 “””</p>

<p>def <strong>init</strong>(self, config):
 self.target_sr = config.get(‘sample_rate’, 16000)
 self.target_duration = config.get(‘target_duration’, None)
 self.enable_noise_reduction = config.get(‘noise_reduction’, True)
 self.enable_vad = config.get(‘vad’, True)
 self.augmentation_enabled = config.get(‘augmentation’, False)</p>

<p>self.stats = {
 ‘processed’: 0,
 ‘failed’: 0,
 ‘clipped’: 0,
 ‘too_short’: 0,
 ‘avg_snr’: []
 }</p>

<p>def process(self, audio_path):
 “””
 Process single audio file</p>

<p>Returns: (processed_audio, metadata, success)
 “””
 metadata = {‘original_path’: audio_path}</p>

<p>try:
 # 1. Load
 audio, orig_sr = librosa.load(audio_path, sr=None)
 metadata[‘original_sr’] = orig_sr
 metadata[‘original_duration’] = len(audio) / orig_sr</p>

<p># 2. Detect issues
 clipped = np.max(np.abs(audio)) &gt;= 0.99
 if clipped:
 audio, _ = detect_and_fix_clipping(audio)
 self.stats[‘clipped’] += 1
 metadata[‘had_clipping’] = True</p>

<p># 3. Resample
 if orig_sr != self.target_sr:
 audio = resample_audio(audio, orig_sr, self.target_sr)
 metadata[‘resampled’] = True</p>

<p># 4. Normalize
 audio = normalize_audio(audio, target_level=-20.0)
 metadata[‘normalized’] = True</p>

<p># 5. Noise reduction
 if self.enable_noise_reduction:
 audio = high_pass_filter(audio, self.target_sr, cutoff_freq=80)
 metadata[‘noise_reduction’] = True</p>

<p># 6. Voice Activity Detection
 if self.enable_vad:
 segments = voice_activity_detection(audio, self.target_sr)
 if segments:
 speech_audio = np.concatenate([
 audio[start:end] for start, end in segments
 ])
 audio = speech_audio
 metadata[‘vad_segments’] = len(segments)
 else:
 # No speech detected
 return None, {‘error’: ‘No speech detected’}, False</p>

<p># 7. Duration handling
 current_duration = len(audio) / self.target_sr</p>

<p>if self.target_duration:
 target_samples = int(self.target_duration * self.target_sr)</p>

<p>if len(audio) &lt; target_samples:
 # Pad
 audio = np.pad(audio, (0, target_samples - len(audio)), mode=’constant’)
 metadata[‘padded’] = True
 elif len(audio) &gt; target_samples:
 # Trim
 audio = audio[:target_samples]
 metadata[‘trimmed’] = True</p>

<p># 8. Quality checks
 if len(audio) &lt; 0.5 * self.target_sr: # Less than 0.5 seconds
 self.stats[‘too_short’] += 1
 return None, {‘error’: ‘Too short after VAD’}, False</p>

<p># 9. Augmentation (training only)
 if self.augmentation_enabled:
 audio = self._augment(audio)
 metadata[‘augmented’] = True</p>

<p># 10. Final normalization
 audio = audio / (np.max(np.abs(audio)) + 1e-8) * 0.95</p>

<p>metadata[‘final_duration’] = len(audio) / self.target_sr
 metadata[‘final_samples’] = len(audio)</p>

<p>self.stats[‘processed’] += 1</p>

<p>return audio, metadata, True</p>

<p>except Exception as e:
 self.stats[‘failed’] += 1
 return None, {‘error’: str(e)}, False</p>

<p>def _augment(self, audio):
 “"”Apply random augmentation”””
 import random</p>

<p>aug_type = random.choice([‘noise’, ‘pitch’, ‘speed’, ‘none’])</p>

<p>if aug_type == ‘noise’:
 audio = add_noise(audio, noise_factor=random.uniform(0.001, 0.01))
 elif aug_type == ‘pitch’:
 steps = random.choice([-2, -1, 1, 2])
 audio = pitch_shift(audio, self.target_sr, n_steps=steps)
 elif aug_type == ‘speed’:
 rate = random.uniform(0.9, 1.1)
 audio = time_stretch(audio, rate=rate)</p>

<p>return audio</p>

<p>def get_stats(self):
 “"”Get processing statistics”””
 return self.stats</p>

<h1 id="usage-3">Usage</h1>
<p>config = {
 ‘sample_rate’: 16000,
 ‘target_duration’: 3.0,
 ‘noise_reduction’: True,
 ‘vad’: True,
 ‘augmentation’: False # True for training
}</p>

<p>pipeline = ProductionAudioPipeline(config)</p>

<h1 id="process-single-file">Process single file</h1>
<p>audio, metadata, success = pipeline.process(‘input.wav’)</p>

<p>if success:
 print(f”✓ Processed successfully”)
 print(f”Duration: {metadata[‘final_duration’]:.2f}s”)
 # Save
 sf.write(‘output.wav’, audio, pipeline.target_sr)
else:
 print(f”✗ Failed: {metadata.get(‘error’)}”)</p>

<h1 id="process-batch">Process batch</h1>
<p>for audio_file in audio_files:
 audio, metadata, success = pipeline.process(audio_file)
 if success:
 save_processed(audio, metadata)</p>

<h1 id="get-statistics">Get statistics</h1>
<p>stats = pipeline.get_stats()
print(f”Processed: {stats[‘processed’]}”)
print(f”Failed: {stats[‘failed’]}”)
print(f”Clipped: {stats[‘clipped’]}”)
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>✅ <strong>Clean audio is critical</strong> - Preprocessing can make/break model performance 
✅ <strong>Standardize formats</strong> - Consistent sample rate, bit depth, mono/stereo 
✅ <strong>Remove noise</strong> - Spectral subtraction, filtering reduce artifacts 
✅ <strong>VAD improves efficiency</strong> - Remove silence saves compute 
✅ <strong>Augmentation boosts robustness</strong> - Time stretch, pitch shift, noise mixing 
✅ <strong>Like feature engineering</strong> - Transform raw data into useful representations 
✅ <strong>Pipeline thinking</strong> - Chain transformations like tree traversal</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0007-audio-preprocessing/">arunbaby.com/speech-tech/0007-audio-preprocessing</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#audio-preprocessing" class="page__taxonomy-item p-category" rel="tag">audio-preprocessing</a><span class="sep">, </span>
    
      <a href="/tags/#data-augmentation" class="page__taxonomy-item p-category" rel="tag">data-augmentation</a><span class="sep">, </span>
    
      <a href="/tags/#noise-reduction" class="page__taxonomy-item p-category" rel="tag">noise-reduction</a><span class="sep">, </span>
    
      <a href="/tags/#signal-processing" class="page__taxonomy-item p-category" rel="tag">signal-processing</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0007-binary-tree-traversal/" rel="permalink">Binary Tree Traversal
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          26 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master the fundamental patterns of tree traversal: the gateway to solving hundreds of tree problems in interviews.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0007-feature-engineering/" rel="permalink">Feature Engineering at Scale
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Feature engineering makes or breaks ML models, learn how to build scalable, production-ready feature pipelines that power real-world systems.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0007-building-your-first-agent/" rel="permalink">Building Your First Agent
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Hello World? No, Hello Agent.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Audio+Preprocessing+%26+Signal+Processing%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0007-audio-preprocessing%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0007-audio-preprocessing%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0007-audio-preprocessing/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0006-text-to-speech-basics/" class="pagination--pager" title="Text-to-Speech (TTS) System Fundamentals">Previous</a>
    
    
      <a href="/speech-tech/0008-streaming-speech-pipeline/" class="pagination--pager" title="Streaming Speech Processing Pipeline">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
