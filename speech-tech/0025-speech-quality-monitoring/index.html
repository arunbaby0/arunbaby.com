<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Speech Quality Monitoring - Arun Baby</title>
<meta name="description" content="How do we know if the audio sounds “good” without asking a human?">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Speech Quality Monitoring">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0025-speech-quality-monitoring/">


  <meta property="og:description" content="How do we know if the audio sounds “good” without asking a human?">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Speech Quality Monitoring">
  <meta name="twitter:description" content="How do we know if the audio sounds “good” without asking a human?">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0025-speech-quality-monitoring/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0025-speech-quality-monitoring/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Speech Quality Monitoring">
    <meta itemprop="description" content="How do we know if the audio sounds “good” without asking a human?">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0025-speech-quality-monitoring/" itemprop="url">Speech Quality Monitoring
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          14 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#the-problem-subjectivity">The Problem: Subjectivity</a></li><li><a href="#intrusive-vs-non-intrusive-metrics">Intrusive vs. Non-Intrusive Metrics</a><ul><li><a href="#1-intrusive-full-reference">1. Intrusive (Full-Reference)</a></li><li><a href="#2-non-intrusive-no-reference">2. Non-Intrusive (No-Reference)</a></li></ul></li><li><a href="#deep-dive-visqol-google">Deep Dive: ViSQOL (Google)</a></li><li><a href="#deep-dive-nisqa-deep-learning-for-quality">Deep Dive: NISQA (Deep Learning for Quality)</a></li><li><a href="#high-level-architecture-real-time-quality-monitor">High-Level Architecture: Real-Time Quality Monitor</a></li><li><a href="#system-design-real-time-quality-monitor-for-voip">System Design: Real-Time Quality Monitor for VoIP</a></li><li><a href="#deep-dive-the-math-of-pesq">Deep Dive: The Math of PESQ</a></li><li><a href="#engineering-component-voice-activity-detection-vad">Engineering Component: Voice Activity Detection (VAD)</a></li><li><a href="#network-engineering-webrtc-internals">Network Engineering: WebRTC Internals</a></li><li><a href="#audio-processing-packet-loss-concealment-plc">Audio Processing: Packet Loss Concealment (PLC)</a></li><li><a href="#deep-dive-dns-mos-microsoft">Deep Dive: DNS-MOS (Microsoft)</a></li><li><a href="#deep-dive-psychoacoustics">Deep Dive: Psychoacoustics</a></li><li><a href="#advanced-metric-stoi-short-time-objective-intelligibility">Advanced Metric: STOI (Short-Time Objective Intelligibility)</a></li><li><a href="#system-design-the-netflix-for-audio-quality-pipeline">System Design: The “Netflix for Audio” Quality Pipeline</a></li><li><a href="#codec-comparison-opus-vs-aac-vs-evs">Codec Comparison: Opus vs. AAC vs. EVS</a></li><li><a href="#appendix-c-subjective-testing-mushra">Appendix C: Subjective Testing (MUSHRA)</a></li><li><a href="#deep-dive-room-acoustics-and-rt60">Deep Dive: Room Acoustics and RT60</a></li><li><a href="#hardware-engineering-microphone-arrays">Hardware Engineering: Microphone Arrays</a></li><li><a href="#advanced-topic-spatial-audio-quality">Advanced Topic: Spatial Audio Quality</a></li><li><a href="#security-audio-watermarking-and-deepfake-detection">Security: Audio Watermarking and Deepfake Detection</a></li><li><a href="#accessibility-hearing-loss-simulation">Accessibility: Hearing Loss Simulation</a></li><li><a href="#deep-dive-audio-codec-internals-mdct">Deep Dive: Audio Codec Internals (MDCT)</a></li><li><a href="#deep-dive-opus-internals-silk--celt">Deep Dive: Opus Internals (SILK + CELT)</a></li><li><a href="#network-engineering-congestion-control-gcc-vs-bbr">Network Engineering: Congestion Control (GCC vs. BBR)</a></li><li><a href="#hardware-mems-vs-condenser-microphones">Hardware: MEMS vs. Condenser Microphones</a></li><li><a href="#appendix-e-the-future-of-quality">Appendix E: The Future of Quality</a></li><li><a href="#appendix-f-interview-questions">Appendix F: Interview Questions</a></li><li><a href="#conclusion">Conclusion</a></li><li><a href="#appendix-a-the-silent-failure-in-speech">Appendix A: The “Silent” Failure in Speech</a></li><li><a href="#conclusion-1">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>How do we know if the audio sounds “good” without asking a human?</strong></p>

<h2 id="the-problem-subjectivity">The Problem: Subjectivity</h2>

<p>In Image Classification, “Accuracy” is objective. Is it a cat? Yes/No.
In Speech, “Quality” is subjective.</p>
<ul>
  <li>“The audio is intelligible but robotic.”</li>
  <li>“The audio is natural but has background noise.”</li>
  <li>“The audio cuts in and out (packet loss).”</li>
</ul>

<p>For decades, the gold standard was <strong>MOS (Mean Opinion Score)</strong>.</p>
<ol>
  <li>Gather 20 humans.</li>
  <li>Play them an audio clip.</li>
  <li>Ask them to rate it 1 (Bad) to 5 (Excellent).</li>
  <li>Average the scores.</li>
</ol>

<p><strong>Problem:</strong> This is slow, expensive, and impossible to run in real-time on a Zoom call. We need <strong>Objective Metrics</strong>.</p>

<h2 id="intrusive-vs-non-intrusive-metrics">Intrusive vs. Non-Intrusive Metrics</h2>

<h3 id="1-intrusive-full-reference">1. Intrusive (Full-Reference)</h3>
<p>You have the “Clean” (Reference) audio and the “Degraded” (Test) audio. You compare them.</p>
<ul>
  <li><strong>Use Case:</strong> Codec development (MP3 vs AAC), Denoising model training.</li>
  <li><strong>Metrics:</strong></li>
  <li><strong>PESQ (Perceptual Evaluation of Speech Quality):</strong> The classic telecom standard. Models human hearing.</li>
  <li><strong>POLQA (Perceptual Objective Listening Quality Analysis):</strong> The successor to PESQ. Handles wideband (HD) voice better.</li>
  <li><strong>ViSQOL (Virtual Speech Quality Objective Listener):</strong> Google’s open-source metric. Uses similarity of spectrograms.</li>
</ul>

<h3 id="2-non-intrusive-no-reference">2. Non-Intrusive (No-Reference)</h3>
<p>You <em>only</em> have the “Degraded” audio. You don’t know what the original sounded like.</p>
<ul>
  <li><strong>Use Case:</strong> Real-time monitoring (Zoom, Discord). You receive audio from the internet; you don’t have the sender’s microphone feed.</li>
  <li><strong>Metrics:</strong></li>
  <li><strong>P.563:</strong> Old standard.</li>
  <li><strong>NISQA (Non-Intrusive Speech Quality Assessment):</strong> Deep Learning based.</li>
  <li><strong>DNS-MOS:</strong> Microsoft’s Deep Noise Suppression MOS predictor.</li>
</ul>

<h2 id="deep-dive-visqol-google">Deep Dive: ViSQOL (Google)</h2>

<p>How does a machine “listen”?
ViSQOL aligns the reference and degraded signals in time, then compares their spectrograms using a <strong>Structural Similarity Index (SSIM)</strong>-like approach.</p>

<ol>
  <li><strong>Spectrogram:</strong> Convert audio to Time-Frequency domain.</li>
  <li><strong>Alignment:</strong> Use dynamic time warping to align the two signals (handling delays/jitter).</li>
  <li><strong>Patch Comparison:</strong> Compare small patches of the spectrograms.</li>
  <li><strong>Mapping:</strong> Map the similarity score to a MOS (1-5).</li>
</ol>

<h2 id="deep-dive-nisqa-deep-learning-for-quality">Deep Dive: NISQA (Deep Learning for Quality)</h2>

<p><strong>NISQA</strong> is a CNN-LSTM model trained to predict human MOS scores directly from raw audio.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Input:</strong> Mel-Spectrogram.</li>
  <li><strong>CNN Layers:</strong> Extract local features (noise, distortion).</li>
  <li><strong>Self-Attention / LSTM:</strong> Capture temporal dependencies (dropouts, silence).</li>
  <li><strong>Output:</strong> Predicted MOS (e.g., 3.8).</li>
</ul>

<p><strong>Why is this revolutionary?</strong>
It allows <strong>Reference-Free</strong> monitoring. You can run this on the client side (in the browser) to tell the user: “Your microphone quality is poor.”</p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>

<h2 id="high-level-architecture-real-time-quality-monitor">High-Level Architecture: Real-Time Quality Monitor</h2>

<p><code class="language-plaintext highlighter-rouge">ascii
+-----------+ +------------+ +-------------+
| VoIP App | --&gt; | Edge Calc | --&gt; | Metrics Agg |
+-----------+ +------------+ +-------------+
(Microphone) (DNS-MOS/VAD) (Prometheus)
 |
 v
+-----------+ +------------+ +-------------+
| Codec Sw | &lt;-- | Alerting | &lt;-- | Dashboard |
+-----------+ +------------+ +-------------+
(Opus Mode) (Slack/PD) (Grafana)
</code></p>

<h2 id="system-design-real-time-quality-monitor-for-voip">System Design: Real-Time Quality Monitor for VoIP</h2>

<p><strong>Scenario:</strong> You are building the quality monitoring system for a Zoom competitor.</p>

<p><strong>1. Client-Side (Edge):</strong></p>
<ul>
  <li><strong>Packet Loss Rate:</strong> Simple counter.</li>
  <li><strong>Jitter:</strong> Variance in packet arrival time.</li>
  <li><strong>Energy Level:</strong> Is the user speaking? (VAD).</li>
  <li><strong>Lightweight ML:</strong> Run a tiny TFLite model (DNS-MOS) every 10 seconds on a 1-second slice.</li>
</ul>

<p><strong>2. Server-Side (Aggregator):</strong></p>
<ul>
  <li>Ingest metrics into <strong>Prometheus</strong>.</li>
  <li><strong>Alerting:</strong> If <code class="language-plaintext highlighter-rouge">Avg MOS &lt; 3.0</code> for a specific region (e.g., “India-South”), trigger an alert. It might be a network outage.</li>
</ul>

<p><strong>3. Feedback Loop:</strong></p>
<ul>
  <li>If quality drops, the client automatically switches codecs (e.g., Opus 48kbps -&gt; Opus 12kbps) or enables aggressive packet loss concealment (PLC).</li>
</ul>

<h2 id="deep-dive-the-math-of-pesq">Deep Dive: The Math of PESQ</h2>

<p>PESQ (ITU-T P.862) isn’t just a simple subtraction. It simulates the human ear.</p>

<p><strong>Steps:</strong></p>
<ol>
  <li><strong>Level Alignment:</strong> Adjust volume so Reference and Degraded are equally loud.</li>
  <li><strong>Input Filter:</strong> Simulate the frequency response of a telephone handset (300Hz - 3400Hz).</li>
  <li><strong>Auditory Transform:</strong>
    <ul>
      <li>Convert FFT to <strong>Bark Scale</strong> (perceptual pitch).</li>
      <li>Convert Amplitude to <strong>Sone Scale</strong> (perceptual loudness).</li>
    </ul>
  </li>
  <li><strong>Disturbance Calculation:</strong>
    <ul>
      <li>Subtract the two “Loudness Spectra”.</li>
      <li><code class="language-plaintext highlighter-rouge">D = |L_ref - L_deg|</code>.</li>
      <li>Apply asymmetric masking (we notice added noise more than missing sound).</li>
    </ul>
  </li>
  <li><strong>Aggregation:</strong> Average the disturbance over time and frequency to get a score ( -0.5 to 4.5).</li>
</ol>

<h2 id="engineering-component-voice-activity-detection-vad">Engineering Component: Voice Activity Detection (VAD)</h2>

<p>You can’t measure quality if no one is talking. Silence is always “perfect quality”.
We need a VAD to filter out silence frames.</p>

<p><strong>Simple Energy-Based VAD (Python):</strong></p>

<p>``python
import numpy as np</p>

<p>def simple_vad(audio, frame_len=160, threshold=0.01):
 # audio: numpy array of samples
 # frame_len: 10ms at 16kHz</p>

<p>frames = []
 for i in range(0, len(audio), frame_len):
 frame = audio[i:i+frame_len]
 energy = np.sum(frame ** 2) / len(frame)</p>

<p>if energy &gt; threshold:
 frames.append(frame)</p>

<p>return np.concatenate(frames) if frames else np.array([])
``</p>

<p><strong>Advanced VAD:</strong> WebRTC VAD uses a Gaussian Mixture Model (GMM) to distinguish speech from noise.</p>

<h2 id="network-engineering-webrtc-internals">Network Engineering: WebRTC Internals</h2>

<p>How does Zoom know your quality is bad? <strong>RTCP (Real-time Transport Control Protocol).</strong></p>

<p>Every few seconds, the receiver sends an <strong>RTCP Receiver Report (RR)</strong> back to the sender.
<strong>Fields:</strong></p>
<ol>
  <li><strong>Fraction Lost:</strong> Percentage of packets lost since last report.</li>
  <li><strong>Cumulative Lost:</strong> Total packets lost.</li>
  <li><strong>Interarrival Jitter:</strong> Variance in packet delay.</li>
</ol>

<p><strong>The Quality Estimator:</strong>
<code class="language-plaintext highlighter-rouge">MOS_est = 4.5 - PacketLossPenalty - JitterPenalty - LatencyPenalty</code>
This is the <strong>E-Model (ITU-T G.107)</strong>. It’s a heuristic formula, not a neural net.</p>

<h2 id="audio-processing-packet-loss-concealment-plc">Audio Processing: Packet Loss Concealment (PLC)</h2>

<p>When a packet is lost, you have a 20ms gap in audio.
<strong>Option 1: Silence.</strong> (Sounds like a click/pop). Bad.
<strong>Option 2: Repeat.</strong> (Repeat last 20ms). Robotic.
<strong>Option 3: Waveform Similarity Overlap-Add (WSOLA).</strong> Stretch the previous packet to cover the gap.
<strong>Option 4: Deep PLC (Packet Loss Concealment).</strong></p>
<ul>
  <li>Use a Generative Model (GAN/RNN) to <em>predict</em> what the missing packet should have been based on context.</li>
  <li><strong>NetEQ (WebRTC):</strong> Uses a jitter buffer and DSP-based PLC to smooth out network bumps.</li>
</ul>

<h2 id="deep-dive-dns-mos-microsoft">Deep Dive: DNS-MOS (Microsoft)</h2>

<p>Microsoft’s <strong>Deep Noise Suppression (DNS)</strong> dataset is huge. They trained a metric to evaluate it.
<strong>Architecture:</strong></p>
<ul>
  <li><strong>Input:</strong> Log-Mel Spectrogram.</li>
  <li><strong>Backbone:</strong> ResNet-18 or Polyphonic Inception.</li>
  <li><strong>Heads:</strong>
    <ol>
      <li><strong>SIG:</strong> Signal Quality (How natural is the speech?).</li>
      <li><strong>BAK:</strong> Background Quality (How intrusive is the noise?).</li>
      <li><strong>OVRL:</strong> Overall Quality.</li>
    </ol>
  </li>
  <li><strong>Training:</strong> Trained on P.808 crowdsourced data (humans rating noisy clips).</li>
</ul>

<p><strong>Why separate SIG and BAK?</strong>
Sometimes a denoiser removes noise (Good BAK) but makes the voice sound muffled (Bad SIG). We need to balance both.</p>

<h2 id="deep-dive-psychoacoustics">Deep Dive: Psychoacoustics</h2>

<p>Why do we need complex metrics like PESQ? Why not just use MSE (Mean Squared Error)?
<strong>MSE is terrible for audio.</strong></p>
<ul>
  <li><strong>Phase Shift:</strong> If you shift a signal by 1ms, MSE is huge, but it sounds identical.</li>
  <li><strong>Masking:</strong> If a loud sound plays at 1000Hz, you can’t hear a quiet sound at 1100Hz.</li>
  <li><strong>Equal Loudness:</strong> A 50Hz tone needs to be much louder than a 1000Hz tone to be heard (Fletcher-Munson curves).</li>
</ul>

<p><strong>Perceptual Loss Functions:</strong>
In Deep Learning (e.g., Speech Enhancement), we minimize a “Perceptual Loss”.
<code class="language-plaintext highlighter-rouge">Loss = |VGG(Ref) - VGG(Deg)|</code>
We pass audio through a pre-trained network (like VGGish or wav2vec) and compare the <em>activations</em>, not the raw pixels/samples.</p>

<h2 id="advanced-metric-stoi-short-time-objective-intelligibility">Advanced Metric: STOI (Short-Time Objective Intelligibility)</h2>

<p>PESQ measures “Quality” (How pleasant?).
STOI measures “Intelligibility” (Can you understand the words?).</p>

<p><strong>Use Case:</strong> Cochlear Implants, Hearing Aids.
A signal can be ugly (robotic) but perfectly intelligible (High STOI, Low PESQ).
A signal can be beautiful but mumbled (Low STOI, High PESQ).</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Decompose signal into TF-units (Time-Frequency).</li>
  <li>Calculate correlation between Reference and Degraded envelopes in each band.</li>
  <li>Average the correlations.</li>
</ol>

<h2 id="system-design-the-netflix-for-audio-quality-pipeline">System Design: The “Netflix for Audio” Quality Pipeline</h2>

<p><strong>Scenario:</strong> You are building Spotify’s ingestion pipeline.
<strong>Goal:</strong> Reject tracks with bad encoding artifacts.</p>

<p><strong>Pipeline:</strong></p>
<ol>
  <li><strong>Ingest:</strong> Upload WAV/FLAC.</li>
  <li><strong>Transcode:</strong> Convert to Ogg Vorbis (320kbps, 160kbps, 96kbps).</li>
  <li><strong>Quality Check (ViSQOL):</strong> Compare Transcoded vs. Original.
    <ul>
      <li>If <code class="language-plaintext highlighter-rouge">ViSQOL &lt; 4.5</code> for 320kbps, something is wrong with the encoder.</li>
    </ul>
  </li>
  <li><strong>Loudness Normalization (LUFS):</strong>
    <ul>
      <li>Measure Integrated Loudness (EBU R128).</li>
      <li>If track is too quiet (-20 LUFS), gain up.</li>
      <li>If track is too loud (-5 LUFS), gain down to target (-14 LUFS).</li>
    </ul>
  </li>
</ol>

<h2 id="codec-comparison-opus-vs-aac-vs-evs">Codec Comparison: Opus vs. AAC vs. EVS</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Feature</th>
      <th style="text-align: left">Opus</th>
      <th style="text-align: left">AAC-LD</th>
      <th style="text-align: left">EVS (5G)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Latency</strong></td>
      <td style="text-align: left">Ultra Low (5ms)</td>
      <td style="text-align: left">Low (20ms)</td>
      <td style="text-align: left">Low (20ms)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Bitrate</strong></td>
      <td style="text-align: left">6kbps - 510kbps</td>
      <td style="text-align: left">32kbps+</td>
      <td style="text-align: left">5.9kbps - 128kbps</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Quality</strong></td>
      <td style="text-align: left">Excellent</td>
      <td style="text-align: left">Good</td>
      <td style="text-align: left">Excellent</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Packet Loss</strong></td>
      <td style="text-align: left">Built-in FEC</td>
      <td style="text-align: left">Poor</td>
      <td style="text-align: left">Channel Aware</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Use Case</strong></td>
      <td style="text-align: left">Zoom, Discord</td>
      <td style="text-align: left">FaceTime</td>
      <td style="text-align: left">VoLTE, 5G Calls</td>
    </tr>
  </tbody>
</table>

<p><strong>Why Opus wins:</strong> It switches modes (SILK for speech, CELT for music) dynamically.</p>

<h2 id="appendix-c-subjective-testing-mushra">Appendix C: Subjective Testing (MUSHRA)</h2>

<p>When MOS isn’t enough, we use <strong>MUSHRA (Multiple Stimuli with Hidden Reference and Anchor)</strong>.</p>
<ol>
  <li><strong>Hidden Reference:</strong> The original audio (should be rated 100).</li>
  <li><strong>Anchor:</strong> A low-pass filtered version (should be rated 20).</li>
  <li><strong>Test Systems:</strong> The models we are testing.</li>
</ol>

<p><strong>Why?</strong> It calibrates the listeners. If a listener rates the Anchor as 80, we disqualify them.</p>

<h2 id="deep-dive-room-acoustics-and-rt60">Deep Dive: Room Acoustics and RT60</h2>

<p>Quality isn’t just about the codec; it’s about the <strong>Room</strong>.
<strong>RT60 (Reverberation Time):</strong> Time it takes for sound to decay by 60dB.</p>
<ul>
  <li><strong>Studio:</strong> 0.3s (Dry).</li>
  <li><strong>Living Room:</strong> 0.5s.</li>
  <li><strong>Cathedral:</strong> 4.0s (Wet).</li>
</ul>

<p><strong>Impact on ASR:</strong>
High RT60 smears the spectrogram. ASR models fail.
<strong>Solution:</strong> Dereverberation (WPE - Weighted Prediction Error).</p>

<h2 id="hardware-engineering-microphone-arrays">Hardware Engineering: Microphone Arrays</h2>

<p>How does Alexa hear you from across the room? <strong>Beamforming.</strong>
Using multiple microphones, we can steer the “listening beam” towards the speaker and nullify noise from the TV.</p>

<p><strong>Metrics:</strong></p>
<ol>
  <li><strong>Directivity Index (DI):</strong> Gain in the look direction vs. average gain.</li>
  <li><strong>White Noise Gain (WNG):</strong> Robustness to sensor noise.</li>
</ol>

<p><strong>MVDR Beamformer (Minimum Variance Distortionless Response):</strong>
Mathematically minimizes output power while maintaining unity gain in the target direction.</p>

<h2 id="advanced-topic-spatial-audio-quality">Advanced Topic: Spatial Audio Quality</h2>

<p>With VR/AR (Apple Vision Pro), audio is 3D.
<strong>HRTF (Head-Related Transfer Function):</strong> How your ears/head filter sound based on direction.</p>

<p><strong>Quality Metrics for Spatial Audio:</strong></p>
<ol>
  <li><strong>Localization Accuracy:</strong> Can the user pinpoint the source?</li>
  <li><strong>Timbral Coloration:</strong> Does the HRTF distort the tone?</li>
  <li><strong>Externalization:</strong> Does it sound like it’s “out there” or “in your head”?</li>
</ol>

<h2 id="security-audio-watermarking-and-deepfake-detection">Security: Audio Watermarking and Deepfake Detection</h2>

<p><strong>The Threat:</strong> AI Voice Cloning (ElevenLabs).
<strong>The Defense:</strong> Watermarking.
Embed an inaudible signal (spread spectrum) into the audio.</p>

<p><strong>Detection:</strong></p>
<ol>
  <li><strong>Artifact Analysis:</strong> GANs leave traces in the high frequencies.</li>
  <li><strong>Phase Continuity:</strong> Natural speech has specific phase relationships. Vocoders often break them.</li>
  <li><strong>Biometrics:</strong> Verify the “Voice Print” against a known enrollment.</li>
</ol>

<h2 id="accessibility-hearing-loss-simulation">Accessibility: Hearing Loss Simulation</h2>

<p>To ensure quality for <em>everyone</em>, we must simulate hearing loss.
<strong>Presbycusis:</strong> Age-related high-frequency loss.
<strong>Recruitment:</strong> Loud sounds become painful quickly.</p>

<p><strong>Testing:</strong>
Run the audio through a “Hearing Loss Simulator” (Low-pass filter + Dynamic Range Compression) and run STOI.
If STOI drops too much, the content is not accessible.</p>

<h2 id="deep-dive-audio-codec-internals-mdct">Deep Dive: Audio Codec Internals (MDCT)</h2>

<p>How does MP3/Opus actually compress audio?
<strong>MDCT (Modified Discrete Cosine Transform).</strong>
It’s like FFT, but with overlapping windows (50% overlap) to prevent “blocking artifacts”.</p>

<p><strong>Process:</strong></p>
<ol>
  <li><strong>Windowing:</strong> Multiply signal by a window function (Sine/Kaiser).</li>
  <li><strong>MDCT:</strong> Convert to frequency domain.</li>
  <li><strong>Quantization:</strong> Round the float values to integers. <strong>This is where loss happens.</strong></li>
  <li><strong>Entropy Coding:</strong> Huffman coding to compress the integers.</li>
</ol>

<p><strong>Psychoacoustic Model:</strong>
The encoder calculates the <strong>Masking Threshold</strong> for each frequency band.
If the quantization noise is <em>below</em> the masking threshold, the human ear can’t hear it.
So, we can quantize heavily (low bitrate) without perceived loss.</p>

<h2 id="deep-dive-opus-internals-silk--celt">Deep Dive: Opus Internals (SILK + CELT)</h2>

<p>Opus is the king of VoIP. Why? It’s a hybrid.</p>

<p><strong>1. SILK (Skype):</strong></p>
<ul>
  <li><strong>Type:</strong> LPC (Linear Predictive Coding).</li>
  <li><strong>Best for:</strong> Speech (Low frequencies).</li>
  <li><strong>Mechanism:</strong> Models the vocal tract as a tube. Transmits the “excitation” (glottis) and “filter” (throat/mouth).</li>
</ul>

<p><strong>2. CELT (Xiph.org):</strong></p>
<ul>
  <li><strong>Type:</strong> MDCT (Transform Coding).</li>
  <li><strong>Best for:</strong> Music (High frequencies).</li>
  <li><strong>Mechanism:</strong> Transmits the spectral energy.</li>
</ul>

<p><strong>Hybrid Mode:</strong>
Opus sends Low Frequencies (&lt; 8kHz) via SILK and High Frequencies (&gt; 8kHz) via CELT. Best of both worlds.</p>

<h2 id="network-engineering-congestion-control-gcc-vs-bbr">Network Engineering: Congestion Control (GCC vs. BBR)</h2>

<p>If the network is congested, sending <em>more</em> data makes it worse.
We need to lower the bitrate.</p>

<p><strong>Google Congestion Control (GCC):</strong></p>
<ul>
  <li><strong>Delay-based:</strong> If RTT increases, reduce bitrate.</li>
  <li><strong>Loss-based:</strong> If packet loss &gt; 2%, reduce bitrate.</li>
  <li><strong>Kalman Filter:</strong> Predicts the network capacity.</li>
</ul>

<p><strong>BBR (Bottleneck Bandwidth and Round-trip propagation time):</strong></p>
<ul>
  <li>Probes the network to find the max bandwidth and min RTT.</li>
  <li>Much more aggressive than GCC. Used in QUIC.</li>
</ul>

<h2 id="hardware-mems-vs-condenser-microphones">Hardware: MEMS vs. Condenser Microphones</h2>

<p>Quality starts at the sensor.
<strong>MEMS (Micro-Electro-Mechanical Systems):</strong></p>
<ul>
  <li><strong>Pros:</strong> Tiny, cheap, solderable (SMD). Used in phones/laptops.</li>
  <li><strong>Cons:</strong> High noise floor (SNR ~65dB).</li>
</ul>

<p><strong>Condenser (Electret):</strong></p>
<ul>
  <li><strong>Pros:</strong> High sensitivity, low noise (SNR &gt; 70dB). Studio quality.</li>
  <li><strong>Cons:</strong> Large, requires phantom power (48V).</li>
</ul>

<p><strong>Clipping:</strong>
If the user screams, the mic diaphragm hits the limit. The signal is “clipped” (flat top).
<strong>Digital Clipping:</strong> Exceeding 0dBFS.
<strong>Solution:</strong> Analog Gain Control (AGC) <em>before</em> the ADC.</p>

<h2 id="appendix-e-the-future-of-quality">Appendix E: The Future of Quality</h2>

<p><strong>Neural Codecs (EnCodec/SoundStream):</strong>
They don’t optimize SNR. They optimize Perceptual Quality.
At 3kbps, they sound better than Opus at 12kbps, but the waveform looks <em>completely different</em>.
<strong>Implication:</strong> Traditional metrics (SNR, PSNR) are dead. We <em>must</em> use Neural Metrics (NISQA, CDPAM).</p>

<h2 id="appendix-f-interview-questions">Appendix F: Interview Questions</h2>

<ol>
  <li><strong>Q:</strong> “How do you handle packet loss in a real-time audio app?”
 <strong>A:</strong>
    <ul>
      <li><strong>Jitter Buffer:</strong> Hold packets for 20-50ms to reorder them.</li>
      <li><strong>FEC (Forward Error Correction):</strong> Send redundant data (XOR of previous packets).</li>
      <li><strong>PLC (Packet Loss Concealment):</strong> Generate fake audio to fill gaps.</li>
    </ul>
  </li>
  <li>
    <p><strong>Q:</strong> “Why is 44.1kHz the standard sample rate?”
 <strong>A:</strong> Nyquist Theorem. Humans hear up to 20kHz. We need <code class="language-plaintext highlighter-rouge">2 * MaxFreq</code> to reconstruct the signal. <code class="language-plaintext highlighter-rouge">2 * 20kHz = 40kHz</code>. The extra 4.1kHz is for anti-aliasing filter roll-off.</p>
  </li>
  <li><strong>Q:</strong> “What is the Cocktail Party Problem?”
 <strong>A:</strong> The ability to focus on one speaker in a noisy room. Humans do it easily (binaural hearing). Machines struggle. Solved using <strong>Blind Source Separation (BSS)</strong> or <strong>Target Speech Extraction (TSE)</strong>.</li>
</ol>

<h2 id="conclusion">Conclusion</h2>

<p>Speech Quality Monitoring is moving from “Signal Processing” (PESQ) to “Deep Learning” (NISQA).
Just like in Vision (Perceptual Loss) and NLP (BERTScore), we are learning that <strong>Neural Networks are the best judges of Neural Networks</strong>.</p>

<p>``python
from pesq import pesq
from scipy.io import wavfile</p>

<h1 id="load-audio-must-be-8k-or-16k-for-pesq">Load audio (must be 8k or 16k for PESQ)</h1>
<p>rate, ref = wavfile.read(“reference.wav”)
rate, deg = wavfile.read(“degraded.wav”)</p>

<h1 id="calculate-pesq-wideband">Calculate PESQ (Wideband)</h1>
<p>score = pesq(rate, ref, deg, ‘wb’)
print(f”PESQ Score: {score:.2f}”)</p>
<h1 id="output-35-fair-to-good">Output: 3.5 (Fair to Good)</h1>
<p>``</p>

<h2 id="appendix-a-the-silent-failure-in-speech">Appendix A: The “Silent” Failure in Speech</h2>

<p>In ASR (Speech-to-Text), a common failure is <strong>Hallucination</strong>.</p>
<ul>
  <li><strong>Silence</strong> -&gt; Model outputs “Thank you very much.”</li>
  <li><strong>Noise</strong> -&gt; Model outputs “I will kill you.” (This actually happens!).</li>
</ul>

<p><strong>Quality Monitoring for ASR:</strong></p>
<ul>
  <li><strong>Log-Likelihood:</strong> If the model is confident.</li>
  <li><strong>Speech-to-Noise Ratio (SNR):</strong> If SNR is too low, don’t transcribe.</li>
  <li><strong>VAD (Voice Activity Detection):</strong> Only send audio that contains speech.</li>
</ul>

<h2 id="conclusion-1">Conclusion</h2>

<p>Speech Quality Monitoring is moving from “Signal Processing” (PESQ) to “Deep Learning” (NISQA).
Just like in Vision (Perceptual Loss) and NLP (BERTScore), we are learning that <strong>Neural Networks are the best judges of Neural Networks</strong>.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0025-speech-quality-monitoring/">arunbaby.com/speech-tech/0025-speech-quality-monitoring</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#audio-quality" class="page__taxonomy-item p-category" rel="tag">audio-quality</a><span class="sep">, </span>
    
      <a href="/tags/#nisqa" class="page__taxonomy-item p-category" rel="tag">nisqa</a><span class="sep">, </span>
    
      <a href="/tags/#pesq" class="page__taxonomy-item p-category" rel="tag">pesq</a><span class="sep">, </span>
    
      <a href="/tags/#polqa" class="page__taxonomy-item p-category" rel="tag">polqa</a><span class="sep">, </span>
    
      <a href="/tags/#visqol" class="page__taxonomy-item p-category" rel="tag">visqol</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#metrics" class="page__taxonomy-item p-category" rel="tag">metrics</a><span class="sep">, </span>
    
      <a href="/categories/#mos" class="page__taxonomy-item p-category" rel="tag">mos</a><span class="sep">, </span>
    
      <a href="/categories/#quality-assurance" class="page__taxonomy-item p-category" rel="tag">quality-assurance</a><span class="sep">, </span>
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0025-validate-bst/" rel="permalink">Validate Binary Search Tree
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The gatekeeper of data integrity. How do we ensure our sorted structures are actually sorted?
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0025-model-monitoring-systems/" rel="permalink">Model Monitoring Systems
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          15 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The silent killer of ML models is not a bug in the code, but a change in the world.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0025-human-in-the-loop-patterns/" rel="permalink">Human-in-the-Loop Patterns
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The safest way to deploy AI: Keep the human in the driver’s seat.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Speech+Quality+Monitoring%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0025-speech-quality-monitoring%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0025-speech-quality-monitoring%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0025-speech-quality-monitoring/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0024-speech-tokenization/" class="pagination--pager" title="Speech Tokenization">Previous</a>
    
    
      <a href="/speech-tech/0026-batch-speech-processing/" class="pagination--pager" title="Batch Speech Processing">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
