<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Streaming ASR Architecture - Arun Baby</title>
<meta name="description" content="Why batch ASR won‚Äôt work for voice assistants, and how streaming models transcribe speech as you speak in under 200ms.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Streaming ASR Architecture">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0001-streaming-asr/">


  <meta property="og:description" content="Why batch ASR won‚Äôt work for voice assistants, and how streaming models transcribe speech as you speak in under 200ms.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Streaming ASR Architecture">
  <meta name="twitter:description" content="Why batch ASR won‚Äôt work for voice assistants, and how streaming models transcribe speech as you speak in under 200ms.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0001-streaming-asr/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T16:05:30+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0001-streaming-asr/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Streaming ASR Architecture">
    <meta itemprop="description" content="Why batch ASR won‚Äôt work for voice assistants, and how streaming models transcribe speech as you speak in under 200ms.">
    <meta itemprop="datePublished" content="2025-12-29T16:05:30+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0001-streaming-asr/" itemprop="url">Streaming ASR Architecture
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#problem-definition">Problem Definition</a><ul><li><a href="#functional-requirements">Functional Requirements</a></li><li><a href="#non-functional-requirements">Non-Functional Requirements</a></li><li><a href="#out-of-scope">Out of Scope</a></li></ul></li><li><a href="#streaming-vs-batch-asr-key-differences">Streaming vs Batch ASR: Key Differences</a><ul><li><a href="#batch-asr-eg-whisper">Batch ASR (e.g., Whisper)</a></li><li><a href="#streaming-asr">Streaming ASR</a></li></ul></li><li><a href="#architecture-overview">Architecture Overview</a></li><li><a href="#component-1-voice-activity-detection-vad">Component 1: Voice Activity Detection (VAD)</a><ul><li><a href="#why-vad-is-critical">Why VAD is Critical</a></li><li><a href="#vad-approaches">VAD Approaches</a></li><li><a href="#production-vad-pipeline">Production VAD Pipeline</a></li></ul></li><li><a href="#component-2-feature-extraction">Component 2: Feature Extraction</a><ul><li><a href="#log-mel-filterbank-features">Log Mel Filterbank Features</a></li><li><a href="#normalization">Normalization</a></li><li><a href="#specaugment-training-only">SpecAugment (Training Only)</a></li></ul></li><li><a href="#component-3-streaming-acoustic-models">Component 3: Streaming Acoustic Models</a><ul><li><a href="#rnn-transducer-rnn-t">RNN-Transducer (RNN-T)</a></li><li><a href="#conformer-encoder">Conformer Encoder</a></li><li><a href="#streaming-constraints">Streaming Constraints</a></li></ul></li><li><a href="#component-4-decoding-and-beam-search">Component 4: Decoding and Beam Search</a><ul><li><a href="#greedy-decoding-fast-suboptimal">Greedy Decoding (Fast, Suboptimal)</a></li><li><a href="#beam-search-better-accuracy">Beam Search (Better Accuracy)</a></li><li><a href="#language-model-fusion">Language Model Fusion</a></li></ul></li><li><a href="#latency-optimization">Latency Optimization</a><ul><li><a href="#target-breakdown">Target Breakdown</a></li><li><a href="#technique-1-model-quantization">Technique 1: Model Quantization</a></li><li><a href="#technique-2-knowledge-distillation">Technique 2: Knowledge Distillation</a></li><li><a href="#technique-3-pruning">Technique 3: Pruning</a></li><li><a href="#technique-4-caching">Technique 4: Caching</a></li></ul></li><li><a href="#scaling-to-millions-of-users">Scaling to Millions of Users</a><ul><li><a href="#throughput-analysis">Throughput Analysis</a></li><li><a href="#strategy-1-batching">Strategy 1: Batching</a></li><li><a href="#strategy-2-regional-deployment">Strategy 2: Regional Deployment</a></li><li><a href="#strategy-3-hybrid-cloud-edge">Strategy 3: Hybrid Cloud-Edge</a></li></ul></li><li><a href="#production-example-putting-it-all-together">Production Example: Putting It All Together</a></li><li><a href="#key-takeaways">Key Takeaways</a></li><li><a href="#further-reading">Further Reading</a></li><li><a href="#conclusion">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Why batch ASR won‚Äôt work for voice assistants, and how streaming models transcribe speech as you speak in under 200ms.</strong></p>

<h2 id="introduction">Introduction</h2>

<p>Every time you say ‚ÄúHey Google‚Äù or ask Alexa a question, you‚Äôre interacting with a streaming Automatic Speech Recognition (ASR) system. Unlike traditional batch ASR systems that wait for you to finish speaking before transcribing, streaming ASR must:</p>

<ul>
  <li>Emit words <strong>as you speak</strong> (not after)</li>
  <li>Maintain <strong>&lt; 200ms latency</strong> for first token</li>
  <li>Handle <strong>millions of concurrent audio streams</strong></li>
  <li>Work reliably in <strong>noisy environments</strong></li>
  <li>Run on both <strong>cloud and edge devices</strong></li>
  <li>Adapt to different <strong>accents and speaking styles</strong></li>
</ul>

<p>This is fundamentally different from batch models like OpenAI‚Äôs Whisper, which achieve amazing accuracy but require the entire utterance before processing. For interactive voice assistants, this delay is unacceptable users expect immediate feedback.</p>

<p><strong>What you‚Äôll learn:</strong></p>
<ul>
  <li>Why streaming requires different model architectures</li>
  <li>RNN-Transducer (RNN-T) and CTC for streaming</li>
  <li>How to maintain state across audio chunks</li>
  <li>Latency optimization techniques (quantization, pruning, caching)</li>
  <li>Scaling to millions of concurrent streams</li>
  <li>Cold start and speaker adaptation</li>
  <li>Real production systems (Google, Amazon, Apple)</li>
</ul>

<hr />

<h2 id="problem-definition">Problem Definition</h2>

<p>Design a production streaming ASR system that transcribes speech in real-time for a voice assistant platform.</p>

<h3 id="functional-requirements">Functional Requirements</h3>

<ol>
  <li><strong>Streaming Transcription</strong>
    <ul>
      <li>Output tokens incrementally as user speaks</li>
      <li>No need to wait for end of utterance</li>
      <li>Partial results updated continuously</li>
    </ul>
  </li>
  <li><strong>Low Latency</strong>
    <ul>
      <li><strong>First token latency:</strong> &lt; 200ms (time from start of speech to first word)</li>
      <li><strong>Per-token latency:</strong> &lt; 100ms (time between subsequent words)</li>
      <li><strong>End-of-utterance latency:</strong> &lt; 500ms (finalized transcript)</li>
    </ul>
  </li>
  <li><strong>No Future Context</strong>
    <ul>
      <li>Cannot ‚Äúlook ahead‚Äù into future audio (non-causal)</li>
      <li>Limited right context window (e.g., 320ms)</li>
      <li>Must work with incomplete information</li>
    </ul>
  </li>
  <li><strong>State Management</strong>
    <ul>
      <li>Maintain conversational context across chunks</li>
      <li>Remember acoustic and linguistic state</li>
      <li>Handle variable-length inputs</li>
    </ul>
  </li>
  <li><strong>Multi-Language Support</strong>
    <ul>
      <li>20+ languages</li>
      <li>Automatic language detection</li>
      <li>Code-switching (mixing languages)</li>
    </ul>
  </li>
</ol>

<h3 id="non-functional-requirements">Non-Functional Requirements</h3>

<ol>
  <li><strong>Accuracy</strong>
    <ul>
      <li><strong>Clean speech:</strong> WER &lt; 5% (Word Error Rate)</li>
      <li><strong>Noisy speech:</strong> WER &lt; 15%</li>
      <li><strong>Accented speech:</strong> WER &lt; 10%</li>
      <li><strong>Far-field:</strong> WER &lt; 20%</li>
    </ul>
  </li>
  <li><strong>Throughput</strong>
    <ul>
      <li>10M concurrent audio streams globally</li>
      <li>10k QPS per regional cluster</li>
      <li>Auto-scaling based on load</li>
    </ul>
  </li>
  <li><strong>Availability</strong>
    <ul>
      <li>99.99% uptime (&lt; 1 hour downtime/year)</li>
      <li>Graceful degradation on failures</li>
      <li>Multi-region failover</li>
    </ul>
  </li>
  <li><strong>Cost Efficiency</strong>
    <ul>
      <li>&lt; $0.01 per minute of audio (cloud)</li>
      <li>&lt; 100ms inference time on edge devices</li>
      <li>GPU/CPU optimization</li>
    </ul>
  </li>
</ol>

<h3 id="out-of-scope">Out of Scope</h3>

<ul>
  <li>Audio storage and archival</li>
  <li>Speaker diarization (who is speaking)</li>
  <li>Speech translation</li>
  <li>Emotion/sentiment detection</li>
  <li>Voice biometric authentication</li>
</ul>

<hr />

<h2 id="streaming-vs-batch-asr-key-differences">Streaming vs Batch ASR: Key Differences</h2>

<h3 id="batch-asr-eg-whisper">Batch ASR (e.g., Whisper)</h3>

<p>``python
def batch_asr(audio):
 # Wait for complete audio
 complete_audio = wait_for_end_of_speech(audio)</p>

<p># Process entire sequence at once
 # Can use bidirectional models, look at future context
 features = extract_features(complete_audio)
 transcript = model(features) # Has access to all frames</p>

<p>return transcript</p>

<h1 id="latency-duration--processing-time">Latency: duration + processing time</h1>
<h1 id="for-10-second-audio-10-seconds--2-seconds--12-seconds">For 10-second audio: 10 seconds + 2 seconds = 12 seconds</h1>
<p>``</p>

<p><strong>Pros:</strong></p>
<ul>
  <li>Can use future context ‚Üí better accuracy</li>
  <li>Simpler architecture (no state management)</li>
  <li>Can use attention over full sequence</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>High latency (must wait for end)</li>
  <li>Poor user experience for voice assistants</li>
  <li>Cannot provide real-time feedback</li>
</ul>

<h3 id="streaming-asr">Streaming ASR</h3>

<p>``python
def streaming_asr(audio_stream):
 state = initialize_state()</p>

<p>for audio_chunk in audio_stream: # Process 100ms chunks
 # Can only look at past + limited future
 features = extract_features(audio_chunk)
 tokens, state = model(features, state) # Causal processing</p>

<p>if tokens:
 yield tokens # Emit immediately</p>

<p># Finalize
 final_tokens = finalize(state)
 yield final_tokens</p>

<h1 id="latency-200ms-for-first-token-100ms-per-subsequent-token">Latency: ~200ms for first token, ~100ms per subsequent token</h1>
<h1 id="for-10-second-audio-200ms--tokens--100ms--2-3-seconds-total">For 10-second audio: 200ms + (tokens * 100ms) ‚âà 2-3 seconds total</h1>
<p>``</p>

<p><strong>Pros:</strong></p>
<ul>
  <li>Low latency (immediate feedback)</li>
  <li>Better user experience</li>
  <li>Can interrupt/correct in real-time</li>
</ul>

<p><strong>Cons:</strong></p>
<ul>
  <li>More complex (state management)</li>
  <li>Slightly lower accuracy (no full future context)</li>
  <li>Harder to train</li>
</ul>

<hr />

<h2 id="architecture-overview">Architecture Overview</h2>

<p><code class="language-plaintext highlighter-rouge">
Audio Input (100ms chunks @ 16kHz)
 ‚Üì
Voice Activity Detection (VAD)
 ‚îú‚îÄ Speech detected ‚Üí Continue
 ‚îî‚îÄ Silence detected ‚Üí Skip processing
 ‚Üì
Feature Extraction
 ‚îú‚îÄ Mel Filterbank (80 dims)
 ‚îú‚îÄ Normalization
 ‚îî‚îÄ Delta features (optional)
 ‚Üì
Streaming Acoustic Model
 ‚îú‚îÄ Encoder (Conformer/RNN)
 ‚îú‚îÄ Prediction Network
 ‚îî‚îÄ Joint Network
 ‚Üì
Decoder (Beam Search)
 ‚îú‚îÄ Language Model Fusion
 ‚îú‚îÄ Beam Management
 ‚îî‚îÄ Token Emission
 ‚Üì
Post-Processing
 ‚îú‚îÄ Punctuation
 ‚îú‚îÄ Capitalization
 ‚îî‚îÄ Inverse Text Normalization
 ‚Üì
Transcription Output
</code></p>

<hr />

<h2 id="component-1-voice-activity-detection-vad">Component 1: Voice Activity Detection (VAD)</h2>

<h3 id="why-vad-is-critical">Why VAD is Critical</h3>

<p><strong>Problem:</strong> Processing silence wastes 50-70% of compute.</p>

<p><strong>Solution:</strong> Filter out non-speech audio before expensive ASR processing.</p>

<p>``python</p>
<h1 id="without-vad">Without VAD</h1>
<p>total_audio = 10 seconds
speech = 3 seconds (30%)
silence = 7 seconds (70% wasted compute)</p>

<h1 id="with-vad">With VAD</h1>
<p>processed_audio = 3 seconds (save 70% compute)
``</p>

<h3 id="vad-approaches">VAD Approaches</h3>

<p><strong>Option 1: Energy-Based (Simple)</strong></p>

<p><code class="language-plaintext highlighter-rouge">python
def energy_vad(audio_chunk, threshold=0.01):
 """
 Classify based on audio energy
 """
 energy = np.sum(audio_chunk ** 2) / len(audio_chunk)
 return energy &gt; threshold
</code></p>

<p><strong>Pros:</strong> Fast (&lt; 1ms), no model needed 
<strong>Cons:</strong> Fails in noisy environments, no semantic understanding</p>

<p><strong>Option 2: ML-Based (Robust)</strong></p>

<p>``python
class SileroVAD:
 ‚Äú‚Äù‚Äù
 Using Silero VAD (open-source, production-ready)
 Model size: 1MB, Latency: ~2ms
 ‚Äú‚Äù‚Äù
 def <strong>init</strong>(self):
 self.model, self.utils = torch.hub.load(
 repo_or_dir=‚Äôsnakers4/silero-vad‚Äô,
 model=‚Äôsilero_vad‚Äô
 )
 self.get_speech_timestamps = self.utils[0]</p>

<p>def is_speech(self, audio, sampling_rate=16000):
 ‚Äú‚Äù‚Äù
 Args:
 audio: torch.Tensor, shape (samples,)
 sampling_rate: int</p>

<p>Returns:
 bool: True if speech detected
 ‚Äú‚Äù‚Äù
 speech_timestamps = self.get_speech_timestamps(
 audio, 
 self.model,
 sampling_rate=sampling_rate,
 threshold=0.5
 )</p>

<p>return len(speech_timestamps) &gt; 0</p>

<h1 id="usage">Usage</h1>
<p>vad = SileroVAD()</p>

<p>for audio_chunk in audio_stream:
 if vad.is_speech(audio_chunk):
 # Process with ASR
 process_asr(audio_chunk)
 else:
 # Skip, save compute
 continue
``</p>

<p><strong>Pros:</strong> Robust to noise, semantic understanding 
<strong>Cons:</strong> Adds 2ms latency, requires model</p>

<h3 id="production-vad-pipeline">Production VAD Pipeline</h3>

<p>``python
class ProductionVAD:
 def <strong>init</strong>(self):
 self.vad = SileroVAD()
 self.speech_buffer = []
 self.silence_frames = 0
 self.max_silence_frames = 30 # 300ms of silence</p>

<p>def process_chunk(self, audio_chunk):
 ‚Äú‚Äù‚Äù
 Buffer management with hysteresis
 ‚Äú‚Äù‚Äù
 is_speech = self.vad.is_speech(audio_chunk)</p>

<p>if is_speech:
 # Reset silence counter
 self.silence_frames = 0</p>

<p># Add to buffer
 self.speech_buffer.append(audio_chunk)</p>

<p>return ‚Äòspeech‚Äô, audio_chunk</p>

<p>else:
 # Increment silence counter
 self.silence_frames += 1</p>

<p># Keep buffering for a bit (hysteresis)
 if self.silence_frames &lt; self.max_silence_frames:
 self.speech_buffer.append(audio_chunk)
 return ‚Äòspeech‚Äô, audio_chunk</p>

<p>else:
 # End of utterance
 if self.speech_buffer:
 complete_utterance = np.concatenate(self.speech_buffer)
 self.speech_buffer = []
 return ‚Äòend_of_utterance‚Äô, complete_utterance</p>

<p>return ‚Äòsilence‚Äô, None
``</p>

<p><strong>Key design decisions:</strong></p>
<ul>
  <li><strong>Hysteresis:</strong> Continue processing for 300ms after silence to avoid cutting off speech</li>
  <li><strong>Buffering:</strong> Accumulate audio for end-of-utterance finalization</li>
  <li><strong>State management:</strong> Track silence duration to detect utterance boundaries</li>
</ul>

<hr />

<h2 id="component-2-feature-extraction">Component 2: Feature Extraction</h2>

<h3 id="log-mel-filterbank-features">Log Mel Filterbank Features</h3>

<p><strong>Why Mel scale?</strong> Human perception of pitch is logarithmic, not linear.</p>

<p>``python
def extract_mel_features(audio, sr=16000, n_mels=80):
 ‚Äú‚Äù‚Äù
 Extract 80-dimensional log mel filterbank features</p>

<p>Args:
 audio: np.array, shape (samples,)
 sr: sampling rate (Hz)
 n_mels: number of mel bands</p>

<p>Returns:
 features: np.array, shape (time, n_mels)
 ‚Äú‚Äù‚Äù
 # Frame audio: 25ms window, 10ms stride
 frame_length = int(0.025 * sr) # 400 samples
 hop_length = int(0.010 * sr) # 160 samples</p>

<p># Short-Time Fourier Transform
 stft = librosa.stft(
 audio,
 n_fft=512,
 hop_length=hop_length,
 win_length=frame_length,
 window=‚Äôhann‚Äô
 )</p>

<p># Magnitude spectrum
 magnitude = np.abs(stft)</p>

<p># Mel filterbank
 mel_basis = librosa.filters.mel(
 sr=sr,
 n_fft=512,
 n_mels=n_mels,
 fmin=0,
 fmax=sr/2
 )</p>

<p># Apply mel filters
 mel_spec = np.dot(mel_basis, magnitude)</p>

<p># Log compression (humans perceive loudness logarithmically)
 log_mel = np.log(mel_spec + 1e-6)</p>

<p># Transpose to (time, frequency)
 return log_mel.T
``</p>

<p><strong>Output:</strong> 100 frames per second (one every 10ms), each with 80 dimensions</p>

<h3 id="normalization">Normalization</h3>

<p>``python
def normalize_features(features, mean=None, std=None):
 ‚Äú‚Äù‚Äù
 Normalize to zero mean, unit variance</p>

<p>Can use global statistics or per-utterance
 ‚Äú‚Äù‚Äù
 if mean is None:
 mean = np.mean(features, axis=0, keepdims=True)
 if std is None:
 std = np.std(features, axis=0, keepdims=True)</p>

<p>normalized = (features - mean) / (std + 1e-6)
 return normalized
``</p>

<p><strong>Global vs Per-Utterance:</strong></p>
<ul>
  <li><strong>Global normalization:</strong> Use statistics from training data (faster, more stable)</li>
  <li><strong>Per-utterance normalization:</strong> Adapt to current speaker/environment (better for diverse conditions)</li>
</ul>

<h3 id="specaugment-training-only">SpecAugment (Training Only)</h3>

<p>``python
def spec_augment(features, time_mask_max=30, freq_mask_max=10):
 ‚Äú‚Äù‚Äù
 Data augmentation for training
 Randomly mask time and frequency bands
 ‚Äú‚Äù‚Äù
 # Time masking
 t_mask_len = np.random.randint(0, time_mask_max)
 t_mask_start = np.random.randint(0, features.shape[0] - t_mask_len)
 features[t_mask_start:t_mask_start+t_mask_len, :] = 0</p>

<p># Frequency masking
 f_mask_len = np.random.randint(0, freq_mask_max)
 f_mask_start = np.random.randint(0, features.shape[1] - f_mask_len)
 features[:, f_mask_start:f_mask_start+f_mask_len] = 0</p>

<p>return features
``</p>

<p><strong>Impact:</strong> Improves robustness by 10-20% relative WER reduction</p>

<hr />

<h2 id="component-3-streaming-acoustic-models">Component 3: Streaming Acoustic Models</h2>

<h3 id="rnn-transducer-rnn-t">RNN-Transducer (RNN-T)</h3>

<p><strong>Why RNN-T for streaming?</strong></p>
<ol>
  <li><strong>Naturally causal:</strong> Doesn‚Äôt need future frames</li>
  <li><strong>Emits tokens dynamically:</strong> Can output 0, 1, or multiple tokens per frame</li>
  <li><strong>No external alignment:</strong> Learns alignment jointly with transcription</li>
</ol>

<p><strong>Architecture:</strong></p>

<p><code class="language-plaintext highlighter-rouge">
 Encoder (processes audio)
 ‚Üì
 h_enc[t] (acoustic embedding)
 ‚Üì
 Prediction Network (processes previous tokens)
 ‚Üì
 h_pred[u] (linguistic embedding)
 ‚Üì
 Joint Network (combines both)
 ‚Üì
 Softmax over vocabulary + blank
</code></p>

<p><strong>Implementation:</strong></p>

<p>``python
import torch
import torch.nn as nn</p>

<p>class StreamingRNNT(nn.Module):
 def <strong>init</strong>(self, vocab_size=1000, enc_dim=512, pred_dim=256, joint_dim=512):
 super().<strong>init</strong>()</p>

<p># Encoder: audio features ‚Üí acoustic representation
 self.encoder = ConformerEncoder(
 input_dim=80,
 output_dim=enc_dim,
 num_layers=18,
 num_heads=8
 )</p>

<p># Prediction network: previous tokens ‚Üí linguistic representation
 self.prediction_net = nn.LSTM(
 input_size=vocab_size,
 hidden_size=pred_dim,
 num_layers=2,
 batch_first=True
 )</p>

<p># Joint network: combine acoustic + linguistic
 self.joint_net = nn.Sequential(
 nn.Linear(enc_dim + pred_dim, joint_dim),
 nn.Tanh(),
 nn.Linear(joint_dim, vocab_size + 1) # +1 for blank token
 )</p>

<p>self.blank_idx = vocab_size</p>

<p>def forward(self, audio_features, prev_tokens, encoder_state=None, predictor_state=None):
 ‚Äú‚Äù‚Äù
 Args:
 audio_features: (batch, time, 80)
 prev_tokens: (batch, seq_len)
 encoder_state: hidden state from previous chunk
 predictor_state: (h, c) from previous tokens</p>

<p>Returns:
 logits: (batch, time, seq_len, vocab_size+1)
 new_encoder_state: updated encoder state
 new_predictor_state: updated predictor state
 ‚Äú‚Äù‚Äù
 # Encode audio
 h_enc, new_encoder_state = self.encoder(audio_features, encoder_state)
 # h_enc: (batch, time, enc_dim)</p>

<p># Encode previous tokens
 # Convert tokens to one-hot
 prev_tokens_onehot = F.one_hot(prev_tokens, num_classes=self.prediction_net.input_size)
 h_pred, new_predictor_state = self.prediction_net(
 prev_tokens_onehot.float(),
 predictor_state
 )
 # h_pred: (batch, seq_len, pred_dim)</p>

<p># Joint network: combine all pairs of (time, token_history)
 # Expand dimensions for broadcasting
 h_enc_exp = h_enc.unsqueeze(2) # (batch, time, 1, enc_dim)
 h_pred_exp = h_pred.unsqueeze(1) # (batch, 1, seq_len, pred_dim)</p>

<p># Concatenate
 h_joint = torch.cat([
 h_enc_exp.expand(-1, -1, h_pred.size(1), -1),
 h_pred_exp.expand(-1, h_enc.size(1), -1, -1)
 ], dim=-1)
 # h_joint: (batch, time, seq_len, enc_dim+pred_dim)</p>

<p># Project to vocabulary
 logits = self.joint_net(h_joint)
 # logits: (batch, time, seq_len, vocab_size+1)</p>

<p>return logits, new_encoder_state, new_predictor_state
``</p>

<h3 id="conformer-encoder">Conformer Encoder</h3>

<p><strong>Why Conformer?</strong> Combines convolution (local patterns) + self-attention (long-range dependencies)</p>

<p>``python
class ConformerEncoder(nn.Module):
 def <strong>init</strong>(self, input_dim=80, output_dim=512, num_layers=18, num_heads=8):
 super().<strong>init</strong>()</p>

<p># Subsampling: 4x downsampling to reduce sequence length
 self.subsampling = Conv2dSubsampling(input_dim, output_dim, factor=4)</p>

<p># Conformer blocks
 self.conformer_blocks = nn.ModuleList([
 ConformerBlock(output_dim, num_heads) 
 for _ in range(num_layers)
 ])</p>

<p>def forward(self, x, state=None):
 # x: (batch, time, input_dim)</p>

<p># Subsampling
 x = self.subsampling(x)
 # x: (batch, time//4, output_dim)</p>

<p># Conformer blocks
 for block in self.conformer_blocks:
 x, state = block(x, state)</p>

<p>return x, state</p>

<p>class ConformerBlock(nn.Module):
 def <strong>init</strong>(self, dim, num_heads):
 super().<strong>init</strong>()</p>

<p># Feed-forward module 1
 self.ff1 = FeedForwardModule(dim)</p>

<p># Multi-head self-attention
 self.attention = MultiHeadSelfAttention(dim, num_heads)</p>

<p># Convolution module
 self.conv = ConvolutionModule(dim, kernel_size=31)</p>

<p># Feed-forward module 2
 self.ff2 = FeedForwardModule(dim)</p>

<p># Layer norms
 self.norm_ff1 = nn.LayerNorm(dim)
 self.norm_att = nn.LayerNorm(dim)
 self.norm_conv = nn.LayerNorm(dim)
 self.norm_ff2 = nn.LayerNorm(dim)
 self.norm_out = nn.LayerNorm(dim)</p>

<p>def forward(self, x, state=None):
 # Feed-forward 1 (half-step residual)
 residual = x
 x = self.norm_ff1(x)
 x = residual + 0.5 * self.ff1(x)</p>

<p># Self-attention
 residual = x
 x = self.norm_att(x)
 x, state = self.attention(x, state)
 x = residual + x</p>

<p># Convolution
 residual = x
 x = self.norm_conv(x)
 x = self.conv(x)
 x = residual + x</p>

<p># Feed-forward 2 (half-step residual)
 residual = x
 x = self.norm_ff2(x)
 x = residual + 0.5 * self.ff2(x)</p>

<p># Final norm
 x = self.norm_out(x)</p>

<p>return x, state
``</p>

<p><strong>Key features:</strong></p>
<ul>
  <li><strong>Macaron-style:</strong> Feed-forward at both beginning and end</li>
  <li><strong>Depthwise convolution:</strong> Captures local patterns efficiently</li>
  <li><strong>Relative positional encoding:</strong> Better for variable-length sequences</li>
</ul>

<h3 id="streaming-constraints">Streaming Constraints</h3>

<p><strong>Problem:</strong> Self-attention in Conformer uses entire sequence ‚Üí not truly streaming</p>

<p><strong>Solution:</strong> Limited lookahead window</p>

<p>``python
class StreamingAttention(nn.Module):
 def <strong>init</strong>(self, dim, num_heads, left_context=1000, right_context=32):
 super().<strong>init</strong>()
 self.attention = nn.MultiheadAttention(dim, num_heads)
 self.left_context = left_context # Look at past 10 seconds
 self.right_context = right_context # Look ahead 320ms</p>

<p>def forward(self, x, cache=None):
 # x: (batch, time, dim)</p>

<p>if cache is not None:
 # Concatenate with cached past frames
 x = torch.cat([cache, x], dim=1)</p>

<p># Apply attention with limited context
 batch_size, seq_len, dim = x.shape</p>

<p># Create attention mask for causal attention with limited right context
 # PyTorch expects attn_mask shape (target_len, source_len)
 mask = self.create_streaming_mask(seq_len, self.right_context).to(x.device)</p>

<p># Attention
 # nn.MultiheadAttention expects (time, batch, dim)
 x_tbf = x.transpose(0, 1)
 x_att_tbf, _ = self.attention(x_tbf, x_tbf, x_tbf, attn_mask=mask)
 x_att = x_att_tbf.transpose(0, 1)</p>

<p># Cache for next chunk
 new_cache = x[:, -self.left_context:, :]</p>

<p># Return only new frames (not cached ones)
 if cache is not None:
 x_att = x_att[:, cache.size(1):, :]</p>

<p>return x_att, new_cache</p>

<p>def create_streaming_mask(self, seq_len, right_context):
 ‚Äú‚Äù‚Äù
 Create mask where each position can attend to:</p>
<ul>
  <li>All past positions</li>
  <li>Up to right_context future positions
 ‚Äú‚Äù‚Äù
 # Start with upper-triangular ones (disallow future)
 mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
 # Allow limited lookahead: zero-out first right_context super-diagonals
 if right_context &gt; 0:
 mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1+right_context)
 # Convert to bool mask where True = disallow
 return mask.bool()
``</li>
</ul>

<hr />

<h2 id="component-4-decoding-and-beam-search">Component 4: Decoding and Beam Search</h2>

<h3 id="greedy-decoding-fast-suboptimal">Greedy Decoding (Fast, Suboptimal)</h3>

<p>``python
def greedy_decode(model, audio_features):
 ‚Äú‚Äù‚Äù
 Always pick highest-probability token
 Fast but misses better hypotheses
 ‚Äú‚Äù‚Äù
 tokens = []
 state = None</p>

<p>for frame in audio_features:
 logits, state = model(frame, tokens, state)
 best_token = torch.argmax(logits)</p>

<p>if best_token != BLANK:
 tokens.append(best_token)</p>

<p>return tokens
``</p>

<p><strong>Pros:</strong> O(T) time, minimal memory 
<strong>Cons:</strong> Can‚Äôt recover from mistakes, 10-20% worse WER</p>

<h3 id="beam-search-better-accuracy">Beam Search (Better Accuracy)</h3>

<p>``python
class BeamSearchDecoder:
 def <strong>init</strong>(self, beam_size=10, blank_idx=0):
 self.beam_size = beam_size
 self.blank_idx = blank_idx</p>

<p>def decode(self, model, audio_features):
 ‚Äú‚Äù‚Äù
 Maintain top-k hypotheses at each time step
 ‚Äú‚Äù‚Äù
 # Initial beam: empty hypothesis
 beams = [Hypothesis(tokens=[], score=0.0, state=None)]</p>

<p>for frame in audio_features:
 candidates = []</p>

<p>for beam in beams:
 # Get logits for this beam
 logits, new_state = model(frame, beam.tokens, beam.state)
 log_probs = F.log_softmax(logits, dim=-1)</p>

<p># Extend with each possible token
 for token_idx, log_prob in enumerate(log_probs):
 if token_idx == self.blank_idx:
 # Blank: don‚Äôt emit token, just update score
 candidates.append(Hypothesis(
 tokens=beam.tokens,
 score=beam.score + log_prob,
 state=beam.state
 ))
 else:
 # Non-blank: emit token
 candidates.append(Hypothesis(
 tokens=beam.tokens + [token_idx],
 score=beam.score + log_prob,
 state=new_state
 ))</p>

<p># Prune to top beam_size hypotheses
 candidates.sort(key=lambda h: h.score, reverse=True)
 beams = candidates[:self.beam_size]</p>

<p># Return best hypothesis
 return beams[0].tokens</p>

<p>class Hypothesis:
 def <strong>init</strong>(self, tokens, score, state):
 self.tokens = tokens
 self.score = score
 self.state = state
``</p>

<p><strong>Complexity:</strong> O(T √ó B √ó V) where T=time, B=beam size, V=vocabulary size 
<strong>Typical parameters:</strong> B=10, V=1000 ‚Üí manageable</p>

<h3 id="language-model-fusion">Language Model Fusion</h3>

<p><strong>Problem:</strong> Acoustic model doesn‚Äôt know linguistic patterns (grammar, common phrases)</p>

<p><strong>Solution:</strong> Integrate language model (LM) scores</p>

<p>``python
def beam_search_with_lm(acoustic_model, lm, audio_features, lm_weight=0.3):
 ‚Äú‚Äù‚Äù
 Combine acoustic model + language model scores
 ‚Äú‚Äù‚Äù
 beams = [Hypothesis(tokens=[], score=0.0, state=None)]</p>

<p>for frame in audio_features:
 candidates = []</p>

<p>for beam in beams:
 logits, new_state = acoustic_model(frame, beam.tokens, beam.state)
 acoustic_log_probs = F.log_softmax(logits, dim=-1)</p>

<p>for token_idx, acoustic_log_prob in enumerate(acoustic_log_probs):
 if token_idx == BLANK:
 # Blank token
 combined_score = beam.score + acoustic_log_prob
 candidates.append(Hypothesis(
 tokens=beam.tokens,
 score=combined_score,
 state=beam.state
 ))
 else:
 # Get LM score for this token
 lm_log_prob = lm.score(beam.tokens + [token_idx])</p>

<p># Combine scores
 combined_score = (
 beam.score +
 acoustic_log_prob +
 lm_weight * lm_log_prob
 )</p>

<p>candidates.append(Hypothesis(
 tokens=beam.tokens + [token_idx],
 score=combined_score,
 state=new_state
 ))</p>

<p>candidates.sort(key=lambda h: h.score, reverse=True)
 beams = candidates[:beam_size]</p>

<p>return beams[0].tokens
``</p>

<p><strong>LM types:</strong></p>
<ul>
  <li><strong>N-gram LM (KenLM):</strong> Fast (&lt; 1ms), large memory (GBs)</li>
  <li><strong>Neural LM (LSTM/Transformer):</strong> Slower (5-20ms), better quality</li>
</ul>

<p><strong>Production choice:</strong> N-gram for first-pass, neural LM for rescoring top hypotheses</p>

<hr />

<h2 id="latency-optimization">Latency Optimization</h2>

<h3 id="target-breakdown">Target Breakdown</h3>

<p><strong>Total latency budget: 200ms</strong>
<code class="language-plaintext highlighter-rouge">
VAD: 2ms
Feature extraction: 5ms
Encoder forward: 80ms ‚Üê Bottleneck
Decoder (beam search): 10ms
Post-processing: 3ms
Network overhead: 20ms
Total: 120ms ‚úì (60ms margin)
</code></p>

<h3 id="technique-1-model-quantization">Technique 1: Model Quantization</h3>

<p><strong>INT8 Quantization:</strong> Convert float32 weights to int8</p>

<p>``python
import torch.quantization as quantization</p>

<h1 id="post-training-quantization-easiest">Post-training quantization (easiest)</h1>
<p>model_fp32 = load_model()
model_fp32.eval()</p>

<h1 id="fuse-operations-convbnrelu--single-op">Fuse operations (Conv+BN+ReLU ‚Üí single op)</h1>
<p>model_fused = quantization.fuse_modules(
 model_fp32,
 [[‚Äòconv‚Äô, ‚Äòbn‚Äô, ‚Äòrelu‚Äô]]
)</p>

<h1 id="quantize">Quantize</h1>
<p>model_int8 = quantization.quantize_dynamic(
 model_fused,
 {nn.Linear, nn.LSTM, nn.Conv2d},
 dtype=torch.qint8
)</p>

<h1 id="save">Save</h1>
<p>torch.save(model_int8.state_dict(), ‚Äòmodel_int8.pth‚Äô)</p>

<h1 id="results">Results:</h1>
<h1 id="--model-size-200mb--50mb-4x-smaller">- Model size: 200MB ‚Üí 50MB (4x smaller)</h1>
<h1 id="--inference-speed-80ms--30ms-27x-faster">- Inference speed: 80ms ‚Üí 30ms (2.7x faster)</h1>
<h1 id="--accuracy-wer-52--54-02-degradation">- Accuracy: WER 5.2% ‚Üí 5.4% (0.2% degradation)</h1>
<p>``</p>

<p><strong>Why quantization works:</strong></p>
<ul>
  <li><strong>Smaller memory footprint:</strong> Fits in L1/L2 cache</li>
  <li><strong>Faster math:</strong> INT8 operations 4x faster than FP32 on CPU</li>
  <li><strong>Minimal accuracy loss:</strong> Neural networks are surprisingly robust</li>
</ul>

<h3 id="technique-2-knowledge-distillation">Technique 2: Knowledge Distillation</h3>

<p><strong>Train small model to mimic large model</strong></p>

<p>``python
def distillation_loss(student_logits, teacher_logits, temperature=3.0):
 ‚Äú‚Äù‚Äù
 Soft targets from teacher help student learn better
 ‚Äú‚Äù‚Äù
 # Soften probabilities with temperature
 student_soft = F.log_softmax(student_logits / temperature, dim=-1)
 teacher_soft = F.softmax(teacher_logits / temperature, dim=-1)</p>

<p># KL divergence
 loss = F.kl_div(student_soft, teacher_soft, reduction=‚Äôbatchmean‚Äô)
 loss = loss * (temperature ** 2)</p>

<p>return loss</p>

<h1 id="training">Training</h1>
<p>teacher = large_model # 18 layers, 80ms inference
student = small_model # 8 layers, 30ms inference</p>

<p>for audio, transcript in training_data:
 # Get teacher predictions (no backprop)
 with torch.no_grad():
 teacher_logits = teacher(audio)</p>

<p># Student predictions
 student_logits = student(audio)</p>

<p># Distillation loss
 loss = distillation_loss(student_logits, teacher_logits)</p>

<p># Optimize
 loss.backward()
 optimizer.step()</p>

<h1 id="results-1">Results:</h1>
<h1 id="--student-8-layers-30ms-wer-58">- Student (8 layers): 30ms, WER 5.8%</h1>
<h1 id="--teacher-18-layers-80ms-wer-50">- Teacher (18 layers): 80ms, WER 5.0%</h1>
<h1 id="--without-distillation-30ms-wer-72">- Without distillation: 30ms, WER 7.2%</h1>
<h1 id="-distillation-closes-the-gap">‚Üí Distillation closes the gap!</h1>
<p>``</p>

<h3 id="technique-3-pruning">Technique 3: Pruning</h3>

<p><strong>Remove unimportant weights</strong></p>

<p>``python
import torch.nn.utils.prune as prune</p>

<p>def prune_model(model, amount=0.4):
 ‚Äú‚Äù‚Äù
 Remove 40% of weights with minimal accuracy loss
 ‚Äú‚Äù‚Äù
 for name, module in model.named_modules():
 if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):
 # L1 unstructured pruning
 prune.l1_unstructured(module, name=‚Äôweight‚Äô, amount=amount)</p>

<p># Remove pruning reparameterization
 prune.remove(module, ‚Äòweight‚Äô)</p>

<p>return model</p>

<h1 id="results-2">Results:</h1>
<h1 id="--40-pruning-wer-50--53-speed-20">- 40% pruning: WER 5.0% ‚Üí 5.3%, Speed +20%</h1>
<h1 id="--60-pruning-wer-50--62-speed-40">- 60% pruning: WER 5.0% ‚Üí 6.2%, Speed +40%</h1>
<p>``</p>

<h3 id="technique-4-caching">Technique 4: Caching</h3>

<p><strong>Cache intermediate results across chunks</strong></p>

<p>``python
class StreamingASRWithCache:
 def <strong>init</strong>(self, model):
 self.model = model
 self.encoder_cache = None
 self.decoder_state = None</p>

<p>def process_chunk(self, audio_chunk):
 # Extract features (no caching needed, fast)
 features = extract_features(audio_chunk)</p>

<p># Encoder: reuse cached hidden states
 encoder_out, self.encoder_cache = self.model.encoder(
 features,
 cache=self.encoder_cache
 )</p>

<p># Decoder: maintain beam state
 tokens, self.decoder_state = self.model.decoder(
 encoder_out,
 state=self.decoder_state
 )</p>

<p>return tokens</p>

<p>def reset(self):
 ‚Äú"‚ÄùCall at end of utterance‚Äù‚Äù‚Äù
 self.encoder_cache = None
 self.decoder_state = None
``</p>

<p><strong>Savings:</strong></p>
<ul>
  <li><strong>Without cache:</strong> Process all frames every chunk ‚Üí 100ms</li>
  <li><strong>With cache:</strong> Process only new frames ‚Üí 30ms (3.3x speedup)</li>
</ul>

<hr />

<h2 id="scaling-to-millions-of-users">Scaling to Millions of Users</h2>

<h3 id="throughput-analysis">Throughput Analysis</h3>

<p><strong>Per-stream compute:</strong></p>
<ul>
  <li>Encoder: 30ms (after optimization)</li>
  <li>Decoder: 10ms</li>
  <li>Total: 40ms per 100ms audio chunk</li>
</ul>

<p><strong>CPU/GPU capacity:</strong></p>
<ul>
  <li>CPU (16 cores): ~50 concurrent streams</li>
  <li>GPU (T4): ~200 concurrent streams</li>
</ul>

<p><strong>For 10M concurrent streams:</strong></p>
<ul>
  <li>GPUs needed: 10M / 200 = 50,000 GPUs</li>
  <li>Cost @ <code class="language-plaintext highlighter-rouge">0.50/hr: </code>25k/hour = $18M/month</li>
</ul>

<p><strong>Way too expensive!</strong> Need further optimization.</p>

<h3 id="strategy-1-batching">Strategy 1: Batching</h3>

<p><strong>Batch multiple streams together</strong></p>

<p>``python
def batch_inference(audio_chunks, batch_size=32):
 ‚Äú‚Äù‚Äù
 Process 32 streams simultaneously on GPU
 ‚Äú‚Äù‚Äù
 # Pad to same length
 max_len = max(len(chunk) for chunk in audio_chunks)
 padded = [
 np.pad(chunk, (0, max_len - len(chunk)))
 for chunk in audio_chunks
 ]</p>

<p># Stack into batch
 batch = torch.tensor(padded) # (32, max_len, 80)</p>

<p># Single forward pass
 outputs = model(batch) # ~40ms for 32 streams</p>

<p>return outputs</p>

<h1 id="results-3">Results:</h1>
<h1 id="--without-batching-40ms-per-stream">- Without batching: 40ms per stream</h1>
<h1 id="--with-batching-32-40ms--32--125ms-per-stream-32x-speedup">- With batching (32): 40ms / 32 = 1.25ms per stream (32x speedup)</h1>
<h1 id="--gpu-needed-10m--200--32--1562-gpus">- GPU needed: 10M / (200 √ó 32) = 1,562 GPUs</h1>
<h1 id="--cost-078mmonth-23x-cheaper">- Cost: $0.78M/month (23x cheaper!)</h1>
<p>``</p>

<h3 id="strategy-2-regional-deployment">Strategy 2: Regional Deployment</h3>

<p><strong>Deploy closer to users to reduce latency</strong></p>

<p>``
North America: 3M users ‚Üí 500 GPUs ‚Üí 3 data centers
Europe: 2M users ‚Üí 330 GPUs ‚Üí 2 data centers
Asia: 4M users ‚Üí 660 GPUs ‚Üí 4 data centers
‚Ä¶</p>

<p>Total: ~1,500 GPUs globally
``</p>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Lower network latency (30ms ‚Üí 10ms)</li>
  <li>Better fault isolation</li>
  <li>Regulatory compliance (data residency)</li>
</ul>

<h3 id="strategy-3-hybrid-cloud-edge">Strategy 3: Hybrid Cloud-Edge</h3>

<p><strong>Run simple queries on-device, complex queries on cloud</strong></p>

<p>``python
def route_request(audio, user_context):
 # Estimate query complexity
 if is_simple_command(audio): # ‚Äúplay music‚Äù, ‚Äúset timer‚Äù
 return on_device_asr(audio) # 30ms, free, offline</p>

<p>elif is_dictation(audio): # Long-form transcription
 return cloud_asr(audio) # 80ms, $0.01/min, high accuracy</p>

<p>else: # Conversational query
 return cloud_asr(audio) # Best quality for complex queries
``</p>

<p><strong>Distribution:</strong></p>
<ul>
  <li>70% simple commands ‚Üí on-device</li>
  <li>30% complex queries ‚Üí cloud</li>
  <li>Effective cloud load: 3M concurrent (70% savings!)</li>
</ul>

<hr />

<h2 id="production-example-putting-it-all-together">Production Example: Putting It All Together</h2>

<p>``python
import asyncio
import websockets
import torch</p>

<p>class ProductionStreamingASR:
 def <strong>init</strong>(self):
 # Load optimized model
 self.model = self.load_optimized_model()</p>

<p># VAD
 self.vad = SileroVAD()</p>

<p># Session management
 self.sessions = {} # session_id ‚Üí StreamingSession</p>

<p># Metrics
 self.metrics = Metrics()</p>

<p>def load_optimized_model(self):
 ‚Äú"‚ÄùLoad quantized, pruned model‚Äù‚Äù‚Äù
 model = StreamingRNNT(vocab_size=1000)</p>

<p># Load pre-trained weights
 checkpoint = torch.load(‚Äòrnnt_optimized.pth‚Äô)
 model.load_state_dict(checkpoint)</p>

<p># Quantize
 model_quantized = torch.quantization.quantize_dynamic(
 model,
 {torch.nn.Linear, torch.nn.LSTM},
 dtype=torch.qint8
 )</p>

<p>model_quantized.eval()
 return model_quantized</p>

<p>async def handle_stream(self, websocket, path):
 ‚Äú"‚ÄùHandle websocket connection from client‚Äù‚Äù‚Äù
 session_id = generate_session_id()
 session = StreamingSession(session_id, self.model, self.vad)
 self.sessions[session_id] = session</p>

<p>try:
 async for message in websocket:
 # Receive audio chunk (binary, 100ms @ 16kHz)
 audio_bytes = message
 audio_array = np.frombuffer(audio_bytes, dtype=np.int16)
 audio_float = audio_array.astype(np.float32) / 32768.0</p>

<p># Process
 start_time = time.time()
 result = session.process_chunk(audio_float)
 latency = (time.time() - start_time) * 1000 # ms</p>

<p># Send partial transcript
 if result:
 await websocket.send(json.dumps({
 ‚Äòtype‚Äô: ‚Äòpartial‚Äô,
 ‚Äòtranscript‚Äô: result[‚Äòtext‚Äô],
 ‚Äòtokens‚Äô: result[‚Äòtokens‚Äô],
 ‚Äòis_final‚Äô: result[‚Äòis_final‚Äô]
 }))</p>

<p># Track metrics
 self.metrics.record_latency(latency)</p>

<p>except websockets.ConnectionClosed:
 # Finalize session
 final_transcript = session.finalize()
 print(f‚ÄùSession {session_id} ended: {final_transcript}‚Äù)</p>

<p>finally:
 # Cleanup
 del self.sessions[session_id]</p>

<p>def run(self, host=‚Äô0.0.0.0‚Äô, port=8765):
 ‚Äú"‚ÄùStart WebSocket server‚Äù‚Äù‚Äù
 start_server = websockets.serve(self.handle_stream, host, port)
 asyncio.get_event_loop().run_until_complete(start_server)
 print(f‚ÄùStreaming ASR server running on ws://{host}:{port}‚Äù)
 asyncio.get_event_loop().run_forever()</p>

<p>class StreamingSession:
 def <strong>init</strong>(self, session_id, model, vad):
 self.session_id = session_id
 self.model = model
 self.vad = vad</p>

<p># State
 self.encoder_cache = None
 self.decoder_state = None
 self.partial_transcript = ‚Äú‚Äù
 self.audio_buffer = []</p>

<p>def process_chunk(self, audio):
 # VAD check
 if not self.vad.is_speech(audio):
 return None</p>

<p># Extract features
 features = extract_mel_features(audio)</p>

<p># Encode
 encoder_out, self.encoder_cache = self.model.encoder(
 features,
 cache=self.encoder_cache
 )</p>

<p># Decode (beam search)
 tokens, self.decoder_state = self.model.decoder(
 encoder_out,
 state=self.decoder_state,
 beam_size=5
 )</p>

<p># Convert tokens to text
 new_text = self.model.tokenizer.decode(tokens)
 self.partial_transcript += new_text</p>

<p>return {
 ‚Äòtext‚Äô: new_text,
 ‚Äòtokens‚Äô: tokens,
 ‚Äòis_final‚Äô: False
 }</p>

<p>def finalize(self):
 ‚Äú"‚ÄùEnd of utterance processing‚Äù‚Äù‚Äù
 # Post-processing
 final_transcript = post_process(self.partial_transcript)</p>

<p># Reset state
 self.encoder_cache = None
 self.decoder_state = None
 self.partial_transcript = ‚Äú‚Äù</p>

<p>return final_transcript</p>

<h1 id="run-server">Run server</h1>
<p>if <strong>name</strong> == ‚Äò<strong>main</strong>‚Äô:
 server = ProductionStreamingASR()
 server.run()
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>‚úÖ <strong>RNN-T architecture</strong> enables true streaming without future context 
‚úÖ <strong>Conformer encoder</strong> combines convolution + attention for best accuracy 
‚úÖ <strong>State management</strong> critical for maintaining context across chunks 
‚úÖ <strong>Quantization + pruning</strong> achieve 4x compression, 3x speedup, &lt; 1% WER loss 
‚úÖ <strong>Batching</strong> provides 32x throughput improvement on GPUs 
‚úÖ <strong>Hybrid cloud-edge</strong> reduces cloud load by 70% 
‚úÖ <strong>VAD</strong> saves 50-70% compute by filtering silence</p>

<hr />

<h2 id="further-reading">Further Reading</h2>

<p><strong>Papers:</strong></p>
<ul>
  <li><a href="https://arxiv.org/abs/1211.3711">RNN-Transducer (Graves 2012)</a></li>
  <li><a href="https://arxiv.org/abs/2005.08100">Conformer (Google 2020)</a></li>
  <li><a href="https://arxiv.org/abs/2005.03191">ContextNet (Google 2020)</a></li>
  <li><a href="https://arxiv.org/abs/1811.06621">Streaming E2E ASR</a></li>
</ul>

<p><strong>Open-Source:</strong></p>
<ul>
  <li><a href="https://github.com/espnet/espnet">ESPnet</a> - End-to-end speech processing</li>
  <li><a href="https://github.com/speechbrain/speechbrain">SpeechBrain</a> - PyTorch-based toolkit</li>
  <li><a href="https://github.com/kaldi-asr/kaldi">Kaldi</a> - Classic ASR toolkit</li>
</ul>

<p><strong>Courses:</strong></p>
<ul>
  <li><a href="http://web.stanford.edu/class/cs224s/">Stanford CS224S: Spoken Language Processing</a></li>
  <li><a href="https://www.coursera.org/learn/nlp-sequence-models">Coursera: Speech Recognition and Synthesis</a></li>
</ul>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>Streaming ASR is a fascinating blend of signal processing, deep learning, and systems engineering. The key challenges low latency, high throughput, and maintaining accuracy without future context require careful architectural choices and aggressive optimization.</p>

<p>As voice interfaces become ubiquitous, streaming ASR systems will continue to evolve. Future directions include:</p>
<ul>
  <li><strong>Multi-modal models</strong> (audio + video for better accuracy)</li>
  <li><strong>Personalization</strong> (adapt to individual speaking styles)</li>
  <li><strong>Emotion recognition</strong> (detect sentiment, stress, sarcasm)</li>
  <li><strong>On-device models</strong> (&lt; 10MB, &lt; 50ms, works offline)</li>
</ul>

<p>The fundamentals covered here RNN-T, streaming architectures, optimization techniques will remain relevant as the field advances.</p>

<p>Now go build a voice assistant that feels truly conversational! üé§üöÄ</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0001-streaming-asr/">arunbaby.com/speech-tech/0001-streaming-asr</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#real-time" class="page__taxonomy-item p-category" rel="tag">real-time</a><span class="sep">, </span>
    
      <a href="/tags/#streaming" class="page__taxonomy-item p-category" rel="tag">streaming</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0001-two-sum/" rel="permalink">Two Sum
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          28 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The hash table trick that makes O(n¬≤) become O(n) and why this pattern appears everywhere from feature stores to embedding lookups.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0001-recommendation-system/" rel="permalink">Recommendation System: Candidate Retrieval
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          29 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How do you narrow down 10 million items to 1000 candidates in under 50ms? The art of fast retrieval at scale.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0001-what-are-ai-agents/" rel="permalink">What are AI Agents?
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">‚ÄúFrom Passive Tools to Active Assistants: The Cognitive Revolution in Software.‚Äù
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Streaming+ASR+Architecture%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0001-streaming-asr%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0001-streaming-asr%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0001-streaming-asr/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="#" class="pagination--pager disabled">Previous</a>
    
    
      <a href="/speech-tech/0002-speech-classification/" class="pagination--pager" title="Speech Command Classification">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
