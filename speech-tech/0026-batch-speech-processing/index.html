<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Batch Speech Processing - Arun Baby</title>
<meta name="description" content="Real-time ASR is hard. Offline ASR is big.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Batch Speech Processing">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0026-batch-speech-processing/">


  <meta property="og:description" content="Real-time ASR is hard. Offline ASR is big.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Batch Speech Processing">
  <meta name="twitter:description" content="Real-time ASR is hard. Offline ASR is big.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0026-batch-speech-processing/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-21T10:58:19+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0026-batch-speech-processing/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Batch Speech Processing">
    <meta itemprop="description" content="Real-time ASR is hard. Offline ASR is big.">
    <meta itemprop="datePublished" content="2025-12-21T10:58:19+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0026-batch-speech-processing/" itemprop="url">Batch Speech Processing
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#the-use-case-transcribe-this-meeting">The Use Case: “Transcribe This Meeting”</a></li><li><a href="#high-level-architecture-the-batch-pipeline">High-Level Architecture: The Batch Pipeline</a></li><li><a href="#pipeline-architecture">Pipeline Architecture</a></li><li><a href="#deep-dive-speaker-diarization">Deep Dive: Speaker Diarization</a></li><li><a href="#deep-dive-forced-alignment">Deep Dive: Forced Alignment</a></li><li><a href="#system-design-youtube-auto-captions">System Design: YouTube Auto-Captions</a></li><li><a href="#python-example-batch-transcription-with-whisper">Python Example: Batch Transcription with Whisper</a></li><li><a href="#deep-dive-whisper-architecture-the-new-standard">Deep Dive: Whisper Architecture (The New Standard)</a></li><li><a href="#deep-dive-word-error-rate-wer">Deep Dive: Word Error Rate (WER)</a></li><li><a href="#engineering-inverse-text-normalization-itn">Engineering: Inverse Text Normalization (ITN)</a></li><li><a href="#advanced-topic-ctc-vs-transducer-vs-attention">Advanced Topic: CTC vs. Transducer vs. Attention</a></li><li><a href="#deep-dive-voice-activity-detection-vad">Deep Dive: Voice Activity Detection (VAD)</a></li><li><a href="#appendix-b-interview-questions">Appendix B: Interview Questions</a></li><li><a href="#deep-dive-conformer-architecture-the-macaron-net">Deep Dive: Conformer Architecture (The “Macaron” Net)</a></li><li><a href="#deep-dive-specaugment-data-augmentation">Deep Dive: SpecAugment (Data Augmentation)</a></li><li><a href="#system-design-privacy-and-pii-redaction">System Design: Privacy and PII Redaction</a></li><li><a href="#engineering-gpu-inference-optimization">Engineering: GPU Inference Optimization</a></li><li><a href="#advanced-topic-multilingual-asr">Advanced Topic: Multilingual ASR</a></li><li><a href="#case-study-spotify-podcast-transcription">Case Study: Spotify Podcast Transcription</a></li><li><a href="#appendix-c-advanced-interview-questions">Appendix C: Advanced Interview Questions</a></li><li><a href="#deep-dive-beam-search-decoding">Deep Dive: Beam Search Decoding</a></li><li><a href="#advanced-topic-language-model-integration-shallow-vs-deep-fusion">Advanced Topic: Language Model Integration (Shallow vs. Deep Fusion)</a></li><li><a href="#system-design-audio-fingerprinting-shazam">System Design: Audio Fingerprinting (Shazam)</a></li><li><a href="#engineering-ffmpeg-tricks-for-speech">Engineering: FFMpeg Tricks for Speech</a></li><li><a href="#case-study-alexas-wake-word-detection-cascade">Case Study: Alexa’s Wake Word Detection (Cascade)</a></li><li><a href="#appendix-d-the-long-tail-problem">Appendix D: The “Long-Tail” Problem</a></li><li><a href="#conclusion">Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Real-time ASR is hard. Offline ASR is big.</strong></p>

<h2 id="the-use-case-transcribe-this-meeting">The Use Case: “Transcribe This Meeting”</h2>

<p>In Real-Time ASR (Siri), latency is king. You sacrifice accuracy for speed.
In Batch ASR (Otter.ai, YouTube Captions), <strong>Accuracy is king</strong>. You have the whole file. You can look ahead. You can run massive models.</p>

<p><strong>Key Differences:</strong></p>
<ul>
  <li><strong>Lookahead:</strong> Batch models can see the <em>future</em> context (Bidirectional RNNs/Transformers). Real-time models are causal (Unidirectional).</li>
  <li><strong>Compute:</strong> Batch can run on a massive GPU cluster overnight.</li>
  <li><strong>Features:</strong> Batch enables Speaker Diarization (“Who said what?”) and Summarization.</li>
</ul>

<h2 id="high-level-architecture-the-batch-pipeline">High-Level Architecture: The Batch Pipeline</h2>

<pre><code class="language-ascii">+-----------+     +------------+     +-------------+
| Raw Audio | --&gt; | Preprocess | --&gt; | Segmentation|
+-----------+     +------------+     +-------------+
(MP3/M4A)         (FFMpeg/VAD)       (30s Chunks)
                                           |
                                           v
+-----------+     +------------+     +-------------+
| Final Txt | &lt;-- | Post-Proc  | &lt;-- | Transcription|
+-----------+     +------------+     +-------------+
(SRT/JSON)        (Diarization)      (Whisper/ASR)
</code></pre>

<h2 id="pipeline-architecture">Pipeline Architecture</h2>

<p>A typical Batch Speech Pipeline has 4 stages:</p>

<ol>
  <li><strong>Preprocessing (FFMpeg):</strong>
    <ul>
      <li>Convert format (MP3/M4A -&gt; WAV).</li>
      <li>Resample (44.1kHz -&gt; 16kHz).</li>
      <li>Mixdown (Stereo -&gt; Mono).</li>
      <li><strong>VAD (Voice Activity Detection):</strong> Remove silence to save compute.</li>
    </ul>
  </li>
  <li><strong>Segmentation:</strong>
    <ul>
      <li>Split long audio (1 hour) into chunks (30 seconds).</li>
      <li>Why? Transformer attention scales quadratically (O(N^2)). You can’t feed 1 hour into BERT.</li>
    </ul>
  </li>
  <li><strong>Transcription (ASR):</strong>
    <ul>
      <li>Run Whisper / Conformer on each chunk.</li>
      <li>Get text + timestamps.</li>
    </ul>
  </li>
  <li><strong>Post-Processing:</strong>
    <ul>
      <li><strong>Diarization:</strong> Cluster segments by speaker.</li>
      <li><strong>Punctuation &amp; Capitalization:</strong> “hello world” -&gt; “Hello world.”</li>
      <li><strong>Inverse Text Normalization (ITN):</strong> “twenty dollars” -&gt; “$20”.</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-speaker-diarization">Deep Dive: Speaker Diarization</h2>

<p><strong>Goal:</strong> Partition the audio stream into homogeneous segments according to the speaker identity.
<strong>Output:</strong> “Speaker A spoke from 0:00 to 0:10. Speaker B spoke from 0:10 to 0:15.”</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Embedding:</strong> Extract a vector (d-vector / x-vector) for every 1-second sliding window.</li>
  <li><strong>Clustering:</strong> Use <strong>Spectral Clustering</strong> or <strong>Agglomerative Hierarchical Clustering (AHC)</strong> to group similar vectors.</li>
  <li><strong>Re-segmentation:</strong> Refine boundaries using a Viterbi decode.</li>
</ol>

<p><strong>Tool:</strong> <code class="language-plaintext highlighter-rouge">pyannote.audio</code> is the industry standard library.</p>

<h2 id="deep-dive-forced-alignment">Deep Dive: Forced Alignment</h2>

<p><strong>Goal:</strong> Align text to audio at the <em>phoneme</em> level.
<strong>Input:</strong> Audio + Transcript (“The cat sat”).
<strong>Output:</strong> “The” (0.1s - 0.2s), “cat” (0.2s - 0.5s)…</p>

<p><strong>How?</strong>
We know the sequence of phonemes (from the text). We just need to find the optimal path through the audio frames that matches this sequence.
This is solved using <strong>Viterbi Alignment</strong> (HMMs) or <strong>CTC Segmentation</strong>.</p>

<p><strong>Use Case:</strong></p>
<ul>
  <li><strong>Karaoke:</strong> Highlighting lyrics.</li>
  <li><strong>Video Editing:</strong> “Delete this word” -&gt; Cuts the audio frame. (Descript).</li>
</ul>

<h2 id="system-design-youtube-auto-captions">System Design: YouTube Auto-Captions</h2>

<p><strong>Scale:</strong> 500 hours of video uploaded <em>every minute</em>.</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Upload:</strong> Video lands in Colossus (Google File System).</li>
  <li><strong>Trigger:</strong> Pub/Sub message to ASR Service.</li>
  <li><strong>Sharding:</strong> Video is split into 5-minute chunks.</li>
  <li><strong>Parallelism:</strong> 12 chunks are processed by 12 TPUs in parallel.</li>
  <li><strong>Merge:</strong> Results are stitched together.</li>
  <li><strong>Indexing:</strong> Captions are indexed for Search (SEO).</li>
</ol>

<h2 id="python-example-batch-transcription-with-whisper">Python Example: Batch Transcription with Whisper</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">whisper</span>

<span class="c1"># Load model (Large-v2 is slow but accurate)
</span><span class="n">model</span> <span class="o">=</span> <span class="n">whisper</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span><span class="sh">"</span><span class="s">large</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Transcribe
# beam_size=5: Better accuracy, slower
</span><span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">transcribe</span><span class="p">(</span><span class="sh">"</span><span class="s">meeting.mp3</span><span class="sh">"</span><span class="p">,</span> <span class="n">beam_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># Segments with timestamps
</span><span class="k">for</span> <span class="n">segment</span> <span class="ow">in</span> <span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">segments</span><span class="sh">"</span><span class="p">]:</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">segment</span><span class="p">[</span><span class="sh">"</span><span class="s">start</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">segment</span><span class="p">[</span><span class="sh">"</span><span class="s">end</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">segment</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">[</span><span class="si">{</span><span class="n">start</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> - </span><span class="si">{</span><span class="n">end</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">]: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="deep-dive-whisper-architecture-the-new-standard">Deep Dive: Whisper Architecture (The New Standard)</h2>

<p>OpenAI’s Whisper changed the game in 2022.
<strong>Architecture:</strong> Standard Transformer Encoder-Decoder.
<strong>Key Innovation:</strong> Weakly Supervised Training on 680,000 hours of internet audio.</p>

<p><strong>Input:</strong></p>
<ul>
  <li>Log-Mel Spectrogram (80 channels).</li>
  <li>30-second windows (padded/truncated).</li>
</ul>

<p><strong>Decoder Tasks (Multitasking):</strong>
The model predicts special tokens to control behavior:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">&lt;|startoftranscript|&gt;</code></li>
  <li><code class="language-plaintext highlighter-rouge">&lt;|en|&gt;</code> (Language ID)</li>
  <li><code class="language-plaintext highlighter-rouge">&lt;|transcribe|&gt;</code> or <code class="language-plaintext highlighter-rouge">&lt;|translate|&gt;</code></li>
  <li><code class="language-plaintext highlighter-rouge">&lt;|timestamps|&gt;</code> (Predict time alignment)</li>
</ul>

<p><strong>Why it wins:</strong>
It’s robust to accents, background noise, and technical jargon because it was trained on “wild” data, not just clean audiobooks (LibriSpeech).</p>

<h2 id="deep-dive-word-error-rate-wer">Deep Dive: Word Error Rate (WER)</h2>

<p>How do we measure accuracy? <strong>Levenshtein Distance.</strong>
<code class="language-plaintext highlighter-rouge">WER = (S + D + I) / N</code></p>
<ul>
  <li><strong>S (Substitution):</strong> “cat” -&gt; “bat”</li>
  <li><strong>D (Deletion):</strong> “the cat” -&gt; “cat”</li>
  <li><strong>I (Insertion):</strong> “cat” -&gt; “the cat”</li>
  <li><strong>N:</strong> Total words in reference.</li>
</ul>

<p><strong>Example:</strong>
Ref: “The cat sat on the mat”
Hyp: “The bat sat on mat”</p>
<ul>
  <li>Sub: cat-&gt;bat (1)</li>
  <li>Del: the (1)</li>
  <li>WER = (1 + 1 + 0) / 6 = 33%</li>
</ul>

<p><strong>Pitfall:</strong> WER doesn’t care about meaning. “I am happy” -&gt; “I am not happy” is a small WER but a huge semantic error.</p>

<h2 id="engineering-inverse-text-normalization-itn">Engineering: Inverse Text Normalization (ITN)</h2>

<p>ASR output: “i have twenty dollars”
User wants: “I have $20.”</p>

<p><strong>ITN</strong> is the process of converting spoken-form text to written-form text.
<strong>Techniques:</strong></p>
<ol>
  <li><strong>FST (Finite State Transducers):</strong> Rule-based grammars (Nvidia NeMo). Fast, deterministic.</li>
  <li><strong>LLM Rewriting:</strong> “Rewrite this transcript to be formatted correctly.” Slower, but handles complex cases (“Call 1-800-FLOWERS”).</li>
</ol>

<h2 id="advanced-topic-ctc-vs-transducer-vs-attention">Advanced Topic: CTC vs. Transducer vs. Attention</h2>

<p>How does the model align Audio (T frames) to Text (L tokens)? <code class="language-plaintext highlighter-rouge">T &gt;&gt; L</code>.</p>

<p><strong>1. CTC (Connectionist Temporal Classification):</strong></p>
<ul>
  <li>Output a probability for every frame.</li>
  <li>Merge repeats: <code class="language-plaintext highlighter-rouge">cc-aa-t</code> -&gt; <code class="language-plaintext highlighter-rouge">cat</code>.</li>
  <li><strong>Pros:</strong> Fast, parallel.</li>
  <li><strong>Cons:</strong> Conditional independence assumption (frame <code class="language-plaintext highlighter-rouge">t</code> doesn’t know about <code class="language-plaintext highlighter-rouge">t-1</code>).</li>
</ul>

<p><strong>2. RNN-Transducer (RNN-T):</strong></p>
<ul>
  <li>Has a “Prediction Network” (Language Model) that feeds back previous tokens.</li>
  <li><strong>Pros:</strong> Streaming friendly, better accuracy than CTC.</li>
  <li><strong>Cons:</strong> Hard to train (memory intensive).</li>
</ul>

<p><strong>3. Attention (Encoder-Decoder):</strong></p>
<ul>
  <li>The Decoder attends to the entire Encoder output.</li>
  <li><strong>Pros:</strong> Best accuracy (Global context).</li>
  <li><strong>Cons:</strong> Not streaming friendly (requires full audio). <strong>Perfect for Batch.</strong></li>
</ul>

<h2 id="deep-dive-voice-activity-detection-vad">Deep Dive: Voice Activity Detection (VAD)</h2>

<p>In Batch processing, VAD is a <strong>Cost Optimization</strong>.
If 50% of the recording is silence, VAD saves 50% of the GPU/API cost.</p>

<p><strong>Silero VAD:</strong>
The current state-of-the-art open-source VAD.</p>
<ul>
  <li><strong>Model:</strong> Enterprise-grade DNN.</li>
  <li><strong>Size:</strong> &lt; 1MB.</li>
  <li><strong>Speed:</strong> &lt; 1ms per chunk.</li>
</ul>

<p><strong>Pipeline:</strong></p>
<ol>
  <li>Run VAD. Get timestamps <code class="language-plaintext highlighter-rouge">[(0, 10), (15, 20)]</code>.</li>
  <li>Crop audio.</li>
  <li>Batch the speech segments.</li>
  <li>Send to Whisper.</li>
  <li>Re-align timestamps to original file.</li>
</ol>

<h2 id="appendix-b-interview-questions">Appendix B: Interview Questions</h2>

<ol>
  <li>
    <p><strong>Q:</strong> “Why is Whisper better than Wav2Vec 2.0?”
<strong>A:</strong> Wav2Vec 2.0 is Self-Supervised (needs fine-tuning). Whisper is Weakly Supervised (trained on labeled data). Whisper handles punctuation and casing out-of-the-box.</p>
  </li>
  <li><strong>Q:</strong> “How do you handle ‘Hallucinations’ in Whisper?”
<strong>A:</strong>
    <ul>
      <li><strong>Beam Search:</strong> Increase beam size.</li>
      <li><strong>Temperature Fallback:</strong> If log-prob is low, increase temperature.</li>
      <li><strong>VAD:</strong> Don’t feed silence to Whisper (it tries to transcribe noise as words).</li>
    </ul>
  </li>
  <li><strong>Q:</strong> “What is the difference between Speaker Diarization and Speaker Identification?”
<strong>A:</strong>
    <ul>
      <li><strong>Diarization:</strong> “Who spoke when?” (Speaker A, Speaker B). No names.</li>
      <li><strong>Identification:</strong> “Is this Elon Musk?” (Matches against a database of voice prints).</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-conformer-architecture-the-macaron-net">Deep Dive: Conformer Architecture (The “Macaron” Net)</h2>

<p>Whisper uses Transformers. But Google uses <strong>Conformers</strong>.
<strong>Idea:</strong> Transformers are good at global context (Attention). CNNs are good at local features (Edges).
<strong>Conformer = CNN + Transformer.</strong></p>

<p><strong>The Block:</strong></p>
<ol>
  <li><strong>Feed Forward (Half-Step):</strong> Like a Macaron sandwich.</li>
  <li><strong>Self-Attention:</strong> Captures long-range dependencies.</li>
  <li><strong>Convolution Module:</strong> Captures local patterns (phonemes).</li>
  <li><strong>Feed Forward (Half-Step).</strong></li>
  <li><strong>Layer Norm.</strong></li>
</ol>

<p><strong>Why?</strong> It converges faster and requires less data than pure Transformers for speech.</p>

<h2 id="deep-dive-specaugment-data-augmentation">Deep Dive: SpecAugment (Data Augmentation)</h2>

<p>How do you prevent overfitting in ASR?
<strong>SpecAugment:</strong> Augment the Spectrogram, not the Audio.</p>

<p><strong>Transformations:</strong></p>
<ol>
  <li><strong>Time Warping:</strong> Stretch/squeeze parts of the spectrogram.</li>
  <li><strong>Frequency Masking:</strong> Zero out a block of frequencies (Simulates a broken mic).</li>
  <li><strong>Time Masking:</strong> Zero out a block of time (Simulates packet loss).</li>
</ol>

<p><strong>Impact:</strong> It forces the model not to rely on any single frequency band or time slice. It learns robust features.</p>

<h2 id="system-design-privacy-and-pii-redaction">System Design: Privacy and PII Redaction</h2>

<p><strong>Scenario:</strong> You are transcribing Call Center audio. It contains Credit Card numbers.
<strong>Requirement:</strong> Redact PII (Personally Identifiable Information).</p>

<p><strong>Pipeline:</strong></p>
<ol>
  <li><strong>ASR:</strong> Transcribe audio to text + timestamps.</li>
  <li><strong>NER (Named Entity Recognition):</strong> Run a BERT model to find <code class="language-plaintext highlighter-rouge">[CREDIT_CARD]</code>, <code class="language-plaintext highlighter-rouge">[PHONE_NUMBER]</code>.</li>
  <li><strong>Redaction:</strong>
    <ul>
      <li><strong>Text:</strong> Replace with <code class="language-plaintext highlighter-rouge">[REDACTED]</code>.</li>
      <li><strong>Audio:</strong> Beep out the segment using the timestamps.</li>
    </ul>
  </li>
</ol>

<p><strong>Challenge:</strong> “My name is <strong>Art</strong>” vs “This is <strong>Art</strong>”. NER is hard on lowercase ASR output.
<strong>Solution:</strong> Use a Truecasing model first.</p>

<h2 id="engineering-gpu-inference-optimization">Engineering: GPU Inference Optimization</h2>

<p>Batch processing is expensive. How do we make it cheaper?</p>

<p><strong>1. Dynamic Batching:</strong></p>
<ul>
  <li>Don’t run 1 file at a time.</li>
  <li>Pack 32 files into a batch.</li>
  <li><strong>Padding:</strong> Pad all files to the length of the longest file.</li>
  <li><strong>Sorting:</strong> Sort files by duration <em>before</em> batching to minimize padding (and wasted compute).</li>
</ul>

<p><strong>2. TensorRT / ONNX Runtime:</strong></p>
<ul>
  <li>Compile the PyTorch model to TensorRT.</li>
  <li>Fuses layers (Conv + ReLU).</li>
  <li>Quantization (FP16 or INT8).</li>
  <li><strong>Speedup:</strong> 2x - 5x.</li>
</ul>

<p><strong>3. Flash Attention:</strong></p>
<ul>
  <li>IO-aware exact attention.</li>
  <li>Reduces memory usage from (O(N^2)) to (O(N)). Allows processing 1-hour files in one go.</li>
</ul>

<h2 id="advanced-topic-multilingual-asr">Advanced Topic: Multilingual ASR</h2>

<p><strong>Problem:</strong> Code-switching (“Hindi-English”).
<strong>Solution:</strong></p>
<ol>
  <li><strong>Language ID:</strong> Predict language every 5 seconds.</li>
  <li><strong>Multilingual Model:</strong> Train one model on 100 languages (Whisper).
    <ul>
      <li>It learns a shared representation of “Speech”.</li>
      <li>It can even do <strong>Zero-Shot Translation</strong> (Speech in French -&gt; Text in English).</li>
    </ul>
  </li>
</ol>

<h2 id="case-study-spotify-podcast-transcription">Case Study: Spotify Podcast Transcription</h2>

<p><strong>Goal:</strong> Transcribe 5 million podcasts for Search.
<strong>Challenges:</strong></p>
<ul>
  <li><strong>Music:</strong> Podcasts have intro music. ASR tries to transcribe lyrics.</li>
  <li><strong>Overlapping Speech:</strong> 3 people laughing.</li>
  <li><strong>Length:</strong> Joe Rogan is 3 hours long.</li>
</ul>

<p><strong>Solution:</strong></p>
<ol>
  <li><strong>Music Detection:</strong> Remove music segments.</li>
  <li><strong>Chunking:</strong> Split into 30s chunks with 5s overlap.</li>
  <li><strong>Deduplication:</strong> Merge the overlap regions using “Longest Common Subsequence”.</li>
</ol>

<h2 id="appendix-c-advanced-interview-questions">Appendix C: Advanced Interview Questions</h2>

<ol>
  <li>
    <p><strong>Q:</strong> “How does CTC Loss handle alignment?”
<strong>A:</strong> It introduces a “blank” token <code class="language-plaintext highlighter-rouge">&lt;eps&gt;</code>. <code class="language-plaintext highlighter-rouge">c-a-t</code> aligns to <code class="language-plaintext highlighter-rouge">c &lt;eps&gt; a &lt;eps&gt; t</code>. It sums over all possible alignments that produce the target text.</p>
  </li>
  <li><strong>Q:</strong> “What is the difference between Online and Offline Diarization?”
<strong>A:</strong>
    <ul>
      <li><strong>Online:</strong> Must decide “Who is speaking?” <em>now</em>. Hard.</li>
      <li><strong>Offline:</strong> Can look at the whole file. Can run clustering (Spectral/AHC) on all embeddings. Much better accuracy.</li>
    </ul>
  </li>
  <li><strong>Q:</strong> “How do you optimize ASR for a specific domain (e.g., Medical)?”
<strong>A:</strong>
    <ul>
      <li><strong>Fine-tuning:</strong> Train the acoustic model on medical audio.</li>
      <li><strong>Language Model Fusion:</strong> Train a text-only LM on medical journals. Fuse it with the ASR output during decoding (Shallow Fusion).</li>
    </ul>
  </li>
</ol>

<h2 id="deep-dive-beam-search-decoding">Deep Dive: Beam Search Decoding</h2>

<p>The model outputs probabilities for each token: <code class="language-plaintext highlighter-rouge">P(token | audio)</code>.
<strong>Greedy Search:</strong> Pick the highest probability token at each step.</p>
<ul>
  <li>Problem: It misses the global optimum. “The” (0.9) -&gt; “cat” (0.1) might be worse than “A” (0.4) -&gt; “dog” (0.8).</li>
</ul>

<p><strong>Beam Search:</strong>
Keep the top <code class="language-plaintext highlighter-rouge">K</code> (Beam Width) hypotheses alive at each step.</p>
<ol>
  <li>Start with <code class="language-plaintext highlighter-rouge">[&lt;s&gt;]</code>.</li>
  <li>Expand all <code class="language-plaintext highlighter-rouge">K</code> paths by 1 token.</li>
  <li>Calculate score: <code class="language-plaintext highlighter-rouge">log(P(path))</code>.</li>
  <li>Keep top <code class="language-plaintext highlighter-rouge">K</code>.</li>
  <li>Repeat.</li>
</ol>

<p><strong>Beam Width Trade-off:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">K=1</code>: Greedy (Fast, Bad).</li>
  <li><code class="language-plaintext highlighter-rouge">K=5</code>: Good balance (Whisper default).</li>
  <li><code class="language-plaintext highlighter-rouge">K=100</code>: Diminishing returns, very slow.</li>
</ul>

<h2 id="advanced-topic-language-model-integration-shallow-vs-deep-fusion">Advanced Topic: Language Model Integration (Shallow vs. Deep Fusion)</h2>

<p>ASR models know phonetics. Language Models (GPT) know grammar.
How do we combine them?</p>

<p><strong>1. Shallow Fusion:</strong>
<code class="language-plaintext highlighter-rouge">Score = log P_ASR(y|x) + lambda * log P_LM(y)</code></p>
<ul>
  <li>We interpolate the scores during Beam Search.</li>
  <li><strong>Pros:</strong> No retraining of ASR. Can swap LMs easily (Medical LM, Legal LM).</li>
</ul>

<p><strong>2. Deep Fusion:</strong></p>
<ul>
  <li>Concatenate the hidden states of ASR and LM <em>before</em> the softmax layer.</li>
  <li><strong>Pros:</strong> Better integration.</li>
  <li><strong>Cons:</strong> Requires retraining.</li>
</ul>

<h2 id="system-design-audio-fingerprinting-shazam">System Design: Audio Fingerprinting (Shazam)</h2>

<p><strong>Problem:</strong> Identify the song in the background.
<strong>Algorithm:</strong></p>
<ol>
  <li><strong>Spectrogram:</strong> Convert audio to Time-Frequency.</li>
  <li><strong>Peaks:</strong> Find local maxima (constellation map).</li>
  <li><strong>Hashes:</strong> Form pairs of peaks (Anchor point + Target point).
    <ul>
      <li><code class="language-plaintext highlighter-rouge">Hash = (Freq1, Freq2, DeltaTime)</code></li>
    </ul>
  </li>
  <li><strong>Database:</strong> Store <code class="language-plaintext highlighter-rouge">Hash -&gt; (SongID, AbsoluteTime)</code>.</li>
  <li><strong>Query:</strong> Match hashes. Find a cluster of matches with consistent time offset.</li>
</ol>

<h2 id="engineering-ffmpeg-tricks-for-speech">Engineering: FFMpeg Tricks for Speech</h2>

<p>FFMpeg is the Swiss Army Knife of Batch Processing.</p>

<p><strong>1. Normalization (Loudness):</strong>
<code class="language-plaintext highlighter-rouge">ffmpeg -i input.wav -filter:a loudnorm output.wav</code>
Ensures consistent volume (-14 LUFS).</p>

<p><strong>2. Silence Removal:</strong>
<code class="language-plaintext highlighter-rouge">ffmpeg -i input.wav -af silenceremove=stop_periods=-1:stop_duration=1:stop_threshold=-50dB output.wav</code>
Trims silence &gt; 1 second.</p>

<p><strong>3. Speed Up (without changing pitch):</strong>
<code class="language-plaintext highlighter-rouge">ffmpeg -i input.wav -filter:a atempo=1.5 output.wav</code>
Listen to podcasts at 1.5x.</p>

<h2 id="case-study-alexas-wake-word-detection-cascade">Case Study: Alexa’s Wake Word Detection (Cascade)</h2>

<p>Alexa is always listening. But sending audio to the cloud is expensive (and creepy).
<strong>Solution: Cascade Architecture.</strong></p>

<ol>
  <li><strong>Stage 1 (DSP):</strong> Low power chip. Detects energy. (mW).</li>
  <li><strong>Stage 2 (Tiny NN):</strong> On-device model. Detects “Alexa”. (High Recall, Low Precision).</li>
  <li><strong>Stage 3 (Cloud):</strong> Full ASR. Verifies “Alexa” and processes the command. (High Precision).</li>
</ol>

<h2 id="appendix-d-the-long-tail-problem">Appendix D: The “Long-Tail” Problem</h2>

<p>ASR works great for “Standard American English”.
It fails for:</p>
<ul>
  <li><strong>Accents:</strong> Scottish, Singaporean.</li>
  <li><strong>Stuttering:</strong> “I… I… want to go”.</li>
  <li><strong>Code Switching:</strong> “Chalo let’s go”.</li>
</ul>

<p><strong>Solution:</strong></p>
<ul>
  <li><strong>Data Augmentation:</strong> Add noise, speed perturbation.</li>
  <li><strong>Transfer Learning:</strong> Fine-tune on specific accent datasets (Common Voice).</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>Batch processing allows us to use the “Heavy Artillery” of AI.
We can use the biggest models, look at the entire context, and perform complex post-processing.
<em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#diarization" class="page__taxonomy-item p-category" rel="tag">diarization</a><span class="sep">, </span>
    
      <a href="/tags/#forced-alignment" class="page__taxonomy-item p-category" rel="tag">forced-alignment</a><span class="sep">, </span>
    
      <a href="/tags/#kaldi" class="page__taxonomy-item p-category" rel="tag">kaldi</a><span class="sep">, </span>
    
      <a href="/tags/#offline-asr" class="page__taxonomy-item p-category" rel="tag">offline-asr</a><span class="sep">, </span>
    
      <a href="/tags/#whisper" class="page__taxonomy-item p-category" rel="tag">whisper</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/categories/#pipelines" class="page__taxonomy-item p-category" rel="tag">pipelines</a><span class="sep">, </span>
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0026-level-order-traversal/" rel="permalink">Binary Tree Level Order Traversal
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How do you print a corporate hierarchy level by level? CEO first, then VPs, then Managers…
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0026-batch-processing-pipelines/" rel="permalink">Batch Processing Pipelines
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Not everything needs to be real-time. Sometimes, “tomorrow morning” is fast enough.
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Batch+Speech+Processing%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0026-batch-speech-processing%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0026-batch-speech-processing%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0026-batch-speech-processing/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0025-speech-quality-monitoring/" class="pagination--pager" title="Speech Quality Monitoring">Previous</a>
    
    
      <a href="/speech-tech/0027-end-to-end-speech-model-design/" class="pagination--pager" title="End-to-End Speech Model Design">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
