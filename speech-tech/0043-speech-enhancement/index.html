<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Speech Enhancement - Arun Baby</title>
<meta name="description" content="“Extracting clear speech from the noise of the real world.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Speech Enhancement">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0043-speech-enhancement/">


  <meta property="og:description" content="“Extracting clear speech from the noise of the real world.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Speech Enhancement">
  <meta name="twitter:description" content="“Extracting clear speech from the noise of the real world.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0043-speech-enhancement/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0043-speech-enhancement/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Speech Enhancement">
    <meta itemprop="description" content="“Extracting clear speech from the noise of the real world.”">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0043-speech-enhancement/" itemprop="url">Speech Enhancement
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction">1. Introduction</a></li><li><a href="#2-types-of-degradation">2. Types of Degradation</a><ul><li><a href="#21-additive-noise">2.1. Additive Noise</a></li><li><a href="#22-reverberation">2.2. Reverberation</a></li><li><a href="#23-clipping--distortion">2.3. Clipping &amp; Distortion</a></li></ul></li><li><a href="#3-classic-signal-processing-approaches">3. Classic Signal Processing Approaches</a><ul><li><a href="#31-spectral-subtraction">3.1. Spectral Subtraction</a></li><li><a href="#32-wiener-filtering">3.2. Wiener Filtering</a></li><li><a href="#33-noise-estimation">3.3. Noise Estimation</a></li></ul></li><li><a href="#4-deep-learning-approaches">4. Deep Learning Approaches</a><ul><li><a href="#41-masking-based-methods">4.1. Masking-Based Methods</a></li><li><a href="#42-mapping-based-methods">4.2. Mapping-Based Methods</a></li><li><a href="#43-waveform-based-methods">4.3. Waveform-Based Methods</a></li></ul></li><li><a href="#5-architectures-for-speech-enhancement">5. Architectures for Speech Enhancement</a><ul><li><a href="#51-u-net">5.1. U-Net</a></li><li><a href="#52-conv-tasnet">5.2. Conv-TasNet</a></li><li><a href="#53-dccrn-deep-complex-cnn">5.3. DCCRN (Deep Complex CNN)</a></li></ul></li><li><a href="#6-loss-functions">6. Loss Functions</a><ul><li><a href="#61-mean-squared-error-mse">6.1. Mean Squared Error (MSE)</a></li><li><a href="#62-scale-invariant-sdr-si-sdr">6.2. Scale-Invariant SDR (SI-SDR)</a></li><li><a href="#63-perceptual-loss">6.3. Perceptual Loss</a></li></ul></li><li><a href="#7-system-design-real-time-denoising">7. System Design: Real-Time Denoising</a></li><li><a href="#8-production-case-study-zoom-noise-cancellation">8. Production Case Study: Zoom Noise Cancellation</a></li><li><a href="#9-production-case-study-apple-airpods-pro">9. Production Case Study: Apple AirPods Pro</a></li><li><a href="#10-datasets">10. Datasets</a></li><li><a href="#11-evaluation-metrics">11. Evaluation Metrics</a></li><li><a href="#12-interview-questions">12. Interview Questions</a></li><li><a href="#13-common-mistakes">13. Common Mistakes</a></li><li><a href="#14-deep-dive-generative-approaches">14. Deep Dive: Generative Approaches</a><ul><li><a href="#141-diffusion-models-for-speech-enhancement">14.1. Diffusion Models for Speech Enhancement</a></li><li><a href="#142-gan-based-enhancement">14.2. GAN-Based Enhancement</a></li></ul></li><li><a href="#15-future-trends">15. Future Trends</a></li><li><a href="#16-conclusion">16. Conclusion</a></li><li><a href="#17-deep-dive-rnnoise">17. Deep Dive: RNNoise</a></li><li><a href="#18-deep-dive-dereverberation">18. Deep Dive: Dereverberation</a></li><li><a href="#19-multi-channel-speech-enhancement">19. Multi-Channel Speech Enhancement</a></li><li><a href="#20-implementation-real-time-enhancement-pipeline">20. Implementation: Real-Time Enhancement Pipeline</a></li><li><a href="#21-training-a-speech-enhancement-model">21. Training a Speech Enhancement Model</a></li><li><a href="#22-handling-difficult-noise-types">22. Handling Difficult Noise Types</a></li><li><a href="#23-integration-with-asr">23. Integration with ASR</a></li><li><a href="#24-latency-analysis">24. Latency Analysis</a></li><li><a href="#25-deployment-considerations">25. Deployment Considerations</a></li><li><a href="#26-mastery-checklist">26. Mastery Checklist</a></li><li><a href="#27-conclusion">27. Conclusion</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Extracting clear speech from the noise of the real world.”</strong></p>

<h2 id="1-introduction">1. Introduction</h2>

<p><strong>Speech Enhancement</strong> is the task of improving the quality and intelligibility of speech signals degraded by noise, reverberation, or other distortions.</p>

<p><strong>Applications:</strong></p>
<ul>
  <li><strong>Voice Assistants:</strong> Improve ASR accuracy in noisy environments.</li>
  <li><strong>Hearing Aids:</strong> Help hearing-impaired users understand speech.</li>
  <li><strong>Video Conferencing:</strong> Remove background noise (Zoom, Teams).</li>
  <li><strong>Telecommunications:</strong> Improve call quality.</li>
  <li><strong>Forensics:</strong> Enhance speech in recordings.</li>
</ul>

<h2 id="2-types-of-degradation">2. Types of Degradation</h2>

<h3 id="21-additive-noise">2.1. Additive Noise</h3>

<p><strong>Model:</strong> <code class="language-plaintext highlighter-rouge">y(t) = x(t) + n(t)</code></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">x(t)</code>: Clean speech.</li>
  <li><code class="language-plaintext highlighter-rouge">n(t)</code>: Noise (fan, traffic, babble).</li>
  <li><code class="language-plaintext highlighter-rouge">y(t)</code>: Noisy speech.</li>
</ul>

<p><strong>Noise Types:</strong></p>
<ul>
  <li><strong>Stationary:</strong> Constant spectrum (fan, AC).</li>
  <li><strong>Non-Stationary:</strong> Changing spectrum (babble, music).</li>
</ul>

<h3 id="22-reverberation">2.2. Reverberation</h3>

<p><strong>Model:</strong> <code class="language-plaintext highlighter-rouge">y(t) = x(t) * h(t)</code></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">h(t)</code>: Room impulse response (RIR).</li>
  <li>Convolution spreads energy over time.</li>
</ul>

<p><strong>Effects:</strong></p>
<ul>
  <li><strong>Early Reflections:</strong> Slight echoes (helpful for perception).</li>
  <li><strong>Late Reverberation:</strong> Smearing, reduced intelligibility.</li>
</ul>

<h3 id="23-clipping--distortion">2.3. Clipping &amp; Distortion</h3>

<p><strong>Cause:</strong> Microphone saturation, codec artifacts.
<strong>Effect:</strong> Waveform is “cut off” at peaks.</p>

<h2 id="3-classic-signal-processing-approaches">3. Classic Signal Processing Approaches</h2>

<h3 id="31-spectral-subtraction">3.1. Spectral Subtraction</h3>

<p><strong>Idea:</strong> Estimate noise spectrum, subtract from noisy spectrum.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Estimate noise spectrum <code class="language-plaintext highlighter-rouge">\hat{N}(f)</code> from silence regions.</li>
  <li>Subtract: <code class="language-plaintext highlighter-rouge">\hat{X}(f) = Y(f) - \alpha \hat{N}(f)</code>.</li>
  <li>Apply flooring to avoid negative values.</li>
</ol>

<p><strong>Problems:</strong></p>
<ul>
  <li><strong>Musical Noise:</strong> Residual tones from random noise estimation errors.</li>
  <li><strong>Non-Stationary Noise:</strong> Fails when noise changes rapidly.</li>
</ul>

<h3 id="32-wiener-filtering">3.2. Wiener Filtering</h3>

<p><strong>Idea:</strong> Optimal linear filter to minimize MSE between estimated and clean speech.</p>

<p><strong>Formula:</strong>
<code class="language-plaintext highlighter-rouge">H(f) = \frac{|X(f)|^2}{|X(f)|^2 + |N(f)|^2} = \frac{\text{SNR}(f)}{\text{SNR}(f) + 1}</code></p>

<p><strong>Interpretation:</strong></p>
<ul>
  <li>High SNR: <code class="language-plaintext highlighter-rouge">H(f) \approx 1</code> (pass signal).</li>
  <li>Low SNR: <code class="language-plaintext highlighter-rouge">H(f) \approx 0</code> (suppress).</li>
</ul>

<h3 id="33-noise-estimation">3.3. Noise Estimation</h3>

<p><strong>VAD-Based:</strong></p>
<ul>
  <li>Detect silence (Voice Activity Detection).</li>
  <li>Update noise estimate during silence.</li>
</ul>

<p><strong>MMSE-Based:</strong></p>
<ul>
  <li>Minimum Mean Square Error estimator.</li>
  <li>Assumes noise is a random variable.</li>
</ul>

<h2 id="4-deep-learning-approaches">4. Deep Learning Approaches</h2>

<h3 id="41-masking-based-methods">4.1. Masking-Based Methods</h3>

<p><strong>Idea:</strong> Learn a mask <code class="language-plaintext highlighter-rouge">M(t, f)</code> to apply to the noisy spectrogram.</p>

<p><code class="language-plaintext highlighter-rouge">\hat{X}(t, f) = M(t, f) \cdot Y(t, f)</code></p>

<p><strong>Mask Types:</strong></p>
<ul>
  <li><strong>Ideal Binary Mask (IBM):</strong> <code class="language-plaintext highlighter-rouge">M = 1</code> if SNR &gt; threshold, else <code class="language-plaintext highlighter-rouge">M = 0</code>.</li>
  <li><strong>Ideal Ratio Mask (IRM):</strong> <code class="language-plaintext highlighter-rouge">M = \frac{|X|^2}{|X|^2 + |N|^2}</code>.</li>
  <li><strong>Complex Ideal Ratio Mask (cIRM):</strong> Operates on complex STFT.</li>
</ul>

<h3 id="42-mapping-based-methods">4.2. Mapping-Based Methods</h3>

<p><strong>Idea:</strong> Directly map noisy spectrogram to clean spectrogram.</p>

<p><code class="language-plaintext highlighter-rouge">\hat{X}(t, f) = f_\theta(Y(t, f))</code></p>

<p><strong>Model:</strong> CNN, LSTM, or U-Net.</p>

<h3 id="43-waveform-based-methods">4.3. Waveform-Based Methods</h3>

<p><strong>Idea:</strong> Process raw waveform directly (no STFT).</p>

<p><strong>Models:</strong></p>
<ul>
  <li><strong>WaveNet:</strong> Dilated convolutions.</li>
  <li><strong>Conv-TasNet:</strong> Learned encoder-decoder.</li>
  <li><strong>DEMUCS:</strong> U-Net on waveform.</li>
</ul>

<p><strong>Pros:</strong> No phase estimation needed.
<strong>Cons:</strong> Computationally expensive.</p>

<h2 id="5-architectures-for-speech-enhancement">5. Architectures for Speech Enhancement</h2>

<h3 id="51-u-net">5.1. U-Net</h3>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Encoder: Downsampling convolutions.</li>
  <li>Decoder: Upsampling convolutions.</li>
  <li>Skip Connections: Connect encoder to decoder.</li>
</ul>

<p><strong>Input:</strong> Noisy spectrogram (magnitude).
<strong>Output:</strong> Enhanced spectrogram (or mask).</p>

<p>``python
class UNet(nn.Module):
 def <strong>init</strong>(self):
 super().<strong>init</strong>()
 # Encoder
 self.enc1 = self.conv_block(1, 64)
 self.enc2 = self.conv_block(64, 128)
 self.enc3 = self.conv_block(128, 256)</p>

<p># Decoder
 self.dec3 = self.upconv_block(256, 128)
 self.dec2 = self.upconv_block(256, 64) # 256 because of skip connection
 self.dec1 = self.upconv_block(128, 1)</p>

<p>def forward(self, x):
 e1 = self.enc1(x)
 e2 = self.enc2(F.max_pool2d(e1, 2))
 e3 = self.enc3(F.max_pool2d(e2, 2))</p>

<p>d3 = self.dec3(e3)
 d2 = self.dec2(torch.cat([d3, e2], dim=1))
 d1 = self.dec1(torch.cat([d2, e1], dim=1))</p>

<p>return d1
``</p>

<h3 id="52-conv-tasnet">5.2. Conv-TasNet</h3>

<p><strong>Architecture (Time-domain):</strong></p>
<ol>
  <li><strong>Encoder:</strong> 1D convolution to learned representation.</li>
  <li><strong>Separator:</strong> Temporal Convolutional Network (TCN) to estimate mask.</li>
  <li><strong>Decoder:</strong> Transposed convolution to reconstruct waveform.</li>
</ol>

<p><strong>Pros:</strong> State-of-the-art for speech separation.
<strong>Cons:</strong> High memory for long audio.</p>

<h3 id="53-dccrn-deep-complex-cnn">5.3. DCCRN (Deep Complex CNN)</h3>

<p><strong>Key Feature:</strong> Operates on complex STFT (real + imaginary).</p>

<p><strong>Benefit:</strong> Better phase estimation than magnitude-only methods.</p>

<h2 id="6-loss-functions">6. Loss Functions</h2>

<h3 id="61-mean-squared-error-mse">6.1. Mean Squared Error (MSE)</h3>

<p><code class="language-plaintext highlighter-rouge">L = \frac{1}{T \cdot F} \sum_{t, f} (\hat{X}(t, f) - X(t, f))^2</code></p>

<p><strong>Pros:</strong> Simple, differentiable.
<strong>Cons:</strong> Doesn’t correlate well with perceptual quality.</p>

<h3 id="62-scale-invariant-sdr-si-sdr">6.2. Scale-Invariant SDR (SI-SDR)</h3>

<p><code class="language-plaintext highlighter-rouge">\text{SI-SDR} = 10 \log_{10} \frac{||\alpha x||^2}{||\hat{x} - \alpha x||^2}</code></p>

<p>Where <code class="language-plaintext highlighter-rouge">\alpha = \frac{\langle \hat{x}, x \rangle}{||x||^2}</code>.</p>

<p><strong>Interpretation:</strong> Higher is better. Measures signal-to-distortion ratio.</p>

<h3 id="63-perceptual-loss">6.3. Perceptual Loss</h3>

<p><strong>PESQ (Perceptual Evaluation of Speech Quality):</strong></p>
<ul>
  <li>Intrusive metric (requires clean reference).</li>
  <li>Scores from 1.0 (bad) to 4.5 (excellent).</li>
</ul>

<p><strong>STOI (Short-Time Objective Intelligibility):</strong></p>
<ul>
  <li>Correlates with human intelligibility.</li>
  <li>Range: 0.0 to 1.0.</li>
</ul>

<p><strong>Differentiable Approximations:</strong></p>
<ul>
  <li>Train a neural network to approximate PESQ/STOI.</li>
  <li>Use as a loss function.</li>
</ul>

<h2 id="7-system-design-real-time-denoising">7. System Design: Real-Time Denoising</h2>

<p><strong>Scenario:</strong> Build a noise suppression module for video conferencing.</p>

<p><strong>Requirements:</strong></p>
<ul>
  <li><strong>Latency:</strong> &lt; 20ms.</li>
  <li><strong>CPU/GPU:</strong> Must run on laptop CPUs.</li>
  <li><strong>Quality:</strong> Preserve speech, remove noise.</li>
</ul>

<p><strong>Architecture:</strong></p>

<p><strong>Step 1: Frame Processing</strong></p>
<ul>
  <li>Audio arrives in 20ms frames.</li>
  <li>STFT with 20ms window, 10ms hop.</li>
</ul>

<p><strong>Step 2: Neural Network</strong></p>
<ul>
  <li>Lightweight CNN (e.g., 10 layers).</li>
  <li>Quantized to INT8 for CPU inference.</li>
</ul>

<p><strong>Step 3: Apply Mask</strong></p>
<ul>
  <li>Multiply noisy STFT by predicted mask.</li>
  <li>Inverse STFT to reconstruct waveform.</li>
</ul>

<p><strong>Step 4: Overlap-Add</strong></p>
<ul>
  <li>Combine overlapping frames smoothly.</li>
</ul>

<p><strong>Step 5: Output</strong></p>
<ul>
  <li>Send enhanced audio to speaker.</li>
</ul>

<h2 id="8-production-case-study-zoom-noise-cancellation">8. Production Case Study: Zoom Noise Cancellation</h2>

<p><strong>Model:</strong> RNNoise-inspired, enhanced with CNN.</p>

<p><strong>Features:</strong></p>
<ul>
  <li><strong>18x real-time:</strong> Processes audio 18x faster than it plays.</li>
  <li><strong>CPU-only:</strong> Runs on low-end laptops.</li>
  <li><strong>Adaptive:</strong> Learns user’s environment over time.</li>
</ul>

<p><strong>Training Data:</strong></p>
<ul>
  <li><strong>Clean:</strong> LibriSpeech, VCTK.</li>
  <li><strong>Noise:</strong> AudioSet, FreeSound.</li>
  <li><strong>Augmentation:</strong> Mix at various SNRs, add reverberation.</li>
</ul>

<h2 id="9-production-case-study-apple-airpods-pro">9. Production Case Study: Apple AirPods Pro</h2>

<p><strong>Features:</strong></p>
<ul>
  <li><strong>Active Noise Cancellation (ANC):</strong> Hardware + DSP.</li>
  <li><strong>Transparency Mode:</strong> Pass through environment.</li>
  <li><strong>Adaptive EQ:</strong> Adjust sound based on ear fit.</li>
</ul>

<p><strong>Enhancement:</strong></p>
<ul>
  <li><strong>Microphones:</strong> 2 external, 1 internal.</li>
  <li><strong>Processing:</strong> On-device neural network.</li>
  <li><strong>Integration:</strong> Optimized for Siri voice input.</li>
</ul>

<h2 id="10-datasets">10. Datasets</h2>

<p><strong>1. VCTK:</strong></p>
<ul>
  <li>109 speakers, clean speech.</li>
  <li>Add noise synthetically.</li>
</ul>

<p><strong>2. DNS Challenge (Microsoft):</strong></p>
<ul>
  <li>Large-scale, diverse noise.</li>
  <li>Training and evaluation sets.</li>
</ul>

<p><strong>3. CHiME:</strong></p>
<ul>
  <li>Real-world noisy recordings.</li>
  <li>Multiple noise conditions.</li>
</ul>

<p><strong>4. LibriMix:</strong></p>
<ul>
  <li>Mixed speech for separation.</li>
  <li>Derived from LibriSpeech.</li>
</ul>

<h2 id="11-evaluation-metrics">11. Evaluation Metrics</h2>

<p><strong>Objective:</strong></p>
<ul>
  <li><strong>PESQ:</strong> Perceptual quality (1.0-4.5).</li>
  <li><strong>STOI:</strong> Intelligibility (0.0-1.0).</li>
  <li><strong>SI-SDR:</strong> Signal-to-distortion ratio (dB).</li>
  <li><strong>POLQA:</strong> Next-gen PESQ.</li>
</ul>

<p><strong>Subjective:</strong></p>
<ul>
  <li><strong>MOS (Mean Opinion Score):</strong> Human ratings (1-5).</li>
  <li><strong>ABX Test:</strong> Which sample sounds better?</li>
</ul>

<h2 id="12-interview-questions">12. Interview Questions</h2>

<ol>
  <li><strong>Spectral Subtraction:</strong> How does it work? What are its limitations?</li>
  <li><strong>Wiener Filter:</strong> Derive the optimal filter.</li>
  <li><strong>Masking vs Mapping:</strong> What’s the difference?</li>
  <li><strong>Real-Time Constraints:</strong> How do you achieve &lt;20ms latency?</li>
  <li><strong>Evaluation:</strong> Explain PESQ and STOI.</li>
  <li><strong>Design:</strong> Design a noise cancellation system for hearing aids.</li>
</ol>

<h2 id="13-common-mistakes">13. Common Mistakes</h2>

<ul>
  <li><strong>Ignoring Phase:</strong> Magnitude-only methods produce artifacts.</li>
  <li><strong>Training/Test Mismatch:</strong> Training on synthetic noise, testing on real.</li>
  <li><strong>Overlooking Latency:</strong> Model too large for real-time.</li>
  <li><strong>Suppressing Speech:</strong> Over-aggressive noise removal.</li>
  <li><strong>Ignoring Reverberation:</strong> Many systems only handle additive noise.</li>
</ul>

<h2 id="14-deep-dive-generative-approaches">14. Deep Dive: Generative Approaches</h2>

<h3 id="141-diffusion-models-for-speech-enhancement">14.1. Diffusion Models for Speech Enhancement</h3>

<p><strong>Idea:</strong> Learn to reverse the noising process.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Forward:</strong> Add Gaussian noise to clean speech.</li>
  <li><strong>Reverse:</strong> Train model to predict clean speech from noisy.</li>
  <li><strong>Inference:</strong> Start with noisy speech, iteratively denoise.</li>
</ol>

<p><strong>Pros:</strong> High-quality, handles complex degradations.
<strong>Cons:</strong> Slow (many diffusion steps).</p>

<h3 id="142-gan-based-enhancement">14.2. GAN-Based Enhancement</h3>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Generator:</strong> U-Net that enhances speech.</li>
  <li><strong>Discriminator:</strong> Classifies real vs enhanced.</li>
</ul>

<p><strong>Loss:</strong></p>
<ul>
  <li>Adversarial loss + MSE/SI-SDR.</li>
  <li>Perceptual loss (from pretrained network).</li>
</ul>

<p><strong>Pros:</strong> Sharper, more natural outputs.
<strong>Cons:</strong> Training instability.</p>

<h2 id="15-future-trends">15. Future Trends</h2>

<p><strong>1. Self-Supervised Learning:</strong></p>
<ul>
  <li>Pretrain on large unlabeled audio.</li>
  <li>Fine-tune for enhancement.</li>
</ul>

<p><strong>2. Multi-Task Learning:</strong></p>
<ul>
  <li>Joint enhancement + ASR.</li>
  <li>Joint enhancement + diarization.</li>
</ul>

<p><strong>3. On-Device Enhancement:</strong></p>
<ul>
  <li>Run on smartphones, earbuds.</li>
  <li>Neural Processing Units (NPUs).</li>
</ul>

<p><strong>4. Personalized Enhancement:</strong></p>
<ul>
  <li>Adapt to user’s voice and environment.</li>
  <li>Few-shot learning.</li>
</ul>

<h2 id="16-conclusion">16. Conclusion</h2>

<p>Speech enhancement is critical for making AI systems work in the real world. Whether it’s helping Siri understand you in a noisy café or enabling clear video calls, enhancement is the first line of defense against acoustic degradation.</p>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Classic Methods:</strong> Spectral subtraction, Wiener filtering.</li>
  <li><strong>Deep Learning:</strong> Masking (U-Net), waveform (Conv-TasNet).</li>
  <li><strong>Metrics:</strong> PESQ, STOI, SI-SDR.</li>
  <li><strong>Production:</strong> Latency, CPU efficiency, generalization.</li>
  <li><strong>Future:</strong> Diffusion models, on-device processing.</li>
</ul>

<p>Mastering speech enhancement enables you to build robust speech systems that work in any environment.</p>

<h2 id="17-deep-dive-rnnoise">17. Deep Dive: RNNoise</h2>

<p><strong>RNNoise</strong> is a lightweight, real-time noise suppression algorithm.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Input:</strong> 22 features (pitch, spectral bands).</li>
  <li><strong>Model:</strong> GRU with 96 units.</li>
  <li><strong>Output:</strong> Gains per frequency band.</li>
</ul>

<p><strong>Key Innovations:</strong></p>
<ul>
  <li><strong>Handcrafted Features:</strong> Instead of spectrogram, use pitch, spectral derivative.</li>
  <li><strong>Pitch Filtering:</strong> Use pitch information to enhance periodic speech.</li>
  <li><strong>Tiny Model:</strong> &lt;100KB, runs on embedded devices.</li>
</ul>

<p><strong>Performance:</strong></p>
<ul>
  <li><strong>18x Real-Time:</strong> On single CPU core.</li>
  <li><strong>Quality:</strong> Comparable to larger neural networks.</li>
</ul>

<p><strong>Code (C with SIMD):</strong>
``c
// RNNoise inference loop
for (int i = 0; i &lt; frame_size; i++) {
 // Extract features
 float features[22] = compute_features(frame[i]);</p>

<p>// GRU inference
 float gains[22] = gru_forward(features);</p>

<p>// Apply gains to frequency bands
 apply_gains(frame[i], gains);
}
``</p>

<h2 id="18-deep-dive-dereverberation">18. Deep Dive: Dereverberation</h2>

<p><strong>Problem:</strong> Remove room reflections from speech.</p>

<p><strong>Approaches:</strong></p>

<p><strong>1. Weighted Prediction Error (WPE):</strong></p>
<ul>
  <li>Model late reverberation as autoregressive process.</li>
  <li>Predict reverberant tail, subtract.</li>
</ul>

<p><strong>2. Neural Dereverberation:</strong></p>
<ul>
  <li>Train on pairs (reverberant, clean).</li>
  <li>Similar architecture to denoising.</li>
</ul>

<p><strong>3. Beamforming:</strong></p>
<ul>
  <li>Use microphone array to focus on direct sound.</li>
  <li>Suppress reflections from other directions.</li>
</ul>

<p><strong>Metric:</strong> Speech-to-Reverberation Ratio (SRR).</p>

<h2 id="19-multi-channel-speech-enhancement">19. Multi-Channel Speech Enhancement</h2>

<p><strong>Scenario:</strong> Multiple microphones (phone with 2 mics, smart speaker with 6 mics).</p>

<p><strong>Algorithm Pipeline:</strong></p>
<ol>
  <li><strong>Beamforming:</strong> Combine channels to enhance direction of interest.</li>
  <li><strong>Post-Filter:</strong> Apply single-channel enhancement to beamformed signal.</li>
</ol>

<p><strong>Beamforming Types:</strong></p>
<ul>
  <li><strong>Delay-and-Sum:</strong> Simple, delays based on geometry.</li>
  <li><strong>MVDR (Minimum Variance Distortionless Response):</strong> Optimal, requires covariance estimation.</li>
  <li><strong>Neural Beamformer:</strong> Learn beamforming weights with neural network.</li>
</ul>

<p><strong>Example (MVDR):</strong>
``python
def mvdr_beamformer(stft, steering_vector, noise_covariance):
 # stft: (channels, time, freq)
 # steering_vector: (channels, freq)
 # noise_covariance: (freq, channels, channels)</p>

<p>output = np.zeros((stft.shape[1], stft.shape[2]), dtype=complex)</p>

<p>for f in range(stft.shape[2]):
 Rn_inv = np.linalg.inv(noise_covariance[f])
 d = steering_vector[:, f]</p>

<p># MVDR weights
 w = Rn_inv @ d / (d.conj().T @ Rn_inv @ d)</p>

<p># Apply to all time frames
 output[:, f] = w.conj().T @ stft[:, :, f]</p>

<p>return output
``</p>

<h2 id="20-implementation-real-time-enhancement-pipeline">20. Implementation: Real-Time Enhancement Pipeline</h2>

<p><strong>Step-by-Step:</strong>
``python
import numpy as np
from scipy.io import wavfile
import torch</p>

<h1 id="1-load-model">1. Load model</h1>
<p>model = load_enhancement_model(‘unet_enhancement.pt’)
model.eval()</p>

<h1 id="2-audio-parameters">2. Audio parameters</h1>
<p>FRAME_SIZE = 512
HOP_SIZE = 256
SAMPLE_RATE = 16000</p>

<h1 id="3-processing-loop">3. Processing loop</h1>
<p>def enhance_audio(input_wav, output_wav):
 sr, audio = wavfile.read(input_wav)
 audio = audio.astype(np.float32) / 32768</p>

<p># STFT
 stft = librosa.stft(audio, n_fft=FRAME_SIZE, hop_length=HOP_SIZE)
 magnitude = np.abs(stft)
 phase = np.angle(stft)</p>

<p># Enhance with model
 with torch.no_grad():
 mag_input = torch.tensor(magnitude).unsqueeze(0).unsqueeze(0)
 mask = model(mag_input).squeeze().numpy()</p>

<p># Apply mask
 enhanced_magnitude = magnitude * mask</p>

<p># Inverse STFT
 enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
 enhanced_audio = librosa.istft(enhanced_stft, hop_length=HOP_SIZE)</p>

<p># Save
 wavfile.write(output_wav, sr, (enhanced_audio * 32768).astype(np.int16))
``</p>

<h2 id="21-training-a-speech-enhancement-model">21. Training a Speech Enhancement Model</h2>

<p><strong>Step 1: Data Preparation</strong>
``python</p>
<h1 id="mix-clean-speech-with-noise-at-random-snr">Mix clean speech with noise at random SNR</h1>
<p>def create_noisy_mixture(clean, noise, snr_db):
 clean_power = np.mean(clean ** 2)
 noise_power = np.mean(noise ** 2)</p>

<p># Calculate required noise scale
 snr_linear = 10 ** (snr_db / 10)
 noise_scale = np.sqrt(clean_power / (snr_linear * noise_power))</p>

<p>noisy = clean + noise_scale * noise
 return noisy, clean
``</p>

<p><strong>Step 2: Define Model (U-Net)</strong>
``python
class EnhancementUNet(nn.Module):
 def <strong>init</strong>(self):
 super().<strong>init</strong>()
 self.encoder = nn.Sequential(
 nn.Conv2d(1, 64, kernel_size=3, padding=1),
 nn.ReLU(),
 nn.MaxPool2d(2),
 # … more layers
 )
 self.decoder = nn.Sequential(
 nn.ConvTranspose2d(256, 64, kernel_size=2, stride=2),
 nn.ReLU(),
 # … more layers
 nn.Conv2d(64, 1, kernel_size=1),
 nn.Sigmoid() # Output mask in [0, 1]
 )</p>

<p>def forward(self, x):
 enc = self.encoder(x)
 mask = self.decoder(enc)
 return mask
``</p>

<p><strong>Step 3: Training Loop</strong>
``python
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)</p>

<p>for epoch in range(100):
 for noisy_batch, clean_batch in dataloader:
 noisy_mag = stft(noisy_batch)
 clean_mag = stft(clean_batch)</p>

<p># Target mask (IRM)
 target_mask = clean_mag ** 2 / (clean_mag ** 2 + noise_mag ** 2 + 1e-8)</p>

<p># Forward
 pred_mask = model(noisy_mag)</p>

<p># Loss
 loss = criterion(pred_mask, target_mask)</p>

<p># Backward
 optimizer.zero_grad()
 loss.backward()
 optimizer.step()
``</p>

<h2 id="22-handling-difficult-noise-types">22. Handling Difficult Noise Types</h2>

<p><strong>Music:</strong></p>
<ul>
  <li>Challenge: Music has similar spectral structure to speech.</li>
  <li>Solution: Train with music as a noise type.</li>
</ul>

<p><strong>Babble:</strong></p>
<ul>
  <li>Challenge: Multiple speakers overlap with target.</li>
  <li>Solution: Speaker separation before enhancement.</li>
</ul>

<p><strong>Impulsive Noise (clicks, pops):</strong></p>
<ul>
  <li>Challenge: Short bursts, hard to estimate.</li>
  <li>Solution: Median filtering + neural enhancement.</li>
</ul>

<p><strong>Wind:</strong></p>
<ul>
  <li>Challenge: Low-frequency, fluctuating.</li>
  <li>Solution: High-pass filter + neural enhancement.</li>
</ul>

<h2 id="23-integration-with-asr">23. Integration with ASR</h2>

<p><strong>Pre-Enhancement:</strong></p>
<ul>
  <li>Enhance audio before feeding to ASR.</li>
  <li>Improves WER in noisy conditions.</li>
</ul>

<p><strong>Joint Training:</strong></p>
<ul>
  <li>Train enhancement + ASR end-to-end.</li>
  <li>Optimize directly for recognition, not perceptual quality.</li>
</ul>

<p><strong>Example (Joint Pipeline):</strong>
<code class="language-plaintext highlighter-rouge">
Audio → Enhancement → ASR → Text
 ↑ ↓
 Joint Loss (WER + Perceptual)
</code></p>

<h2 id="24-latency-analysis">24. Latency Analysis</h2>

<p><strong>Pipeline Latency:</strong></p>
<ul>
  <li><strong>Frame Size:</strong> 20ms (typical).</li>
  <li><strong>STFT:</strong> 10ms (computation).</li>
  <li><strong>Neural Network:</strong> 5-20ms (depends on model size).</li>
  <li><strong>Inverse STFT:</strong> 5ms.</li>
  <li><strong>Total:</strong> 40-55ms (not including buffer delays).</li>
</ul>

<p><strong>Reducing Latency:</strong></p>
<ul>
  <li>Smaller models (quantized, pruned).</li>
  <li>Smaller frame sizes (10ms).</li>
  <li>GPU/NPU acceleration.</li>
</ul>

<h2 id="25-deployment-considerations">25. Deployment Considerations</h2>

<p><strong>Mobile (iOS/Android):</strong></p>
<ul>
  <li>Use TensorFlow Lite or Core ML.</li>
  <li>Quantize to INT8.</li>
  <li>Target: &lt;10ms per frame.</li>
</ul>

<p><strong>Embedded (Raspberry Pi, STM32):</strong></p>
<ul>
  <li>Use C/C++ with SIMD.</li>
  <li>Very small model (&lt;100KB).</li>
  <li>Target: &lt;5ms per frame.</li>
</ul>

<p><strong>Cloud:</strong></p>
<ul>
  <li>Batch processing for efficiency.</li>
  <li>GPU for high-throughput.</li>
</ul>

<h2 id="26-mastery-checklist">26. Mastery Checklist</h2>

<p><strong>Mastery Checklist:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Explain spectral subtraction and Wiener filtering</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement a U-Net for speech enhancement</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Train on noisy/clean pairs</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Evaluate with PESQ and STOI</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement real-time processing (&lt;20ms latency)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Understand CTC/RNN-T integration for ASR</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Handle different noise types</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Deploy on mobile (TFLite/Core ML)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement multi-channel enhancement</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Understand diffusion-based enhancement</li>
</ul>

<h2 id="27-conclusion">27. Conclusion</h2>

<p>Speech enhancement is the unsung hero of speech technology. Without it, voice assistants wouldn’t work in noisy environments, video calls would be unusable, and hearing aids would be ineffective.</p>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Classic Methods:</strong> Spectral subtraction, Wiener filter—foundation of understanding.</li>
  <li><strong>Deep Learning:</strong> Masking and mapping with CNNs, U-Nets, and waveform models.</li>
  <li><strong>Production:</strong> Real-time constraints, CPU efficiency, generalization to unseen noise.</li>
  <li><strong>Metrics:</strong> PESQ (quality), STOI (intelligibility), SI-SDR (distortion).</li>
  <li><strong>Multi-Channel:</strong> Beamforming + post-filtering for best results.</li>
</ul>

<p>The future is on-device, personalized, and multi-modal. As edge AI becomes more powerful, speech enhancement will happen entirely on your device, preserving privacy while delivering crystal-clear audio. Mastering these techniques is essential for any speech engineer.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0043-speech-enhancement/">arunbaby.com/speech-tech/0043-speech-enhancement</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#audio-processing" class="page__taxonomy-item p-category" rel="tag">audio-processing</a><span class="sep">, </span>
    
      <a href="/tags/#deep-learning" class="page__taxonomy-item p-category" rel="tag">deep-learning</a><span class="sep">, </span>
    
      <a href="/tags/#denoising" class="page__taxonomy-item p-category" rel="tag">denoising</a><span class="sep">, </span>
    
      <a href="/tags/#signal-processing" class="page__taxonomy-item p-category" rel="tag">signal-processing</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0043-median-of-two-sorted-arrays/" rel="permalink">Median of Two Sorted Arrays
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Finding the middle ground between two ordered worlds.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0043-vector-databases/" rel="permalink">Vector Databases
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The infrastructure for semantic search and AI-native applications.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0043-testing-ai-agents/" rel="permalink">Testing AI Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Test agents like systems: validate tool calls, pin behaviors with replayable traces, and catch regressions before users do.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Speech+Enhancement%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0043-speech-enhancement%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0043-speech-enhancement%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0043-speech-enhancement/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0042-asr-decoding/" class="pagination--pager" title="Automatic Speech Recognition (ASR) Decoding">Previous</a>
    
    
      <a href="/speech-tech/0044-voice-conversion/" class="pagination--pager" title="Voice Conversion">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
