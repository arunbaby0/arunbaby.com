<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Multi-task Speech Learning - Arun Baby</title>
<meta name="description" content="“One model to rule them all: ASR, Translation, and Understanding.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Multi-task Speech Learning">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/">


  <meta property="og:description" content="“One model to rule them all: ASR, Translation, and Understanding.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Multi-task Speech Learning">
  <meta name="twitter:description" content="“One model to rule them all: ASR, Translation, and Understanding.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-14T22:12:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Multi-task Speech Learning">
    <meta itemprop="description" content="“One model to rule them all: ASR, Translation, and Understanding.”">
    <meta itemprop="datePublished" content="2025-12-14T22:12:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/" itemprop="url">Multi-task Speech Learning
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-the-concept-why-multi-task">1. The Concept: Why Multi-task?</a></li><li><a href="#2-architectures">2. Architectures</a><ul><li><a href="#1-shared-encoder-separate-decoders">1. Shared Encoder, Separate Decoders</a></li><li><a href="#2-token-based-task-specification-the-whisper-way">2. Token-Based Task Specification (The “Whisper” Way)</a></li></ul></li><li><a href="#3-training-strategies">3. Training Strategies</a><ul><li><a href="#1-loss-weighting">1. Loss Weighting</a></li><li><a href="#2-gradient-surgery">2. Gradient Surgery</a></li><li><a href="#3-curriculum-learning">3. Curriculum Learning</a></li></ul></li><li><a href="#4-deep-dive-openai-whisper">4. Deep Dive: OpenAI Whisper</a></li><li><a href="#5-deep-dive-unispeech-microsoft">5. Deep Dive: UniSpeech (Microsoft)</a></li><li><a href="#6-system-design-unified-speech-api">6. System Design: Unified Speech API</a></li><li><a href="#7-challenges">7. Challenges</a></li><li><a href="#8-summary">8. Summary</a></li><li><a href="#9-deep-dive-gradient-surgery-pcgrad">9. Deep Dive: Gradient Surgery (PCGrad)</a></li><li><a href="#10-deep-dive-uncertainty-weighting">10. Deep Dive: Uncertainty Weighting</a></li><li><a href="#11-deep-dive-adapter-modules">11. Deep Dive: Adapter Modules</a></li><li><a href="#12-deep-dive-whispers-weak-supervision-strategy">12. Deep Dive: Whisper’s Weak Supervision Strategy</a></li><li><a href="#13-code-pytorch-multi-task-model">13. Code: PyTorch Multi-task Model</a></li><li><a href="#14-system-design-real-time-translation-babelfish">14. System Design: Real-Time Translation (Babelfish)</a></li><li><a href="#15-deep-dive-joint-ctc-attention-training">15. Deep Dive: Joint CTC-Attention Training</a></li><li><a href="#16-deep-dive-multilingual-asr-as-multi-task-learning">16. Deep Dive: Multilingual ASR as Multi-task Learning</a></li><li><a href="#17-deep-dive-voice-activity-detection-vad-integration">17. Deep Dive: Voice Activity Detection (VAD) Integration</a></li><li><a href="#18-deep-dive-speaker-diarization-as-an-auxiliary-task">18. Deep Dive: Speaker Diarization as an Auxiliary Task</a></li><li><a href="#19-case-study-metas-massively-multilingual-speech-mms">19. Case Study: Meta’s Massively Multilingual Speech (MMS)</a></li><li><a href="#20-case-study-googles-universal-speech-model-usm">20. Case Study: Google’s Universal Speech Model (USM)</a></li><li><a href="#21-system-design-scalable-multi-task-training-pipeline">21. System Design: Scalable Multi-task Training Pipeline</a></li><li><a href="#22-deep-dive-evaluation-metrics-for-multi-task-models">22. Deep Dive: Evaluation Metrics for Multi-task Models</a></li><li><a href="#23-code-implementing-uncertainty-weighting">23. Code: Implementing Uncertainty Weighting</a></li><li><a href="#24-future-trends-foundation-models-speechllm">24. Future Trends: Foundation Models (SpeechLLM)</a></li><li><a href="#25-deep-dive-spoken-language-understanding-slu-tasks">25. Deep Dive: Spoken Language Understanding (SLU) Tasks</a></li><li><a href="#26-deep-dive-emotion-recognition-as-an-auxiliary-task">26. Deep Dive: Emotion Recognition as an Auxiliary Task</a></li><li><a href="#27-deep-dive-accent-classification">27. Deep Dive: Accent Classification</a></li><li><a href="#28-code-implementing-gradient-surgery-pcgrad">28. Code: Implementing Gradient Surgery (PCGrad)</a></li><li><a href="#29-checklist-for-multi-task-training">29. Checklist for Multi-task Training</a></li><li><a href="#31-deep-dive-zero-shot-transfer-in-multi-task-models">31. Deep Dive: Zero-Shot Transfer in Multi-task Models</a></li><li><a href="#32-deep-dive-the-curse-of-multilingualism">32. Deep Dive: The “Curse of Multilingualism”</a></li><li><a href="#33-deep-dive-adapterfusion">33. Deep Dive: AdapterFusion</a></li><li><a href="#34-case-study-tuning-whisper-for-code-switching">34. Case Study: Tuning Whisper for Code-Switching</a></li><li><a href="#35-future-trends-speechllm-and-in-context-multi-tasking">35. Future Trends: SpeechLLM and “In-Context” Multi-tasking</a></li><li><a href="#36-deep-dive-the-cocktail-party-problem-source-separation">36. Deep Dive: The “Cocktail Party Problem” (Source Separation)</a></li><li><a href="#37-deep-dive-audio-visual-multi-task-learning">37. Deep Dive: Audio-Visual Multi-task Learning</a></li><li><a href="#38-deep-dive-self-training-noisy-student">38. Deep Dive: Self-Training (Noisy Student)</a></li><li><a href="#39-the-economics-of-multi-task-models">39. The Economics of Multi-task Models</a></li><li><a href="#40-checklist-for-deployment">40. Checklist for Deployment</a></li><li><a href="#41-ethical-considerations">41. Ethical Considerations</a></li><li><a href="#42-further-reading">42. Further Reading</a></li><li><a href="#43-conclusion">43. Conclusion</a></li><li><a href="#44-summary">44. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“One model to rule them all: ASR, Translation, and Understanding.”</strong></p>

<h2 id="1-the-concept-why-multi-task">1. The Concept: Why Multi-task?</h2>

<p>Traditionally, we built separate models:</p>
<ol>
  <li><strong>ASR:</strong> Audio -&gt; Text.</li>
  <li><strong>ST (Speech Translation):</strong> Audio -&gt; Foreign Text.</li>
  <li><strong>SLU (Spoken Language Understanding):</strong> Audio -&gt; Intent/Slots.</li>
  <li><strong>VAD:</strong> Audio -&gt; Speech/Silence.</li>
</ol>

<p><strong>Multi-task Learning (MTL)</strong> trains a single model to perform multiple tasks simultaneously.</p>

<p><strong>Benefits:</strong></p>
<ul>
  <li><strong>Regularization:</strong> Learning to translate helps the model understand semantics, which improves ASR.</li>
  <li><strong>Data Efficiency:</strong> “Low-resource” tasks (e.g., Swahili ASR) benefit from “High-resource” tasks (e.g., English ASR) via shared representations.</li>
  <li><strong>Simplified Deployment:</strong> Deploy one model instead of four.</li>
</ul>

<h2 id="2-architectures">2. Architectures</h2>

<h3 id="1-shared-encoder-separate-decoders">1. Shared Encoder, Separate Decoders</h3>
<ul>
  <li><strong>Encoder:</strong> Processes audio (Spectrogram -&gt; Hidden States). Shared across all tasks.</li>
  <li><strong>Decoder A:</strong> ASR (predicts English tokens).</li>
  <li><strong>Decoder B:</strong> ST (predicts French tokens).</li>
  <li><strong>Decoder C:</strong> SLU (predicts Intent labels).</li>
  <li><strong>Pros:</strong> Specialized output heads.</li>
  <li><strong>Cons:</strong> Increases parameter count with each new task.</li>
</ul>

<h3 id="2-token-based-task-specification-the-whisper-way">2. Token-Based Task Specification (The “Whisper” Way)</h3>
<ul>
  <li><strong>Single Encoder-Decoder Transformer.</strong></li>
  <li><strong>Task Tokens:</strong> The first token fed to the decoder tells it what to do.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">&lt;|transcribe|&gt;</code> -&gt; Output English text.</li>
      <li><code class="language-plaintext highlighter-rouge">&lt;|translate|&gt;</code> -&gt; Output French text.</li>
      <li><code class="language-plaintext highlighter-rouge">&lt;|timestamps|&gt;</code> -&gt; Output time-aligned text.</li>
    </ul>
  </li>
  <li><strong>Pros:</strong> Extremely flexible. Zero-shot transfer.</li>
  <li><strong>Cons:</strong> Balancing tasks during training is tricky.</li>
</ul>

<h2 id="3-training-strategies">3. Training Strategies</h2>

<h3 id="1-loss-weighting">1. Loss Weighting</h3>
<p>\(L_{total} = \lambda_1 L_{ASR} + \lambda_2 L_{ST} + \lambda_3 L_{SLU}\)</p>
<ul>
  <li><strong>Challenge:</strong> If $L_{ASR}$ is large, the model ignores ST.</li>
  <li><strong>Solution:</strong> Dynamic Weight Averaging (DWA) or Uncertainty Weighting (learn $\lambda$ as parameters).</li>
</ul>

<h3 id="2-gradient-surgery">2. Gradient Surgery</h3>
<ul>
  <li><strong>Problem:</strong> Task A wants to move weights “Left”, Task B wants “Right”. Gradients conflict.</li>
  <li><strong>PCGrad (Project Conflicting Gradients):</strong> If gradients point in opposite directions, project one onto the normal plane of the other. Removes destructive interference.</li>
</ul>

<h3 id="3-curriculum-learning">3. Curriculum Learning</h3>
<ul>
  <li>Start with easy tasks (ASR).</li>
  <li>Gradually introduce hard tasks (Translation).</li>
</ul>

<h2 id="4-deep-dive-openai-whisper">4. Deep Dive: OpenAI Whisper</h2>

<p><strong>Whisper</strong> is the ultimate example of Multi-task Speech Learning.</p>

<ul>
  <li><strong>Tasks:</strong>
    <ol>
      <li>English Transcription.</li>
      <li>Any-to-English Translation.</li>
      <li>Language Identification.</li>
      <li>Voice Activity Detection (timestamp prediction).</li>
    </ol>
  </li>
  <li><strong>Architecture:</strong> Standard Transformer Encoder-Decoder.</li>
  <li><strong>Input:</strong> Log-Mel Spectrogram (30 seconds).</li>
  <li><strong>Decoder Prompt:</strong>
<code class="language-plaintext highlighter-rouge">[&lt;|startoftranscript|&gt;, &lt;|en|&gt;, &lt;|transcribe|&gt;, &lt;|notimestamps|&gt;]</code></li>
</ul>

<p><strong>Key Insight:</strong> By training on 680k hours of weak supervision (internet audio), the model learns robust representations that generalize across tasks.</p>

<h2 id="5-deep-dive-unispeech-microsoft">5. Deep Dive: UniSpeech (Microsoft)</h2>

<p><strong>UniSpeech</strong> combines:</p>
<ol>
  <li><strong>Self-Supervised Learning (SSL):</strong> Contrastive loss on unlabeled audio (like wav2vec 2.0).</li>
  <li><strong>Supervised Learning:</strong> ASR/ST loss on labeled data.</li>
</ol>

<p><strong>Unified Representation:</strong></p>
<ul>
  <li>Forces the model to learn phonetically rich representations (for ASR) and semantically rich representations (for Translation).</li>
</ul>

<h2 id="6-system-design-unified-speech-api">6. System Design: Unified Speech API</h2>

<p><strong>Scenario:</strong> Build an API that takes audio and returns Transcript + Sentiment + Language.</p>

<p><strong>Approach 1: Pipeline</strong>
<code class="language-plaintext highlighter-rouge">Audio -&gt; ASR Model -&gt; Text -&gt; Sentiment Model -&gt; Label</code></p>
<ul>
  <li><strong>Latency:</strong> Sum of latencies.</li>
  <li><strong>Error Propagation:</strong> If ASR fails, Sentiment fails.</li>
</ul>

<p><strong>Approach 2: Multi-task Model</strong>
<code class="language-plaintext highlighter-rouge">Audio -&gt; [Shared Encoder] -&gt; [ASR Head, Sentiment Head, LID Head]</code></p>
<ul>
  <li><strong>Latency:</strong> Encoder (expensive) runs once. Heads (cheap) run in parallel.</li>
  <li><strong>Robustness:</strong> Sentiment head works directly on audio features (prosody, tone), not just text.</li>
</ul>

<h2 id="7-challenges">7. Challenges</h2>

<ol>
  <li><strong>Catastrophic Forgetting:</strong> Fine-tuning on Task B makes it forget Task A.
    <ul>
      <li><strong>Fix:</strong> Replay buffers (mix old data with new).</li>
    </ul>
  </li>
  <li><strong>Negative Transfer:</strong> Task A hurts Task B.
    <ul>
      <li>Example: Speaker Identification (needs speaker info) vs. ASR (needs to ignore speaker info).</li>
      <li><strong>Fix:</strong> Task-specific adapters.</li>
    </ul>
  </li>
</ol>

<h2 id="8-summary">8. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Feature</th>
      <th style="text-align: left">Single-Task</th>
      <th style="text-align: left">Multi-Task</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Performance</strong></td>
      <td style="text-align: left">High on specific task</td>
      <td style="text-align: left">High on all (usually)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Data Req</strong></td>
      <td style="text-align: left">High labeled data</td>
      <td style="text-align: left">Can leverage auxiliary data</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Deployment</strong></td>
      <td style="text-align: left">N models</td>
      <td style="text-align: left">1 model</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Training</strong></td>
      <td style="text-align: left">Simple</td>
      <td style="text-align: left">Complex (balancing)</td>
    </tr>
  </tbody>
</table>

<h2 id="9-deep-dive-gradient-surgery-pcgrad">9. Deep Dive: Gradient Surgery (PCGrad)</h2>

<p>When training on Task A (ASR) and Task B (Translation), the gradients might conflict.</p>
<ul>
  <li>$\nabla L_A$ says “increase weight $w$”.</li>
  <li>$\nabla L_B$ says “decrease weight $w$”.</li>
  <li>Result: They cancel out, or the model oscillates.</li>
</ul>

<p><strong>PCGrad Algorithm:</strong></p>
<ol>
  <li>Compute gradients $g_A$ and $g_B$ independently.</li>
  <li>Check cosine similarity: if $g_A \cdot g_B &lt; 0$ (angle &gt; 90 degrees), they conflict.</li>
  <li>Project $g_A$ onto the normal plane of $g_B$:
\(g_A' = g_A - \frac{g_A \cdot g_B}{\|g_B\|^2} g_B\)</li>
  <li>Do the same for $g_B$.</li>
  <li>Update weights with $g_A’ + g_B’$.</li>
</ol>

<p><strong>Effect:</strong> The optimization trajectory follows a “zigzag” path that satisfies both tasks without destructive interference.</p>

<h2 id="10-deep-dive-uncertainty-weighting">10. Deep Dive: Uncertainty Weighting</h2>

<p>How do we set $\lambda_1, \lambda_2$ in the loss function?
\(L = \lambda_1 L_{ASR} + \lambda_2 L_{ST}\)</p>

<p><strong>Homoscedastic Uncertainty:</strong></p>
<ul>
  <li>Assume the task noise is constant.</li>
  <li>Learn $\sigma_1, \sigma_2$ (variance) as trainable parameters.</li>
  <li>Loss becomes:
\(L = \frac{1}{2\sigma_1^2} L_{ASR} + \frac{1}{2\sigma_2^2} L_{ST} + \log \sigma_1 + \log \sigma_2\)</li>
  <li>If a task is noisy (high loss), the model increases $\sigma$ to reduce its weight.</li>
  <li><strong>Result:</strong> Automatic, dynamic balancing.</li>
</ul>

<h2 id="11-deep-dive-adapter-modules">11. Deep Dive: Adapter Modules</h2>

<p>Fine-tuning a massive Multi-task model (like Whisper) for a new task is expensive.
<strong>Adapters</strong> allow efficient transfer learning.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Freeze the pre-trained Transformer layers.</li>
  <li>Insert small “Adapter” layers between Feed-Forward and Self-Attention blocks.</li>
  <li>Adapter = <code class="language-plaintext highlighter-rouge">Linear(d -&gt; d/r) -&gt; ReLU -&gt; Linear(d/r -&gt; d)</code>.</li>
  <li>Only train the Adapters (few parameters).</li>
</ul>

<p><strong>Task-Specific Adapters:</strong></p>
<ul>
  <li>Train one set of adapters for “Medical ASR”.</li>
  <li>Train another set for “Legal ASR”.</li>
  <li>Switch adapters at runtime based on user domain.</li>
</ul>

<h2 id="12-deep-dive-whispers-weak-supervision-strategy">12. Deep Dive: Whisper’s Weak Supervision Strategy</h2>

<p>Most ASR models are trained on LibriSpeech (clean, read audio). They fail on real-world noise.
Whisper trained on <strong>680,000 hours</strong> of internet audio.</p>

<p><strong>Data Filtering:</strong></p>
<ol>
  <li><strong>Language ID:</strong> Discard if audio language doesn’t match transcript language.</li>
  <li><strong>No Speech:</strong> Discard if VAD detects silence.</li>
  <li><strong>Machine Generated:</strong> Discard if transcript looks like output of another ASR system (to avoid learning errors).</li>
</ol>

<p><strong>Result:</strong></p>
<ul>
  <li>Whisper is not SOTA on LibriSpeech (Clean).</li>
  <li>But it is <strong>SOTA on Robustness</strong> (Accents, Noise, Music).</li>
</ul>

<h2 id="13-code-pytorch-multi-task-model">13. Code: PyTorch Multi-task Model</h2>

<p>A simple implementation of a shared encoder with multiple heads.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MultiTaskSpeechModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_intents</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="c1"># Shared Encoder (e.g., Conformer or LSTM)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="c1"># Task 1: ASR (CTC Head)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">asr_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
        
        <span class="c1"># Task 2: Intent Classification (SLU Head)
</span>        <span class="c1"># Use the last hidden state for classification
</span>        <span class="n">self</span><span class="p">.</span><span class="n">intent_head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">num_intents</span><span class="p">)</span>
        <span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (Batch, Time, Feats)
</span>        <span class="n">encoder_out</span><span class="p">,</span> <span class="p">(</span><span class="n">h_n</span><span class="p">,</span> <span class="n">c_n</span><span class="p">)</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># ASR Output: (Batch, Time, Vocab)
</span>        <span class="n">asr_logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">asr_head</span><span class="p">(</span><span class="n">encoder_out</span><span class="p">)</span>
        
        <span class="c1"># Intent Output: (Batch, Num_Intents)
</span>        <span class="c1"># Pool over time (e.g., take last state or mean)
</span>        <span class="c1"># Here taking mean of encoder outputs
</span>        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">encoder_out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">intent_logits</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">intent_head</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">asr_logits</span><span class="p">,</span> <span class="n">intent_logits</span>

<span class="c1"># Loss Calculation
</span><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">asr_logits</span><span class="p">,</span> <span class="n">asr_targets</span><span class="p">,</span> <span class="n">intent_logits</span><span class="p">,</span> <span class="n">intent_targets</span><span class="p">):</span>
    <span class="n">loss_asr</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CTCLoss</span><span class="p">()(</span><span class="n">asr_logits</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">asr_targets</span><span class="p">,</span> <span class="p">...)</span>
    <span class="n">loss_intent</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()(</span><span class="n">intent_logits</span><span class="p">,</span> <span class="n">intent_targets</span><span class="p">)</span>
    
    <span class="c1"># Simple weighting
</span>    <span class="k">return</span> <span class="n">loss_asr</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">loss_intent</span>
</code></pre></div></div>

<h2 id="14-system-design-real-time-translation-babelfish">14. System Design: Real-Time Translation (Babelfish)</h2>

<p><strong>Scenario:</strong> Build a “Universal Translator” device.
<strong>Input:</strong> Audio (Spanish). <strong>Output:</strong> Audio (English).</p>

<p><strong>Pipeline:</strong></p>
<ol>
  <li><strong>VAD:</strong> Detect speech.</li>
  <li><strong>S2ST Model (Speech-to-Speech Translation):</strong>
    <ul>
      <li><strong>Direct S2ST (Translatotron 2):</strong> Audio -&gt; Spectrogram. No text intermediate.</li>
      <li><strong>Cascaded:</strong> ASR -&gt; MT -&gt; TTS.</li>
    </ul>
  </li>
  <li><strong>Streaming:</strong>
    <ul>
      <li>Use <strong>Wait-k Policy</strong>: Wait for $k$ words before translating.</li>
      <li>Trade-off: Latency vs. Context (Accuracy).</li>
    </ul>
  </li>
</ol>

<ul>
  <li>Trade-off: Latency vs. Context (Accuracy).</li>
</ul>

<h2 id="15-deep-dive-joint-ctc-attention-training">15. Deep Dive: Joint CTC-Attention Training</h2>

<p><strong>The Hybrid Approach:</strong>
Most modern ASR systems (ESPnet, WeNet) use <strong>Hybrid CTC/Attention</strong>.</p>

<p><strong>Why?</strong></p>
<ul>
  <li><strong>Attention:</strong> Good at modeling long-range dependencies and language semantics. Bad at monotonic alignment (can loop or skip).</li>
  <li><strong>CTC:</strong> Enforces monotonic alignment. Good at timing. Bad at conditional dependence.</li>
</ul>

<p><strong>Training Objective:</strong>
\(L = \alpha L_{CTC} + (1 - \alpha) L_{Attn}\)</p>
<ul>
  <li>Typically $\alpha = 0.3$.</li>
  <li>The Encoder is shared.</li>
  <li>The CTC head branches off the Encoder.</li>
  <li>The Attention Decoder sits on top of the Encoder.</li>
</ul>

<p><strong>Inference (Decoding):</strong></p>
<ul>
  <li><strong>Attention Rescoring:</strong>
    <ol>
      <li>Generate top-k hypotheses using CTC (fast).</li>
      <li>Rescore them using the Attention Decoder (accurate).</li>
    </ol>
  </li>
  <li><strong>Joint Decoding:</strong>
    <ul>
      <li>Combine scores at each beam search step:
\(Score = \lambda \log P_{CTC}(y|x) + (1-\lambda) \log P_{Attn}(y|x)\)</li>
    </ul>
  </li>
</ul>

<h2 id="16-deep-dive-multilingual-asr-as-multi-task-learning">16. Deep Dive: Multilingual ASR as Multi-task Learning</h2>

<p>Training on 100 languages is effectively a 100-task problem.</p>

<p><strong>Strategies:</strong></p>
<ol>
  <li><strong>Language ID (LID) Prediction:</strong>
    <ul>
      <li>Add an auxiliary head to predict the language.</li>
      <li>Helps the encoder separate language-specific features (phonemes) from language-agnostic features (speaker voice).</li>
    </ul>
  </li>
  <li><strong>Shared vs. Specific Layers:</strong>
    <ul>
      <li><strong>Shared:</strong> Bottom layers (acoustic features are universal).</li>
      <li><strong>Specific:</strong> Top layers (phonotactics and grammar vary).</li>
      <li><strong>Adapter Modules:</strong> Insert language-specific adapters.</li>
    </ul>
  </li>
  <li><strong>Script Unification:</strong>
    <ul>
      <li>Map all languages to IPA (International Phonetic Alphabet) or use a shared SentencePiece vocabulary (e.g., 100k tokens covering all scripts).</li>
    </ul>
  </li>
</ol>

<h2 id="17-deep-dive-voice-activity-detection-vad-integration">17. Deep Dive: Voice Activity Detection (VAD) Integration</h2>

<p><strong>Problem:</strong> ASR models hallucinate text during silence.
<strong>Solution:</strong> Multi-task learning with VAD.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Main Task:</strong> ASR (Seq2Seq).</li>
  <li><strong>Auxiliary Task:</strong> Frame-level binary classification (Speech vs. Silence).</li>
  <li><strong>Loss:</strong> $L_{ASR} + \lambda L_{VAD}$.</li>
</ul>

<p><strong>Benefit:</strong></p>
<ul>
  <li>Forces the encoder to learn a robust “speech presence” feature.</li>
  <li>During inference, the VAD head can be used to gate the decoder (don’t decode if VAD &lt; 0.5).</li>
</ul>

<h2 id="18-deep-dive-speaker-diarization-as-an-auxiliary-task">18. Deep Dive: Speaker Diarization as an Auxiliary Task</h2>

<p><strong>Scenario:</strong> “Who spoke when?”</p>

<p><strong>E2E ASR-Diarization (E2E-ASR-DA):</strong></p>
<ul>
  <li><strong>Output:</strong> Instead of just text, output <code class="language-plaintext highlighter-rouge">(Speaker_ID, Word)</code>.</li>
  <li>Example: <code class="language-plaintext highlighter-rouge">&lt;spk:1&gt; Hello &lt;spk:2&gt; Hi there</code>.</li>
</ul>

<p><strong>Serialized Output Training (SOT):</strong></p>
<ul>
  <li>For overlapping speech.</li>
  <li>Output Speaker 1’s utterance first, then Speaker 2’s.</li>
  <li>Separated by a delimiter token <code class="language-plaintext highlighter-rouge">&lt;sep&gt;</code>.</li>
</ul>

<p><strong>Multi-task Benefit:</strong></p>
<ul>
  <li>Learning to distinguish speakers helps ASR in “Cocktail Party” scenarios (overlapping speech).</li>
</ul>

<h2 id="19-case-study-metas-massively-multilingual-speech-mms">19. Case Study: Meta’s Massively Multilingual Speech (MMS)</h2>

<p><strong>Goal:</strong> ASR and TTS for <strong>1,100+ languages</strong>.</p>

<p><strong>Data:</strong></p>
<ul>
  <li><strong>Religious Texts:</strong> The Bible is translated into thousands of languages.</li>
  <li><strong>Alignment:</strong> Use Forced Alignment on Bible audiobooks to create labeled data.</li>
</ul>

<p><strong>Model:</strong></p>
<ul>
  <li><strong>Wav2Vec 2.0</strong> pre-training on 500k hours of unlabeled audio (1,400 languages).</li>
  <li><strong>Fine-tuning:</strong> Linear layers (adapters) for each language.</li>
</ul>

<p><strong>Result:</strong></p>
<ul>
  <li>Half the WER of Whisper on low-resource languages.</li>
  <li>Proves that <strong>Self-Supervised Learning + Multi-task Fine-tuning</strong> is the recipe for scale.</li>
</ul>

<h2 id="20-case-study-googles-universal-speech-model-usm">20. Case Study: Google’s Universal Speech Model (USM)</h2>

<p><strong>Architecture:</strong> Conformer (2 Billion parameters).</p>

<p><strong>Training Objectives (MOST):</strong></p>
<ol>
  <li><strong>BEST-RQ:</strong> Self-supervised learning (BERT on audio).</li>
  <li><strong>Text-Injection:</strong> Train the encoder on text-only data (by upsampling text to match audio length).</li>
  <li><strong>ASR:</strong> Supervised learning on 300 languages.</li>
  <li><strong>Automatic Speech Translation (AST):</strong> Translate audio directly to English text.</li>
</ol>

<p><strong>Key Innovation:</strong></p>
<ul>
  <li><strong>Chunk-wise Attention:</strong> To handle long audio (YouTube videos).</li>
  <li><strong>Result:</strong> SOTA on YouTube captioning.</li>
</ul>

<h2 id="21-system-design-scalable-multi-task-training-pipeline">21. System Design: Scalable Multi-task Training Pipeline</h2>

<p><strong>Challenge:</strong> Training on 10 datasets (ASR, ST, VAD) with different sizes and formats.</p>

<p><strong>Pipeline:</strong></p>
<ol>
  <li><strong>Data Loader (The “Mixer”):</strong>
    <ul>
      <li>Samples batches from different datasets based on a probability distribution $p_i$.</li>
      <li>
        <table>
          <tbody>
            <tr>
              <td><strong>Temperature Sampling:</strong> $p_i \propto</td>
              <td>D_i</td>
              <td>^{1/T}$.</td>
            </tr>
          </tbody>
        </table>
        <ul>
          <li>$T=1$: Proportional to dataset size (starves small tasks).</li>
          <li>$T=\infty$: Uniform sampling (overfits small tasks).</li>
          <li>$T=5$: Good balance.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Bucketing:</strong>
    <ul>
      <li>Group audio files by length to minimize padding.</li>
      <li>Crucial for GPU efficiency.</li>
    </ul>
  </li>
  <li><strong>Distributed Training:</strong>
    <ul>
      <li><strong>FSDP (Fully Sharded Data Parallel):</strong> Shard model parameters across GPUs.</li>
      <li><strong>Gradient Accumulation:</strong> Simulate large batch sizes.</li>
    </ul>
  </li>
</ol>

<h2 id="22-deep-dive-evaluation-metrics-for-multi-task-models">22. Deep Dive: Evaluation Metrics for Multi-task Models</h2>

<p>How do you evaluate a “Swiss Army Knife” model?</p>

<ol>
  <li><strong>Average Performance:</strong>
    <ul>
      <li>Normalize metrics (WER, BLEU, F1) to 0-100 scale.</li>
      <li>Compute geometric mean.</li>
    </ul>
  </li>
  <li><strong>Pareto Frontier:</strong>
    <ul>
      <li>Plot ASR Accuracy vs. ST Accuracy.</li>
      <li>Does improving one hurt the other? (Negative Transfer).</li>
    </ul>
  </li>
  <li><strong>Zero-Shot Transfer:</strong>
    <ul>
      <li>Train on English ASR + French ASR.</li>
      <li>Test on English-to-French Translation.</li>
      <li>(Emergent ability).</li>
    </ul>
  </li>
</ol>

<h2 id="23-code-implementing-uncertainty-weighting">23. Code: Implementing Uncertainty Weighting</h2>

<p>Let’s implement the learnable loss weights in PyTorch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MultiTaskLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_tasks</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># Learnable log variances (sigma^2)
</span>        <span class="c1"># Initialize to 0 (variance = 1)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">log_vars</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">num_tasks</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">losses</span><span class="p">):</span>
        <span class="c1"># losses: list of scalar tensors [L1, L2, ...]
</span>        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">loss</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">losses</span><span class="p">):</span>
            <span class="c1"># Precision = 1 / (2 * sigma^2)
</span>            <span class="n">precision</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">log_vars</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            
            <span class="c1"># L = precision * loss + log(sigma)
</span>            <span class="c1"># log(sigma) = 0.5 * log_var
</span>            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">precision</span> <span class="o">*</span> <span class="n">loss</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">log_vars</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            
        <span class="k">return</span> <span class="n">total_loss</span>

<span class="c1"># Usage
</span><span class="n">mtl_criterion</span> <span class="o">=</span> <span class="nc">MultiTaskLoss</span><span class="p">(</span><span class="n">num_tasks</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span>
    <span class="nf">list</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">())</span> <span class="o">+</span> <span class="nf">list</span><span class="p">(</span><span class="n">mtl_criterion</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()),</span> 
    <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span>
<span class="p">)</span>

<span class="c1"># Training Loop
</span><span class="n">loss_asr</span> <span class="o">=</span> <span class="nf">criterion_asr</span><span class="p">(</span><span class="n">pred_asr</span><span class="p">,</span> <span class="n">target_asr</span><span class="p">)</span>
<span class="n">loss_st</span> <span class="o">=</span> <span class="nf">criterion_st</span><span class="p">(</span><span class="n">pred_st</span><span class="p">,</span> <span class="n">target_st</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="nf">mtl_criterion</span><span class="p">([</span><span class="n">loss_asr</span><span class="p">,</span> <span class="n">loss_st</span><span class="p">])</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="24-future-trends-foundation-models-speechllm">24. Future Trends: Foundation Models (SpeechLLM)</h2>

<p><strong>The End of Task-Specific Heads?</strong></p>

<p><strong>SpeechLLM (2024+):</strong></p>
<ul>
  <li><strong>Input:</strong> Audio Tokens + Text Tokens.</li>
  <li><strong>Output:</strong> Text Tokens (or Audio Tokens).</li>
  <li><strong>Task Specification:</strong> Just a text prompt.
    <ul>
      <li>“Transcribe this audio.”</li>
      <li>“Who is speaking?”</li>
      <li>“Is the speaker angry?”</li>
    </ul>
  </li>
  <li><strong>Architecture:</strong> Decoder-only Transformer (GPT-4 style).</li>
  <li><strong>Training:</strong> Next-token prediction on massive mixed-modal data.</li>
</ul>

<p><strong>Implication:</strong> Multi-task learning becomes implicit. The model learns tasks as “in-context learning”.</p>

<ul>
  <li><strong>Training:</strong> Next-token prediction on massive mixed-modal data.</li>
  <li><strong>Implication:</strong> Multi-task learning becomes implicit. The model learns tasks as “in-context learning”.</li>
</ul>

<h2 id="25-deep-dive-spoken-language-understanding-slu-tasks">25. Deep Dive: Spoken Language Understanding (SLU) Tasks</h2>

<p>SLU goes beyond transcription. It extracts meaning.</p>

<p><strong>1. Intent Classification:</strong></p>
<ul>
  <li><strong>Input:</strong> Audio.</li>
  <li><strong>Output:</strong> Class label (e.g., <code class="language-plaintext highlighter-rouge">PlayMusic</code>, <code class="language-plaintext highlighter-rouge">SetAlarm</code>).</li>
  <li><strong>Multi-task Benefit:</strong> ASR learns phonemes; Intent learns semantics. Together, they are robust to noise.</li>
</ul>

<p><strong>2. Slot Filling:</strong></p>
<ul>
  <li><strong>Input:</strong> Audio.</li>
  <li><strong>Output:</strong> Sequence of tags (BIO format).</li>
  <li><code class="language-plaintext highlighter-rouge">Play [Song: Despacito] by [Artist: Luis Fonsi]</code>.</li>
  <li><strong>Architecture:</strong> ASR Encoder -&gt; Slot Decoder (CRF or LSTM).</li>
</ul>

<p><strong>3. Sentiment Analysis:</strong></p>
<ul>
  <li><strong>Input:</strong> Audio.</li>
  <li><strong>Output:</strong> Positive/Negative/Neutral.</li>
  <li><strong>Why Audio?</strong> Sarcasm (“Yeah, right”) is detected via pitch/tone, not text.</li>
  <li><strong>Fusion:</strong> Concatenate Text Embeddings (from ASR) + Audio Embeddings (from Encoder) -&gt; Classifier.</li>
</ul>

<h2 id="26-deep-dive-emotion-recognition-as-an-auxiliary-task">26. Deep Dive: Emotion Recognition as an Auxiliary Task</h2>

<p><strong>SER (Speech Emotion Recognition):</strong></p>
<ul>
  <li>Classes: Happy, Sad, Angry, Neutral.</li>
</ul>

<p><strong>Why Multi-task with ASR?</strong></p>
<ul>
  <li><strong>ASR helps SER:</strong> Knowing <em>what</em> is said helps determine emotion.</li>
  <li><strong>SER helps ASR:</strong> Emotional speech (shouting, crying) has different acoustic properties. Explicitly modeling emotion helps the encoder normalize these variations.</li>
</ul>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Shared Encoder:</strong> Wav2Vec 2.0.</li>
  <li><strong>ASR Head:</strong> CTC/Attention.</li>
  <li><strong>SER Head:</strong> Pooling Layer -&gt; Linear -&gt; Softmax.</li>
</ul>

<h2 id="27-deep-dive-accent-classification">27. Deep Dive: Accent Classification</h2>

<p><strong>Problem:</strong> ASR fails on unseen accents.
<strong>Solution:</strong> Multi-task learning with Accent ID.</p>

<p><strong>Method:</strong></p>
<ol>
  <li><strong>Auxiliary Task:</strong> Predict accent (US, UK, Indian, Australian).</li>
  <li><strong>Gradient Reversal Layer (GRL):</strong>
    <ul>
      <li>We want the encoder to be <strong>Accent-Invariant</strong>.</li>
      <li>Add a GRL before the Accent Classifier.</li>
      <li>During backprop, flip the gradient sign.</li>
      <li>The encoder tries to <em>maximize</em> accent classification error (remove accent info), while the classifier tries to minimize it.</li>
      <li><strong>Result:</strong> Robust, accent-agnostic features.</li>
    </ul>
  </li>
</ol>

<h2 id="28-code-implementing-gradient-surgery-pcgrad">28. Code: Implementing Gradient Surgery (PCGrad)</h2>

<p>Here is a simplified implementation of PCGrad in PyTorch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">random</span>

<span class="k">class</span> <span class="nc">PCGradOptimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">objectives</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        objectives: list of losses [loss_task1, loss_task2]
        </span><span class="sh">"""</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        
        <span class="c1"># 1. Compute gradients for each task independently
</span>        <span class="k">for</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">objectives</span><span class="p">:</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">grad_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">param</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">grad_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">clone</span><span class="p">())</span>
                    <span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span> <span class="c1"># Clear for next task
</span>            <span class="n">grads</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">grad_list</span><span class="p">)</span>
            
        <span class="c1"># 2. Project conflicting gradients
</span>        <span class="c1"># Shuffle order to avoid bias
</span>        <span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        
        <span class="n">final_grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span><span class="p">[:]</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">]</span> <span class="c1"># Deep copy
</span>        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">grads</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">grads</span><span class="p">)):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">j</span><span class="p">:</span> <span class="k">continue</span>
                
                <span class="c1"># Flatten gradients to compute dot product
</span>                <span class="n">g_i_flat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">g</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
                <span class="n">g_j_flat</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">g</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">[</span><span class="n">j</span><span class="p">]])</span>
                
                <span class="n">dot_prod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">g_i_flat</span><span class="p">,</span> <span class="n">g_j_flat</span><span class="p">)</span>
                
                <span class="k">if</span> <span class="n">dot_prod</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># Conflict!
</span>                    <span class="c1"># Project g_i onto normal plane of g_j
</span>                    <span class="c1"># g_i = g_i - (g_i . g_j) / ||g_j||^2 * g_j
</span>                    <span class="n">norm_sq</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">g_j_flat</span><span class="p">,</span> <span class="n">g_j_flat</span><span class="p">)</span>
                    <span class="n">scale</span> <span class="o">=</span> <span class="n">dot_prod</span> <span class="o">/</span> <span class="n">norm_sq</span>
                    
                    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="n">i</span><span class="p">])):</span>
                        <span class="n">final_grads</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
                        
        <span class="c1"># 3. Apply final gradients
</span>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">]):</span>
            <span class="k">if</span> <span class="n">param</span><span class="p">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">param</span><span class="p">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
            
            <span class="c1"># Sum projected gradients from all tasks
</span>            <span class="k">for</span> <span class="n">task_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">grads</span><span class="p">)):</span>
                <span class="n">param</span><span class="p">.</span><span class="n">grad</span> <span class="o">+=</span> <span class="n">final_grads</span><span class="p">[</span><span class="n">task_idx</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="29-checklist-for-multi-task-training">29. Checklist for Multi-task Training</h2>

<p>Before you start training:</p>

<ol class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Data Balance:</strong> Are tasks roughly equal in size? If not, use temperature sampling.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Loss Scale:</strong> Do losses have similar magnitude? (e.g., ASR loss is 100, Class loss is 1). Normalize them.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Capacity:</strong> Is the model big enough? Multi-tasking requires more capacity than single-task.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Scheduling:</strong> Should you start with all tasks, or introduce them sequentially (Curriculum)?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Evaluation:</strong> Do you have a separate validation set for <em>each</em> task?</li>
</ol>

<table>
  <tbody>
    <tr>
      <td><strong>Robustness</strong></td>
      <td>Low</td>
      <td>High (Accent/Noise invariant)</td>
    </tr>
  </tbody>
</table>

<h2 id="31-deep-dive-zero-shot-transfer-in-multi-task-models">31. Deep Dive: Zero-Shot Transfer in Multi-task Models</h2>

<p><strong>The Magic:</strong> Train on Task A and B, test on Task C (which was never seen).</p>

<p><strong>Example: Zero-Shot Speech Translation</strong></p>
<ul>
  <li><strong>Train:</strong>
    <ol>
      <li>English Audio -&gt; English Text (ASR).</li>
      <li>English Text -&gt; French Text (MT).</li>
    </ol>
  </li>
  <li><strong>Test:</strong> English Audio -&gt; French Text (ST).</li>
  <li><strong>Mechanism:</strong> If the model learns a shared embedding space for “Audio” and “Text”, it can bridge the gap.</li>
</ul>

<p><strong>Example: Zero-Shot Language Transfer</strong></p>
<ul>
  <li><strong>Train:</strong>
    <ol>
      <li>English ASR.</li>
      <li>French ASR.</li>
      <li>English-to-Spanish Translation.</li>
    </ol>
  </li>
  <li><strong>Test:</strong> French-to-Spanish Translation.</li>
  <li><strong>Mechanism:</strong> The “Translation” head learns to map semantic concepts to Spanish, regardless of the source language (if the encoder is language-agnostic).</li>
</ul>

<h2 id="32-deep-dive-the-curse-of-multilingualism">32. Deep Dive: The “Curse of Multilingualism”</h2>

<p><strong>Observation:</strong> Adding languages improves performance initially (transfer learning), but eventually degrades it (interference).</p>

<p><strong>The Capacity Bottleneck:</strong></p>
<ul>
  <li>A fixed-size model has limited capacity.</li>
  <li>English takes up 50% of the weights.</li>
  <li>Adding 99 more languages forces them to fight for the remaining 50%.</li>
  <li><strong>Result:</strong> High-resource languages (English) degrade slightly; low-resource languages improve massively.</li>
</ul>

<p><strong>Solution:</strong></p>
<ol>
  <li><strong>Increase Model Size:</strong> 1B -&gt; 10B parameters.</li>
  <li><strong>Mixture of Experts (MoE):</strong>
    <ul>
      <li>Have 100 “Expert” FFN layers.</li>
      <li>For each token, route it to the top-2 experts.</li>
      <li><strong>Result:</strong> Massive capacity (1 Trillion params) with low inference cost (active params are small).</li>
    </ul>
  </li>
</ol>

<h2 id="33-deep-dive-adapterfusion">33. Deep Dive: AdapterFusion</h2>

<p><strong>Problem:</strong> We have separate adapters for ASR, ST, and VAD. Can we combine them?</p>

<p><strong>AdapterFusion (Pfeiffer et al.):</strong></p>
<ol>
  <li><strong>Train:</strong> Train adapters for each task independently.</li>
  <li><strong>Fuse:</strong> Freeze adapters. Learn a <strong>Fusion Layer</strong> (Attention) that combines their outputs.
\(h_{fused} = \text{Attn}(h_{enc}, [h_{ASR}, h_{ST}, h_{VAD}])\)</li>
  <li><strong>Benefit:</strong> The model dynamically decides: “For this noisy frame, I’ll trust the VAD adapter more. For this clear speech, I’ll trust the ASR adapter.”</li>
</ol>

<h2 id="34-case-study-tuning-whisper-for-code-switching">34. Case Study: Tuning Whisper for Code-Switching</h2>

<p><strong>Scenario:</strong> “Hinglish” (Hindi + English mixed).</p>
<ul>
  <li>“Main kal market jaaunga to buy vegetables.”</li>
</ul>

<p><strong>Challenge:</strong></p>
<ul>
  <li>Monolingual ASR fails (expects only Hindi or only English).</li>
  <li>Language ID flips rapidly.</li>
</ul>

<p><strong>Multi-task Solution:</strong></p>
<ol>
  <li><strong>Data:</strong> Synthetic Code-Switching.
    <ul>
      <li>Take English sentence.</li>
      <li>Replace random nouns with Hindi translations.</li>
      <li>Generate audio using TTS.</li>
    </ul>
  </li>
  <li><strong>Training:</strong> Fine-tune Whisper on this mixed data.</li>
  <li><strong>Result:</strong> The model learns to handle intra-sentence language switching without explicit language tags.</li>
</ol>

<h2 id="35-future-trends-speechllm-and-in-context-multi-tasking">35. Future Trends: SpeechLLM and “In-Context” Multi-tasking</h2>

<p><strong>Current:</strong> Explicit heads for ASR, ST.
<strong>Future:</strong> Text Prompting.</p>

<p><strong>AudioPaLM (Google):</strong></p>
<ul>
  <li>Unified vocabulary of Text Tokens and Audio Tokens.</li>
  <li><strong>Task:</strong> “Translate this audio to German.”</li>
  <li><strong>Input:</strong> <code class="language-plaintext highlighter-rouge">[AudioTokens] [Text: Translate to German]</code></li>
  <li><strong>Output:</strong> <code class="language-plaintext highlighter-rouge">[Text: German Translation]</code></li>
  <li>
    <p><strong>In-Context Learning:</strong> Provide 3 examples in the prompt, and the model learns the task on the fly without weight updates.</p>
  </li>
  <li><strong>In-Context Learning:</strong> Provide 3 examples in the prompt, and the model learns the task on the fly without weight updates.</li>
</ul>

<h2 id="36-deep-dive-the-cocktail-party-problem-source-separation">36. Deep Dive: The “Cocktail Party Problem” (Source Separation)</h2>

<p><strong>Scenario:</strong> Two people talking at once.
<strong>Goal:</strong> Separate them into two clean audio streams.</p>

<p><strong>Multi-task Approach:</strong></p>
<ul>
  <li><strong>Task 1:</strong> Separation (PIT - Permutation Invariant Training).</li>
  <li><strong>Task 2:</strong> ASR on separated streams.</li>
  <li><strong>Joint Training:</strong> Backpropagate ASR loss through the separator.</li>
  <li><strong>Result:</strong> The separator learns to output streams that are “ASR-friendly” (even if they sound slightly unnatural to humans).</li>
</ul>

<h2 id="37-deep-dive-audio-visual-multi-task-learning">37. Deep Dive: Audio-Visual Multi-task Learning</h2>

<p><strong>Lip Reading (Visual Speech Recognition):</strong></p>
<ul>
  <li><strong>Input:</strong> Audio + Video of lips.</li>
  <li><strong>Tasks:</strong>
    <ol>
      <li>Audio ASR.</li>
      <li>Video ASR.</li>
      <li>Audio-Visual Fusion.</li>
    </ol>
  </li>
  <li><strong>Benefit:</strong> When audio is noisy (0dB SNR), the model relies on the video stream (lip movement) to disambiguate phonemes (e.g., “P” vs “B”).</li>
</ul>

<h2 id="38-deep-dive-self-training-noisy-student">38. Deep Dive: Self-Training (Noisy Student)</h2>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Train Teacher on labeled data (ASR).</li>
  <li>Teacher generates pseudo-labels for unlabeled data.</li>
  <li><strong>Multi-task Student:</strong>
    <ul>
      <li>Train Student on Labeled Data (Supervised Loss).</li>
      <li>Train Student on Unlabeled Data (Consistency Loss).</li>
      <li><strong>Augmentation:</strong> Add noise (SpecAugment) to Student input, force it to match Teacher output.</li>
    </ul>
  </li>
  <li>Iterate.</li>
</ol>

<p><strong>Result:</strong> Massive improvements in robustness without new human labels.</p>

<ul>
  <li><strong>Result:</strong> Massive improvements in robustness without new human labels.</li>
  <li><strong>Augmentation:</strong> Add noise (SpecAugment) to Student input, force it to match Teacher output.</li>
</ul>

<h2 id="39-the-economics-of-multi-task-models">39. The Economics of Multi-task Models</h2>

<p><strong>Cost:</strong></p>
<ul>
  <li><strong>Training:</strong> Extremely expensive. Training Whisper-Large took thousands of GPU-days.</li>
  <li><strong>Inference:</strong> Large models (1B+ params) are slow and require expensive GPUs (A100).</li>
</ul>

<p><strong>Benefit:</strong></p>
<ul>
  <li><strong>Maintenance:</strong> Maintaining 1 model is cheaper than maintaining 10 specialized models.</li>
  <li><strong>Data Efficiency:</strong> You save millions on labeling costs because the model learns from unlabeled data and transfer learning.</li>
  <li><strong>User Experience:</strong> Seamless switching between languages and tasks (ASR -&gt; Translation) without latency spikes.</li>
</ul>

<p><strong>Verdict:</strong> For large tech companies, Multi-task is a no-brainer. For startups, fine-tuning a pre-trained Multi-task model (like Whisper) is the way to go.</p>

<h2 id="40-checklist-for-deployment">40. Checklist for Deployment</h2>

<ol class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Model Size:</strong> Can you afford to run <code class="language-plaintext highlighter-rouge">large-v3</code> (1.5GB VRAM) or do you need <code class="language-plaintext highlighter-rouge">tiny</code> (75MB)?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Quantization:</strong> Use <code class="language-plaintext highlighter-rouge">int8</code> or <code class="language-plaintext highlighter-rouge">float16</code> to reduce memory by 2-4x with minimal accuracy loss.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Batching:</strong> Use dynamic batching (e.g., TorchServe) to saturate the GPU.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Caching:</strong> Cache common audio queries (hash the audio file).</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Fallback:</strong> If the model fails (low confidence), do you have a fallback (e.g., a simpler model or human-in-the-loop)?</li>
</ol>

<ul>
  <li><strong>Fallback:</strong> If the model fails (low confidence), do you have a fallback (e.g., a simpler model or human-in-the-loop)?</li>
  <li><strong>Bias:</strong> Multi-task models can amplify bias. If the training data has more male speakers for ASR and female for TTS, the model might associate “male” with “input” and “female” with “output”.</li>
</ul>

<h2 id="41-ethical-considerations">41. Ethical Considerations</h2>

<p><strong>1. Bias Amplification:</strong></p>
<ul>
  <li>Multi-task models trained on internet data (Whisper) inherit internet biases.</li>
  <li><strong>Example:</strong> Translating “The doctor called the nurse” into a gendered language might default to “Male Doctor” and “Female Nurse” purely based on statistical co-occurrence, even if incorrect.</li>
</ul>

<p><strong>2. Representation:</strong></p>
<ul>
  <li>Low-resource languages often get “overwritten” by high-resource ones in shared capacity models.</li>
  <li><strong>Fix:</strong> Ensure strict data balancing and evaluation on <em>all</em> languages, not just the top 10.</li>
</ul>

<p><strong>3. Dual Use:</strong></p>
<ul>
  <li>A model good at Voice Cloning (TTS) and ASR can be used for deepfakes.</li>
  <li><strong>Safeguards:</strong> Release models with watermarking or restricted licenses.</li>
</ul>

<h2 id="42-further-reading">42. Further Reading</h2>

<ol>
  <li><strong>“Whisper: Robust Speech Recognition via Large-Scale Weak Supervision” (Radford et al., 2022):</strong> The bible of multi-task speech.</li>
  <li><strong>“UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data” (Wang et al., 2021):</strong> Combining SSL and Supervised learning.</li>
  <li><strong>“Gradient Surgery for Multi-Task Learning” (Yu et al., 2020):</strong> The PCGrad paper.</li>
  <li><strong>“Massively Multilingual Speech” (Pratap et al., 2023):</strong> Scaling to 1000 languages.</li>
  <li><strong>“AudioPaLM: A Large Language Model that Can Speak and Listen” (Rubenstein et al., 2023):</strong> The future of SpeechLLMs.</li>
</ol>

<h2 id="43-conclusion">43. Conclusion</h2>

<p>Multi-task learning is the key to building <strong>general-purpose speech systems</strong>. Instead of building one model for ASR, one for Translation, and one for VAD, we are moving towards <strong>Unified Foundation Models</strong> that can handle any speech task via prompting. The challenges of gradient conflict and capacity bottlenecks are being solved by techniques like PCGrad and Mixture of Experts. The future is not just multi-task, but <strong>multi-modal</strong> (Speech + Text + Vision).</p>

<h2 id="44-summary">44. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Feature</th>
      <th style="text-align: left">Single-Task</th>
      <th style="text-align: left">Multi-Task</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Performance</strong></td>
      <td style="text-align: left">High on specific task</td>
      <td style="text-align: left">High on all (usually)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Data Req</strong></td>
      <td style="text-align: left">High labeled data</td>
      <td style="text-align: left">Can leverage auxiliary data</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Deployment</strong></td>
      <td style="text-align: left">N models</td>
      <td style="text-align: left">1 model</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Training</strong></td>
      <td style="text-align: left">Simple</td>
      <td style="text-align: left">Complex (balancing)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Optimization</strong></td>
      <td style="text-align: left">Standard SGD</td>
      <td style="text-align: left">PCGrad, Uncertainty Weighting</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Robustness</strong></td>
      <td style="text-align: left">Low</td>
      <td style="text-align: left">High (Accent/Noise invariant)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Future</strong></td>
      <td style="text-align: left">Specialized Models</td>
      <td style="text-align: left">Foundation Models (SpeechLLM)</td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/">arunbaby.com/speech-tech/0036-multi-task-speech-learning</a></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#multi-task-learning" class="page__taxonomy-item p-category" rel="tag">multi-task-learning</a><span class="sep">, </span>
    
      <a href="/tags/#slu" class="page__taxonomy-item p-category" rel="tag">slu</a><span class="sep">, </span>
    
      <a href="/tags/#translation" class="page__taxonomy-item p-category" rel="tag">translation</a><span class="sep">, </span>
    
      <a href="/tags/#whisper" class="page__taxonomy-item p-category" rel="tag">whisper</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech_tech</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Multi-task+Speech+Learning%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0036-multi-task-speech-learning%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0036-multi-task-speech-learning%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0035-speech-boundary-detection/" class="pagination--pager" title="Speech Boundary Detection">Previous</a>
    
    
      <a href="/speech-tech/0037-sequence-to-sequence-speech/" class="pagination--pager" title="Sequence-to-Sequence Speech Models">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
