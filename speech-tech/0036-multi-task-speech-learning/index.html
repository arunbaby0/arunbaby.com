<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Multi-task Speech Learning - Arun Baby</title>
<meta name="description" content="“One model to rule them all: ASR, Translation, and Understanding.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Multi-task Speech Learning">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/">


  <meta property="og:description" content="“One model to rule them all: ASR, Translation, and Understanding.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Multi-task Speech Learning">
  <meta name="twitter:description" content="“One model to rule them all: ASR, Translation, and Understanding.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T16:05:30+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Multi-task Speech Learning">
    <meta itemprop="description" content="“One model to rule them all: ASR, Translation, and Understanding.”">
    <meta itemprop="datePublished" content="2025-12-29T16:05:30+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/" itemprop="url">Multi-task Speech Learning
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-the-concept-why-multi-task">1. The Concept: Why Multi-task?</a></li><li><a href="#2-architectures">2. Architectures</a><ul><li><a href="#1-shared-encoder-separate-decoders">1. Shared Encoder, Separate Decoders</a></li><li><a href="#2-token-based-task-specification-the-whisper-way">2. Token-Based Task Specification (The “Whisper” Way)</a></li></ul></li><li><a href="#3-training-strategies">3. Training Strategies</a><ul><li><a href="#1-loss-weighting">1. Loss Weighting</a></li><li><a href="#2-gradient-surgery">2. Gradient Surgery</a></li><li><a href="#3-curriculum-learning">3. Curriculum Learning</a></li></ul></li><li><a href="#4-deep-dive-openai-whisper">4. Deep Dive: OpenAI Whisper</a></li><li><a href="#5-deep-dive-unispeech-microsoft">5. Deep Dive: UniSpeech (Microsoft)</a></li><li><a href="#6-system-design-unified-speech-api">6. System Design: Unified Speech API</a></li><li><a href="#7-challenges">7. Challenges</a></li><li><a href="#8-summary">8. Summary</a></li><li><a href="#9-deep-dive-gradient-surgery-pcgrad">9. Deep Dive: Gradient Surgery (PCGrad)</a></li><li><a href="#10-deep-dive-uncertainty-weighting">10. Deep Dive: Uncertainty Weighting</a></li><li><a href="#11-deep-dive-adapter-modules">11. Deep Dive: Adapter Modules</a></li><li><a href="#12-deep-dive-whispers-weak-supervision-strategy">12. Deep Dive: Whisper’s Weak Supervision Strategy</a></li><li><a href="#13-code-pytorch-multi-task-model">13. Code: PyTorch Multi-task Model</a></li><li><a href="#14-system-design-real-time-translation-babelfish">14. System Design: Real-Time Translation (Babelfish)</a></li><li><a href="#15-deep-dive-joint-ctc-attention-training">15. Deep Dive: Joint CTC-Attention Training</a></li><li><a href="#16-deep-dive-multilingual-asr-as-multi-task-learning">16. Deep Dive: Multilingual ASR as Multi-task Learning</a></li><li><a href="#17-deep-dive-voice-activity-detection-vad-integration">17. Deep Dive: Voice Activity Detection (VAD) Integration</a></li><li><a href="#18-deep-dive-speaker-diarization-as-an-auxiliary-task">18. Deep Dive: Speaker Diarization as an Auxiliary Task</a></li><li><a href="#19-case-study-metas-massively-multilingual-speech-mms">19. Case Study: Meta’s Massively Multilingual Speech (MMS)</a></li><li><a href="#20-case-study-googles-universal-speech-model-usm">20. Case Study: Google’s Universal Speech Model (USM)</a></li><li><a href="#21-system-design-scalable-multi-task-training-pipeline">21. System Design: Scalable Multi-task Training Pipeline</a></li><li><a href="#22-deep-dive-evaluation-metrics-for-multi-task-models">22. Deep Dive: Evaluation Metrics for Multi-task Models</a></li><li><a href="#23-code-implementing-uncertainty-weighting">23. Code: Implementing Uncertainty Weighting</a></li><li><a href="#24-future-trends-foundation-models-speechllm">24. Future Trends: Foundation Models (SpeechLLM)</a></li><li><a href="#25-deep-dive-spoken-language-understanding-slu-tasks">25. Deep Dive: Spoken Language Understanding (SLU) Tasks</a></li><li><a href="#26-deep-dive-emotion-recognition-as-an-auxiliary-task">26. Deep Dive: Emotion Recognition as an Auxiliary Task</a></li><li><a href="#27-deep-dive-accent-classification">27. Deep Dive: Accent Classification</a></li><li><a href="#28-code-implementing-gradient-surgery-pcgrad">28. Code: Implementing Gradient Surgery (PCGrad)</a></li><li><a href="#29-checklist-for-multi-task-training">29. Checklist for Multi-task Training</a></li><li><a href="#31-deep-dive-zero-shot-transfer-in-multi-task-models">31. Deep Dive: Zero-Shot Transfer in Multi-task Models</a></li><li><a href="#32-deep-dive-the-curse-of-multilingualism">32. Deep Dive: The “Curse of Multilingualism”</a></li><li><a href="#33-deep-dive-adapterfusion">33. Deep Dive: AdapterFusion</a></li><li><a href="#34-case-study-tuning-whisper-for-code-switching">34. Case Study: Tuning Whisper for Code-Switching</a></li><li><a href="#35-future-trends-speechllm-and-in-context-multi-tasking">35. Future Trends: SpeechLLM and “In-Context” Multi-tasking</a></li><li><a href="#36-deep-dive-the-cocktail-party-problem-source-separation">36. Deep Dive: The “Cocktail Party Problem” (Source Separation)</a></li><li><a href="#37-deep-dive-audio-visual-multi-task-learning">37. Deep Dive: Audio-Visual Multi-task Learning</a></li><li><a href="#38-deep-dive-self-training-noisy-student">38. Deep Dive: Self-Training (Noisy Student)</a></li><li><a href="#39-the-economics-of-multi-task-models">39. The Economics of Multi-task Models</a></li><li><a href="#40-checklist-for-deployment">40. Checklist for Deployment</a></li><li><a href="#41-ethical-considerations">41. Ethical Considerations</a></li><li><a href="#42-further-reading">42. Further Reading</a></li><li><a href="#43-conclusion">43. Conclusion</a></li><li><a href="#44-summary">44. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“One model to rule them all: ASR, Translation, and Understanding.”</strong></p>

<h2 id="1-the-concept-why-multi-task">1. The Concept: Why Multi-task?</h2>

<p>Traditionally, we built separate models:</p>
<ol>
  <li><strong>ASR:</strong> Audio -&gt; Text.</li>
  <li><strong>ST (Speech Translation):</strong> Audio -&gt; Foreign Text.</li>
  <li><strong>SLU (Spoken Language Understanding):</strong> Audio -&gt; Intent/Slots.</li>
  <li><strong>VAD:</strong> Audio -&gt; Speech/Silence.</li>
</ol>

<p><strong>Multi-task Learning (MTL)</strong> trains a single model to perform multiple tasks simultaneously.</p>

<p><strong>Benefits:</strong></p>
<ul>
  <li><strong>Regularization:</strong> Learning to translate helps the model understand semantics, which improves ASR.</li>
  <li><strong>Data Efficiency:</strong> “Low-resource” tasks (e.g., Swahili ASR) benefit from “High-resource” tasks (e.g., English ASR) via shared representations.</li>
  <li><strong>Simplified Deployment:</strong> Deploy one model instead of four.</li>
</ul>

<h2 id="2-architectures">2. Architectures</h2>

<h3 id="1-shared-encoder-separate-decoders">1. Shared Encoder, Separate Decoders</h3>
<ul>
  <li><strong>Encoder:</strong> Processes audio (Spectrogram -&gt; Hidden States). Shared across all tasks.</li>
  <li><strong>Decoder A:</strong> ASR (predicts English tokens).</li>
  <li><strong>Decoder B:</strong> ST (predicts French tokens).</li>
  <li><strong>Decoder C:</strong> SLU (predicts Intent labels).</li>
  <li><strong>Pros:</strong> Specialized output heads.</li>
  <li><strong>Cons:</strong> Increases parameter count with each new task.</li>
</ul>

<h3 id="2-token-based-task-specification-the-whisper-way">2. Token-Based Task Specification (The “Whisper” Way)</h3>
<ul>
  <li><strong>Single Encoder-Decoder Transformer.</strong></li>
  <li><strong>Task Tokens:</strong> The first token fed to the decoder tells it what to do.</li>
  <li><code class="language-plaintext highlighter-rouge">&lt;|transcribe|&gt;</code> -&gt; Output English text.</li>
  <li><code class="language-plaintext highlighter-rouge">&lt;|translate|&gt;</code> -&gt; Output French text.</li>
  <li><code class="language-plaintext highlighter-rouge">&lt;|timestamps|&gt;</code> -&gt; Output time-aligned text.</li>
  <li><strong>Pros:</strong> Extremely flexible. Zero-shot transfer.</li>
  <li><strong>Cons:</strong> Balancing tasks during training is tricky.</li>
</ul>

<h2 id="3-training-strategies">3. Training Strategies</h2>

<h3 id="1-loss-weighting">1. Loss Weighting</h3>
<p><code class="language-plaintext highlighter-rouge">L_{total} = \lambda_1 L_{ASR} + \lambda_2 L_{ST} + \lambda_3 L_{SLU}</code></p>
<ul>
  <li><strong>Challenge:</strong> If <code class="language-plaintext highlighter-rouge">L_{ASR}</code> is large, the model ignores ST.</li>
  <li><strong>Solution:</strong> Dynamic Weight Averaging (DWA) or Uncertainty Weighting (learn <code class="language-plaintext highlighter-rouge">\lambda</code> as parameters).</li>
</ul>

<h3 id="2-gradient-surgery">2. Gradient Surgery</h3>
<ul>
  <li><strong>Problem:</strong> Task A wants to move weights “Left”, Task B wants “Right”. Gradients conflict.</li>
  <li><strong>PCGrad (Project Conflicting Gradients):</strong> If gradients point in opposite directions, project one onto the normal plane of the other. Removes destructive interference.</li>
</ul>

<h3 id="3-curriculum-learning">3. Curriculum Learning</h3>
<ul>
  <li>Start with easy tasks (ASR).</li>
  <li>Gradually introduce hard tasks (Translation).</li>
</ul>

<h2 id="4-deep-dive-openai-whisper">4. Deep Dive: OpenAI Whisper</h2>

<p><strong>Whisper</strong> is the ultimate example of Multi-task Speech Learning.</p>

<ul>
  <li><strong>Tasks:</strong>
    <ol>
      <li>English Transcription.</li>
      <li>Any-to-English Translation.</li>
      <li>Language Identification.</li>
      <li>Voice Activity Detection (timestamp prediction).</li>
    </ol>
  </li>
  <li><strong>Architecture:</strong> Standard Transformer Encoder-Decoder.</li>
  <li><strong>Input:</strong> Log-Mel Spectrogram (30 seconds).</li>
  <li><strong>Decoder Prompt:</strong>
 <code class="language-plaintext highlighter-rouge">[&lt;|startoftranscript|&gt;, &lt;|en|&gt;, &lt;|transcribe|&gt;, &lt;|notimestamps|&gt;]</code></li>
</ul>

<p><strong>Key Insight:</strong> By training on 680k hours of weak supervision (internet audio), the model learns robust representations that generalize across tasks.</p>

<h2 id="5-deep-dive-unispeech-microsoft">5. Deep Dive: UniSpeech (Microsoft)</h2>

<p><strong>UniSpeech</strong> combines:</p>
<ol>
  <li><strong>Self-Supervised Learning (SSL):</strong> Contrastive loss on unlabeled audio (like wav2vec 2.0).</li>
  <li><strong>Supervised Learning:</strong> ASR/ST loss on labeled data.</li>
</ol>

<p><strong>Unified Representation:</strong></p>
<ul>
  <li>Forces the model to learn phonetically rich representations (for ASR) and semantically rich representations (for Translation).</li>
</ul>

<h2 id="6-system-design-unified-speech-api">6. System Design: Unified Speech API</h2>

<p><strong>Scenario:</strong> Build an API that takes audio and returns Transcript + Sentiment + Language.</p>

<p><strong>Approach 1: Pipeline</strong>
<code class="language-plaintext highlighter-rouge">Audio -&gt; ASR Model -&gt; Text -&gt; Sentiment Model -&gt; Label</code></p>
<ul>
  <li><strong>Latency:</strong> Sum of latencies.</li>
  <li><strong>Error Propagation:</strong> If ASR fails, Sentiment fails.</li>
</ul>

<p><strong>Approach 2: Multi-task Model</strong>
<code class="language-plaintext highlighter-rouge">Audio -&gt; [Shared Encoder] -&gt; [ASR Head, Sentiment Head, LID Head]</code></p>
<ul>
  <li><strong>Latency:</strong> Encoder (expensive) runs once. Heads (cheap) run in parallel.</li>
  <li><strong>Robustness:</strong> Sentiment head works directly on audio features (prosody, tone), not just text.</li>
</ul>

<h2 id="7-challenges">7. Challenges</h2>

<ol>
  <li><strong>Catastrophic Forgetting:</strong> Fine-tuning on Task B makes it forget Task A.
    <ul>
      <li><strong>Fix:</strong> Replay buffers (mix old data with new).</li>
    </ul>
  </li>
  <li><strong>Negative Transfer:</strong> Task A hurts Task B.
    <ul>
      <li>Example: Speaker Identification (needs speaker info) vs. ASR (needs to ignore speaker info).</li>
      <li><strong>Fix:</strong> Task-specific adapters.</li>
    </ul>
  </li>
</ol>

<h2 id="8-summary">8. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Feature</th>
      <th style="text-align: left">Single-Task</th>
      <th style="text-align: left">Multi-Task</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Performance</strong></td>
      <td style="text-align: left">High on specific task</td>
      <td style="text-align: left">High on all (usually)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Data Req</strong></td>
      <td style="text-align: left">High labeled data</td>
      <td style="text-align: left">Can leverage auxiliary data</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Deployment</strong></td>
      <td style="text-align: left">N models</td>
      <td style="text-align: left">1 model</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Training</strong></td>
      <td style="text-align: left">Simple</td>
      <td style="text-align: left">Complex (balancing)</td>
    </tr>
  </tbody>
</table>

<h2 id="9-deep-dive-gradient-surgery-pcgrad">9. Deep Dive: Gradient Surgery (PCGrad)</h2>

<p>When training on Task A (ASR) and Task B (Translation), the gradients might conflict.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">\nabla L_A</code> says “increase weight <code class="language-plaintext highlighter-rouge">w</code>”.</li>
  <li><code class="language-plaintext highlighter-rouge">\nabla L_B</code> says “decrease weight <code class="language-plaintext highlighter-rouge">w</code>”.</li>
  <li>Result: They cancel out, or the model oscillates.</li>
</ul>

<p><strong>PCGrad Algorithm:</strong></p>
<ol>
  <li>Compute gradients <code class="language-plaintext highlighter-rouge">g_A</code> and <code class="language-plaintext highlighter-rouge">g_B</code> independently.</li>
  <li>Check cosine similarity: if <code class="language-plaintext highlighter-rouge">g_A \cdot g_B &lt; 0</code> (angle &gt; 90 degrees), they conflict.</li>
  <li>Project <code class="language-plaintext highlighter-rouge">g_A</code> onto the normal plane of <code class="language-plaintext highlighter-rouge">g_B</code>:
 <code class="language-plaintext highlighter-rouge">g_A' = g_A - \frac{g_A \cdot g_B}{\|g_B\|^2} g_B</code></li>
  <li>Do the same for <code class="language-plaintext highlighter-rouge">g_B</code>.</li>
  <li>Update weights with <code class="language-plaintext highlighter-rouge">g_A' + g_B'</code>.</li>
</ol>

<p><strong>Effect:</strong> The optimization trajectory follows a “zigzag” path that satisfies both tasks without destructive interference.</p>

<h2 id="10-deep-dive-uncertainty-weighting">10. Deep Dive: Uncertainty Weighting</h2>

<p>How do we set <code class="language-plaintext highlighter-rouge">\lambda_1, \lambda_2</code> in the loss function?
<code class="language-plaintext highlighter-rouge">L = \lambda_1 L_{ASR} + \lambda_2 L_{ST}</code></p>

<p><strong>Homoscedastic Uncertainty:</strong></p>
<ul>
  <li>Assume the task noise is constant.</li>
  <li>Learn <code class="language-plaintext highlighter-rouge">\sigma_1, \sigma_2</code> (variance) as trainable parameters.</li>
  <li>Loss becomes:
 <code class="language-plaintext highlighter-rouge">L = \frac{1}{2\sigma_1^2} L_{ASR} + \frac{1}{2\sigma_2^2} L_{ST} + \log \sigma_1 + \log \sigma_2</code></li>
  <li>If a task is noisy (high loss), the model increases <code class="language-plaintext highlighter-rouge">\sigma</code> to reduce its weight.</li>
  <li><strong>Result:</strong> Automatic, dynamic balancing.</li>
</ul>

<h2 id="11-deep-dive-adapter-modules">11. Deep Dive: Adapter Modules</h2>

<p>Fine-tuning a massive Multi-task model (like Whisper) for a new task is expensive.
<strong>Adapters</strong> allow efficient transfer learning.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li>Freeze the pre-trained Transformer layers.</li>
  <li>Insert small “Adapter” layers between Feed-Forward and Self-Attention blocks.</li>
  <li>Adapter = <code class="language-plaintext highlighter-rouge">Linear(d -&gt; d/r) -&gt; ReLU -&gt; Linear(d/r -&gt; d)</code>.</li>
  <li>Only train the Adapters (few parameters).</li>
</ul>

<p><strong>Task-Specific Adapters:</strong></p>
<ul>
  <li>Train one set of adapters for “Medical ASR”.</li>
  <li>Train another set for “Legal ASR”.</li>
  <li>Switch adapters at runtime based on user domain.</li>
</ul>

<h2 id="12-deep-dive-whispers-weak-supervision-strategy">12. Deep Dive: Whisper’s Weak Supervision Strategy</h2>

<p>Most ASR models are trained on LibriSpeech (clean, read audio). They fail on real-world noise.
Whisper trained on <strong>680,000 hours</strong> of internet audio.</p>

<p><strong>Data Filtering:</strong></p>
<ol>
  <li><strong>Language ID:</strong> Discard if audio language doesn’t match transcript language.</li>
  <li><strong>No Speech:</strong> Discard if VAD detects silence.</li>
  <li><strong>Machine Generated:</strong> Discard if transcript looks like output of another ASR system (to avoid learning errors).</li>
</ol>

<p><strong>Result:</strong></p>
<ul>
  <li>Whisper is not SOTA on LibriSpeech (Clean).</li>
  <li>But it is <strong>SOTA on Robustness</strong> (Accents, Noise, Music).</li>
</ul>

<h2 id="13-code-pytorch-multi-task-model">13. Code: PyTorch Multi-task Model</h2>

<p>A simple implementation of a shared encoder with multiple heads.</p>

<p>``python
import torch
import torch.nn as nn</p>

<p>class MultiTaskSpeechModel(nn.Module):
 def <strong>init</strong>(self, input_dim, vocab_size, num_intents):
 super().<strong>init</strong>()</p>

<p># Shared Encoder (e.g., Conformer or LSTM)
 self.encoder = nn.LSTM(input_dim, 512, num_layers=3, batch_first=True, bidirectional=True)</p>

<p># Task 1: ASR (CTC Head)
 self.asr_head = nn.Linear(1024, vocab_size)</p>

<p># Task 2: Intent Classification (SLU Head)
 # Use the last hidden state for classification
 self.intent_head = nn.Sequential(
 nn.Linear(1024, 256),
 nn.ReLU(),
 nn.Linear(256, num_intents)
 )</p>

<p>def forward(self, x):
 # x: (Batch, Time, Feats)
 encoder_out, (h_n, c_n) = self.encoder(x)</p>

<p># ASR Output: (Batch, Time, Vocab)
 asr_logits = self.asr_head(encoder_out)</p>

<p># Intent Output: (Batch, Num_Intents)
 # Pool over time (e.g., take last state or mean)
 # Here taking mean of encoder outputs
 context = torch.mean(encoder_out, dim=1)
 intent_logits = self.intent_head(context)</p>

<p>return asr_logits, intent_logits</p>

<h1 id="loss-calculation">Loss Calculation</h1>
<p>def compute_loss(asr_logits, asr_targets, intent_logits, intent_targets):
 loss_asr = nn.CTCLoss()(asr_logits.transpose(0, 1), asr_targets, …)
 loss_intent = nn.CrossEntropyLoss()(intent_logits, intent_targets)</p>

<p># Simple weighting
 return loss_asr + 0.5 * loss_intent
``</p>

<h2 id="14-system-design-real-time-translation-babelfish">14. System Design: Real-Time Translation (Babelfish)</h2>

<p><strong>Scenario:</strong> Build a “Universal Translator” device.
<strong>Input:</strong> Audio (Spanish). <strong>Output:</strong> Audio (English).</p>

<p><strong>Pipeline:</strong></p>
<ol>
  <li><strong>VAD:</strong> Detect speech.</li>
  <li><strong>S2ST Model (Speech-to-Speech Translation):</strong>
    <ul>
      <li><strong>Direct S2ST (Translatotron 2):</strong> Audio -&gt; Spectrogram. No text intermediate.</li>
      <li><strong>Cascaded:</strong> ASR -&gt; MT -&gt; TTS.</li>
    </ul>
  </li>
  <li><strong>Streaming:</strong>
    <ul>
      <li>Use <strong>Wait-k Policy</strong>: Wait for <code class="language-plaintext highlighter-rouge">k</code> words before translating.</li>
      <li>Trade-off: Latency vs. Context (Accuracy).</li>
    </ul>
  </li>
</ol>

<ul>
  <li>Trade-off: Latency vs. Context (Accuracy).</li>
</ul>

<h2 id="15-deep-dive-joint-ctc-attention-training">15. Deep Dive: Joint CTC-Attention Training</h2>

<p><strong>The Hybrid Approach:</strong>
Most modern ASR systems (ESPnet, WeNet) use <strong>Hybrid CTC/Attention</strong>.</p>

<p><strong>Why?</strong></p>
<ul>
  <li><strong>Attention:</strong> Good at modeling long-range dependencies and language semantics. Bad at monotonic alignment (can loop or skip).</li>
  <li><strong>CTC:</strong> Enforces monotonic alignment. Good at timing. Bad at conditional dependence.</li>
</ul>

<p><strong>Training Objective:</strong>
<code class="language-plaintext highlighter-rouge">L = \alpha L_{CTC} + (1 - \alpha) L_{Attn}</code></p>
<ul>
  <li>Typically <code class="language-plaintext highlighter-rouge">\alpha = 0.3</code>.</li>
  <li>The Encoder is shared.</li>
  <li>The CTC head branches off the Encoder.</li>
  <li>The Attention Decoder sits on top of the Encoder.</li>
</ul>

<p><strong>Inference (Decoding):</strong></p>
<ul>
  <li><strong>Attention Rescoring:</strong>
    <ol>
      <li>Generate top-k hypotheses using CTC (fast).</li>
      <li>Rescore them using the Attention Decoder (accurate).</li>
    </ol>
  </li>
  <li><strong>Joint Decoding:</strong></li>
  <li>Combine scores at each beam search step:
 <code class="language-plaintext highlighter-rouge">Score = \lambda \log P_{CTC}(y|x) + (1-\lambda) \log P_{Attn}(y|x)</code></li>
</ul>

<h2 id="16-deep-dive-multilingual-asr-as-multi-task-learning">16. Deep Dive: Multilingual ASR as Multi-task Learning</h2>

<p>Training on 100 languages is effectively a 100-task problem.</p>

<p><strong>Strategies:</strong></p>
<ol>
  <li><strong>Language ID (LID) Prediction:</strong>
    <ul>
      <li>Add an auxiliary head to predict the language.</li>
      <li>Helps the encoder separate language-specific features (phonemes) from language-agnostic features (speaker voice).</li>
    </ul>
  </li>
  <li><strong>Shared vs. Specific Layers:</strong>
    <ul>
      <li><strong>Shared:</strong> Bottom layers (acoustic features are universal).</li>
      <li><strong>Specific:</strong> Top layers (phonotactics and grammar vary).</li>
      <li><strong>Adapter Modules:</strong> Insert language-specific adapters.</li>
    </ul>
  </li>
  <li><strong>Script Unification:</strong>
    <ul>
      <li>Map all languages to IPA (International Phonetic Alphabet) or use a shared SentencePiece vocabulary (e.g., 100k tokens covering all scripts).</li>
    </ul>
  </li>
</ol>

<h2 id="17-deep-dive-voice-activity-detection-vad-integration">17. Deep Dive: Voice Activity Detection (VAD) Integration</h2>

<p><strong>Problem:</strong> ASR models hallucinate text during silence.
<strong>Solution:</strong> Multi-task learning with VAD.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Main Task:</strong> ASR (Seq2Seq).</li>
  <li><strong>Auxiliary Task:</strong> Frame-level binary classification (Speech vs. Silence).</li>
  <li><strong>Loss:</strong> <code class="language-plaintext highlighter-rouge">L_{ASR} + \lambda L_{VAD}</code>.</li>
</ul>

<p><strong>Benefit:</strong></p>
<ul>
  <li>Forces the encoder to learn a robust “speech presence” feature.</li>
  <li>During inference, the VAD head can be used to gate the decoder (don’t decode if VAD &lt; 0.5).</li>
</ul>

<h2 id="18-deep-dive-speaker-diarization-as-an-auxiliary-task">18. Deep Dive: Speaker Diarization as an Auxiliary Task</h2>

<p><strong>Scenario:</strong> “Who spoke when?”</p>

<p><strong>E2E ASR-Diarization (E2E-ASR-DA):</strong></p>
<ul>
  <li><strong>Output:</strong> Instead of just text, output <code class="language-plaintext highlighter-rouge">(Speaker_ID, Word)</code>.</li>
  <li>Example: <code class="language-plaintext highlighter-rouge">&lt;spk:1&gt; Hello &lt;spk:2&gt; Hi there</code>.</li>
</ul>

<p><strong>Serialized Output Training (SOT):</strong></p>
<ul>
  <li>For overlapping speech.</li>
  <li>Output Speaker 1’s utterance first, then Speaker 2’s.</li>
  <li>Separated by a delimiter token <code class="language-plaintext highlighter-rouge">&lt;sep&gt;</code>.</li>
</ul>

<p><strong>Multi-task Benefit:</strong></p>
<ul>
  <li>Learning to distinguish speakers helps ASR in “Cocktail Party” scenarios (overlapping speech).</li>
</ul>

<h2 id="19-case-study-metas-massively-multilingual-speech-mms">19. Case Study: Meta’s Massively Multilingual Speech (MMS)</h2>

<p><strong>Goal:</strong> ASR and TTS for <strong>1,100+ languages</strong>.</p>

<p><strong>Data:</strong></p>
<ul>
  <li><strong>Religious Texts:</strong> The Bible is translated into thousands of languages.</li>
  <li><strong>Alignment:</strong> Use Forced Alignment on Bible audiobooks to create labeled data.</li>
</ul>

<p><strong>Model:</strong></p>
<ul>
  <li><strong>Wav2Vec 2.0</strong> pre-training on 500k hours of unlabeled audio (1,400 languages).</li>
  <li><strong>Fine-tuning:</strong> Linear layers (adapters) for each language.</li>
</ul>

<p><strong>Result:</strong></p>
<ul>
  <li>Half the WER of Whisper on low-resource languages.</li>
  <li>Proves that <strong>Self-Supervised Learning + Multi-task Fine-tuning</strong> is the recipe for scale.</li>
</ul>

<h2 id="20-case-study-googles-universal-speech-model-usm">20. Case Study: Google’s Universal Speech Model (USM)</h2>

<p><strong>Architecture:</strong> Conformer (2 Billion parameters).</p>

<p><strong>Training Objectives (MOST):</strong></p>
<ol>
  <li><strong>BEST-RQ:</strong> Self-supervised learning (BERT on audio).</li>
  <li><strong>Text-Injection:</strong> Train the encoder on text-only data (by upsampling text to match audio length).</li>
  <li><strong>ASR:</strong> Supervised learning on 300 languages.</li>
  <li><strong>Automatic Speech Translation (AST):</strong> Translate audio directly to English text.</li>
</ol>

<p><strong>Key Innovation:</strong></p>
<ul>
  <li><strong>Chunk-wise Attention:</strong> To handle long audio (YouTube videos).</li>
  <li><strong>Result:</strong> SOTA on YouTube captioning.</li>
</ul>

<h2 id="21-system-design-scalable-multi-task-training-pipeline">21. System Design: Scalable Multi-task Training Pipeline</h2>

<p><strong>Challenge:</strong> Training on 10 datasets (ASR, ST, VAD) with different sizes and formats.</p>

<p><strong>Pipeline:</strong></p>
<ol>
  <li><strong>Data Loader (The “Mixer”):</strong>
    <ul>
      <li>Samples batches from different datasets based on a probability distribution <code class="language-plaintext highlighter-rouge">p_i</code>.</li>
      <li><strong>Temperature Sampling:</strong> <code class="language-plaintext highlighter-rouge">p_i \propto |D_i|^{1/T}</code>.</li>
      <li><code class="language-plaintext highlighter-rouge">T=1</code>: Proportional to dataset size (starves small tasks).</li>
      <li><code class="language-plaintext highlighter-rouge">T=\infty</code>: Uniform sampling (overfits small tasks).</li>
      <li><code class="language-plaintext highlighter-rouge">T=5</code>: Good balance.</li>
    </ul>
  </li>
  <li><strong>Bucketing:</strong>
    <ul>
      <li>Group audio files by length to minimize padding.</li>
      <li>Crucial for GPU efficiency.</li>
    </ul>
  </li>
  <li><strong>Distributed Training:</strong>
    <ul>
      <li><strong>FSDP (Fully Sharded Data Parallel):</strong> Shard model parameters across GPUs.</li>
      <li><strong>Gradient Accumulation:</strong> Simulate large batch sizes.</li>
    </ul>
  </li>
</ol>

<h2 id="22-deep-dive-evaluation-metrics-for-multi-task-models">22. Deep Dive: Evaluation Metrics for Multi-task Models</h2>

<p>How do you evaluate a “Swiss Army Knife” model?</p>

<ol>
  <li><strong>Average Performance:</strong>
    <ul>
      <li>Normalize metrics (WER, BLEU, F1) to 0-100 scale.</li>
      <li>Compute geometric mean.</li>
    </ul>
  </li>
  <li><strong>Pareto Frontier:</strong>
    <ul>
      <li>Plot ASR Accuracy vs. ST Accuracy.</li>
      <li>Does improving one hurt the other? (Negative Transfer).</li>
    </ul>
  </li>
  <li><strong>Zero-Shot Transfer:</strong>
    <ul>
      <li>Train on English ASR + French ASR.</li>
      <li>Test on English-to-French Translation.</li>
      <li>(Emergent ability).</li>
    </ul>
  </li>
</ol>

<h2 id="23-code-implementing-uncertainty-weighting">23. Code: Implementing Uncertainty Weighting</h2>

<p>Let’s implement the learnable loss weights in PyTorch.</p>

<p>``python
import torch
import torch.nn as nn</p>

<p>class MultiTaskLoss(nn.Module):
 def <strong>init</strong>(self, num_tasks):
 super().<strong>init</strong>()
 # Learnable log variances (sigma^2)
 # Initialize to 0 (variance = 1)
 self.log_vars = nn.Parameter(torch.zeros(num_tasks))</p>

<p>def forward(self, losses):
 # losses: list of scalar tensors [L1, L2, …]
 total_loss = 0
 for i, loss in enumerate(losses):
 # Precision = 1 / (2 * sigma^2)
 precision = 0.5 * torch.exp(-self.log_vars[i])</p>

<p># L = precision * loss + log(sigma)
 # log(sigma) = 0.5 * log_var
 total_loss += precision * loss + 0.5 * self.log_vars[i]</p>

<p>return total_loss</p>

<h1 id="usage">Usage</h1>
<p>mtl_criterion = MultiTaskLoss(num_tasks=2)
optimizer = torch.optim.Adam(
 list(model.parameters()) + list(mtl_criterion.parameters()), 
 lr=1e-4
)</p>

<h1 id="training-loop">Training Loop</h1>
<p>loss_asr = criterion_asr(pred_asr, target_asr)
loss_st = criterion_st(pred_st, target_st)</p>

<p>loss = mtl_criterion([loss_asr, loss_st])
loss.backward()
optimizer.step()
``</p>

<h2 id="24-future-trends-foundation-models-speechllm">24. Future Trends: Foundation Models (SpeechLLM)</h2>

<p><strong>The End of Task-Specific Heads?</strong></p>

<p><strong>SpeechLLM (2024+):</strong></p>
<ul>
  <li><strong>Input:</strong> Audio Tokens + Text Tokens.</li>
  <li><strong>Output:</strong> Text Tokens (or Audio Tokens).</li>
  <li><strong>Task Specification:</strong> Just a text prompt.</li>
  <li>“Transcribe this audio.”</li>
  <li>“Who is speaking?”</li>
  <li>“Is the speaker angry?”</li>
  <li><strong>Architecture:</strong> Decoder-only Transformer (GPT-4 style).</li>
  <li><strong>Training:</strong> Next-token prediction on massive mixed-modal data.</li>
</ul>

<p><strong>Implication:</strong> Multi-task learning becomes implicit. The model learns tasks as “in-context learning”.</p>

<ul>
  <li><strong>Training:</strong> Next-token prediction on massive mixed-modal data.</li>
  <li><strong>Implication:</strong> Multi-task learning becomes implicit. The model learns tasks as “in-context learning”.</li>
</ul>

<h2 id="25-deep-dive-spoken-language-understanding-slu-tasks">25. Deep Dive: Spoken Language Understanding (SLU) Tasks</h2>

<p>SLU goes beyond transcription. It extracts meaning.</p>

<p><strong>1. Intent Classification:</strong></p>
<ul>
  <li><strong>Input:</strong> Audio.</li>
  <li><strong>Output:</strong> Class label (e.g., <code class="language-plaintext highlighter-rouge">PlayMusic</code>, <code class="language-plaintext highlighter-rouge">SetAlarm</code>).</li>
  <li><strong>Multi-task Benefit:</strong> ASR learns phonemes; Intent learns semantics. Together, they are robust to noise.</li>
</ul>

<p><strong>2. Slot Filling:</strong></p>
<ul>
  <li><strong>Input:</strong> Audio.</li>
  <li><strong>Output:</strong> Sequence of tags (BIO format).</li>
  <li><code class="language-plaintext highlighter-rouge">Play [Song: Despacito] by [Artist: Luis Fonsi]</code>.</li>
  <li><strong>Architecture:</strong> ASR Encoder -&gt; Slot Decoder (CRF or LSTM).</li>
</ul>

<p><strong>3. Sentiment Analysis:</strong></p>
<ul>
  <li><strong>Input:</strong> Audio.</li>
  <li><strong>Output:</strong> Positive/Negative/Neutral.</li>
  <li><strong>Why Audio?</strong> Sarcasm (“Yeah, right”) is detected via pitch/tone, not text.</li>
  <li><strong>Fusion:</strong> Concatenate Text Embeddings (from ASR) + Audio Embeddings (from Encoder) -&gt; Classifier.</li>
</ul>

<h2 id="26-deep-dive-emotion-recognition-as-an-auxiliary-task">26. Deep Dive: Emotion Recognition as an Auxiliary Task</h2>

<p><strong>SER (Speech Emotion Recognition):</strong></p>
<ul>
  <li>Classes: Happy, Sad, Angry, Neutral.</li>
</ul>

<p><strong>Why Multi-task with ASR?</strong></p>
<ul>
  <li><strong>ASR helps SER:</strong> Knowing <em>what</em> is said helps determine emotion.</li>
  <li><strong>SER helps ASR:</strong> Emotional speech (shouting, crying) has different acoustic properties. Explicitly modeling emotion helps the encoder normalize these variations.</li>
</ul>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Shared Encoder:</strong> Wav2Vec 2.0.</li>
  <li><strong>ASR Head:</strong> CTC/Attention.</li>
  <li><strong>SER Head:</strong> Pooling Layer -&gt; Linear -&gt; Softmax.</li>
</ul>

<h2 id="27-deep-dive-accent-classification">27. Deep Dive: Accent Classification</h2>

<p><strong>Problem:</strong> ASR fails on unseen accents.
<strong>Solution:</strong> Multi-task learning with Accent ID.</p>

<p><strong>Method:</strong></p>
<ol>
  <li><strong>Auxiliary Task:</strong> Predict accent (US, UK, Indian, Australian).</li>
  <li><strong>Gradient Reversal Layer (GRL):</strong>
    <ul>
      <li>We want the encoder to be <strong>Accent-Invariant</strong>.</li>
      <li>Add a GRL before the Accent Classifier.</li>
      <li>During backprop, flip the gradient sign.</li>
      <li>The encoder tries to <em>maximize</em> accent classification error (remove accent info), while the classifier tries to minimize it.</li>
      <li><strong>Result:</strong> Robust, accent-agnostic features.</li>
    </ul>
  </li>
</ol>

<h2 id="28-code-implementing-gradient-surgery-pcgrad">28. Code: Implementing Gradient Surgery (PCGrad)</h2>

<p>Here is a simplified implementation of PCGrad in PyTorch.</p>

<p>``python
import torch
import random</p>

<p>class PCGradOptimizer:
 def <strong>init</strong>(self, optimizer):
 self.optimizer = optimizer</p>

<p>def step(self, objectives):
 “””
 objectives: list of losses [loss_task1, loss_task2]
 “””
 grads = []
 self.optimizer.zero_grad()</p>

<p># 1. Compute gradients for each task independently
 for loss in objectives:
 loss.backward(retain_graph=True)
 grad_list = []
 for param in self.optimizer.param_groups[0][‘params’]:
 if param.grad is not None:
 grad_list.append(param.grad.clone())
 param.grad.zero_() # Clear for next task
 grads.append(grad_list)</p>

<p># 2. Project conflicting gradients
 # Shuffle order to avoid bias
 random.shuffle(grads)</p>

<p>final_grads = [g[:] for g in grads] # Deep copy</p>

<p>for i in range(len(grads)):
 for j in range(len(grads)):
 if i == j: continue</p>

<p># Flatten gradients to compute dot product
 g_i_flat = torch.cat([g.flatten() for g in grads[i]])
 g_j_flat = torch.cat([g.flatten() for g in grads[j]])</p>

<p>dot_prod = torch.dot(g_i_flat, g_j_flat)</p>

<p>if dot_prod &lt; 0: # Conflict!
 # Project g_i onto normal plane of g_j
 # g_i = g_i - (g_i . g_j) / ||g_j||^2 * g_j
 norm_sq = torch.dot(g_j_flat, g_j_flat)
 scale = dot_prod / norm_sq</p>

<p>for k in range(len(grads[i])):
 final_grads[i][k] -= scale * grads[j][k]</p>

<p># 3. Apply final gradients
 for i, param in enumerate(self.optimizer.param_groups[0][‘params’]):
 if param.grad is None:
 param.grad = torch.zeros_like(param)</p>

<p># Sum projected gradients from all tasks
 for task_idx in range(len(grads)):
 param.grad += final_grads[task_idx][i]</p>

<p>self.optimizer.step()
``</p>

<h2 id="29-checklist-for-multi-task-training">29. Checklist for Multi-task Training</h2>

<p>Before you start training:</p>

<ol class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Data Balance:</strong> Are tasks roughly equal in size? If not, use temperature sampling.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Loss Scale:</strong> Do losses have similar magnitude? (e.g., ASR loss is 100, Class loss is 1). Normalize them.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Capacity:</strong> Is the model big enough? Multi-tasking requires more capacity than single-task.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Scheduling:</strong> Should you start with all tasks, or introduce them sequentially (Curriculum)?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Evaluation:</strong> Do you have a separate validation set for <em>each</em> task?</li>
</ol>

<table>
  <tbody>
    <tr>
      <td><strong>Robustness</strong></td>
      <td>Low</td>
      <td>High (Accent/Noise invariant)</td>
    </tr>
  </tbody>
</table>

<h2 id="31-deep-dive-zero-shot-transfer-in-multi-task-models">31. Deep Dive: Zero-Shot Transfer in Multi-task Models</h2>

<p><strong>The Magic:</strong> Train on Task A and B, test on Task C (which was never seen).</p>

<p><strong>Example: Zero-Shot Speech Translation</strong></p>
<ul>
  <li><strong>Train:</strong>
    <ol>
      <li>English Audio -&gt; English Text (ASR).</li>
      <li>English Text -&gt; French Text (MT).</li>
    </ol>
  </li>
  <li><strong>Test:</strong> English Audio -&gt; French Text (ST).</li>
  <li><strong>Mechanism:</strong> If the model learns a shared embedding space for “Audio” and “Text”, it can bridge the gap.</li>
</ul>

<p><strong>Example: Zero-Shot Language Transfer</strong></p>
<ul>
  <li><strong>Train:</strong>
    <ol>
      <li>English ASR.</li>
      <li>French ASR.</li>
      <li>English-to-Spanish Translation.</li>
    </ol>
  </li>
  <li><strong>Test:</strong> French-to-Spanish Translation.</li>
  <li><strong>Mechanism:</strong> The “Translation” head learns to map semantic concepts to Spanish, regardless of the source language (if the encoder is language-agnostic).</li>
</ul>

<h2 id="32-deep-dive-the-curse-of-multilingualism">32. Deep Dive: The “Curse of Multilingualism”</h2>

<p><strong>Observation:</strong> Adding languages improves performance initially (transfer learning), but eventually degrades it (interference).</p>

<p><strong>The Capacity Bottleneck:</strong></p>
<ul>
  <li>A fixed-size model has limited capacity.</li>
  <li>English takes up 50% of the weights.</li>
  <li>Adding 99 more languages forces them to fight for the remaining 50%.</li>
  <li><strong>Result:</strong> High-resource languages (English) degrade slightly; low-resource languages improve massively.</li>
</ul>

<p><strong>Solution:</strong></p>
<ol>
  <li><strong>Increase Model Size:</strong> 1B -&gt; 10B parameters.</li>
  <li><strong>Mixture of Experts (MoE):</strong>
    <ul>
      <li>Have 100 “Expert” FFN layers.</li>
      <li>For each token, route it to the top-2 experts.</li>
      <li><strong>Result:</strong> Massive capacity (1 Trillion params) with low inference cost (active params are small).</li>
    </ul>
  </li>
</ol>

<h2 id="33-deep-dive-adapterfusion">33. Deep Dive: AdapterFusion</h2>

<p><strong>Problem:</strong> We have separate adapters for ASR, ST, and VAD. Can we combine them?</p>

<p><strong>AdapterFusion (Pfeiffer et al.):</strong></p>
<ol>
  <li><strong>Train:</strong> Train adapters for each task independently.</li>
  <li><strong>Fuse:</strong> Freeze adapters. Learn a <strong>Fusion Layer</strong> (Attention) that combines their outputs.
 <code class="language-plaintext highlighter-rouge">h_{fused} = \text{Attn}(h_{enc}, [h_{ASR}, h_{ST}, h_{VAD}])</code></li>
  <li><strong>Benefit:</strong> The model dynamically decides: “For this noisy frame, I’ll trust the VAD adapter more. For this clear speech, I’ll trust the ASR adapter.”</li>
</ol>

<h2 id="34-case-study-tuning-whisper-for-code-switching">34. Case Study: Tuning Whisper for Code-Switching</h2>

<p><strong>Scenario:</strong> “Hinglish” (Hindi + English mixed).</p>
<ul>
  <li>“Main kal market jaaunga to buy vegetables.”</li>
</ul>

<p><strong>Challenge:</strong></p>
<ul>
  <li>Monolingual ASR fails (expects only Hindi or only English).</li>
  <li>Language ID flips rapidly.</li>
</ul>

<p><strong>Multi-task Solution:</strong></p>
<ol>
  <li><strong>Data:</strong> Synthetic Code-Switching.
    <ul>
      <li>Take English sentence.</li>
      <li>Replace random nouns with Hindi translations.</li>
      <li>Generate audio using TTS.</li>
    </ul>
  </li>
  <li><strong>Training:</strong> Fine-tune Whisper on this mixed data.</li>
  <li><strong>Result:</strong> The model learns to handle intra-sentence language switching without explicit language tags.</li>
</ol>

<h2 id="35-future-trends-speechllm-and-in-context-multi-tasking">35. Future Trends: SpeechLLM and “In-Context” Multi-tasking</h2>

<p><strong>Current:</strong> Explicit heads for ASR, ST.
<strong>Future:</strong> Text Prompting.</p>

<p><strong>AudioPaLM (Google):</strong></p>
<ul>
  <li>Unified vocabulary of Text Tokens and Audio Tokens.</li>
  <li><strong>Task:</strong> “Translate this audio to German.”</li>
  <li><strong>Input:</strong> <code class="language-plaintext highlighter-rouge">[AudioTokens] [Text: Translate to German]</code></li>
  <li><strong>Output:</strong> <code class="language-plaintext highlighter-rouge">[Text: German Translation]</code></li>
  <li>
    <p><strong>In-Context Learning:</strong> Provide 3 examples in the prompt, and the model learns the task on the fly without weight updates.</p>
  </li>
  <li><strong>In-Context Learning:</strong> Provide 3 examples in the prompt, and the model learns the task on the fly without weight updates.</li>
</ul>

<h2 id="36-deep-dive-the-cocktail-party-problem-source-separation">36. Deep Dive: The “Cocktail Party Problem” (Source Separation)</h2>

<p><strong>Scenario:</strong> Two people talking at once.
<strong>Goal:</strong> Separate them into two clean audio streams.</p>

<p><strong>Multi-task Approach:</strong></p>
<ul>
  <li><strong>Task 1:</strong> Separation (PIT - Permutation Invariant Training).</li>
  <li><strong>Task 2:</strong> ASR on separated streams.</li>
  <li><strong>Joint Training:</strong> Backpropagate ASR loss through the separator.</li>
  <li><strong>Result:</strong> The separator learns to output streams that are “ASR-friendly” (even if they sound slightly unnatural to humans).</li>
</ul>

<h2 id="37-deep-dive-audio-visual-multi-task-learning">37. Deep Dive: Audio-Visual Multi-task Learning</h2>

<p><strong>Lip Reading (Visual Speech Recognition):</strong></p>
<ul>
  <li><strong>Input:</strong> Audio + Video of lips.</li>
  <li><strong>Tasks:</strong>
    <ol>
      <li>Audio ASR.</li>
      <li>Video ASR.</li>
      <li>Audio-Visual Fusion.</li>
    </ol>
  </li>
  <li><strong>Benefit:</strong> When audio is noisy (0dB SNR), the model relies on the video stream (lip movement) to disambiguate phonemes (e.g., “P” vs “B”).</li>
</ul>

<h2 id="38-deep-dive-self-training-noisy-student">38. Deep Dive: Self-Training (Noisy Student)</h2>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Train Teacher on labeled data (ASR).</li>
  <li>Teacher generates pseudo-labels for unlabeled data.</li>
  <li><strong>Multi-task Student:</strong>
    <ul>
      <li>Train Student on Labeled Data (Supervised Loss).</li>
      <li>Train Student on Unlabeled Data (Consistency Loss).</li>
      <li><strong>Augmentation:</strong> Add noise (SpecAugment) to Student input, force it to match Teacher output.</li>
    </ul>
  </li>
  <li>Iterate.</li>
</ol>

<p><strong>Result:</strong> Massive improvements in robustness without new human labels.</p>

<ul>
  <li><strong>Result:</strong> Massive improvements in robustness without new human labels.</li>
  <li><strong>Augmentation:</strong> Add noise (SpecAugment) to Student input, force it to match Teacher output.</li>
</ul>

<h2 id="39-the-economics-of-multi-task-models">39. The Economics of Multi-task Models</h2>

<p><strong>Cost:</strong></p>
<ul>
  <li><strong>Training:</strong> Extremely expensive. Training Whisper-Large took thousands of GPU-days.</li>
  <li><strong>Inference:</strong> Large models (1B+ params) are slow and require expensive GPUs (A100).</li>
</ul>

<p><strong>Benefit:</strong></p>
<ul>
  <li><strong>Maintenance:</strong> Maintaining 1 model is cheaper than maintaining 10 specialized models.</li>
  <li><strong>Data Efficiency:</strong> You save millions on labeling costs because the model learns from unlabeled data and transfer learning.</li>
  <li><strong>User Experience:</strong> Seamless switching between languages and tasks (ASR -&gt; Translation) without latency spikes.</li>
</ul>

<p><strong>Verdict:</strong> For large tech companies, Multi-task is a no-brainer. For startups, fine-tuning a pre-trained Multi-task model (like Whisper) is the way to go.</p>

<h2 id="40-checklist-for-deployment">40. Checklist for Deployment</h2>

<ol class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Model Size:</strong> Can you afford to run <code class="language-plaintext highlighter-rouge">large-v3</code> (1.5GB VRAM) or do you need <code class="language-plaintext highlighter-rouge">tiny</code> (75MB)?</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Quantization:</strong> Use <code class="language-plaintext highlighter-rouge">int8</code> or <code class="language-plaintext highlighter-rouge">float16</code> to reduce memory by 2-4x with minimal accuracy loss.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Batching:</strong> Use dynamic batching (e.g., TorchServe) to saturate the GPU.</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Caching:</strong> Cache common audio queries (hash the audio file).</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" /><strong>Fallback:</strong> If the model fails (low confidence), do you have a fallback (e.g., a simpler model or human-in-the-loop)?</li>
</ol>

<ul>
  <li><strong>Fallback:</strong> If the model fails (low confidence), do you have a fallback (e.g., a simpler model or human-in-the-loop)?</li>
  <li><strong>Bias:</strong> Multi-task models can amplify bias. If the training data has more male speakers for ASR and female for TTS, the model might associate “male” with “input” and “female” with “output”.</li>
</ul>

<h2 id="41-ethical-considerations">41. Ethical Considerations</h2>

<p><strong>1. Bias Amplification:</strong></p>
<ul>
  <li>Multi-task models trained on internet data (Whisper) inherit internet biases.</li>
  <li><strong>Example:</strong> Translating “The doctor called the nurse” into a gendered language might default to “Male Doctor” and “Female Nurse” purely based on statistical co-occurrence, even if incorrect.</li>
</ul>

<p><strong>2. Representation:</strong></p>
<ul>
  <li>Low-resource languages often get “overwritten” by high-resource ones in shared capacity models.</li>
  <li><strong>Fix:</strong> Ensure strict data balancing and evaluation on <em>all</em> languages, not just the top 10.</li>
</ul>

<p><strong>3. Dual Use:</strong></p>
<ul>
  <li>A model good at Voice Cloning (TTS) and ASR can be used for deepfakes.</li>
  <li><strong>Safeguards:</strong> Release models with watermarking or restricted licenses.</li>
</ul>

<h2 id="42-further-reading">42. Further Reading</h2>

<ol>
  <li><strong>“Whisper: Robust Speech Recognition via Large-Scale Weak Supervision” (Radford et al., 2022):</strong> The bible of multi-task speech.</li>
  <li><strong>“UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data” (Wang et al., 2021):</strong> Combining SSL and Supervised learning.</li>
  <li><strong>“Gradient Surgery for Multi-Task Learning” (Yu et al., 2020):</strong> The PCGrad paper.</li>
  <li><strong>“Massively Multilingual Speech” (Pratap et al., 2023):</strong> Scaling to 1000 languages.</li>
  <li><strong>“AudioPaLM: A Large Language Model that Can Speak and Listen” (Rubenstein et al., 2023):</strong> The future of SpeechLLMs.</li>
</ol>

<h2 id="43-conclusion">43. Conclusion</h2>

<p>Multi-task learning is the key to building <strong>general-purpose speech systems</strong>. Instead of building one model for ASR, one for Translation, and one for VAD, we are moving towards <strong>Unified Foundation Models</strong> that can handle any speech task via prompting. The challenges of gradient conflict and capacity bottlenecks are being solved by techniques like PCGrad and Mixture of Experts. The future is not just multi-task, but <strong>multi-modal</strong> (Speech + Text + Vision).</p>

<h2 id="44-summary">44. Summary</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Feature</th>
      <th style="text-align: left">Single-Task</th>
      <th style="text-align: left">Multi-Task</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left"><strong>Performance</strong></td>
      <td style="text-align: left">High on specific task</td>
      <td style="text-align: left">High on all (usually)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Data Req</strong></td>
      <td style="text-align: left">High labeled data</td>
      <td style="text-align: left">Can leverage auxiliary data</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Deployment</strong></td>
      <td style="text-align: left">N models</td>
      <td style="text-align: left">1 model</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Training</strong></td>
      <td style="text-align: left">Simple</td>
      <td style="text-align: left">Complex (balancing)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Optimization</strong></td>
      <td style="text-align: left">Standard SGD</td>
      <td style="text-align: left">PCGrad, Uncertainty Weighting</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Robustness</strong></td>
      <td style="text-align: left">Low</td>
      <td style="text-align: left">High (Accent/Noise invariant)</td>
    </tr>
    <tr>
      <td style="text-align: left"><strong>Future</strong></td>
      <td style="text-align: left">Specialized Models</td>
      <td style="text-align: left">Foundation Models (SpeechLLM)</td>
    </tr>
  </tbody>
</table>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/">arunbaby.com/speech-tech/0036-multi-task-speech-learning</a></p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#multi-task-learning" class="page__taxonomy-item p-category" rel="tag">multi-task-learning</a><span class="sep">, </span>
    
      <a href="/tags/#slu" class="page__taxonomy-item p-category" rel="tag">slu</a><span class="sep">, </span>
    
      <a href="/tags/#translation" class="page__taxonomy-item p-category" rel="tag">translation</a><span class="sep">, </span>
    
      <a href="/tags/#whisper" class="page__taxonomy-item p-category" rel="tag">whisper</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech_tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0036-partition-equal-subset-sum/" rel="permalink">Partition Equal Subset Sum
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Can you split the treasure evenly?”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0036-resource-partitioning/" rel="permalink">Resource Partitioning in ML Clusters
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“How to share a supercomputer without stepping on each other’s toes.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0036-web-browsing-agents/" rel="permalink">Web Browsing Agents
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          18 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Turn the open web into a reliable tool: browse, extract, verify, and cite—without getting prompt-injected.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Multi-task+Speech+Learning%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0036-multi-task-speech-learning%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0036-multi-task-speech-learning%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0036-multi-task-speech-learning/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0035-speech-boundary-detection/" class="pagination--pager" title="Speech Boundary Detection">Previous</a>
    
    
      <a href="/speech-tech/0037-sequence-to-sequence-speech/" class="pagination--pager" title="Sequence-to-Sequence Speech Models">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
