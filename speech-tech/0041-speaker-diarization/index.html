<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Speaker Diarization - Arun Baby</title>
<meta name="description" content="“Who spoke when? The art of untangling voices.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Speaker Diarization">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0041-speaker-diarization/">


  <meta property="og:description" content="“Who spoke when? The art of untangling voices.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Speaker Diarization">
  <meta name="twitter:description" content="“Who spoke when? The art of untangling voices.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0041-speaker-diarization/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-14T22:21:53+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0041-speaker-diarization/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Speaker Diarization">
    <meta itemprop="description" content="“Who spoke when? The art of untangling voices.”">
    <meta itemprop="datePublished" content="2025-12-14T22:21:53+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0041-speaker-diarization/" itemprop="url">Speaker Diarization
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          20 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction">1. Introduction</a></li><li><a href="#2-problem-formulation">2. Problem Formulation</a></li><li><a href="#3-traditional-pipeline">3. Traditional Pipeline</a><ul><li><a href="#31-speech-activity-detection-sad">3.1. Speech Activity Detection (SAD)</a></li><li><a href="#32-segmentation">3.2. Segmentation</a></li><li><a href="#33-embedding-extraction">3.3. Embedding Extraction</a></li><li><a href="#34-clustering">3.4. Clustering</a></li></ul></li><li><a href="#4-deep-dive-x-vectors">4. Deep Dive: X-Vectors</a></li><li><a href="#5-deep-dive-clustering-algorithms">5. Deep Dive: Clustering Algorithms</a><ul><li><a href="#51-agglomerative-hierarchical-clustering-ahc">5.1. Agglomerative Hierarchical Clustering (AHC)</a></li><li><a href="#52-spectral-clustering">5.2. Spectral Clustering</a></li></ul></li><li><a href="#6-end-to-end-neural-diarization-eend">6. End-to-End Neural Diarization (EEND)</a></li><li><a href="#7-handling-overlapping-speech">7. Handling Overlapping Speech</a></li><li><a href="#8-system-design-real-time-diarization-for-video-conferencing">8. System Design: Real-Time Diarization for Video Conferencing</a></li><li><a href="#9-deep-dive-plda-probabilistic-linear-discriminant-analysis">9. Deep Dive: PLDA (Probabilistic Linear Discriminant Analysis)</a></li><li><a href="#10-evaluation-metrics">10. Evaluation Metrics</a></li><li><a href="#11-datasets">11. Datasets</a></li><li><a href="#12-interview-questions">12. Interview Questions</a></li><li><a href="#13-common-mistakes">13. Common Mistakes</a></li><li><a href="#14-advanced-eend-architectures">14. Advanced EEND Architectures</a><ul><li><a href="#141-eend-eda-encoder-decoder-attractor">14.1. EEND-EDA (Encoder-Decoder Attractor)</a></li><li><a href="#142-eend-with-self-attention">14.2. EEND with Self-Attention</a></li><li><a href="#143-eend-with-target-speaker-voice-activity-detection-ts-vad">14.3. EEND with Target-Speaker Voice Activity Detection (TS-VAD)</a></li></ul></li><li><a href="#15-production-case-study-zoom-diarization">15. Production Case Study: Zoom Diarization</a></li><li><a href="#16-multi-modal-diarization-audio--video">16. Multi-Modal Diarization (Audio + Video)</a></li><li><a href="#17-deep-dive-online-diarization">17. Deep Dive: Online Diarization</a></li><li><a href="#18-privacy--security">18. Privacy &amp; Security</a></li><li><a href="#19-deep-dive-speaker-change-detection">19. Deep Dive: Speaker Change Detection</a></li><li><a href="#20-production-monitoring">20. Production Monitoring</a></li><li><a href="#21-cost-analysis">21. Cost Analysis</a></li><li><a href="#22-advanced-technique-few-shot-speaker-diarization">22. Advanced Technique: Few-Shot Speaker Diarization</a></li><li><a href="#23-future-trends">23. Future Trends</a></li><li><a href="#24-benchmarking">24. Benchmarking</a></li><li><a href="#25-conclusion">25. Conclusion</a></li><li><a href="#26-deep-dive-resnet-based-x-vector-architecture">26. Deep Dive: ResNet-based X-Vector Architecture</a></li><li><a href="#27-deep-dive-voxceleb-dataset">27. Deep Dive: VoxCeleb Dataset</a></li><li><a href="#28-production-optimization-batch-processing">28. Production Optimization: Batch Processing</a></li><li><a href="#29-advanced-evaluation-detailed-error-analysis">29. Advanced Evaluation: Detailed Error Analysis</a></li><li><a href="#30-deep-dive-permutation-invariant-training-pit">30. Deep Dive: Permutation Invariant Training (PIT)</a></li><li><a href="#31-production-case-study-call-center-diarization">31. Production Case Study: Call Center Diarization</a></li><li><a href="#32-advanced-technique-self-supervised-learning-for-diarization">32. Advanced Technique: Self-Supervised Learning for Diarization</a></li><li><a href="#33-interview-deep-dive-diarization-vs-speaker-recognition">33. Interview Deep Dive: Diarization vs Speaker Recognition</a></li><li><a href="#34-future-trends-transformer-based-diarization">34. Future Trends: Transformer-based Diarization</a></li><li><a href="#35-conclusion--best-practices">35. Conclusion &amp; Best Practices</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Who spoke when? The art of untangling voices.”</strong></p>

<h2 id="1-introduction">1. Introduction</h2>

<p><strong>Speaker Diarization</strong> is the task of partitioning an audio stream into homogeneous segments according to the speaker identity. In simple terms: “Who spoke when?”</p>

<p><strong>Input:</strong> Audio recording (e.g., meeting, podcast, phone call).
<strong>Output:</strong> Timeline with speaker labels.</p>

<p><strong>Example:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[00:00 - 00:15] Speaker A: "Hello, how are you?"
[00:15 - 00:30] Speaker B: "I'm doing well, thanks!"
[00:30 - 00:45] Speaker A: "Great to hear."
</code></pre></div></div>

<p><strong>Applications:</strong></p>
<ul>
  <li><strong>Meeting Transcription:</strong> Zoom, Google Meet, Microsoft Teams.</li>
  <li><strong>Call Center Analytics:</strong> Separate agent from customer.</li>
  <li><strong>Podcast Indexing:</strong> Identify different speakers for search.</li>
  <li><strong>Forensics:</strong> Identify speakers in surveillance recordings.</li>
</ul>

<h2 id="2-problem-formulation">2. Problem Formulation</h2>

<p><strong>Challenges:</strong></p>
<ol>
  <li><strong>Unknown Number of Speakers:</strong> We don’t know how many people are speaking.</li>
  <li><strong>Overlapping Speech:</strong> Multiple people speaking simultaneously.</li>
  <li><strong>Variable Segment Length:</strong> Speakers may talk for 1 second or 10 minutes.</li>
  <li><strong>Acoustic Variability:</strong> Background noise, channel effects, emotion.</li>
</ol>

<p><strong>Metrics:</strong></p>
<ul>
  <li><strong>Diarization Error Rate (DER):</strong> Percentage of time that is incorrectly attributed.
    <ul>
      <li>$DER = \frac{FA + MISS + CONF}{TOTAL}$</li>
      <li><strong>FA (False Alarm):</strong> Non-speech detected as speech.</li>
      <li><strong>MISS:</strong> Speech detected as non-speech.</li>
      <li><strong>CONF (Confusion):</strong> Speech attributed to wrong speaker.</li>
    </ul>
  </li>
</ul>

<h2 id="3-traditional-pipeline">3. Traditional Pipeline</h2>

<p>The classic diarization system has 4 stages:</p>

<h3 id="31-speech-activity-detection-sad">3.1. Speech Activity Detection (SAD)</h3>
<ul>
  <li><strong>Goal:</strong> Remove silence and non-speech (music, noise).</li>
  <li><strong>Method:</strong> Energy-based VAD or DNN-based VAD.</li>
</ul>

<h3 id="32-segmentation">3.2. Segmentation</h3>
<ul>
  <li><strong>Goal:</strong> Split audio into short, homogeneous segments (e.g., 1-2 seconds).</li>
  <li><strong>Method:</strong>
    <ul>
      <li><strong>Fixed-Length:</strong> Simple, but may split mid-sentence.</li>
      <li><strong>Change Point Detection:</strong> Detect speaker changes using Bayesian Information Criterion (BIC) or GLR (Generalized Likelihood Ratio).</li>
    </ul>
  </li>
</ul>

<h3 id="33-embedding-extraction">3.3. Embedding Extraction</h3>
<ul>
  <li><strong>Goal:</strong> Convert each segment into a fixed-size vector (embedding) that represents the speaker’s voice.</li>
  <li><strong>Method:</strong>
    <ul>
      <li><strong>i-vectors:</strong> Traditional approach using GMM-UBM.</li>
      <li><strong>x-vectors:</strong> Deep learning approach using TDNN (Time-Delay Neural Network).</li>
      <li><strong>d-vectors:</strong> Trained with triplet loss.</li>
    </ul>
  </li>
</ul>

<h3 id="34-clustering">3.4. Clustering</h3>
<ul>
  <li><strong>Goal:</strong> Group segments by speaker.</li>
  <li><strong>Method:</strong>
    <ul>
      <li><strong>Agglomerative Hierarchical Clustering (AHC):</strong> Bottom-up clustering.</li>
      <li><strong>Spectral Clustering:</strong> Graph-based clustering.</li>
      <li><strong>PLDA (Probabilistic Linear Discriminant Analysis):</strong> Probabilistic scoring.</li>
    </ul>
  </li>
</ul>

<h2 id="4-deep-dive-x-vectors">4. Deep Dive: X-Vectors</h2>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Input:</strong> Mel-Frequency Cepstral Coefficients (MFCCs) or Mel-Filterbanks.</li>
  <li><strong>Layers:</strong>
    <ol>
      <li><strong>Frame-level TDNN:</strong> 5 layers with temporal context (e.g., [-2, +2] frames).</li>
      <li><strong>Statistics Pooling:</strong> Compute mean and standard deviation across time.</li>
      <li><strong>Segment-level Fully Connected:</strong> 2 layers.</li>
      <li><strong>Output:</strong> 512-dimensional embedding.</li>
    </ol>
  </li>
</ul>

<p><strong>Training:</strong></p>
<ul>
  <li><strong>Dataset:</strong> VoxCeleb (1M+ utterances, 7000+ speakers).</li>
  <li><strong>Loss:</strong> Softmax (speaker classification) or AAM-Softmax (Additive Angular Margin).</li>
</ul>

<p><strong>Inference:</strong></p>
<ul>
  <li>Extract x-vector for each segment.</li>
  <li>Compute cosine similarity between x-vectors.</li>
  <li>Cluster based on similarity.</li>
</ul>

<h2 id="5-deep-dive-clustering-algorithms">5. Deep Dive: Clustering Algorithms</h2>

<h3 id="51-agglomerative-hierarchical-clustering-ahc">5.1. Agglomerative Hierarchical Clustering (AHC)</h3>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Start with each segment as its own cluster.</li>
  <li><strong>Merge Step:</strong> Find the two closest clusters and merge them.</li>
  <li>Repeat until a stopping criterion is met (e.g., threshold on distance, or target number of clusters).</li>
</ol>

<p><strong>Distance Metrics:</strong></p>
<ul>
  <li><strong>Average Linkage:</strong> Average distance between all pairs.</li>
  <li><strong>Complete Linkage:</strong> Maximum distance between any pair.</li>
  <li><strong>PLDA Score:</strong> Probabilistic score.</li>
</ul>

<p><strong>Stopping Criterion:</strong></p>
<ul>
  <li><strong>Threshold:</strong> Stop when the minimum distance &gt; threshold.</li>
  <li><strong>Eigengap:</strong> Use spectral clustering to estimate the number of speakers.</li>
</ul>

<h3 id="52-spectral-clustering">5.2. Spectral Clustering</h3>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Construct an <strong>Affinity Matrix</strong> $A$ where $A_{ij} = \text{similarity}(x_i, x_j)$.</li>
  <li>Compute the <strong>Graph Laplacian</strong> $L = D - A$ where $D$ is the degree matrix.</li>
  <li>Compute the eigenvectors of $L$.</li>
  <li>Use the first $k$ eigenvectors as features.</li>
  <li>Apply k-means clustering.</li>
</ol>

<p><strong>Advantage:</strong> Can handle non-convex clusters.</p>

<h2 id="6-end-to-end-neural-diarization-eend">6. End-to-End Neural Diarization (EEND)</h2>

<p><strong>Idea:</strong> Train a single neural network to directly predict speaker labels for each frame.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Input:</strong> Mel-Filterbanks (e.g., 500 frames).</li>
  <li><strong>Encoder:</strong> Transformer or LSTM.</li>
  <li><strong>Output:</strong> $T \times K$ matrix where $T$ is time frames, $K$ is max number of speakers. Each entry is the probability that speaker $k$ is active at time $t$.</li>
</ul>

<p><strong>Training:</strong></p>
<ul>
  <li><strong>Loss:</strong> Binary Cross-Entropy for each speaker.</li>
  <li><strong>Permutation Invariant Training (PIT):</strong> Since speaker labels are arbitrary, we need to find the best permutation of predicted labels to match ground truth.</li>
</ul>

<p><strong>Advantage:</strong></p>
<ul>
  <li>No need for separate segmentation and clustering.</li>
  <li>Can handle overlapping speech naturally.</li>
</ul>

<p><strong>Disadvantage:</strong></p>
<ul>
  <li>Requires large amounts of labeled data.</li>
  <li>Fixed maximum number of speakers.</li>
</ul>

<h2 id="7-handling-overlapping-speech">7. Handling Overlapping Speech</h2>

<p>Traditional diarization assumes only one speaker at a time. In reality, people interrupt each other.</p>

<p><strong>Solutions:</strong></p>

<p><strong>1. Multi-Label Classification:</strong></p>
<ul>
  <li>Instead of assigning one speaker per frame, allow multiple speakers.</li>
  <li>Output: $T \times K$ binary matrix.</li>
</ul>

<p><strong>2. Source Separation:</strong></p>
<ul>
  <li>Use a speech separation model (e.g., Conv-TasNet) to separate overlapping speakers.</li>
  <li>Then run diarization on each separated stream.</li>
</ul>

<p><strong>3. EEND with Overlap:</strong></p>
<ul>
  <li>Train EEND to predict overlapping speakers.</li>
</ul>

<h2 id="8-system-design-real-time-diarization-for-video-conferencing">8. System Design: Real-Time Diarization for Video Conferencing</h2>

<p><strong>Scenario:</strong> Zoom meeting with 10 participants. We want to display speaker labels in real-time.</p>

<p><strong>Constraints:</strong></p>
<ul>
  <li><strong>Latency:</strong> &lt; 1 second.</li>
  <li><strong>Accuracy:</strong> DER &lt; 10%.</li>
  <li><strong>Scalability:</strong> Handle 100+ concurrent meetings.</li>
</ul>

<p><strong>Architecture:</strong></p>

<p><strong>Step 1: Audio Capture</strong></p>
<ul>
  <li>Each participant’s audio is streamed to the server.</li>
</ul>

<p><strong>Step 2: VAD</strong></p>
<ul>
  <li>Run a lightweight VAD model (e.g., WebRTC VAD) to detect speech.</li>
</ul>

<p><strong>Step 3: Embedding Extraction</strong></p>
<ul>
  <li>Use a streaming x-vector model.</li>
  <li>Extract embeddings every 1 second (with 0.5s overlap).</li>
</ul>

<p><strong>Step 4: Online Clustering</strong></p>
<ul>
  <li>Use <strong>Online AHC</strong> or <strong>Online Spectral Clustering</strong>.</li>
  <li>Update clusters incrementally as new embeddings arrive.</li>
</ul>

<p><strong>Step 5: Speaker Tracking</strong></p>
<ul>
  <li>Maintain a “speaker profile” for each participant.</li>
  <li>Match new embeddings to existing profiles.</li>
</ul>

<p><strong>Step 6: Display</strong></p>
<ul>
  <li>Send speaker labels back to clients.</li>
  <li>Display “Speaker A is talking” in the UI.</li>
</ul>

<p><strong>Optimization:</strong></p>
<ul>
  <li><strong>Batching:</strong> Process multiple meetings in parallel on GPU.</li>
  <li><strong>Caching:</strong> Cache embeddings for known speakers.</li>
</ul>

<h2 id="9-deep-dive-plda-probabilistic-linear-discriminant-analysis">9. Deep Dive: PLDA (Probabilistic Linear Discriminant Analysis)</h2>

<p>PLDA is a probabilistic model for speaker verification and diarization.</p>

<p><strong>Model:</strong>
$x = m + Fy + \epsilon$</p>
<ul>
  <li>$x$: Observed embedding (x-vector).</li>
  <li>$m$: Global mean.</li>
  <li>$F$: Factor loading matrix (speaker variability).</li>
  <li>$y$: Latent speaker factor.</li>
  <li>$\epsilon$: Residual noise.</li>
</ul>

<p><strong>Scoring:</strong>
Given two embeddings $x_1$ and $x_2$, compute the log-likelihood ratio:
$\text{score} = \log \frac{P(x_1, x_2 | \text{same speaker})}{P(x_1, x_2 | \text{different speakers})}$</p>

<p><strong>Use in Diarization:</strong></p>
<ul>
  <li>Use PLDA score as the distance metric in AHC.</li>
</ul>

<h2 id="10-evaluation-metrics">10. Evaluation Metrics</h2>

<p><strong>Diarization Error Rate (DER):</strong>
$DER = \frac{\text{False Alarm} + \text{Missed Speech} + \text{Speaker Confusion}}{\text{Total Speech Time}}$</p>

<p><strong>Jaccard Error Rate (JER):</strong>
Measures overlap between predicted and ground truth speaker segments.</p>

<p><strong>Optimal Mapping:</strong>
Since speaker labels are arbitrary (Speaker A vs Speaker 1), we need to find the optimal mapping between predicted and ground truth labels (Hungarian algorithm).</p>

<h2 id="11-datasets">11. Datasets</h2>

<p><strong>1. CALLHOME:</strong></p>
<ul>
  <li>Telephone conversations.</li>
  <li>2-7 speakers per conversation.</li>
  <li>Used in NIST evaluations.</li>
</ul>

<p><strong>2. AMI Meeting Corpus:</strong></p>
<ul>
  <li>Meeting recordings.</li>
  <li>4 speakers per meeting.</li>
  <li>Includes video and slides.</li>
</ul>

<p><strong>3. DIHARD:</strong></p>
<ul>
  <li>Diverse domains (meetings, interviews, broadcasts, YouTube).</li>
  <li>Challenging: overlapping speech, noise.</li>
</ul>

<p><strong>4. VoxConverse:</strong></p>
<ul>
  <li>YouTube videos.</li>
  <li>Variable number of speakers.</li>
</ul>

<h2 id="12-interview-questions">12. Interview Questions</h2>

<ol>
  <li><strong>Explain Speaker Diarization.</strong> What are the main challenges?</li>
  <li><strong>X-Vectors vs I-Vectors.</strong> What are the differences?</li>
  <li><strong>Clustering.</strong> Why use AHC instead of k-means?</li>
  <li><strong>Overlapping Speech.</strong> How do you handle it?</li>
  <li><strong>Real-Time Diarization.</strong> How would you design a system for live meetings?</li>
  <li><strong>Calculate DER.</strong> Given ground truth and predictions, compute DER.</li>
</ol>

<h2 id="13-common-mistakes">13. Common Mistakes</h2>

<ul>
  <li><strong>Ignoring Overlapping Speech:</strong> Assuming only one speaker at a time leads to high confusion errors.</li>
  <li><strong>Fixed Number of Speakers:</strong> Using k-means with a fixed $k$ when the number of speakers is unknown.</li>
  <li><strong>Poor VAD:</strong> If VAD misses speech or includes noise, diarization will fail.</li>
  <li><strong>Not Normalizing Embeddings:</strong> Cosine similarity requires normalized embeddings.</li>
  <li><strong>Overfitting to Domain:</strong> A model trained on telephone speech may fail on meeting recordings.</li>
</ul>

<h2 id="14-advanced-eend-architectures">14. Advanced EEND Architectures</h2>

<h3 id="141-eend-eda-encoder-decoder-attractor">14.1. EEND-EDA (Encoder-Decoder Attractor)</h3>

<p><strong>Problem:</strong> Standard EEND has a fixed maximum number of speakers.</p>

<p><strong>Solution:</strong> Use an encoder-decoder architecture with <strong>attractors</strong>.</p>

<p><strong>Architecture:</strong></p>
<ol>
  <li><strong>Encoder:</strong> Transformer encodes the audio.</li>
  <li><strong>Attractor Estimation:</strong> A decoder estimates the number of speakers and their “attractors” (prototype embeddings).</li>
  <li><strong>Speaker Assignment:</strong> For each frame, compute similarity to each attractor. Assign to the closest attractor.</li>
</ol>

<p><strong>Benefit:</strong> Handles variable number of speakers without retraining.</p>

<h3 id="142-eend-with-self-attention">14.2. EEND with Self-Attention</h3>

<p><strong>Idea:</strong> Use self-attention to model long-range dependencies between frames.</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Input:</strong> Mel-Filterbanks.</li>
  <li><strong>Encoder:</strong> Multi-head self-attention (Transformer).</li>
  <li><strong>Output:</strong> Speaker activity matrix.</li>
</ul>

<p><strong>Training:</strong></p>
<ul>
  <li><strong>Permutation Invariant Training (PIT):</strong> Find the best permutation of predicted speakers to match ground truth.</li>
  <li><strong>Loss:</strong> Binary Cross-Entropy for each speaker.</li>
</ul>

<p><strong>Result:</strong> State-of-the-art performance on CALLHOME (DER &lt; 10%).</p>

<h3 id="143-eend-with-target-speaker-voice-activity-detection-ts-vad">14.3. EEND with Target-Speaker Voice Activity Detection (TS-VAD)</h3>

<p><strong>Idea:</strong> Given a target speaker’s enrollment audio, detect when that speaker is active.</p>

<p><strong>Use Case:</strong> “Show me all segments where Alice spoke.”</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Input:</strong> (1) Meeting audio, (2) Enrollment audio of Alice.</li>
  <li><strong>Encoder:</strong> Extract embeddings for both.</li>
  <li><strong>Attention:</strong> Cross-attention between meeting and enrollment.</li>
  <li><strong>Output:</strong> Binary mask (Alice active or not).</li>
</ul>

<h2 id="15-production-case-study-zoom-diarization">15. Production Case Study: Zoom Diarization</h2>

<p><strong>Scenario:</strong> Zoom meeting with 10 participants. Display speaker labels in real-time.</p>

<p><strong>Challenges:</strong></p>
<ul>
  <li><strong>Latency:</strong> &lt; 1 second.</li>
  <li><strong>Accuracy:</strong> DER &lt; 10%.</li>
  <li><strong>Scalability:</strong> 100K+ concurrent meetings.</li>
</ul>

<p><strong>Solution:</strong></p>

<p><strong>Step 1: Audio Capture</strong></p>
<ul>
  <li>Each participant’s audio is streamed to the server (separate tracks).</li>
</ul>

<p><strong>Step 2: VAD</strong></p>
<ul>
  <li>Run WebRTC VAD on each track.</li>
</ul>

<p><strong>Step 3: Embedding Extraction</strong></p>
<ul>
  <li>Use a streaming x-vector model (ResNet-based).</li>
  <li>Extract embeddings every 1 second.</li>
</ul>

<p><strong>Step 4: Online Clustering</strong></p>
<ul>
  <li>Use <strong>Online AHC</strong> with PLDA scoring.</li>
  <li>Update clusters incrementally.</li>
</ul>

<p><strong>Step 5: Speaker Tracking</strong></p>
<ul>
  <li>Maintain a “speaker profile” for each participant.</li>
  <li>Use speaker verification to match new embeddings to profiles.</li>
</ul>

<p><strong>Step 6: Display</strong></p>
<ul>
  <li>Send speaker labels to clients via WebSocket.</li>
</ul>

<p><strong>Optimization:</strong></p>
<ul>
  <li><strong>GPU Batching:</strong> Process 100 meetings in parallel on a single V100.</li>
  <li><strong>Caching:</strong> Cache embeddings for known speakers (reduces compute by 50%).</li>
</ul>

<p><strong>Result:</strong></p>
<ul>
  <li><strong>Latency:</strong> 500ms.</li>
  <li><strong>DER:</strong> 8%.</li>
  <li><strong>Cost:</strong> $0.01/hour/meeting.</li>
</ul>

<h2 id="16-multi-modal-diarization-audio--video">16. Multi-Modal Diarization (Audio + Video)</h2>

<p><strong>Idea:</strong> Use visual cues (lip movement, face detection) to improve diarization.</p>

<p><strong>Approach:</strong></p>

<p><strong>1. Face Detection:</strong></p>
<ul>
  <li>Use MTCNN or RetinaFace to detect faces in each frame.</li>
</ul>

<p><strong>2. Active Speaker Detection (ASD):</strong></p>
<ul>
  <li>Train a model to predict if a face is speaking based on lip movement.</li>
  <li><strong>Input:</strong> Face crop + audio.</li>
  <li><strong>Output:</strong> Probability of speaking.</li>
</ul>

<p><strong>3. Fusion:</strong></p>
<ul>
  <li>Combine audio-based diarization with video-based ASD.</li>
  <li><strong>Rule:</strong> If audio says Speaker A is active AND face detection says Person 1 is speaking, then Person 1 = Speaker A.</li>
</ul>

<p><strong>Benefit:</strong> Reduces confusion errors by 30-50% in meetings with video.</p>

<h2 id="17-deep-dive-online-diarization">17. Deep Dive: Online Diarization</h2>

<p><strong>Challenge:</strong> Traditional diarization is offline (requires the entire audio). For live meetings, we need online diarization.</p>

<p><strong>Approach:</strong></p>

<p><strong>1. Sliding Window:</strong></p>
<ul>
  <li>Process audio in chunks (e.g., 10 seconds).</li>
  <li>Run diarization on each chunk.</li>
</ul>

<p><strong>2. Speaker Linking:</strong></p>
<ul>
  <li>Link speakers across chunks using embeddings.</li>
  <li><strong>Challenge:</strong> Speaker labels may change across chunks (Speaker A in chunk 1 = Speaker B in chunk 2).</li>
  <li><strong>Solution:</strong> Use a global speaker tracker.</li>
</ul>

<p><strong>3. Incremental Clustering:</strong></p>
<ul>
  <li>Use <strong>Online AHC</strong> or <strong>DBSCAN</strong>.</li>
  <li>Add new segments to existing clusters or create new clusters.</li>
</ul>

<h2 id="18-privacy--security">18. Privacy &amp; Security</h2>

<p><strong>Concerns:</strong></p>
<ul>
  <li><strong>Voice Biometrics:</strong> Speaker embeddings can be used to identify individuals.</li>
  <li><strong>Surveillance:</strong> Diarization enables tracking who said what.</li>
</ul>

<p><strong>Solutions:</strong></p>

<p><strong>1. On-Device Diarization:</strong></p>
<ul>
  <li>Run diarization locally (no audio sent to cloud).</li>
  <li><strong>Challenge:</strong> Requires lightweight models.</li>
</ul>

<p><strong>2. Differential Privacy:</strong></p>
<ul>
  <li>Add noise to embeddings to prevent re-identification.</li>
  <li><strong>Trade-off:</strong> Slight accuracy drop.</li>
</ul>

<p><strong>3. Anonymization:</strong></p>
<ul>
  <li>Replace speaker labels with pseudonyms (Speaker 1, Speaker 2).</li>
  <li>Don’t store raw audio, only transcripts.</li>
</ul>

<h2 id="19-deep-dive-speaker-change-detection">19. Deep Dive: Speaker Change Detection</h2>

<p><strong>Problem:</strong> Detect when the speaker changes (boundary detection).</p>

<p><strong>Approaches:</strong></p>

<p><strong>1. Bayesian Information Criterion (BIC):</strong></p>
<ul>
  <li>For each potential change point $t$, compute:
    <ul>
      <li>$BIC(t) = \log P(X_{1:t}) + \log P(X_{t+1:T}) - \log P(X_{1:T})$</li>
    </ul>
  </li>
  <li>If $BIC(t) &gt; \theta$, there’s a speaker change at $t$.</li>
</ul>

<p><strong>2. Generalized Likelihood Ratio (GLR):</strong></p>
<ul>
  <li>Similar to BIC, but uses likelihood ratio.</li>
</ul>

<p><strong>3. Neural Change Detection:</strong></p>
<ul>
  <li>Train a CNN to predict speaker change points.</li>
  <li><strong>Input:</strong> Spectrogram.</li>
  <li><strong>Output:</strong> Binary label (change or no change).</li>
</ul>

<h2 id="20-production-monitoring">20. Production Monitoring</h2>

<p><strong>Metrics to Track:</strong></p>
<ul>
  <li><strong>DER:</strong> Aggregate across all meetings. Alert if DER &gt; 15%.</li>
  <li><strong>Latency:</strong> P95 latency. Alert if &gt; 2 seconds.</li>
  <li><strong>Throughput:</strong> Meetings processed per second.</li>
  <li><strong>Error Analysis:</strong> Which types of errors are most common? (FA, MISS, CONF)</li>
</ul>

<p><strong>Dashboards:</strong></p>
<ul>
  <li><strong>Grafana:</strong> Real-time metrics.</li>
  <li><strong>Kibana:</strong> Log analysis (search for “speaker change detected”).</li>
</ul>

<p><strong>A/B Testing:</strong></p>
<ul>
  <li>Deploy new model to 5% of meetings.</li>
  <li>Compare DER with baseline.</li>
</ul>

<h2 id="21-cost-analysis">21. Cost Analysis</h2>

<p><strong>Scenario:</strong> Diarization for 1M meetings/month (average 30 minutes each).</p>

<p><strong>Baseline (Cloud-based):</strong></p>
<ul>
  <li><strong>Compute:</strong> 1M meetings × 30 min = 500K hours.</li>
  <li><strong>Cost:</strong> 500K hours × $0.10/hour (GPU) = $50K/month.</li>
</ul>

<p><strong>Optimized (Batching + Caching):</strong></p>
<ul>
  <li><strong>Batching:</strong> Process 100 meetings in parallel. Reduces GPU hours by 10x.</li>
  <li><strong>Caching:</strong> 40% cache hit rate. Reduces compute by 40%.</li>
  <li><strong>Cost:</strong> 500K hours × 0.1 (batching) × 0.6 (caching) × $0.10 = $3K/month.</li>
</ul>

<p><strong>Savings:</strong> $47K/month (94% cost reduction).</p>

<h2 id="22-advanced-technique-few-shot-speaker-diarization">22. Advanced Technique: Few-Shot Speaker Diarization</h2>

<p><strong>Problem:</strong> Diarization fails when speakers have very short utterances (&lt; 1 second).</p>

<p><strong>Solution:</strong> Use few-shot learning.</p>

<p><strong>Approach:</strong></p>
<ol>
  <li><strong>Meta-Learning:</strong> Train a model on many diarization tasks.</li>
  <li><strong>Prototypical Networks:</strong> Learn a metric space where speakers cluster tightly.</li>
  <li><strong>Inference:</strong> Given a few examples of each speaker, classify new segments.</li>
</ol>

<p><strong>Benefit:</strong> Works with as few as 3 seconds of enrollment audio per speaker.</p>

<h2 id="23-future-trends">23. Future Trends</h2>

<p><strong>1. Zero-Shot Diarization:</strong></p>
<ul>
  <li>Diarize without any training data for the target domain.</li>
  <li>Use pre-trained models (e.g., Wav2Vec 2.0) as feature extractors.</li>
</ul>

<p><strong>2. Continuous Learning:</strong></p>
<ul>
  <li>Model adapts to new speakers over time.</li>
  <li><strong>Challenge:</strong> Catastrophic forgetting.</li>
</ul>

<p><strong>3. Multi-Lingual Diarization:</strong></p>
<ul>
  <li>Handle meetings where speakers switch languages.</li>
  <li><strong>Challenge:</strong> Language-specific acoustic features.</li>
</ul>

<p><strong>4. Emotion-Aware Diarization:</strong></p>
<ul>
  <li>Not just “who spoke” but “who spoke angrily/happily”.</li>
  <li><strong>Use Case:</strong> Call center analytics.</li>
</ul>

<h2 id="24-benchmarking">24. Benchmarking</h2>

<p><strong>Dataset:</strong> CALLHOME (2-speaker telephone conversations).</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>DER (%)</th>
      <th>Latency (s)</th>
      <th>Model Size (MB)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>i-vector + AHC</td>
      <td>12.3</td>
      <td>5.0</td>
      <td>50</td>
    </tr>
    <tr>
      <td>x-vector + AHC</td>
      <td>9.8</td>
      <td>3.0</td>
      <td>20</td>
    </tr>
    <tr>
      <td>x-vector + Spectral</td>
      <td>8.5</td>
      <td>4.0</td>
      <td>20</td>
    </tr>
    <tr>
      <td>EEND</td>
      <td>7.2</td>
      <td>2.0</td>
      <td>100</td>
    </tr>
    <tr>
      <td>EEND-EDA</td>
      <td>6.5</td>
      <td>2.5</td>
      <td>120</td>
    </tr>
  </tbody>
</table>

<p><strong>Observation:</strong> EEND achieves the best DER but requires more compute.</p>

<h2 id="25-conclusion">25. Conclusion</h2>

<p>Speaker diarization is a critical component of modern speech systems. From meeting transcription to call center analytics, the ability to answer “who spoke when” unlocks powerful applications.</p>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Traditional Pipeline:</strong> SAD → Segmentation → Embedding → Clustering.</li>
  <li><strong>EEND:</strong> End-to-end neural approach. Handles overlapping speech naturally.</li>
  <li><strong>X-Vectors:</strong> State-of-the-art embeddings for speaker recognition.</li>
  <li><strong>Clustering:</strong> AHC with PLDA scoring is the gold standard.</li>
  <li><strong>Production:</strong> Real-time diarization requires online clustering and caching.</li>
  <li><strong>Multi-Modal:</strong> Combining audio and video improves accuracy.</li>
</ul>

<p>The future of diarization is multi-modal, privacy-preserving, and adaptive. As remote work becomes the norm, the demand for accurate, real-time diarization will only grow. Mastering these techniques is essential for speech engineers.</p>

<h2 id="26-deep-dive-resnet-based-x-vector-architecture">26. Deep Dive: ResNet-based X-Vector Architecture</h2>

<p><strong>Detailed Architecture:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: 40-dim MFCCs (T frames)
↓
Frame-level Layers:
  - TDNN1: 512 units, context [-2, +2]
  - TDNN2: 512 units, context [-2, 0, +2]
  - TDNN3: 512 units, context [-3, 0, +3]
  - TDNN4: 512 units, context {0}
  - TDNN5: 1500 units, context {0}
↓
Statistics Pooling: [mean, std] → 3000-dim
↓
Segment-level Layers:
  - FC1: 512 units
  - FC2: 512 units (x-vector embedding)
↓
Output: 7000 units (speaker classification)
</code></pre></div></div>

<p><strong>Training Details:</strong></p>
<ul>
  <li><strong>Dataset:</strong> VoxCeleb1 + VoxCeleb2 (7000+ speakers, 1M+ utterances).</li>
  <li><strong>Augmentation:</strong> Add noise, reverb, codec distortion.</li>
  <li><strong>Loss:</strong> Softmax or AAM-Softmax (Additive Angular Margin).</li>
  <li><strong>Optimizer:</strong> Adam with learning rate 0.001.</li>
  <li><strong>Batch Size:</strong> 128 utterances.</li>
</ul>

<p><strong>Inference:</strong></p>
<ul>
  <li>Extract the 512-dim embedding from FC2.</li>
  <li>Normalize to unit length.</li>
  <li>Use cosine similarity for clustering.</li>
</ul>

<h2 id="27-deep-dive-voxceleb-dataset">27. Deep Dive: VoxCeleb Dataset</h2>

<p><strong>VoxCeleb1:</strong></p>
<ul>
  <li><strong>Speakers:</strong> 1,251.</li>
  <li><strong>Utterances:</strong> 153K.</li>
  <li><strong>Duration:</strong> 352 hours.</li>
  <li><strong>Source:</strong> YouTube celebrity interviews.</li>
</ul>

<p><strong>VoxCeleb2:</strong></p>
<ul>
  <li><strong>Speakers:</strong> 6,112.</li>
  <li><strong>Utterances:</strong> 1.1M.</li>
  <li><strong>Duration:</strong> 2,442 hours.</li>
</ul>

<p><strong>Challenges:</strong></p>
<ul>
  <li><strong>In-the-Wild:</strong> Background noise, music, laughter.</li>
  <li><strong>Multi-Speaker:</strong> Some videos have multiple speakers.</li>
  <li><strong>Variable Length:</strong> Utterances range from 1 second to 10 minutes.</li>
</ul>

<p><strong>Preprocessing:</strong></p>
<ul>
  <li><strong>VAD:</strong> Remove silence using WebRTC VAD.</li>
  <li><strong>Segmentation:</strong> Split into 2-3 second chunks.</li>
  <li><strong>Normalization:</strong> Mean-variance normalization of MFCCs.</li>
</ul>

<h2 id="28-production-optimization-batch-processing">28. Production Optimization: Batch Processing</h2>

<p><strong>Problem:</strong> Processing 1M meetings sequentially takes days.</p>

<p><strong>Solution:</strong> Batch processing on GPU.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Collect</strong> 100 meetings.</li>
  <li><strong>Pad</strong> all audio to the same length (e.g., 30 minutes).</li>
  <li><strong>Extract</strong> MFCCs in parallel (GPU).</li>
  <li><strong>Batch Inference:</strong> Run x-vector model on all 100 meetings simultaneously.</li>
  <li><strong>Clustering:</strong> Run AHC on each meeting in parallel (CPU).</li>
</ol>

<p><strong>Speedup:</strong> 100x faster than sequential processing.</p>

<p><strong>Implementation (PyTorch):</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="c1"># Batch of 100 meetings, each 30 minutes (180K frames)
</span><span class="n">mfccs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">180000</span><span class="p">,</span> <span class="mi">40</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>

<span class="c1"># X-vector model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">XVectorModel</span><span class="p">().</span><span class="nf">cuda</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="c1"># Batch inference
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">mfccs</span><span class="p">)</span>  <span class="c1"># (100, T', 512)
</span>
<span class="c1"># Post-process each meeting
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">emb</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># (T', 512)
</span>    <span class="c1"># Run clustering
</span>    <span class="n">labels</span> <span class="o">=</span> <span class="nf">cluster</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="29-advanced-evaluation-detailed-error-analysis">29. Advanced Evaluation: Detailed Error Analysis</h2>

<p><strong>DER Breakdown:</strong></p>
<ul>
  <li><strong>False Alarm (FA):</strong> 2% (non-speech detected as speech).</li>
  <li><strong>Missed Speech (MISS):</strong> 3% (speech detected as non-speech).</li>
  <li><strong>Speaker Confusion (CONF):</strong> 5% (wrong speaker label).</li>
  <li><strong>Total DER:</strong> 10%.</li>
</ul>

<p><strong>Error Analysis:</strong></p>
<ul>
  <li><strong>FA:</strong> Mostly music and laughter.
    <ul>
      <li><strong>Fix:</strong> Improve VAD with music detection.</li>
    </ul>
  </li>
  <li><strong>MISS:</strong> Mostly whispered speech.
    <ul>
      <li><strong>Fix:</strong> Train VAD on whispered speech data.</li>
    </ul>
  </li>
  <li><strong>CONF:</strong> Mostly overlapping speech.
    <ul>
      <li><strong>Fix:</strong> Use EEND to handle overlaps.</li>
    </ul>
  </li>
</ul>

<h2 id="30-deep-dive-permutation-invariant-training-pit">30. Deep Dive: Permutation Invariant Training (PIT)</h2>

<p><strong>Problem:</strong> In EEND, speaker labels are arbitrary. Ground truth might be [A, B], but prediction might be [B, A].</p>

<p><strong>Solution:</strong> PIT finds the best permutation.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li><strong>Predict:</strong> Model outputs $P \in \mathbb{R}^{T \times K}$ (K speakers).</li>
  <li><strong>Ground Truth:</strong> $Y \in \mathbb{R}^{T \times K}$.</li>
  <li><strong>Enumerate Permutations:</strong> For K=2, there are 2! = 2 permutations.</li>
  <li><strong>Compute Loss for Each Permutation:</strong>
    <ul>
      <li>Perm 1: $L_1 = BCE(P[:, 0], Y[:, 0]) + BCE(P[:, 1], Y[:, 1])$</li>
      <li>Perm 2: $L_2 = BCE(P[:, 0], Y[:, 1]) + BCE(P[:, 1], Y[:, 0])$</li>
    </ul>
  </li>
  <li><strong>Choose Minimum:</strong> $L = \min(L_1, L_2)$.</li>
</ol>

<p><strong>Complexity:</strong> $O(K!)$ for K speakers. For K&gt;3, use Hungarian algorithm.</p>

<h2 id="31-production-case-study-call-center-diarization">31. Production Case Study: Call Center Diarization</h2>

<p><strong>Scenario:</strong> Analyze 10K calls/day to separate agent from customer.</p>

<p><strong>Challenges:</strong></p>
<ul>
  <li><strong>2-Speaker:</strong> Always agent + customer.</li>
  <li><strong>Overlapping Speech:</strong> Frequent interruptions.</li>
  <li><strong>Background Noise:</strong> Office noise, typing.</li>
</ul>

<p><strong>Solution:</strong></p>

<p><strong>Step 1: Stereo Audio</strong></p>
<ul>
  <li>Agent and customer are on separate channels (stereo).</li>
  <li><strong>Benefit:</strong> No need for diarization! Just label left=agent, right=customer.</li>
</ul>

<p><strong>Step 2: Mono Audio (Fallback)</strong></p>
<ul>
  <li>If stereo is unavailable, use diarization.</li>
  <li><strong>Optimization:</strong> Since K=2, use a simpler clustering algorithm (k-means with k=2).</li>
</ul>

<p><strong>Step 3: Speaker Verification</strong></p>
<ul>
  <li>Verify that the agent is who they claim to be (security).</li>
  <li>Extract x-vector from agent’s speech.</li>
  <li>Compare with enrolled agent profile.</li>
</ul>

<p><strong>Result:</strong></p>
<ul>
  <li><strong>Latency:</strong> 10 seconds (for a 5-minute call).</li>
  <li><strong>DER:</strong> 5% (better than general diarization because K is known).</li>
</ul>

<h2 id="32-advanced-technique-self-supervised-learning-for-diarization">32. Advanced Technique: Self-Supervised Learning for Diarization</h2>

<p><strong>Problem:</strong> Labeled diarization data is expensive (need to manually annotate who spoke when).</p>

<p><strong>Solution:</strong> Use self-supervised learning.</p>

<p><strong>Approach:</strong></p>
<ol>
  <li><strong>Pretext Task:</strong> Train a model to predict if two segments are from the same speaker.</li>
  <li><strong>Contrastive Learning:</strong> Pull embeddings of the same speaker together, push different speakers apart.</li>
  <li><strong>Fine-Tuning:</strong> Fine-tune on a small labeled dataset.</li>
</ol>

<p><strong>Example: SimCLR for Speaker Embeddings:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Positive pair: Two segments from the same speaker
</span><span class="n">emb1</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">segment1</span><span class="p">)</span>
<span class="n">emb2</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">segment2</span><span class="p">)</span>

<span class="c1"># Negative pairs: Segments from different speakers
</span><span class="n">emb3</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">segment3</span><span class="p">)</span>

<span class="c1"># Contrastive loss
</span><span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="nf">log</span><span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="nf">sim</span><span class="p">(</span><span class="n">emb1</span><span class="p">,</span> <span class="n">emb2</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="nf">sim</span><span class="p">(</span><span class="n">emb1</span><span class="p">,</span> <span class="n">emb2</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)</span> <span class="o">+</span> <span class="nf">exp</span><span class="p">(</span><span class="nf">sim</span><span class="p">(</span><span class="n">emb1</span><span class="p">,</span> <span class="n">emb3</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span><span class="p">)))</span>
</code></pre></div></div>

<h2 id="33-interview-deep-dive-diarization-vs-speaker-recognition">33. Interview Deep Dive: Diarization vs Speaker Recognition</h2>

<p><strong>Q: What’s the difference between speaker diarization and speaker recognition?</strong></p>

<p><strong>A:</strong></p>
<ul>
  <li><strong>Diarization:</strong> “Who spoke when?” Unknown speakers. Clustering problem.</li>
  <li><strong>Recognition:</strong> “Is this speaker Alice?” Known speakers. Classification problem.</li>
</ul>

<p><strong>Q: Can you use the same model for both?</strong></p>

<p><strong>A:</strong> Yes! X-vectors can be used for both.</p>
<ul>
  <li><strong>Diarization:</strong> Cluster x-vectors.</li>
  <li><strong>Recognition:</strong> Compare x-vector to enrolled speaker’s x-vector.</li>
</ul>

<h2 id="34-future-trends-transformer-based-diarization">34. Future Trends: Transformer-based Diarization</h2>

<p><strong>EEND with Conformer:</strong></p>
<ul>
  <li>Replace LSTM with Conformer (Convolution + Transformer).</li>
  <li><strong>Benefit:</strong> Better long-range dependencies.</li>
  <li><strong>Result:</strong> DER &lt; 5% on CALLHOME.</li>
</ul>

<p><strong>Wav2Vec 2.0 for Diarization:</strong></p>
<ul>
  <li>Use pre-trained Wav2Vec 2.0 as feature extractor.</li>
  <li><strong>Benefit:</strong> No need for MFCCs. Learn features end-to-end.</li>
  <li><strong>Challenge:</strong> Large model (300M params). Need compression for production.</li>
</ul>

<h2 id="35-conclusion--best-practices">35. Conclusion &amp; Best Practices</h2>

<p><strong>Best Practices:</strong></p>
<ol>
  <li><strong>Start with X-Vectors + AHC:</strong> Proven, reliable, easy to implement.</li>
  <li><strong>Use PLDA Scoring:</strong> Better than cosine similarity for clustering.</li>
  <li><strong>Handle Overlapping Speech:</strong> Use EEND or multi-label classification.</li>
  <li><strong>Optimize for Production:</strong> Batch processing, caching, GPU acceleration.</li>
  <li><strong>Monitor DER:</strong> Track FA, MISS, CONF separately for targeted improvements.</li>
</ol>

<p><strong>Diarization Checklist:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement VAD (WebRTC or DNN-based)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Extract x-vectors (train or use pre-trained)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement AHC with PLDA scoring</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Evaluate on CALLHOME (target DER &lt; 10%)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Handle overlapping speech (EEND or multi-label)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Optimize for real-time (online clustering)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Add multi-modal (video) if available</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Monitor in production (DER, latency, cost)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Set up A/B testing</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Iterate based on error analysis</li>
</ul>

<p>The journey from “who spoke when” to production-ready diarization involves mastering embeddings, clustering, and system design. As meetings move online and voice interfaces proliferate, diarization will become even more critical. The techniques you’ve learned here—from x-vectors to EEND to multi-modal fusion—will serve you well in building the next generation of speech systems.</p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#clustering" class="page__taxonomy-item p-category" rel="tag">clustering</a><span class="sep">, </span>
    
      <a href="/tags/#embeddings" class="page__taxonomy-item p-category" rel="tag">embeddings</a><span class="sep">, </span>
    
      <a href="/tags/#segmentation" class="page__taxonomy-item p-category" rel="tag">segmentation</a><span class="sep">, </span>
    
      <a href="/tags/#speaker-recognition" class="page__taxonomy-item p-category" rel="tag">speaker-recognition</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Speaker+Diarization%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0041-speaker-diarization%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0041-speaker-diarization%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0041-speaker-diarization/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0040-wake-word-detection/" class="pagination--pager" title="Wake Word Detection">Previous</a>
    
    
      <a href="/speech-tech/0042-asr-decoding/" class="pagination--pager" title="Automatic Speech Recognition (ASR) Decoding">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
