<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Automatic Speech Recognition (ASR) Decoding - Arun Baby</title>
<meta name="description" content="“Turning acoustic probabilities into coherent text.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Automatic Speech Recognition (ASR) Decoding">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0042-asr-decoding/">


  <meta property="og:description" content="“Turning acoustic probabilities into coherent text.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Automatic Speech Recognition (ASR) Decoding">
  <meta name="twitter:description" content="“Turning acoustic probabilities into coherent text.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0042-asr-decoding/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T16:05:30+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0042-asr-decoding/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Automatic Speech Recognition (ASR) Decoding">
    <meta itemprop="description" content="“Turning acoustic probabilities into coherent text.”">
    <meta itemprop="datePublished" content="2025-12-29T16:05:30+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0042-asr-decoding/" itemprop="url">Automatic Speech Recognition (ASR) Decoding
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          16 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction">1. Introduction</a></li><li><a href="#2-the-decoding-problem">2. The Decoding Problem</a></li><li><a href="#3-decoding-strategies">3. Decoding Strategies</a><ul><li><a href="#31-greedy-decoding">3.1. Greedy Decoding</a></li><li><a href="#32-beam-search">3.2. Beam Search</a></li><li><a href="#33-ctc-decoding">3.3. CTC Decoding</a></li><li><a href="#34-attention-based-decoding">3.4. Attention-Based Decoding</a></li></ul></li><li><a href="#4-language-model-integration">4. Language Model Integration</a><ul><li><a href="#41-shallow-fusion">4.1. Shallow Fusion</a></li><li><a href="#42-deep-fusion">4.2. Deep Fusion</a></li><li><a href="#43-rescoring-n-best-rescoring">4.3. Rescoring (N-Best Rescoring)</a></li></ul></li><li><a href="#5-ctc-beam-search-with-language-model">5. CTC Beam Search with Language Model</a></li><li><a href="#6-system-design-production-asr-decoder">6. System Design: Production ASR Decoder</a></li><li><a href="#7-deep-dive-weighted-finite-state-transducers-wfst">7. Deep Dive: Weighted Finite State Transducers (WFST)</a></li><li><a href="#8-deep-dive-streaming-vs-non-streaming">8. Deep Dive: Streaming vs Non-Streaming</a></li><li><a href="#9-word-error-rate-wer">9. Word Error Rate (WER)</a></li><li><a href="#10-decoding-optimizations">10. Decoding Optimizations</a><ul><li><a href="#101-pruning">10.1. Pruning</a></li><li><a href="#102-lattice-generation">10.2. Lattice Generation</a></li><li><a href="#103-gpu-acceleration">10.3. GPU Acceleration</a></li></ul></li><li><a href="#11-production-case-study-whisper">11. Production Case Study: Whisper</a></li><li><a href="#12-interview-questions">12. Interview Questions</a></li><li><a href="#13-common-mistakes">13. Common Mistakes</a></li><li><a href="#14-deep-dive-confidence-estimation">14. Deep Dive: Confidence Estimation</a></li><li><a href="#15-deep-dive-hot-words-contextual-biasing">15. Deep Dive: Hot Words (Contextual Biasing)</a></li><li><a href="#16-future-trends">16. Future Trends</a></li><li><a href="#17-conclusion">17. Conclusion</a></li><li><a href="#18-deep-dive-rnn-transducer-rnn-t-decoding">18. Deep Dive: RNN-Transducer (RNN-T) Decoding</a></li><li><a href="#19-production-case-study-google-voice-search">19. Production Case Study: Google Voice Search</a></li><li><a href="#20-production-case-study-amazon-alexa">20. Production Case Study: Amazon Alexa</a></li><li><a href="#21-benchmarking-asr-decoders">21. Benchmarking ASR Decoders</a></li><li><a href="#22-decoding-for-low-resource-languages">22. Decoding for Low-Resource Languages</a></li><li><a href="#23-advanced-minimum-bayes-risk-mbr-decoding">23. Advanced: Minimum Bayes Risk (MBR) Decoding</a></li><li><a href="#24-deep-dive-subword-units">24. Deep Dive: Subword Units</a></li><li><a href="#25-decoding-with-word-timestamps">25. Decoding with Word Timestamps</a></li><li><a href="#26-decoding-for-dictation-vs-conversation">26. Decoding for Dictation vs Conversation</a></li><li><a href="#27-implementation-pytorch-ctc-beam-search">27. Implementation: PyTorch CTC Beam Search</a></li><li><a href="#28-interview-strategy-asr-decoding">28. Interview Strategy: ASR Decoding</a></li><li><a href="#29-testing-asr-decoders">29. Testing ASR Decoders</a></li><li><a href="#30-conclusion--mastery-checklist">30. Conclusion &amp; Mastery Checklist</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Turning acoustic probabilities into coherent text.”</strong></p>

<h2 id="1-introduction">1. Introduction</h2>

<p><strong>Automatic Speech Recognition (ASR)</strong> converts speech audio into text. The <strong>decoding</strong> stage is where we take the acoustic model’s output (probabilities over characters/tokens) and produce the final transcription.</p>

<p><strong>Pipeline:</strong>
<code class="language-plaintext highlighter-rouge">
Audio → Feature Extraction (MFCC) → Acoustic Model → Decoder → Text
</code></p>

<p>The decoder’s job is to find the most likely text sequence given the acoustic observations.</p>

<h2 id="2-the-decoding-problem">2. The Decoding Problem</h2>

<p><strong>Mathematically:</strong>
<code class="language-plaintext highlighter-rouge">\hat{W} = \arg\max_W P(W | O)</code></p>

<p>Where:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">O</code> = Acoustic observations (audio features).</li>
  <li><code class="language-plaintext highlighter-rouge">W</code> = Word sequence (transcription).</li>
</ul>

<p>Using Bayes’ theorem:
<code class="language-plaintext highlighter-rouge">P(W | O) = \frac{P(O | W) P(W)}{P(O)}</code></p>

<p>Since <code class="language-plaintext highlighter-rouge">P(O)</code> is constant, we maximize:
<code class="language-plaintext highlighter-rouge">\hat{W} = \arg\max_W P(O | W) \cdot P(W)</code></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">P(O | W)</code> = <strong>Acoustic Model</strong> (AM): How likely is this audio given these words?</li>
  <li><code class="language-plaintext highlighter-rouge">P(W)</code> = <strong>Language Model</strong> (LM): How likely is this word sequence?</li>
</ul>

<h2 id="3-decoding-strategies">3. Decoding Strategies</h2>

<h3 id="31-greedy-decoding">3.1. Greedy Decoding</h3>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>At each time step, pick the most likely token.</li>
  <li>Concatenate to form the output.</li>
</ol>

<p><code class="language-plaintext highlighter-rouge">python
def greedy_decode(probs):
 # probs: (T, vocab_size)
 tokens = []
 for t in range(len(probs)):
 token = np.argmax(probs[t])
 tokens.append(token)
 return tokens
</code></p>

<p><strong>Pros:</strong> Fast, simple.
<strong>Cons:</strong> Doesn’t consider context. Often produces suboptimal results.</p>

<h3 id="32-beam-search">3.2. Beam Search</h3>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Maintain top-k hypotheses (beams).</li>
  <li>At each step, extend each hypothesis with all possible tokens.</li>
  <li>Keep only the top-k scoring hypotheses.</li>
  <li>Repeat until end-of-sequence.</li>
</ol>

<p>``python
def beam_search(probs, beam_width=5):
 # probs: (T, vocab_size)
 sequences = [[[], 0.0]] # (tokens, score)</p>

<p>for t in range(len(probs)):
 all_candidates = []
 for seq, score in sequences:
 for token in range(len(probs[t])):
 new_seq = seq + [token]
 new_score = score + np.log(probs[t][token])
 all_candidates.append((new_seq, new_score))</p>

<p># Keep top-k
 sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]</p>

<p>return sequences[0][0] # Return best sequence
``</p>

<p><strong>Pros:</strong> Considers multiple hypotheses. Usually better than greedy.
<strong>Cons:</strong> Computationally expensive. Still not globally optimal.</p>

<h3 id="33-ctc-decoding">3.3. CTC Decoding</h3>

<p><strong>Connectionist Temporal Classification (CTC)</strong> is a loss function for sequence-to-sequence tasks where alignment is unknown.</p>

<p><strong>Key Idea:</strong></p>
<ul>
  <li>Add a “blank” token (ε) to handle alignment.</li>
  <li>Multiple CTC paths map to the same output (e.g., “aa-ab” and “a-aab” both → “ab”).</li>
</ul>

<p><strong>CTC Decoding:</strong></p>
<ol>
  <li>Apply greedy or beam search to CTC output.</li>
  <li>Collapse consecutive duplicate tokens.</li>
  <li>Remove blank tokens.</li>
</ol>

<p>``python
def ctc_decode(probs, blank_token=0):
 tokens = []
 prev = -1</p>

<p>for t in range(len(probs)):
 token = np.argmax(probs[t])
 if token != prev and token != blank_token:
 tokens.append(token)
 prev = token</p>

<p>return tokens
``</p>

<h3 id="34-attention-based-decoding">3.4. Attention-Based Decoding</h3>

<p>In encoder-decoder models (like Whisper, Seq2Seq), the decoder uses attention to focus on relevant parts of the encoder output.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Encoder processes audio → hidden states.</li>
  <li>Decoder autoregressively generates tokens.</li>
  <li>At each step, attention weights determine which encoder states to focus on.</li>
</ol>

<p>``python
def attention_decode(encoder_output, decoder, max_length=100):
 tokens = [BOS_TOKEN] # Begin of sequence</p>

<p>for _ in range(max_length):
 output = decoder(tokens, encoder_output)
 next_token = np.argmax(output[-1])
 tokens.append(next_token)</p>

<p>if next_token == EOS_TOKEN:
 break</p>

<p>return tokens[1:-1] # Remove BOS and EOS
``</p>

<h2 id="4-language-model-integration">4. Language Model Integration</h2>

<p><strong>Problem:</strong> Acoustic model output may be noisy. Language models can correct grammatical errors.</p>

<p><strong>Approaches:</strong></p>

<h3 id="41-shallow-fusion">4.1. Shallow Fusion</h3>

<p><strong>Idea:</strong> Combine AM and LM scores during beam search.</p>

<p><code class="language-plaintext highlighter-rouge">\text{score} = \log P_{AM}(y | x) + \lambda \log P_{LM}(y)</code></p>

<p>Where <code class="language-plaintext highlighter-rouge">\lambda</code> is a tunable weight.</p>

<p>``python
def beam_search_with_lm(am_probs, lm, beam_width=5, lm_weight=0.5):
 sequences = [[[], 0.0, 0.0]] # (tokens, am_score, lm_score)</p>

<p>for t in range(len(am_probs)):
 all_candidates = []
 for seq, am_score, lm_score in sequences:
 for token in range(len(am_probs[t])):
 new_seq = seq + [token]
 new_am_score = am_score + np.log(am_probs[t][token])
 new_lm_score = lm_score + lm.score(new_seq)</p>

<p>combined = new_am_score + lm_weight * new_lm_score
 all_candidates.append((new_seq, new_am_score, new_lm_score, combined))</p>

<p># Keep top-k by combined score
 sequences = sorted(all_candidates, key=lambda x: x[3], reverse=True)[:beam_width]</p>

<p>return sequences[0][0]
``</p>

<h3 id="42-deep-fusion">4.2. Deep Fusion</h3>

<p><strong>Idea:</strong> Train the AM and LM jointly. The LM’s hidden states are fed into the AM.</p>

<p><strong>Benefits:</strong> Tighter integration, better performance.
<strong>Drawbacks:</strong> Requires retraining. Less flexible.</p>

<h3 id="43-rescoring-n-best-rescoring">4.3. Rescoring (N-Best Rescoring)</h3>

<p><strong>Idea:</strong></p>
<ol>
  <li>Generate N-best hypotheses with AM only.</li>
  <li>Rescore each hypothesis with a powerful LM (e.g., GPT).</li>
  <li>Return the highest-scoring hypothesis.</li>
</ol>

<p><strong>Benefits:</strong> Can use large, pretrained LMs.
<strong>Drawbacks:</strong> Two-pass (slower).</p>

<h2 id="5-ctc-beam-search-with-language-model">5. CTC Beam Search with Language Model</h2>

<p><strong>Prefix Beam Search:</strong>
Handles CTC’s blank tokens and collapsed outputs properly.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Track two scores for each prefix:
    <ul>
      <li><strong>Pb (blank):</strong> Probability of prefix ending with blank.</li>
      <li><strong>Pnb (non-blank):</strong> Probability of prefix not ending with blank.</li>
    </ul>
  </li>
  <li>At each step, extend prefixes by:
    <ul>
      <li>Adding a blank (only affects Pb).</li>
      <li>Repeating the last character (affects Pb/Pnb transition).</li>
      <li>Adding a new character (affects Pnb).</li>
    </ul>
  </li>
</ol>

<p>``python
def ctc_beam_search_with_lm(probs, lm, beam_width=10, blank=0, lm_weight=0.5):
 # probs: (T, vocab_size)
 # Initialize
 beams = {(): (1.0, 0.0)} # prefix: (pb, pnb)</p>

<p>for t in range(len(probs)):
 new_beams = defaultdict(lambda: (0.0, 0.0))</p>

<p>for prefix, (pb, pnb) in beams.items():
 for c in range(len(probs[t])):
 p = probs[t][c]</p>

<p>if c == blank:
 # Extend with blank
 key = prefix
 new_beams[key] = (new_beams[key][0] + (pb + pnb) * p, new_beams[key][1])</p>

<p>elif prefix and c == prefix[-1]:
 # Repeat character (needs blank before)
 new_beams[prefix] = (new_beams[prefix][0], new_beams[prefix][1] + pb * p)
 # Or new character after blank
 new_key = prefix + (c,)
 new_beams[new_key] = (new_beams[new_key][0], new_beams[new_key][1] + pnb * p)</p>

<p>else:
 # New character
 new_key = prefix + (c,)
 lm_factor = lm.score(new_key) ** lm_weight
 new_beams[new_key] = (new_beams[new_key][0], new_beams[new_key][1] + (pb + pnb) * p * lm_factor)</p>

<p># Prune to beam_width
 beams = dict(sorted(new_beams.items(), key=lambda x: sum(x[1]), reverse=True)[:beam_width])</p>

<p># Return best prefix
 best_prefix = max(beams, key=lambda x: sum(beams[x]))
 return list(best_prefix)
``</p>

<h2 id="6-system-design-production-asr-decoder">6. System Design: Production ASR Decoder</h2>

<p><strong>Scenario:</strong> Build a real-time ASR system for voice assistants.</p>

<p><strong>Requirements:</strong></p>
<ul>
  <li><strong>Latency:</strong> &lt; 200ms from end of utterance to transcription.</li>
  <li><strong>Accuracy:</strong> WER &lt; 10%.</li>
  <li><strong>Streaming:</strong> Start transcribing before the user finishes speaking.</li>
</ul>

<p><strong>Architecture:</strong></p>

<p><strong>Step 1: Streaming Audio Input</strong></p>
<ul>
  <li>Audio arrives in 20ms chunks.</li>
  <li>Feature extraction (MFCC/Mel) in real-time.</li>
</ul>

<p><strong>Step 2: Streaming Acoustic Model</strong></p>
<ul>
  <li>Use a streaming-compatible architecture (e.g., Conformer with lookahead).</li>
  <li>Output probabilities every 20ms.</li>
</ul>

<p><strong>Step 3: Online Decoding</strong></p>
<ul>
  <li>Run CTC beam search incrementally.</li>
  <li>Emit partial results as hypotheses stabilize.</li>
</ul>

<p><strong>Step 4: Endpointing</strong></p>
<ul>
  <li>Detect when the user stops speaking.</li>
  <li>Finalize the transcription.</li>
</ul>

<p><strong>Step 5: Rescoring (Optional)</strong></p>
<ul>
  <li>Rescore final hypothesis with a larger LM.</li>
</ul>

<h2 id="7-deep-dive-weighted-finite-state-transducers-wfst">7. Deep Dive: Weighted Finite State Transducers (WFST)</h2>

<p><strong>WFST</strong> is a mathematical framework for composing AM, LM, and lexicon.</p>

<p><strong>Components:</strong></p>
<ul>
  <li><strong>H (HMM):</strong> Maps senones to context-dependent phones.</li>
  <li><strong>C (Context):</strong> Maps context-dependent phones to context-independent phones.</li>
  <li><strong>L (Lexicon):</strong> Maps phones to words.</li>
  <li><strong>G (Grammar):</strong> Language model (n-gram).</li>
</ul>

<p><strong>Composition:</strong>
<code class="language-plaintext highlighter-rouge">\text{Decoding Graph} = H \circ C \circ L \circ G</code></p>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Efficient decoding using Viterbi algorithm on the composed graph.</li>
  <li>Caching: Precompute and store the composed graph.</li>
</ul>

<p><strong>Tools:</strong></p>
<ul>
  <li><strong>Kaldi:</strong> Open-source ASR toolkit using WFST.</li>
  <li><strong>OpenFst:</strong> Library for finite-state transducers.</li>
</ul>

<h2 id="8-deep-dive-streaming-vs-non-streaming">8. Deep Dive: Streaming vs Non-Streaming</h2>

<p><strong>Non-Streaming (Offline):</strong></p>
<ul>
  <li>Process entire audio at once.</li>
  <li>Can use bidirectional models (look at future).</li>
  <li>Higher accuracy.</li>
</ul>

<p><strong>Streaming (Online):</strong></p>
<ul>
  <li>Process audio chunk by chunk.</li>
  <li>Cannot look at future (causality constraint).</li>
  <li>Lower latency, slightly lower accuracy.</li>
</ul>

<p><strong>Hybrid (Look-Ahead):</strong></p>
<ul>
  <li>Allow small lookahead (e.g., 200ms).</li>
  <li>Balance between latency and accuracy.</li>
</ul>

<h2 id="9-word-error-rate-wer">9. Word Error Rate (WER)</h2>

<p><strong>Definition:</strong>
<code class="language-plaintext highlighter-rouge">WER = \frac{S + D + I}{N}</code></p>

<p>Where:</p>
<ul>
  <li><strong>S</strong> = Substitutions (wrong word).</li>
  <li><strong>D</strong> = Deletions (missing word).</li>
  <li><strong>I</strong> = Insertions (extra word).</li>
  <li><strong>N</strong> = Total words in reference.</li>
</ul>

<p><strong>Example:</strong></p>
<ul>
  <li>Reference: “the cat sat on the mat”</li>
  <li>Hypothesis: “the cat hat on a mat”</li>
  <li>Errors: S=1 (sat→hat), S=1 (the→a)</li>
  <li>WER = 2 / 6 = 33.3%</li>
</ul>

<h2 id="10-decoding-optimizations">10. Decoding Optimizations</h2>

<h3 id="101-pruning">10.1. Pruning</h3>

<p><strong>Beam Pruning:</strong> Keep only top-k hypotheses.
<strong>Threshold Pruning:</strong> Discard hypotheses with score &lt; best_score - threshold.
<strong>Histogram Pruning:</strong> Limit hypotheses per frame.</p>

<h3 id="102-lattice-generation">10.2. Lattice Generation</h3>

<p>Instead of outputting a single transcription, output a <strong>lattice</strong>—a compact representation of all hypotheses.</p>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Rescoring: Apply different LMs later.</li>
  <li>Confidence Estimation: Compute confidence scores.</li>
  <li>N-Best Lists: Extract top-N transcriptions.</li>
</ul>

<h3 id="103-gpu-acceleration">10.3. GPU Acceleration</h3>

<ul>
  <li><strong>Batched Beam Search:</strong> Process multiple utterances in parallel.</li>
  <li><strong>CUDA Implementations:</strong> Use GPU for matrix operations.</li>
  <li><strong>TensorRT:</strong> Optimize inference.</li>
</ul>

<h2 id="11-production-case-study-whisper">11. Production Case Study: Whisper</h2>

<p><strong>Whisper (OpenAI):</strong></p>
<ul>
  <li><strong>Architecture:</strong> Encoder-Decoder Transformer.</li>
  <li><strong>Decoding:</strong> Greedy or beam search with temperature.</li>
  <li><strong>Language Model:</strong> Implicit in the decoder.</li>
  <li><strong>Multilingual:</strong> 99 languages.</li>
</ul>

<p><strong>Decoding Features:</strong></p>
<ul>
  <li><strong>Timestamps:</strong> Predict start/end times for each word.</li>
  <li><strong>Temperature Fallback:</strong> If greedy fails (repetition), retry with temperature sampling.</li>
  <li><strong>Compression Threshold:</strong> Skip segments with too much compression (repetition).</li>
</ul>

<h2 id="12-interview-questions">12. Interview Questions</h2>

<ol>
  <li><strong>Greedy vs Beam Search:</strong> When would you use each?</li>
  <li><strong>CTC Decoding:</strong> Explain how blank tokens work.</li>
  <li><strong>Language Model Integration:</strong> What is shallow fusion?</li>
  <li><strong>Streaming ASR:</strong> How do you handle causality constraints?</li>
  <li><strong>WFST:</strong> What is the role of the lexicon (L)?</li>
  <li><strong>Calculate WER:</strong> Given reference and hypothesis, compute WER.</li>
</ol>

<h2 id="13-common-mistakes">13. Common Mistakes</h2>

<ul>
  <li><strong>Ignoring Blanks in CTC:</strong> Not collapsing consecutive tokens.</li>
  <li><strong>Beam Width Too Small:</strong> Missing the correct hypothesis.</li>
  <li><strong>LM Weight Too High:</strong> Over-relying on LM, ignoring AM.</li>
  <li><strong>Not Handling OOV Words:</strong> Out-of-vocabulary words cause errors.</li>
  <li><strong>Ignoring Latency:</strong> Production systems have strict latency requirements.</li>
</ul>

<h2 id="14-deep-dive-confidence-estimation">14. Deep Dive: Confidence Estimation</h2>

<p><strong>Problem:</strong> Not all transcriptions are equally reliable. How do we estimate confidence?</p>

<p><strong>Approaches:</strong></p>

<p><strong>1. Posterior Probability:</strong></p>
<ul>
  <li>Use the probability of the best hypothesis.</li>
  <li><code class="language-plaintext highlighter-rouge">\text{confidence} = P(W | O)</code>.</li>
</ul>

<p><strong>2. Entropy:</strong></p>
<ul>
  <li>High entropy = low confidence (many competing hypotheses).</li>
</ul>

<p><strong>3. Word-Level Confidence:</strong></p>
<ul>
  <li>Compute confidence for each word using lattice posteriors.</li>
</ul>

<p><strong>4. Model-Based:</strong></p>
<ul>
  <li>Train a separate model to predict confidence from ASR features.</li>
</ul>

<h2 id="15-deep-dive-hot-words-contextual-biasing">15. Deep Dive: Hot Words (Contextual Biasing)</h2>

<p><strong>Problem:</strong> ASR fails on domain-specific terms (e.g., product names, jargon).</p>

<p><strong>Solution:</strong> Bias decoding towards expected words.</p>

<p><strong>Approaches:</strong></p>
<ol>
  <li><strong>N-Gram Boosting:</strong> Increase LM probability for hot words.</li>
  <li><strong>Contextual LM:</strong> Condition LM on context (e.g., user’s recent queries).</li>
  <li><strong>Shallow Fusion with Boosted LM:</strong> Add bonus score for hot words.</li>
</ol>

<p><strong>Example:</strong>
``python
hot_words = [“Alexa”, “Siri”, “Cortana”]
hot_word_boost = 2.0 # Log-scale boost</p>

<p>def score_with_hotwords(seq, am_score, lm_score):
 bonus = sum(hot_word_boost for word in seq if word in hot_words)
 return am_score + lm_weight * lm_score + bonus
``</p>

<h2 id="16-future-trends">16. Future Trends</h2>

<p><strong>1. End-to-End LM Integration:</strong></p>
<ul>
  <li>Train AM and LM jointly (e.g., Hybrid Transducers).</li>
</ul>

<p><strong>2. LLM for Error Correction:</strong></p>
<ul>
  <li>Use GPT/LLaMA to post-process ASR output.</li>
</ul>

<p><strong>3. Multimodal Decoding:</strong></p>
<ul>
  <li>Use video (lip reading) to improve decoding.</li>
</ul>

<p><strong>4. On-Device Decoding:</strong></p>
<ul>
  <li>Run decoding on mobile devices (requires efficient implementations).</li>
</ul>

<h2 id="17-conclusion">17. Conclusion</h2>

<p>ASR decoding is the bridge between acoustic probabilities and human-readable text. The key is balancing:</p>
<ul>
  <li><strong>Accuracy:</strong> Use beam search and language models.</li>
  <li><strong>Latency:</strong> Use streaming architectures and pruning.</li>
  <li><strong>Flexibility:</strong> Use lattices and rescoring for adaptability.</li>
</ul>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Greedy:</strong> Fast but suboptimal.</li>
  <li><strong>Beam Search:</strong> Better quality, higher cost.</li>
  <li><strong>CTC:</strong> Handles alignment automatically.</li>
  <li><strong>LM Integration:</strong> Shallow fusion, deep fusion, rescoring.</li>
  <li><strong>WFST:</strong> Classical approach for composing AM/LM/Lexicon.</li>
  <li><strong>Streaming:</strong> Essential for real-time applications.</li>
</ul>

<p>Mastering decoding is essential for building production ASR systems. The techniques here apply to any sequence-to-sequence task: machine translation, speech synthesis, and beyond.</p>

<h2 id="18-deep-dive-rnn-transducer-rnn-t-decoding">18. Deep Dive: RNN-Transducer (RNN-T) Decoding</h2>

<p><strong>RNN-T</strong> is a popular architecture for streaming ASR (used by Google, Meta).</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Encoder:</strong> Processes audio, outputs acoustic features.</li>
  <li><strong>Prediction Network:</strong> RNN that processes output history.</li>
  <li><strong>Joint Network:</strong> Combines encoder + prediction network outputs.</li>
  <li><strong>Output:</strong> Probability over vocabulary + blank.</li>
</ul>

<p><strong>Decoding Algorithm:</strong>
``python
def rnnt_greedy_decode(encoder_output, predictor, joint):
 T = len(encoder_output)
 U = 0 # Output index
 outputs = []
 pred_state = predictor.initial_state()</p>

<p>t = 0
 while t &lt; T:
 # Get encoder output at time t
 enc_t = encoder_output[t]</p>

<p># Get prediction network output
 pred_output, pred_state = predictor(outputs[-1:] if outputs else [BOS], pred_state)</p>

<p># Joint network
 logits = joint(enc_t, pred_output)
 token = np.argmax(logits)</p>

<p>if token == BLANK:
 t += 1 # Advance time
 else:
 outputs.append(token)
 # Don’t advance time (can emit multiple tokens at same time step)</p>

<p>return outputs
``</p>

<p><strong>Key Difference from CTC:</strong></p>
<ul>
  <li>CTC: One output per time step.</li>
  <li>RNN-T: Can emit multiple tokens per time step (or none).</li>
</ul>

<h2 id="19-production-case-study-google-voice-search">19. Production Case Study: Google Voice Search</h2>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Model:</strong> Conformer encoder + RNN-T decoder.</li>
  <li><strong>Decoding:</strong> Beam search with shallow fusion LM.</li>
  <li><strong>Latency:</strong> &lt; 200ms.</li>
  <li><strong>WER:</strong> &lt; 5% on conversational speech.</li>
</ul>

<p><strong>Optimizations:</strong></p>
<ul>
  <li><strong>Quantized Model:</strong> INT8 for mobile.</li>
  <li><strong>Speculative Decoding:</strong> Start decoding before audio ends.</li>
  <li><strong>Hot Word Boosting:</strong> Boost contact names, app names.</li>
  <li><strong>Contextual LM:</strong> Condition on user’s search history.</li>
</ul>

<h2 id="20-production-case-study-amazon-alexa">20. Production Case Study: Amazon Alexa</h2>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>First Pass:</strong> CTC decoding with small LM.</li>
  <li><strong>Second Pass:</strong> Rescoring with large LM.</li>
  <li><strong>Third Pass:</strong> NLU (intent recognition).</li>
</ul>

<p><strong>Latency Breakdown:</strong></p>
<ul>
  <li>Audio capture: 50ms.</li>
  <li>CTC decoding: 100ms.</li>
  <li>LM rescoring: 50ms.</li>
  <li>Total: ~200ms.</li>
</ul>

<p><strong>Optimization:</strong></p>
<ul>
  <li><strong>Endpointing:</strong> Detect speech end quickly.</li>
  <li><strong>Prefetch:</strong> Start cloud processing during endpointing.</li>
</ul>

<h2 id="21-benchmarking-asr-decoders">21. Benchmarking ASR Decoders</h2>

<p><strong>Metrics:</strong></p>
<ul>
  <li><strong>WER:</strong> Word Error Rate.</li>
  <li><strong>Latency:</strong> Time from audio end to transcription.</li>
  <li><strong>RTF (Real-Time Factor):</strong> Processing time / Audio duration. RTF &lt; 1 means faster than real-time.</li>
</ul>

<p><strong>Benchmark (LibriSpeech):</strong>
| Model | WER (test-clean) | RTF (GPU) | RTF (CPU) |
|——-|——————|———–|———–|
| Wav2Vec 2.0 + Greedy | 2.7% | 0.1 | 1.5 |
| Whisper (small) + Beam | 3.0% | 0.2 | 3.0 |
| Conformer + RNN-T | 2.1% | 0.15 | 2.0 |</p>

<h2 id="22-decoding-for-low-resource-languages">22. Decoding for Low-Resource Languages</h2>

<p><strong>Challenge:</strong> Language models may not exist for low-resource languages.</p>

<p><strong>Solutions:</strong></p>
<ol>
  <li><strong>Multilingual Models:</strong> Use models trained on 100+ languages.</li>
  <li><strong>Cross-Lingual Transfer:</strong> Fine-tune on target language with limited data.</li>
  <li><strong>Character-Level LM:</strong> Train a small character LM.</li>
  <li><strong>Phoneme-Based LM:</strong> Use phoneme sequences instead of words.</li>
</ol>

<h2 id="23-advanced-minimum-bayes-risk-mbr-decoding">23. Advanced: Minimum Bayes Risk (MBR) Decoding</h2>

<p><strong>Problem:</strong> Beam search finds the most likely sequence, but not necessarily the one with the lowest WER.</p>

<p><strong>MBR Approach:</strong></p>
<ul>
  <li>Generate N-best hypotheses.</li>
  <li>For each hypothesis, compute expected WER against all other hypotheses.</li>
  <li>Return the hypothesis with lowest expected WER.</li>
</ul>

<p><strong>Algorithm:</strong>
``python
def mbr_decode(n_best_list):
 # n_best_list: [(hypothesis, probability), …]
 best_hyp = None
 best_risk = float(‘inf’)</p>

<p>for hyp, prob in n_best_list:
 risk = 0
 for other_hyp, other_prob in n_best_list:
 risk += other_prob * word_error_rate(hyp, other_hyp)</p>

<p>if risk &lt; best_risk:
 best_risk = risk
 best_hyp = hyp</p>

<p>return best_hyp
``</p>

<p><strong>Use Case:</strong> When WER is more important than likelihood.</p>

<h2 id="24-deep-dive-subword-units">24. Deep Dive: Subword Units</h2>

<p><strong>Problem:</strong> Character-level models have long sequences. Word-level models have OOV issues.</p>

<p><strong>Solution:</strong> Subword units (BPE, SentencePiece).</p>

<p><strong>Algorithm (BPE):</strong></p>
<ol>
  <li>Start with characters.</li>
  <li>Iteratively merge the most frequent adjacent pair.</li>
  <li>Stop when vocabulary size reached.</li>
</ol>

<p><strong>Example:</strong></p>
<ul>
  <li>“lower” → “l o w e r” → “lo we r” → “low er” → “lower”</li>
</ul>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Handles rare words.</li>
  <li>Shorter sequences than characters.</li>
  <li>No OOV (can spell any word).</li>
</ul>

<h2 id="25-decoding-with-word-timestamps">25. Decoding with Word Timestamps</h2>

<p><strong>Problem:</strong> Beyond transcription, we need to know when each word was spoken.</p>

<p><strong>Approaches:</strong></p>

<p><strong>1. CTC Alignment:</strong></p>
<ul>
  <li>After decoding, use forced alignment to find timestamps.</li>
</ul>

<p><strong>2. Attention Weights:</strong></p>
<ul>
  <li>Use encoder-decoder attention to infer timestamps.</li>
</ul>

<p><strong>3. Whisper-Style:</strong></p>
<ul>
  <li>Predict <code class="language-plaintext highlighter-rouge">&lt;|start_time|&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;|end_time|&gt;</code> tokens.</li>
</ul>

<h2 id="26-decoding-for-dictation-vs-conversation">26. Decoding for Dictation vs Conversation</h2>

<p><strong>Dictation:</strong></p>
<ul>
  <li>User speaks clearly, slowly.</li>
  <li>Expects exact transcription.</li>
  <li>Punctuation and formatting expected.</li>
</ul>

<p><strong>Conversation:</strong></p>
<ul>
  <li>Multiple speakers, overlapping speech.</li>
  <li>Disfluencies (uh, um, repetitions).</li>
  <li>Informal language.</li>
</ul>

<p><strong>Decoding Differences:</strong></p>
<ul>
  <li><strong>Dictation:</strong> Use strong LM, inverse text normalization (numbers, dates).</li>
  <li><strong>Conversation:</strong> Use disfluency model, speaker diarization.</li>
</ul>

<h2 id="27-implementation-pytorch-ctc-beam-search">27. Implementation: PyTorch CTC Beam Search</h2>

<p>``python
import torch
from torchaudio.models.decoder import ctc_decoder</p>

<h1 id="load-lexicon-and-lm">Load lexicon and LM</h1>
<p>decoder = ctc_decoder(
 lexicon=”lexicon.txt”,
 tokens=”tokens.txt”,
 lm=”language_model.arpa”,
 beam_size=50,
 lm_weight=0.5,
 word_score=-0.2
)</p>

<h1 id="decode">Decode</h1>
<p>emissions = model(audio) # (T, vocab_size)
hypotheses = decoder(emissions)</p>

<h1 id="get-best-hypothesis">Get best hypothesis</h1>
<p>best = hypotheses[0][0]
text = “ “.join(best.words)
``</p>

<h2 id="28-interview-strategy-asr-decoding">28. Interview Strategy: ASR Decoding</h2>

<p><strong>Step-by-Step:</strong></p>
<ol>
  <li><strong>Explain the Problem:</strong> AM gives probabilities, need to find best text.</li>
  <li><strong>Greedy:</strong> Simple but suboptimal.</li>
  <li><strong>Beam Search:</strong> Better, introduce beam width.</li>
  <li><strong>CTC Specifics:</strong> Blank token, collapsing.</li>
  <li><strong>LM Integration:</strong> Shallow fusion, rescoring.</li>
  <li><strong>Streaming:</strong> Discuss latency constraints.</li>
  <li><strong>Trade-offs:</strong> Accuracy vs latency vs compute.</li>
</ol>

<h2 id="29-testing-asr-decoders">29. Testing ASR Decoders</h2>

<p><strong>Unit Tests:</strong></p>
<ul>
  <li>Test CTC collapsing: “aa-a-bb” → “ab”.</li>
  <li>Test beam search ranking.</li>
  <li>Test LM weight behavior.</li>
</ul>

<p><strong>Integration Tests:</strong></p>
<ul>
  <li>End-to-end: audio → transcription.</li>
  <li>Compare with baseline (e.g., Whisper).</li>
</ul>

<p><strong>Regression Tests:</strong></p>
<ul>
  <li>Test on standard benchmarks (LibriSpeech, Common Voice).</li>
  <li>Alert if WER increases.</li>
</ul>

<h2 id="30-conclusion--mastery-checklist">30. Conclusion &amp; Mastery Checklist</h2>

<p><strong>Mastery Checklist:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement greedy decoding</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement beam search</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement CTC decoding (with blank collapsing)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Integrate language model (shallow fusion)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Understand RNN-T decoding</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement N-best list generation</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Calculate WER correctly</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Handle streaming ASR</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement hot word boosting</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Benchmark RTF and WER</li>
</ul>

<p>ASR decoding is where the magic happens—turning probabilities into words. The techniques you’ve learned here are the foundation for building voice assistants, transcription services, and real-time translation systems. As LLMs become more integrated with speech, the importance of efficient, accurate decoding will only grow.</p>

<p><strong>Next Steps:</strong></p>
<ul>
  <li>Implement a CTC decoder from scratch.</li>
  <li>Add LM integration.</li>
  <li>Build a streaming decoder.</li>
  <li>Explore RNN-T for production systems.</li>
  <li>Study WFST-based decoding (Kaldi).</li>
</ul>

<p>The journey from “audio in” to “text out” is complex but rewarding. Master it, and you’ll have the skills to build world-class speech systems.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0042-asr-decoding/">arunbaby.com/speech-tech/0042-asr-decoding</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#beam-search" class="page__taxonomy-item p-category" rel="tag">beam-search</a><span class="sep">, </span>
    
      <a href="/tags/#decoding" class="page__taxonomy-item p-category" rel="tag">decoding</a><span class="sep">, </span>
    
      <a href="/tags/#language-model" class="page__taxonomy-item p-category" rel="tag">language-model</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0042-merge-k-sorted-lists/" rel="permalink">Merge K Sorted Lists
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Combining order from chaos, one element at a time.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0042-feature-stores/" rel="permalink">Feature Stores
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          14 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The centralized truth for machine learning features.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0042-agent-evaluation-frameworks/" rel="permalink">Agent Evaluation Frameworks
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          15 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“If you can’t measure an agent, you can’t improve it: build evals for success, safety, cost, and regressions.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Automatic+Speech+Recognition+%28ASR%29+Decoding%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0042-asr-decoding%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0042-asr-decoding%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0042-asr-decoding/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0041-speaker-diarization/" class="pagination--pager" title="Speaker Diarization">Previous</a>
    
    
      <a href="/speech-tech/0043-speech-enhancement/" class="pagination--pager" title="Speech Enhancement">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
