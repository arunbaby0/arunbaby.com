<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Automatic Speech Recognition (ASR) Decoding - Arun Baby</title>
<meta name="description" content="“Turning acoustic probabilities into coherent text.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Automatic Speech Recognition (ASR) Decoding">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0042-asr-decoding/">


  <meta property="og:description" content="“Turning acoustic probabilities into coherent text.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Automatic Speech Recognition (ASR) Decoding">
  <meta name="twitter:description" content="“Turning acoustic probabilities into coherent text.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0042-asr-decoding/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-29T13:53:22+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0042-asr-decoding/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Automatic Speech Recognition (ASR) Decoding">
    <meta itemprop="description" content="“Turning acoustic probabilities into coherent text.”">
    <meta itemprop="datePublished" content="2025-12-29T13:53:22+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0042-asr-decoding/" itemprop="url">Automatic Speech Recognition (ASR) Decoding
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          16 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction">1. Introduction</a></li><li><a href="#2-the-decoding-problem">2. The Decoding Problem</a></li><li><a href="#3-decoding-strategies">3. Decoding Strategies</a><ul><li><a href="#31-greedy-decoding">3.1. Greedy Decoding</a></li><li><a href="#32-beam-search">3.2. Beam Search</a></li><li><a href="#33-ctc-decoding">3.3. CTC Decoding</a></li><li><a href="#34-attention-based-decoding">3.4. Attention-Based Decoding</a></li></ul></li><li><a href="#4-language-model-integration">4. Language Model Integration</a><ul><li><a href="#41-shallow-fusion">4.1. Shallow Fusion</a></li><li><a href="#42-deep-fusion">4.2. Deep Fusion</a></li><li><a href="#43-rescoring-n-best-rescoring">4.3. Rescoring (N-Best Rescoring)</a></li></ul></li><li><a href="#5-ctc-beam-search-with-language-model">5. CTC Beam Search with Language Model</a></li><li><a href="#6-system-design-production-asr-decoder">6. System Design: Production ASR Decoder</a></li><li><a href="#7-deep-dive-weighted-finite-state-transducers-wfst">7. Deep Dive: Weighted Finite State Transducers (WFST)</a></li><li><a href="#8-deep-dive-streaming-vs-non-streaming">8. Deep Dive: Streaming vs Non-Streaming</a></li><li><a href="#9-word-error-rate-wer">9. Word Error Rate (WER)</a></li><li><a href="#10-decoding-optimizations">10. Decoding Optimizations</a><ul><li><a href="#101-pruning">10.1. Pruning</a></li><li><a href="#102-lattice-generation">10.2. Lattice Generation</a></li><li><a href="#103-gpu-acceleration">10.3. GPU Acceleration</a></li></ul></li><li><a href="#11-production-case-study-whisper">11. Production Case Study: Whisper</a></li><li><a href="#12-interview-questions">12. Interview Questions</a></li><li><a href="#13-common-mistakes">13. Common Mistakes</a></li><li><a href="#14-deep-dive-confidence-estimation">14. Deep Dive: Confidence Estimation</a></li><li><a href="#15-deep-dive-hot-words-contextual-biasing">15. Deep Dive: Hot Words (Contextual Biasing)</a></li><li><a href="#16-future-trends">16. Future Trends</a></li><li><a href="#17-conclusion">17. Conclusion</a></li><li><a href="#18-deep-dive-rnn-transducer-rnn-t-decoding">18. Deep Dive: RNN-Transducer (RNN-T) Decoding</a></li><li><a href="#19-production-case-study-google-voice-search">19. Production Case Study: Google Voice Search</a></li><li><a href="#20-production-case-study-amazon-alexa">20. Production Case Study: Amazon Alexa</a></li><li><a href="#21-benchmarking-asr-decoders">21. Benchmarking ASR Decoders</a></li><li><a href="#22-decoding-for-low-resource-languages">22. Decoding for Low-Resource Languages</a></li><li><a href="#23-advanced-minimum-bayes-risk-mbr-decoding">23. Advanced: Minimum Bayes Risk (MBR) Decoding</a></li><li><a href="#24-deep-dive-subword-units">24. Deep Dive: Subword Units</a></li><li><a href="#25-decoding-with-word-timestamps">25. Decoding with Word Timestamps</a></li><li><a href="#26-decoding-for-dictation-vs-conversation">26. Decoding for Dictation vs Conversation</a></li><li><a href="#27-implementation-pytorch-ctc-beam-search">27. Implementation: PyTorch CTC Beam Search</a></li><li><a href="#28-interview-strategy-asr-decoding">28. Interview Strategy: ASR Decoding</a></li><li><a href="#29-testing-asr-decoders">29. Testing ASR Decoders</a></li><li><a href="#30-conclusion--mastery-checklist">30. Conclusion &amp; Mastery Checklist</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Turning acoustic probabilities into coherent text.”</strong></p>

<h2 id="1-introduction">1. Introduction</h2>

<p><strong>Automatic Speech Recognition (ASR)</strong> converts speech audio into text. The <strong>decoding</strong> stage is where we take the acoustic model’s output (probabilities over characters/tokens) and produce the final transcription.</p>

<p><strong>Pipeline:</strong></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Audio → Feature Extraction (MFCC) → Acoustic Model → Decoder → Text
</code></pre></div></div>

<p>The decoder’s job is to find the most likely text sequence given the acoustic observations.</p>

<h2 id="2-the-decoding-problem">2. The Decoding Problem</h2>

<p><strong>Mathematically:</strong>
\(\hat{W} = \arg\max_W P(W | O)\)</p>

<p>Where:</p>
<ul>
  <li>$O$ = Acoustic observations (audio features).</li>
  <li>$W$ = Word sequence (transcription).</li>
</ul>

<p>Using Bayes’ theorem:
\(P(W | O) = \frac{P(O | W) P(W)}{P(O)}\)</p>

<p>Since $P(O)$ is constant, we maximize:
\(\hat{W} = \arg\max_W P(O | W) \cdot P(W)\)</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$P(O</td>
          <td>W)$ = <strong>Acoustic Model</strong> (AM): How likely is this audio given these words?</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>$P(W)$ = <strong>Language Model</strong> (LM): How likely is this word sequence?</li>
</ul>

<h2 id="3-decoding-strategies">3. Decoding Strategies</h2>

<h3 id="31-greedy-decoding">3.1. Greedy Decoding</h3>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>At each time step, pick the most likely token.</li>
  <li>Concatenate to form the output.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">greedy_decode</span><span class="p">(</span><span class="n">probs</span><span class="p">):</span>
    <span class="c1"># probs: (T, vocab_size)
</span>    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">probs</span><span class="p">)):</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
        <span class="n">tokens</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div></div>

<p><strong>Pros:</strong> Fast, simple.
<strong>Cons:</strong> Doesn’t consider context. Often produces suboptimal results.</p>

<h3 id="32-beam-search">3.2. Beam Search</h3>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Maintain top-k hypotheses (beams).</li>
  <li>At each step, extend each hypothesis with all possible tokens.</li>
  <li>Keep only the top-k scoring hypotheses.</li>
  <li>Repeat until end-of-sequence.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">beam_search</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">beam_width</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># probs: (T, vocab_size)
</span>    <span class="n">sequences</span> <span class="o">=</span> <span class="p">[[[],</span> <span class="mf">0.0</span><span class="p">]]</span>  <span class="c1"># (tokens, score)
</span>    
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">probs</span><span class="p">)):</span>
        <span class="n">all_candidates</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">seq</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">t</span><span class="p">])):</span>
                <span class="n">new_seq</span> <span class="o">=</span> <span class="n">seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">token</span><span class="p">]</span>
                <span class="n">new_score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">token</span><span class="p">])</span>
                <span class="n">all_candidates</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">new_seq</span><span class="p">,</span> <span class="n">new_score</span><span class="p">))</span>
        
        <span class="c1"># Keep top-k
</span>        <span class="n">sequences</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_candidates</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="n">beam_width</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Return best sequence
</span></code></pre></div></div>

<p><strong>Pros:</strong> Considers multiple hypotheses. Usually better than greedy.
<strong>Cons:</strong> Computationally expensive. Still not globally optimal.</p>

<h3 id="33-ctc-decoding">3.3. CTC Decoding</h3>

<p><strong>Connectionist Temporal Classification (CTC)</strong> is a loss function for sequence-to-sequence tasks where alignment is unknown.</p>

<p><strong>Key Idea:</strong></p>
<ul>
  <li>Add a “blank” token (ε) to handle alignment.</li>
  <li>Multiple CTC paths map to the same output (e.g., “aa-ab” and “a-aab” both → “ab”).</li>
</ul>

<p><strong>CTC Decoding:</strong></p>
<ol>
  <li>Apply greedy or beam search to CTC output.</li>
  <li>Collapse consecutive duplicate tokens.</li>
  <li>Remove blank tokens.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ctc_decode</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">blank_token</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">prev</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">probs</span><span class="p">)):</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">token</span> <span class="o">!=</span> <span class="n">prev</span> <span class="ow">and</span> <span class="n">token</span> <span class="o">!=</span> <span class="n">blank_token</span><span class="p">:</span>
            <span class="n">tokens</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
        <span class="n">prev</span> <span class="o">=</span> <span class="n">token</span>
    
    <span class="k">return</span> <span class="n">tokens</span>
</code></pre></div></div>

<h3 id="34-attention-based-decoding">3.4. Attention-Based Decoding</h3>

<p>In encoder-decoder models (like Whisper, Seq2Seq), the decoder uses attention to focus on relevant parts of the encoder output.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Encoder processes audio → hidden states.</li>
  <li>Decoder autoregressively generates tokens.</li>
  <li>At each step, attention weights determine which encoder states to focus on.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">attention_decode</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">BOS_TOKEN</span><span class="p">]</span>  <span class="c1"># Begin of sequence
</span>    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">)</span>
        <span class="n">next_token</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">tokens</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">next_token</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">next_token</span> <span class="o">==</span> <span class="n">EOS_TOKEN</span><span class="p">:</span>
            <span class="k">break</span>
    
    <span class="k">return</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Remove BOS and EOS
</span></code></pre></div></div>

<h2 id="4-language-model-integration">4. Language Model Integration</h2>

<p><strong>Problem:</strong> Acoustic model output may be noisy. Language models can correct grammatical errors.</p>

<p><strong>Approaches:</strong></p>

<h3 id="41-shallow-fusion">4.1. Shallow Fusion</h3>

<p><strong>Idea:</strong> Combine AM and LM scores during beam search.</p>

\[\text{score} = \log P_{AM}(y | x) + \lambda \log P_{LM}(y)\]

<p>Where $\lambda$ is a tunable weight.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">beam_search_with_lm</span><span class="p">(</span><span class="n">am_probs</span><span class="p">,</span> <span class="n">lm</span><span class="p">,</span> <span class="n">beam_width</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">lm_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">sequences</span> <span class="o">=</span> <span class="p">[[[],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]]</span>  <span class="c1"># (tokens, am_score, lm_score)
</span>    
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">am_probs</span><span class="p">)):</span>
        <span class="n">all_candidates</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">seq</span><span class="p">,</span> <span class="n">am_score</span><span class="p">,</span> <span class="n">lm_score</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">am_probs</span><span class="p">[</span><span class="n">t</span><span class="p">])):</span>
                <span class="n">new_seq</span> <span class="o">=</span> <span class="n">seq</span> <span class="o">+</span> <span class="p">[</span><span class="n">token</span><span class="p">]</span>
                <span class="n">new_am_score</span> <span class="o">=</span> <span class="n">am_score</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">am_probs</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">token</span><span class="p">])</span>
                <span class="n">new_lm_score</span> <span class="o">=</span> <span class="n">lm_score</span> <span class="o">+</span> <span class="n">lm</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">new_seq</span><span class="p">)</span>
                
                <span class="n">combined</span> <span class="o">=</span> <span class="n">new_am_score</span> <span class="o">+</span> <span class="n">lm_weight</span> <span class="o">*</span> <span class="n">new_lm_score</span>
                <span class="n">all_candidates</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">new_seq</span><span class="p">,</span> <span class="n">new_am_score</span><span class="p">,</span> <span class="n">new_lm_score</span><span class="p">,</span> <span class="n">combined</span><span class="p">))</span>
        
        <span class="c1"># Keep top-k by combined score
</span>        <span class="n">sequences</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">all_candidates</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="n">beam_width</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="42-deep-fusion">4.2. Deep Fusion</h3>

<p><strong>Idea:</strong> Train the AM and LM jointly. The LM’s hidden states are fed into the AM.</p>

<p><strong>Benefits:</strong> Tighter integration, better performance.
<strong>Drawbacks:</strong> Requires retraining. Less flexible.</p>

<h3 id="43-rescoring-n-best-rescoring">4.3. Rescoring (N-Best Rescoring)</h3>

<p><strong>Idea:</strong></p>
<ol>
  <li>Generate N-best hypotheses with AM only.</li>
  <li>Rescore each hypothesis with a powerful LM (e.g., GPT).</li>
  <li>Return the highest-scoring hypothesis.</li>
</ol>

<p><strong>Benefits:</strong> Can use large, pretrained LMs.
<strong>Drawbacks:</strong> Two-pass (slower).</p>

<h2 id="5-ctc-beam-search-with-language-model">5. CTC Beam Search with Language Model</h2>

<p><strong>Prefix Beam Search:</strong>
Handles CTC’s blank tokens and collapsed outputs properly.</p>

<p><strong>Algorithm:</strong></p>
<ol>
  <li>Track two scores for each prefix:
    <ul>
      <li><strong>Pb (blank):</strong> Probability of prefix ending with blank.</li>
      <li><strong>Pnb (non-blank):</strong> Probability of prefix not ending with blank.</li>
    </ul>
  </li>
  <li>At each step, extend prefixes by:
    <ul>
      <li>Adding a blank (only affects Pb).</li>
      <li>Repeating the last character (affects Pb/Pnb transition).</li>
      <li>Adding a new character (affects Pnb).</li>
    </ul>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ctc_beam_search_with_lm</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">lm</span><span class="p">,</span> <span class="n">beam_width</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lm_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="c1"># probs: (T, vocab_size)
</span>    <span class="c1"># Initialize
</span>    <span class="n">beams</span> <span class="o">=</span> <span class="p">{():</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)}</span>  <span class="c1"># prefix: (pb, pnb)
</span>    
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">probs</span><span class="p">)):</span>
        <span class="n">new_beams</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">))</span>
        
        <span class="k">for</span> <span class="n">prefix</span><span class="p">,</span> <span class="p">(</span><span class="n">pb</span><span class="p">,</span> <span class="n">pnb</span><span class="p">)</span> <span class="ow">in</span> <span class="n">beams</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="n">t</span><span class="p">])):</span>
                <span class="n">p</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[</span><span class="n">t</span><span class="p">][</span><span class="n">c</span><span class="p">]</span>
                
                <span class="k">if</span> <span class="n">c</span> <span class="o">==</span> <span class="n">blank</span><span class="p">:</span>
                    <span class="c1"># Extend with blank
</span>                    <span class="n">key</span> <span class="o">=</span> <span class="n">prefix</span>
                    <span class="n">new_beams</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_beams</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">pb</span> <span class="o">+</span> <span class="n">pnb</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span><span class="p">,</span> <span class="n">new_beams</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                
                <span class="k">elif</span> <span class="n">prefix</span> <span class="ow">and</span> <span class="n">c</span> <span class="o">==</span> <span class="n">prefix</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
                    <span class="c1"># Repeat character (needs blank before)
</span>                    <span class="n">new_beams</span><span class="p">[</span><span class="n">prefix</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_beams</span><span class="p">[</span><span class="n">prefix</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">new_beams</span><span class="p">[</span><span class="n">prefix</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">pb</span> <span class="o">*</span> <span class="n">p</span><span class="p">)</span>
                    <span class="c1"># Or new character after blank
</span>                    <span class="n">new_key</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span><span class="p">,)</span>
                    <span class="n">new_beams</span><span class="p">[</span><span class="n">new_key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_beams</span><span class="p">[</span><span class="n">new_key</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">new_beams</span><span class="p">[</span><span class="n">new_key</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">pnb</span> <span class="o">*</span> <span class="n">p</span><span class="p">)</span>
                
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># New character
</span>                    <span class="n">new_key</span> <span class="o">=</span> <span class="n">prefix</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span><span class="p">,)</span>
                    <span class="n">lm_factor</span> <span class="o">=</span> <span class="n">lm</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">new_key</span><span class="p">)</span> <span class="o">**</span> <span class="n">lm_weight</span>
                    <span class="n">new_beams</span><span class="p">[</span><span class="n">new_key</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_beams</span><span class="p">[</span><span class="n">new_key</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">new_beams</span><span class="p">[</span><span class="n">new_key</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">pb</span> <span class="o">+</span> <span class="n">pnb</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span> <span class="o">*</span> <span class="n">lm_factor</span><span class="p">)</span>
        
        <span class="c1"># Prune to beam_width
</span>        <span class="n">beams</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">sorted</span><span class="p">(</span><span class="n">new_beams</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="n">beam_width</span><span class="p">])</span>
    
    <span class="c1"># Return best prefix
</span>    <span class="n">best_prefix</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">beams</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">sum</span><span class="p">(</span><span class="n">beams</span><span class="p">[</span><span class="n">x</span><span class="p">]))</span>
    <span class="k">return</span> <span class="nf">list</span><span class="p">(</span><span class="n">best_prefix</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="6-system-design-production-asr-decoder">6. System Design: Production ASR Decoder</h2>

<p><strong>Scenario:</strong> Build a real-time ASR system for voice assistants.</p>

<p><strong>Requirements:</strong></p>
<ul>
  <li><strong>Latency:</strong> &lt; 200ms from end of utterance to transcription.</li>
  <li><strong>Accuracy:</strong> WER &lt; 10%.</li>
  <li><strong>Streaming:</strong> Start transcribing before the user finishes speaking.</li>
</ul>

<p><strong>Architecture:</strong></p>

<p><strong>Step 1: Streaming Audio Input</strong></p>
<ul>
  <li>Audio arrives in 20ms chunks.</li>
  <li>Feature extraction (MFCC/Mel) in real-time.</li>
</ul>

<p><strong>Step 2: Streaming Acoustic Model</strong></p>
<ul>
  <li>Use a streaming-compatible architecture (e.g., Conformer with lookahead).</li>
  <li>Output probabilities every 20ms.</li>
</ul>

<p><strong>Step 3: Online Decoding</strong></p>
<ul>
  <li>Run CTC beam search incrementally.</li>
  <li>Emit partial results as hypotheses stabilize.</li>
</ul>

<p><strong>Step 4: Endpointing</strong></p>
<ul>
  <li>Detect when the user stops speaking.</li>
  <li>Finalize the transcription.</li>
</ul>

<p><strong>Step 5: Rescoring (Optional)</strong></p>
<ul>
  <li>Rescore final hypothesis with a larger LM.</li>
</ul>

<h2 id="7-deep-dive-weighted-finite-state-transducers-wfst">7. Deep Dive: Weighted Finite State Transducers (WFST)</h2>

<p><strong>WFST</strong> is a mathematical framework for composing AM, LM, and lexicon.</p>

<p><strong>Components:</strong></p>
<ul>
  <li><strong>H (HMM):</strong> Maps senones to context-dependent phones.</li>
  <li><strong>C (Context):</strong> Maps context-dependent phones to context-independent phones.</li>
  <li><strong>L (Lexicon):</strong> Maps phones to words.</li>
  <li><strong>G (Grammar):</strong> Language model (n-gram).</li>
</ul>

<p><strong>Composition:</strong>
\(\text{Decoding Graph} = H \circ C \circ L \circ G\)</p>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Efficient decoding using Viterbi algorithm on the composed graph.</li>
  <li>Caching: Precompute and store the composed graph.</li>
</ul>

<p><strong>Tools:</strong></p>
<ul>
  <li><strong>Kaldi:</strong> Open-source ASR toolkit using WFST.</li>
  <li><strong>OpenFst:</strong> Library for finite-state transducers.</li>
</ul>

<h2 id="8-deep-dive-streaming-vs-non-streaming">8. Deep Dive: Streaming vs Non-Streaming</h2>

<p><strong>Non-Streaming (Offline):</strong></p>
<ul>
  <li>Process entire audio at once.</li>
  <li>Can use bidirectional models (look at future).</li>
  <li>Higher accuracy.</li>
</ul>

<p><strong>Streaming (Online):</strong></p>
<ul>
  <li>Process audio chunk by chunk.</li>
  <li>Cannot look at future (causality constraint).</li>
  <li>Lower latency, slightly lower accuracy.</li>
</ul>

<p><strong>Hybrid (Look-Ahead):</strong></p>
<ul>
  <li>Allow small lookahead (e.g., 200ms).</li>
  <li>Balance between latency and accuracy.</li>
</ul>

<h2 id="9-word-error-rate-wer">9. Word Error Rate (WER)</h2>

<p><strong>Definition:</strong>
\(WER = \frac{S + D + I}{N}\)</p>

<p>Where:</p>
<ul>
  <li><strong>S</strong> = Substitutions (wrong word).</li>
  <li><strong>D</strong> = Deletions (missing word).</li>
  <li><strong>I</strong> = Insertions (extra word).</li>
  <li><strong>N</strong> = Total words in reference.</li>
</ul>

<p><strong>Example:</strong></p>
<ul>
  <li>Reference: “the cat sat on the mat”</li>
  <li>Hypothesis: “the cat hat on a mat”</li>
  <li>Errors: S=1 (sat→hat), S=1 (the→a)</li>
  <li>WER = 2 / 6 = 33.3%</li>
</ul>

<h2 id="10-decoding-optimizations">10. Decoding Optimizations</h2>

<h3 id="101-pruning">10.1. Pruning</h3>

<p><strong>Beam Pruning:</strong> Keep only top-k hypotheses.
<strong>Threshold Pruning:</strong> Discard hypotheses with score &lt; best_score - threshold.
<strong>Histogram Pruning:</strong> Limit hypotheses per frame.</p>

<h3 id="102-lattice-generation">10.2. Lattice Generation</h3>

<p>Instead of outputting a single transcription, output a <strong>lattice</strong>—a compact representation of all hypotheses.</p>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Rescoring: Apply different LMs later.</li>
  <li>Confidence Estimation: Compute confidence scores.</li>
  <li>N-Best Lists: Extract top-N transcriptions.</li>
</ul>

<h3 id="103-gpu-acceleration">10.3. GPU Acceleration</h3>

<ul>
  <li><strong>Batched Beam Search:</strong> Process multiple utterances in parallel.</li>
  <li><strong>CUDA Implementations:</strong> Use GPU for matrix operations.</li>
  <li><strong>TensorRT:</strong> Optimize inference.</li>
</ul>

<h2 id="11-production-case-study-whisper">11. Production Case Study: Whisper</h2>

<p><strong>Whisper (OpenAI):</strong></p>
<ul>
  <li><strong>Architecture:</strong> Encoder-Decoder Transformer.</li>
  <li><strong>Decoding:</strong> Greedy or beam search with temperature.</li>
  <li><strong>Language Model:</strong> Implicit in the decoder.</li>
  <li><strong>Multilingual:</strong> 99 languages.</li>
</ul>

<p><strong>Decoding Features:</strong></p>
<ul>
  <li><strong>Timestamps:</strong> Predict start/end times for each word.</li>
  <li><strong>Temperature Fallback:</strong> If greedy fails (repetition), retry with temperature sampling.</li>
  <li><strong>Compression Threshold:</strong> Skip segments with too much compression (repetition).</li>
</ul>

<h2 id="12-interview-questions">12. Interview Questions</h2>

<ol>
  <li><strong>Greedy vs Beam Search:</strong> When would you use each?</li>
  <li><strong>CTC Decoding:</strong> Explain how blank tokens work.</li>
  <li><strong>Language Model Integration:</strong> What is shallow fusion?</li>
  <li><strong>Streaming ASR:</strong> How do you handle causality constraints?</li>
  <li><strong>WFST:</strong> What is the role of the lexicon (L)?</li>
  <li><strong>Calculate WER:</strong> Given reference and hypothesis, compute WER.</li>
</ol>

<h2 id="13-common-mistakes">13. Common Mistakes</h2>

<ul>
  <li><strong>Ignoring Blanks in CTC:</strong> Not collapsing consecutive tokens.</li>
  <li><strong>Beam Width Too Small:</strong> Missing the correct hypothesis.</li>
  <li><strong>LM Weight Too High:</strong> Over-relying on LM, ignoring AM.</li>
  <li><strong>Not Handling OOV Words:</strong> Out-of-vocabulary words cause errors.</li>
  <li><strong>Ignoring Latency:</strong> Production systems have strict latency requirements.</li>
</ul>

<h2 id="14-deep-dive-confidence-estimation">14. Deep Dive: Confidence Estimation</h2>

<p><strong>Problem:</strong> Not all transcriptions are equally reliable. How do we estimate confidence?</p>

<p><strong>Approaches:</strong></p>

<p><strong>1. Posterior Probability:</strong></p>
<ul>
  <li>Use the probability of the best hypothesis.</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$\text{confidence} = P(W</td>
          <td>O)$.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p><strong>2. Entropy:</strong></p>
<ul>
  <li>High entropy = low confidence (many competing hypotheses).</li>
</ul>

<p><strong>3. Word-Level Confidence:</strong></p>
<ul>
  <li>Compute confidence for each word using lattice posteriors.</li>
</ul>

<p><strong>4. Model-Based:</strong></p>
<ul>
  <li>Train a separate model to predict confidence from ASR features.</li>
</ul>

<h2 id="15-deep-dive-hot-words-contextual-biasing">15. Deep Dive: Hot Words (Contextual Biasing)</h2>

<p><strong>Problem:</strong> ASR fails on domain-specific terms (e.g., product names, jargon).</p>

<p><strong>Solution:</strong> Bias decoding towards expected words.</p>

<p><strong>Approaches:</strong></p>
<ol>
  <li><strong>N-Gram Boosting:</strong> Increase LM probability for hot words.</li>
  <li><strong>Contextual LM:</strong> Condition LM on context (e.g., user’s recent queries).</li>
  <li><strong>Shallow Fusion with Boosted LM:</strong> Add bonus score for hot words.</li>
</ol>

<p><strong>Example:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">hot_words</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Alexa</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Siri</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Cortana</span><span class="sh">"</span><span class="p">]</span>
<span class="n">hot_word_boost</span> <span class="o">=</span> <span class="mf">2.0</span>  <span class="c1"># Log-scale boost
</span>
<span class="k">def</span> <span class="nf">score_with_hotwords</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">am_score</span><span class="p">,</span> <span class="n">lm_score</span><span class="p">):</span>
    <span class="n">bonus</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">hot_word_boost</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">seq</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">hot_words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">am_score</span> <span class="o">+</span> <span class="n">lm_weight</span> <span class="o">*</span> <span class="n">lm_score</span> <span class="o">+</span> <span class="n">bonus</span>
</code></pre></div></div>

<h2 id="16-future-trends">16. Future Trends</h2>

<p><strong>1. End-to-End LM Integration:</strong></p>
<ul>
  <li>Train AM and LM jointly (e.g., Hybrid Transducers).</li>
</ul>

<p><strong>2. LLM for Error Correction:</strong></p>
<ul>
  <li>Use GPT/LLaMA to post-process ASR output.</li>
</ul>

<p><strong>3. Multimodal Decoding:</strong></p>
<ul>
  <li>Use video (lip reading) to improve decoding.</li>
</ul>

<p><strong>4. On-Device Decoding:</strong></p>
<ul>
  <li>Run decoding on mobile devices (requires efficient implementations).</li>
</ul>

<h2 id="17-conclusion">17. Conclusion</h2>

<p>ASR decoding is the bridge between acoustic probabilities and human-readable text. The key is balancing:</p>
<ul>
  <li><strong>Accuracy:</strong> Use beam search and language models.</li>
  <li><strong>Latency:</strong> Use streaming architectures and pruning.</li>
  <li><strong>Flexibility:</strong> Use lattices and rescoring for adaptability.</li>
</ul>

<p><strong>Key Takeaways:</strong></p>
<ul>
  <li><strong>Greedy:</strong> Fast but suboptimal.</li>
  <li><strong>Beam Search:</strong> Better quality, higher cost.</li>
  <li><strong>CTC:</strong> Handles alignment automatically.</li>
  <li><strong>LM Integration:</strong> Shallow fusion, deep fusion, rescoring.</li>
  <li><strong>WFST:</strong> Classical approach for composing AM/LM/Lexicon.</li>
  <li><strong>Streaming:</strong> Essential for real-time applications.</li>
</ul>

<p>Mastering decoding is essential for building production ASR systems. The techniques here apply to any sequence-to-sequence task: machine translation, speech synthesis, and beyond.</p>

<h2 id="18-deep-dive-rnn-transducer-rnn-t-decoding">18. Deep Dive: RNN-Transducer (RNN-T) Decoding</h2>

<p><strong>RNN-T</strong> is a popular architecture for streaming ASR (used by Google, Meta).</p>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Encoder:</strong> Processes audio, outputs acoustic features.</li>
  <li><strong>Prediction Network:</strong> RNN that processes output history.</li>
  <li><strong>Joint Network:</strong> Combines encoder + prediction network outputs.</li>
  <li><strong>Output:</strong> Probability over vocabulary + blank.</li>
</ul>

<p><strong>Decoding Algorithm:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rnnt_greedy_decode</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">,</span> <span class="n">predictor</span><span class="p">,</span> <span class="n">joint</span><span class="p">):</span>
    <span class="n">T</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span>
    <span class="n">U</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Output index
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pred_state</span> <span class="o">=</span> <span class="n">predictor</span><span class="p">.</span><span class="nf">initial_state</span><span class="p">()</span>
    
    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">T</span><span class="p">:</span>
        <span class="c1"># Get encoder output at time t
</span>        <span class="n">enc_t</span> <span class="o">=</span> <span class="n">encoder_output</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
        
        <span class="c1"># Get prediction network output
</span>        <span class="n">pred_output</span><span class="p">,</span> <span class="n">pred_state</span> <span class="o">=</span> <span class="nf">predictor</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="k">if</span> <span class="n">outputs</span> <span class="k">else</span> <span class="p">[</span><span class="n">BOS</span><span class="p">],</span> <span class="n">pred_state</span><span class="p">)</span>
        
        <span class="c1"># Joint network
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="nf">joint</span><span class="p">(</span><span class="n">enc_t</span><span class="p">,</span> <span class="n">pred_output</span><span class="p">)</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="n">BLANK</span><span class="p">:</span>
            <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Advance time
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="c1"># Don't advance time (can emit multiple tokens at same time step)
</span>    
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<p><strong>Key Difference from CTC:</strong></p>
<ul>
  <li>CTC: One output per time step.</li>
  <li>RNN-T: Can emit multiple tokens per time step (or none).</li>
</ul>

<h2 id="19-production-case-study-google-voice-search">19. Production Case Study: Google Voice Search</h2>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>Model:</strong> Conformer encoder + RNN-T decoder.</li>
  <li><strong>Decoding:</strong> Beam search with shallow fusion LM.</li>
  <li><strong>Latency:</strong> &lt; 200ms.</li>
  <li><strong>WER:</strong> &lt; 5% on conversational speech.</li>
</ul>

<p><strong>Optimizations:</strong></p>
<ul>
  <li><strong>Quantized Model:</strong> INT8 for mobile.</li>
  <li><strong>Speculative Decoding:</strong> Start decoding before audio ends.</li>
  <li><strong>Hot Word Boosting:</strong> Boost contact names, app names.</li>
  <li><strong>Contextual LM:</strong> Condition on user’s search history.</li>
</ul>

<h2 id="20-production-case-study-amazon-alexa">20. Production Case Study: Amazon Alexa</h2>

<p><strong>Architecture:</strong></p>
<ul>
  <li><strong>First Pass:</strong> CTC decoding with small LM.</li>
  <li><strong>Second Pass:</strong> Rescoring with large LM.</li>
  <li><strong>Third Pass:</strong> NLU (intent recognition).</li>
</ul>

<p><strong>Latency Breakdown:</strong></p>
<ul>
  <li>Audio capture: 50ms.</li>
  <li>CTC decoding: 100ms.</li>
  <li>LM rescoring: 50ms.</li>
  <li>Total: ~200ms.</li>
</ul>

<p><strong>Optimization:</strong></p>
<ul>
  <li><strong>Endpointing:</strong> Detect speech end quickly.</li>
  <li><strong>Prefetch:</strong> Start cloud processing during endpointing.</li>
</ul>

<h2 id="21-benchmarking-asr-decoders">21. Benchmarking ASR Decoders</h2>

<p><strong>Metrics:</strong></p>
<ul>
  <li><strong>WER:</strong> Word Error Rate.</li>
  <li><strong>Latency:</strong> Time from audio end to transcription.</li>
  <li><strong>RTF (Real-Time Factor):</strong> Processing time / Audio duration. RTF &lt; 1 means faster than real-time.</li>
</ul>

<p><strong>Benchmark (LibriSpeech):</strong>
| Model | WER (test-clean) | RTF (GPU) | RTF (CPU) |
|——-|——————|———–|———–|
| Wav2Vec 2.0 + Greedy | 2.7% | 0.1 | 1.5 |
| Whisper (small) + Beam | 3.0% | 0.2 | 3.0 |
| Conformer + RNN-T | 2.1% | 0.15 | 2.0 |</p>

<h2 id="22-decoding-for-low-resource-languages">22. Decoding for Low-Resource Languages</h2>

<p><strong>Challenge:</strong> Language models may not exist for low-resource languages.</p>

<p><strong>Solutions:</strong></p>
<ol>
  <li><strong>Multilingual Models:</strong> Use models trained on 100+ languages.</li>
  <li><strong>Cross-Lingual Transfer:</strong> Fine-tune on target language with limited data.</li>
  <li><strong>Character-Level LM:</strong> Train a small character LM.</li>
  <li><strong>Phoneme-Based LM:</strong> Use phoneme sequences instead of words.</li>
</ol>

<h2 id="23-advanced-minimum-bayes-risk-mbr-decoding">23. Advanced: Minimum Bayes Risk (MBR) Decoding</h2>

<p><strong>Problem:</strong> Beam search finds the most likely sequence, but not necessarily the one with the lowest WER.</p>

<p><strong>MBR Approach:</strong></p>
<ul>
  <li>Generate N-best hypotheses.</li>
  <li>For each hypothesis, compute expected WER against all other hypotheses.</li>
  <li>Return the hypothesis with lowest expected WER.</li>
</ul>

<p><strong>Algorithm:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mbr_decode</span><span class="p">(</span><span class="n">n_best_list</span><span class="p">):</span>
    <span class="c1"># n_best_list: [(hypothesis, probability), ...]
</span>    <span class="n">best_hyp</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">best_risk</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">hyp</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">n_best_list</span><span class="p">:</span>
        <span class="n">risk</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">other_hyp</span><span class="p">,</span> <span class="n">other_prob</span> <span class="ow">in</span> <span class="n">n_best_list</span><span class="p">:</span>
            <span class="n">risk</span> <span class="o">+=</span> <span class="n">other_prob</span> <span class="o">*</span> <span class="nf">word_error_rate</span><span class="p">(</span><span class="n">hyp</span><span class="p">,</span> <span class="n">other_hyp</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">risk</span> <span class="o">&lt;</span> <span class="n">best_risk</span><span class="p">:</span>
            <span class="n">best_risk</span> <span class="o">=</span> <span class="n">risk</span>
            <span class="n">best_hyp</span> <span class="o">=</span> <span class="n">hyp</span>
    
    <span class="k">return</span> <span class="n">best_hyp</span>
</code></pre></div></div>

<p><strong>Use Case:</strong> When WER is more important than likelihood.</p>

<h2 id="24-deep-dive-subword-units">24. Deep Dive: Subword Units</h2>

<p><strong>Problem:</strong> Character-level models have long sequences. Word-level models have OOV issues.</p>

<p><strong>Solution:</strong> Subword units (BPE, SentencePiece).</p>

<p><strong>Algorithm (BPE):</strong></p>
<ol>
  <li>Start with characters.</li>
  <li>Iteratively merge the most frequent adjacent pair.</li>
  <li>Stop when vocabulary size reached.</li>
</ol>

<p><strong>Example:</strong></p>
<ul>
  <li>“lower” → “l o w e r” → “lo we r” → “low er” → “lower”</li>
</ul>

<p><strong>Benefits:</strong></p>
<ul>
  <li>Handles rare words.</li>
  <li>Shorter sequences than characters.</li>
  <li>No OOV (can spell any word).</li>
</ul>

<h2 id="25-decoding-with-word-timestamps">25. Decoding with Word Timestamps</h2>

<p><strong>Problem:</strong> Beyond transcription, we need to know when each word was spoken.</p>

<p><strong>Approaches:</strong></p>

<p><strong>1. CTC Alignment:</strong></p>
<ul>
  <li>After decoding, use forced alignment to find timestamps.</li>
</ul>

<p><strong>2. Attention Weights:</strong></p>
<ul>
  <li>Use encoder-decoder attention to infer timestamps.</li>
</ul>

<p><strong>3. Whisper-Style:</strong></p>
<ul>
  <li>Predict <code class="language-plaintext highlighter-rouge">&lt;|start_time|&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;|end_time|&gt;</code> tokens.</li>
</ul>

<h2 id="26-decoding-for-dictation-vs-conversation">26. Decoding for Dictation vs Conversation</h2>

<p><strong>Dictation:</strong></p>
<ul>
  <li>User speaks clearly, slowly.</li>
  <li>Expects exact transcription.</li>
  <li>Punctuation and formatting expected.</li>
</ul>

<p><strong>Conversation:</strong></p>
<ul>
  <li>Multiple speakers, overlapping speech.</li>
  <li>Disfluencies (uh, um, repetitions).</li>
  <li>Informal language.</li>
</ul>

<p><strong>Decoding Differences:</strong></p>
<ul>
  <li><strong>Dictation:</strong> Use strong LM, inverse text normalization (numbers, dates).</li>
  <li><strong>Conversation:</strong> Use disfluency model, speaker diarization.</li>
</ul>

<h2 id="27-implementation-pytorch-ctc-beam-search">27. Implementation: PyTorch CTC Beam Search</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torchaudio.models.decoder</span> <span class="kn">import</span> <span class="n">ctc_decoder</span>

<span class="c1"># Load lexicon and LM
</span><span class="n">decoder</span> <span class="o">=</span> <span class="nf">ctc_decoder</span><span class="p">(</span>
    <span class="n">lexicon</span><span class="o">=</span><span class="sh">"</span><span class="s">lexicon.txt</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tokens</span><span class="o">=</span><span class="sh">"</span><span class="s">tokens.txt</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">lm</span><span class="o">=</span><span class="sh">"</span><span class="s">language_model.arpa</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">beam_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">lm_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">word_score</span><span class="o">=-</span><span class="mf">0.2</span>
<span class="p">)</span>

<span class="c1"># Decode
</span><span class="n">emissions</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>  <span class="c1"># (T, vocab_size)
</span><span class="n">hypotheses</span> <span class="o">=</span> <span class="nf">decoder</span><span class="p">(</span><span class="n">emissions</span><span class="p">)</span>

<span class="c1"># Get best hypothesis
</span><span class="n">best</span> <span class="o">=</span> <span class="n">hypotheses</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">best</span><span class="p">.</span><span class="n">words</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="28-interview-strategy-asr-decoding">28. Interview Strategy: ASR Decoding</h2>

<p><strong>Step-by-Step:</strong></p>
<ol>
  <li><strong>Explain the Problem:</strong> AM gives probabilities, need to find best text.</li>
  <li><strong>Greedy:</strong> Simple but suboptimal.</li>
  <li><strong>Beam Search:</strong> Better, introduce beam width.</li>
  <li><strong>CTC Specifics:</strong> Blank token, collapsing.</li>
  <li><strong>LM Integration:</strong> Shallow fusion, rescoring.</li>
  <li><strong>Streaming:</strong> Discuss latency constraints.</li>
  <li><strong>Trade-offs:</strong> Accuracy vs latency vs compute.</li>
</ol>

<h2 id="29-testing-asr-decoders">29. Testing ASR Decoders</h2>

<p><strong>Unit Tests:</strong></p>
<ul>
  <li>Test CTC collapsing: “aa-a-bb” → “ab”.</li>
  <li>Test beam search ranking.</li>
  <li>Test LM weight behavior.</li>
</ul>

<p><strong>Integration Tests:</strong></p>
<ul>
  <li>End-to-end: audio → transcription.</li>
  <li>Compare with baseline (e.g., Whisper).</li>
</ul>

<p><strong>Regression Tests:</strong></p>
<ul>
  <li>Test on standard benchmarks (LibriSpeech, Common Voice).</li>
  <li>Alert if WER increases.</li>
</ul>

<h2 id="30-conclusion--mastery-checklist">30. Conclusion &amp; Mastery Checklist</h2>

<p><strong>Mastery Checklist:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement greedy decoding</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement beam search</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement CTC decoding (with blank collapsing)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Integrate language model (shallow fusion)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Understand RNN-T decoding</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement N-best list generation</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Calculate WER correctly</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Handle streaming ASR</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Implement hot word boosting</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Benchmark RTF and WER</li>
</ul>

<p>ASR decoding is where the magic happens—turning probabilities into words. The techniques you’ve learned here are the foundation for building voice assistants, transcription services, and real-time translation systems. As LLMs become more integrated with speech, the importance of efficient, accurate decoding will only grow.</p>

<p><strong>Next Steps:</strong></p>
<ul>
  <li>Implement a CTC decoder from scratch.</li>
  <li>Add LM integration.</li>
  <li>Build a streaming decoder.</li>
  <li>Explore RNN-T for production systems.</li>
  <li>Study WFST-based decoding (Kaldi).</li>
</ul>

<p>The journey from “audio in” to “text out” is complex but rewarding. Master it, and you’ll have the skills to build world-class speech systems.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0042-asr-decoding/">arunbaby.com/speech-tech/0042-asr-decoding</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asr" class="page__taxonomy-item p-category" rel="tag">asr</a><span class="sep">, </span>
    
      <a href="/tags/#beam-search" class="page__taxonomy-item p-category" rel="tag">beam-search</a><span class="sep">, </span>
    
      <a href="/tags/#decoding" class="page__taxonomy-item p-category" rel="tag">decoding</a><span class="sep">, </span>
    
      <a href="/tags/#language-model" class="page__taxonomy-item p-category" rel="tag">language-model</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Automatic+Speech+Recognition+%28ASR%29+Decoding%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0042-asr-decoding%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0042-asr-decoding%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0042-asr-decoding/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0041-speaker-diarization/" class="pagination--pager" title="Speaker Diarization">Previous</a>
    
    
      <a href="/speech-tech/0043-speech-enhancement/" class="pagination--pager" title="Speech Enhancement">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
