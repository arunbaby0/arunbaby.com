<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Streaming Speech Processing Pipeline - Arun Baby</title>
<meta name="description" content="Build real-time speech processing pipelines that handle audio streams with minimal latency for live transcription and voice interfaces.">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Streaming Speech Processing Pipeline">
<meta property="og:url" content="https://www.arunbaby.com/speech-tech/0008-streaming-speech-pipeline/">


  <meta property="og:description" content="Build real-time speech processing pipelines that handle audio streams with minimal latency for live transcription and voice interfaces.">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Streaming Speech Processing Pipeline">
  <meta name="twitter:description" content="Build real-time speech processing pipelines that handle audio streams with minimal latency for live transcription and voice interfaces.">
  <meta name="twitter:url" content="https://www.arunbaby.com/speech-tech/0008-streaming-speech-pipeline/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-31T09:51:02+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/speech-tech/0008-streaming-speech-pipeline/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Streaming Speech Processing Pipeline">
    <meta itemprop="description" content="Build real-time speech processing pipelines that handle audio streams with minimal latency for live transcription and voice interfaces.">
    <meta itemprop="datePublished" content="2025-12-31T09:51:02+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/speech-tech/0008-streaming-speech-pipeline/" itemprop="url">Streaming Speech Processing Pipeline
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#streaming-pipeline-architecture">Streaming Pipeline Architecture</a></li><li><a href="#audio-capture--chunking">Audio Capture &amp; Chunking</a><ul><li><a href="#real-time-audio-capture">Real-time Audio Capture</a></li><li><a href="#chunk-buffering-strategy">Chunk Buffering Strategy</a></li></ul></li><li><a href="#websocket-based-streaming">WebSocket-Based Streaming</a><ul><li><a href="#server-side">Server Side</a></li><li><a href="#client-side">Client Side</a></li></ul></li><li><a href="#latency-optimization">Latency Optimization</a><ul><li><a href="#latency-breakdown">Latency Breakdown</a></li><li><a href="#optimization-strategies">Optimization Strategies</a></li></ul></li><li><a href="#state-management">State Management</a><ul><li><a href="#stateful-streaming">Stateful Streaming</a></li></ul></li><li><a href="#error-handling--recovery">Error Handling &amp; Recovery</a><ul><li><a href="#robust-streaming">Robust Streaming</a></li></ul></li><li><a href="#connection-to-model-serving-ml">Connection to Model Serving (ML)</a></li><li><a href="#production-patterns">Production Patterns</a><ul><li><a href="#1-multi-channel-audio-streaming">1. Multi-Channel Audio Streaming</a></li><li><a href="#2-adaptive-chunk-size">2. Adaptive Chunk Size</a></li><li><a href="#3-buffering-strategy-for-unreliable-networks">3. Buffering Strategy for Unreliable Networks</a></li></ul></li><li><a href="#advanced-optimization-techniques">Advanced Optimization Techniques</a><ul><li><a href="#1-model-warm-up">1. Model Warm-Up</a></li><li><a href="#2-gpu-batching-for-throughput">2. GPU Batching for Throughput</a></li><li><a href="#3-quantized-models-for-edge-devices">3. Quantized Models for Edge Devices</a></li></ul></li><li><a href="#real-world-integration-examples">Real-World Integration Examples</a><ul><li><a href="#1-zoom-like-meeting-transcription">1. Zoom-like Meeting Transcription</a></li><li><a href="#2-voice-assistant-backend">2. Voice Assistant Backend</a></li></ul></li><li><a href="#performance-metrics--slas">Performance Metrics &amp; SLAs</a><ul><li><a href="#latency-tracking">Latency Tracking</a></li></ul></li><li><a href="#key-takeaways">Key Takeaways</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>Build real-time speech processing pipelines that handle audio streams with minimal latency for live transcription and voice interfaces.</strong></p>

<h2 id="introduction">Introduction</h2>

<p><strong>Streaming speech processing</strong> handles audio in real-time as itâ€™s captured, without waiting for the entire recording.</p>

<p><strong>Why streaming matters:</strong></p>
<ul>
  <li><strong>Low latency:</strong> Start processing immediately (&lt; 100ms)</li>
  <li><strong>Live applications:</strong> Transcription, translation, voice assistants</li>
  <li><strong>Memory efficiency:</strong> Process chunks, not entire recordings</li>
  <li><strong>Better UX:</strong> Instant feedback to users</li>
</ul>

<p><strong>Challenges:</strong></p>
<ul>
  <li>Chunking audio correctly</li>
  <li>Managing state across chunks</li>
  <li>Handling network delays</li>
  <li>Synchronization issues</li>
</ul>

<hr />

<h2 id="streaming-pipeline-architecture">Streaming Pipeline Architecture</h2>

<p><code class="language-plaintext highlighter-rouge">
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Microphone â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
 â”‚ Audio stream (PCM)
 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Audio Chunker â”‚ â† Split into chunks (e.g., 100ms)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚ Chunks
 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Preprocessor â”‚ â† Normalize, filter
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Extract â”‚ â† MFCC, Mel-spec
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ML Model â”‚ â† ASR, classification
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Post-processing â”‚ â† Smoothing, formatting
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output â”‚ â† Transcription, action
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></p>

<hr />

<h2 id="audio-capture--chunking">Audio Capture &amp; Chunking</h2>

<h3 id="real-time-audio-capture">Real-time Audio Capture</h3>

<p>``python
import pyaudio
import numpy as np
from queue import Queue
import threading</p>

<p>class AudioStreamer:
 â€œâ€â€
 Capture audio from microphone in real-time</p>

<p>Buffers chunks for processing
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, sample_rate=16000, chunk_duration_ms=100):
 self.sample_rate = sample_rate
 self.chunk_duration_ms = chunk_duration_ms
 self.chunk_size = int(sample_rate * chunk_duration_ms / 1000)</p>

<p>self.audio_queue = Queue()
 self.stream = None
 self.running = False</p>

<p>def _audio_callback(self, in_data, frame_count, time_info, status):
 â€œâ€â€
 Callback called by PyAudio for each audio chunk</p>

<p>Runs in separate thread
 â€œâ€â€
 if status:
 print(fâ€Audio status: {status}â€)</p>

<p># Convert bytes to numpy array
 audio_data = np.frombuffer(in_data, dtype=np.int16)</p>

<p># Normalize to [-1, 1]
 audio_data = audio_data.astype(np.float32) / 32768.0</p>

<p># Add to queue
 self.audio_queue.put(audio_data)</p>

<p>return (in_data, pyaudio.paContinue)</p>

<p>def start(self):
 â€œ"â€Start capturing audioâ€â€â€
 self.running = True</p>

<p>p = pyaudio.PyAudio()</p>

<p>self.stream = p.open(
 format=pyaudio.paInt16,
 channels=1,
 rate=self.sample_rate,
 input=True,
 frames_per_buffer=self.chunk_size,
 stream_callback=self._audio_callback
 )</p>

<p>self.stream.start_stream()
 print(fâ€Started audio capture (chunk={self.chunk_duration_ms}ms)â€)</p>

<p>def stop(self):
 â€œ"â€Stop capturing audioâ€â€â€
 self.running = False
 if self.stream:
 self.stream.stop_stream()
 self.stream.close()
 print(â€œStopped audio captureâ€)</p>

<p>def get_chunk(self, timeout=1.0):
 â€œâ€â€
 Get next audio chunk</p>

<p>Returns: numpy array of audio samples
 â€œâ€â€
 try:
 return self.audio_queue.get(timeout=timeout)
 except:
 return None</p>

<h1 id="usage">Usage</h1>
<p>streamer = AudioStreamer(sample_rate=16000, chunk_duration_ms=100)
streamer.start()</p>

<h1 id="process-chunks-in-real-time">Process chunks in real-time</h1>
<p>while True:
 chunk = streamer.get_chunk()
 if chunk is not None:
 # Process chunk
 process_audio_chunk(chunk)
``</p>

<h3 id="chunk-buffering-strategy">Chunk Buffering Strategy</h3>

<p>``python
class ChunkBuffer:
 â€œâ€â€
 Buffer audio chunks with overlap</p>

<p>Helps models that need context from previous chunks
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, buffer_size=3, overlap_size=1):
 â€œâ€â€
 Args:
 buffer_size: Number of chunks to keep
 overlap_size: Number of chunks to overlap
 â€œâ€â€
 self.buffer_size = buffer_size
 self.overlap_size = overlap_size
 self.chunks = []</p>

<p>def add_chunk(self, chunk):
 â€œ"â€Add new chunk to bufferâ€â€â€
 self.chunks.append(chunk)</p>

<p># Keep only recent chunks
 if len(self.chunks) &gt; self.buffer_size:
 self.chunks.pop(0)</p>

<p>def get_buffered_audio(self):
 â€œâ€â€
 Get concatenated audio with overlap</p>

<p>Returns: numpy array
 â€œâ€â€
 if not self.chunks:
 return None</p>

<p>return np.concatenate(self.chunks)</p>

<p>def get_latest_with_context(self):
 â€œâ€â€
 Get latest chunk with context from previous chunks</p>

<p>Useful for models that need history
 â€œâ€â€
 if len(self.chunks) &lt; 2:
 return self.chunks[-1] if self.chunks else None</p>

<p># Return last â€˜overlap_size + 1â€™ chunks
 context_chunks = self.chunks[-(self.overlap_size + 1):]
 return np.concatenate(context_chunks)</p>

<h1 id="usage-1">Usage</h1>
<p>buffer = ChunkBuffer(buffer_size=3, overlap_size=1)</p>

<p>for chunk in audio_chunks:
 buffer.add_chunk(chunk)
 audio_with_context = buffer.get_latest_with_context()
 # Process audio with context
``</p>

<hr />

<h2 id="websocket-based-streaming">WebSocket-Based Streaming</h2>

<h3 id="server-side">Server Side</h3>

<p>``python
import asyncio
import websockets
import json
import numpy as np
import time</p>

<p>class StreamingASRServer:
 â€œâ€â€
 WebSocket server for streaming ASR</p>

<p>Clients send audio chunks, server returns transcriptions
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model, port=8765):
 self.model = model
 self.port = port
 self.active_connections = set()</p>

<p>async def handle_client(self, websocket, path):
 â€œ"â€Handle single client connectionâ€â€â€
 client_id = id(websocket)
 self.active_connections.add(websocket)
 print(fâ€Client {client_id} connectedâ€)</p>

<p>try:
 async for message in websocket:
 # Decode message
 data = json.loads(message)</p>

<p>if data[â€˜typeâ€™] == â€˜audioâ€™:
 # Process audio chunk
 audio_bytes = bytes.fromhex(data[â€˜audioâ€™])
 audio_chunk = np.frombuffer(audio_bytes, dtype=np.float32)</p>

<p># Run inference
 transcription = await self.process_chunk(audio_chunk)</p>

<p># Send result
 response = {
 â€˜typeâ€™: â€˜transcriptionâ€™,
 â€˜textâ€™: transcription,
 â€˜is_finalâ€™: data.get(â€˜is_finalâ€™, False)
 }
 await websocket.send(json.dumps(response))</p>

<p>elif data[â€˜typeâ€™] == â€˜endâ€™:
 # Session ended
 break</p>

<p>except websockets.exceptions.ConnectionClosed:
 print(fâ€Client {client_id} disconnectedâ€)</p>

<p>finally:
 self.active_connections.remove(websocket)</p>

<p>async def process_chunk(self, audio_chunk):
 â€œâ€â€
 Process audio chunk</p>

<p>Returns: Transcription text
 â€œâ€â€
 # Extract features
 # Placeholder feature extractor (should match your modelâ€™s expected input)
 features = extract_features(audio_chunk)</p>

<p># Run model inference
 transcription = self.model.predict(features)</p>

<p>return transcription</p>

<p>def start(self):
 â€œ"â€Start WebSocket serverâ€â€â€
 print(fâ€Starting ASR server on port {self.port}â€)</p>

<p>start_server = websockets.serve(
 self.handle_client,
 â€˜localhostâ€™,
 self.port
 )</p>

<p>asyncio.get_event_loop().run_until_complete(start_server)
 asyncio.get_event_loop().run_forever()</p>

<h1 id="usage-2">Usage</h1>
<p>server = StreamingASRServer(model=asr_model, port=8765)
server.start()
``</p>

<h3 id="client-side">Client Side</h3>

<p>``python
import asyncio
import websockets
import json
import numpy as np</p>

<p>class StreamingASRClient:
 â€œâ€â€
 WebSocket client for streaming ASR</p>

<p>Sends audio chunks and receives transcriptions
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, server_url=â€™ws://localhost:8765â€™):
 self.server_url = server_url
 self.websocket = None</p>

<p>async def connect(self):
 â€œ"â€Connect to serverâ€â€â€
 self.websocket = await websockets.connect(self.server_url)
 print(fâ€Connected to {self.server_url}â€)</p>

<p>async def send_audio_chunk(self, audio_chunk, is_final=False):
 â€œâ€â€
 Send audio chunk to server</p>

<p>Args:
 audio_chunk: numpy array
 is_final: Whether this is the last chunk
 â€œâ€â€
 # Convert to bytes
 audio_bytes = audio_chunk.astype(np.float32).tobytes()
 audio_hex = audio_bytes.hex()</p>

<p># Create message
 message = {
 â€˜typeâ€™: â€˜audioâ€™,
 â€˜audioâ€™: audio_hex,
 â€˜is_finalâ€™: is_final
 }</p>

<p># Send
 await self.websocket.send(json.dumps(message))</p>

<p>async def receive_transcription(self):
 â€œâ€â€
 Receive transcription from server</p>

<p>Returns: Dict with transcription
 â€œâ€â€
 response = await self.websocket.recv()
 return json.loads(response)</p>

<p>async def close(self):
 â€œ"â€Close connectionâ€â€â€
 if self.websocket:
 await self.websocket.send(json.dumps({â€˜typeâ€™: â€˜endâ€™}))
 await self.websocket.close()</p>

<h1 id="usage-3">Usage</h1>
<p>async def stream_audio():
 client = StreamingASRClient()
 await client.connect()</p>

<p># Stream audio chunks
 streamer = AudioStreamer()
 streamer.start()</p>

<p>try:
 while True:
 chunk = streamer.get_chunk()
 if chunk is None:
 break</p>

<p># Send chunk
 await client.send_audio_chunk(chunk)</p>

<p># Receive transcription
 result = await client.receive_transcription()
 print(fâ€Transcription: {result[â€˜textâ€™]}â€)</p>

<p>finally:
 streamer.stop()
 await client.close()</p>

<h1 id="run">Run</h1>
<p>asyncio.run(stream_audio())
``</p>

<hr />

<h2 id="latency-optimization">Latency Optimization</h2>

<h3 id="latency-breakdown">Latency Breakdown</h3>

<p>``
Total Latency = Audio Capture + Network + Processing + Network + Display</p>

<p>Typical values:</p>
<ul>
  <li>Audio capture: 10-50ms (chunk duration)</li>
  <li>Network (client â†’ server): 10-30ms</li>
  <li>Feature extraction: 5-10ms</li>
  <li>Model inference: 20-100ms (depends on model)</li>
  <li>Network (server â†’ client): 10-30ms</li>
  <li>Display: 1-5ms</li>
</ul>

<p>Total: 56-225ms (aim for &lt; 100ms)
``</p>

<h3 id="optimization-strategies">Optimization Strategies</h3>

<p>``python
class OptimizedStreamingPipeline:
 â€œâ€â€
 Optimized streaming pipeline</p>

<p>Techniques:</p>
<ul>
  <li>Smaller chunks</li>
  <li>Model quantization</li>
  <li>Batch processing</li>
  <li>Prefetching
 â€œâ€â€</li>
</ul>

<p>def <strong>init</strong>(self, model, chunk_duration_ms=50):
 â€œâ€â€
 Args:
 chunk_duration_ms: Smaller chunks = lower latency
 â€œâ€â€
 self.model = model
 self.chunk_duration_ms = chunk_duration_ms</p>

<p># Prefetch buffer
 self.prefetch_buffer = asyncio.Queue(maxsize=3)</p>

<p># Start prefetching thread
 self.prefetch_task = None</p>

<p>async def start_prefetching(self, audio_source):
 â€œâ€â€
 Prefetch audio chunks</p>

<p>Reduces waiting time
 â€œâ€â€
 async for chunk in audio_source:
 await self.prefetch_buffer.put(chunk)</p>

<p>async def process_stream(self, audio_source):
 â€œâ€â€
 Process audio stream with optimizations
 â€œâ€â€
 # Start prefetching
 self.prefetch_task = asyncio.create_task(
 self.start_prefetching(audio_source)
 )</p>

<p>while True:
 # Get prefetched chunk (zero wait!)
 chunk = await self.prefetch_buffer.get()</p>

<p>if chunk is None:
 break</p>

<p># Process chunk
 result = await self.process_chunk_optimized(chunk)</p>

<p>yield result</p>

<p>async def process_chunk_optimized(self, chunk):
 â€œâ€â€
 Optimized chunk processing</p>

<p>Uses quantized model for faster inference
 â€œâ€â€
 # Extract features (optimized)
 features = self.extract_features_fast(chunk)</p>

<p># Run inference (quantized model)
 result = self.model.predict(features)</p>

<p>return result</p>

<p>def extract_features_fast(self, audio):
 â€œâ€â€
 Fast feature extraction</p>

<p>Uses caching and vectorization
 â€œâ€â€
 # Vectorized operations are faster
 mfcc = librosa.feature.mfcc(
 y=audio,
 sr=16000,
 n_mfcc=13,
 hop_length=160 # Smaller hop = more features
 )</p>

<p>return mfcc
``</p>

<hr />

<h2 id="state-management">State Management</h2>

<h3 id="stateful-streaming">Stateful Streaming</h3>

<p>``python
class StatefulStreamingProcessor:
 â€œâ€â€
 Maintain state across chunks</p>

<p>Important for context-dependent models
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model):
 self.model = model
 self.state = None # Hidden state for RNN/LSTM models
 self.previous_chunks = []
 self.partial_results = []</p>

<p>def process_chunk(self, audio_chunk):
 â€œâ€â€
 Process chunk with state</p>

<p>Returns: (result, is_complete)
 â€œâ€â€
 # Extract features
 features = extract_features(audio_chunk)</p>

<p># Run model with state
 if hasattr(self.model, â€˜predict_statefulâ€™):
 result, self.state = self.model.predict_stateful(
 features,
 previous_state=self.state
 )
 else:
 # Fallback: concatenate with previous chunks
 self.previous_chunks.append(audio_chunk)
 if len(self.previous_chunks) &gt; 5:
 self.previous_chunks.pop(0)</p>

<p>combined_audio = np.concatenate(self.previous_chunks)
 combined_features = extract_features(combined_audio)
 result = self.model.predict(combined_features)</p>

<p># Determine if result is complete
 is_complete = self.check_completeness(result)</p>

<p>if is_complete:
 self.partial_results.append(result)</p>

<p>return result, is_complete</p>

<p>def check_completeness(self, result):
 â€œâ€â€
 Check if result is a complete utterance</p>

<p>Uses heuristics:</p>
<ul>
  <li>Pause detection</li>
  <li>Confidence threshold</li>
  <li>Length limits
 â€œâ€â€
 # Simple heuristic: check for pause
 # (In practice, use more sophisticated methods)
 if hasattr(result, â€˜confidenceâ€™) and result.confidence &gt; 0.9:
 return True</li>
</ul>

<p>return False</p>

<p>def reset_state(self):
 â€œ"â€Reset state (e.g., after complete utterance)â€â€â€
 self.state = None
 self.previous_chunks = []
 self.partial_results = []
``</p>

<hr />

<h2 id="error-handling--recovery">Error Handling &amp; Recovery</h2>

<h3 id="robust-streaming">Robust Streaming</h3>

<p>``python
class RobustStreamingPipeline:
 â€œâ€â€
 Streaming pipeline with error handling</p>

<p>Handles:</p>
<ul>
  <li>Network failures</li>
  <li>Audio glitches</li>
  <li>Model errors
 â€œâ€â€</li>
</ul>

<p>def <strong>init</strong>(self, model):
 self.model = model
 self.error_count = 0
 self.max_errors = 10</p>

<p>async def process_stream_robust(self, audio_source):
 â€œâ€â€
 Process stream with error recovery
 â€œâ€â€
 retry_count = 0
 max_retries = 3</p>

<p>async for chunk in audio_source:
 try:
 # Process chunk
 result = await self.process_chunk_safe(chunk)</p>

<p># Reset retry count on success
 retry_count = 0</p>

<p>yield result</p>

<p>except AudioGlitchError as e:
 # Audio glitch: skip chunk
 print(fâ€Audio glitch detected: {e}â€)
 self.error_count += 1
 continue</p>

<p>except ModelInferenceError as e:
 # Model error: retry with fallback
 print(fâ€Model inference failed: {e}â€)</p>

<p>if retry_count &lt; max_retries:
 retry_count += 1
 # Use simpler fallback model
 result = await self.fallback_inference(chunk)
 yield result
 else:
 # Give up after max retries
 print(â€œMax retries exceeded, skipping chunkâ€)
 retry_count = 0</p>

<p>except Exception as e:
 # Unexpected error
 print(fâ€Unexpected error: {e}â€)
 self.error_count += 1</p>

<p>if self.error_count &gt; self.max_errors:
 raise RuntimeError(â€œToo many errors, stopping streamâ€)</p>

<p>async def process_chunk_safe(self, chunk):
 â€œâ€â€
 Process chunk with validation
 â€œâ€â€
 # Validate chunk
 if not self.validate_chunk(chunk):
 raise AudioGlitchError(â€œInvalid audio chunkâ€)</p>

<p># Process
 try:
 features = extract_features(chunk)
 result = self.model.predict(features)
 return result
 except Exception as e:
 raise ModelInferenceError(fâ€Inference failed: {e}â€)</p>

<p>def validate_chunk(self, chunk):
 â€œâ€â€
 Validate audio chunk</p>

<p>Checks for:</p>
<ul>
  <li>Correct length</li>
  <li>Valid range</li>
  <li>No NaN values
 â€œâ€â€
 if chunk is None or len(chunk) == 0:
 return False</li>
</ul>

<p>if np.any(np.isnan(chunk)):
 return False</p>

<p>if np.max(np.abs(chunk)) &gt; 10: # Suspiciously large
 return False</p>

<p>return True</p>

<p>async def fallback_inference(self, chunk):
 â€œâ€â€
 Fallback inference with simpler model</p>

<p>Trades accuracy for reliability
 â€œâ€â€
 # Use cached results or simple heuristics
 return {â€œtextâ€: â€œ[processingâ€¦]â€, â€œconfidenceâ€: 0.5}</p>

<p>class AudioGlitchError(Exception):
 pass</p>

<p>class ModelInferenceError(Exception):
 pass
``</p>

<hr />

<h2 id="connection-to-model-serving-ml">Connection to Model Serving (ML)</h2>

<p>Streaming speech pipelines use model serving patterns:</p>

<p>``python
class StreamingSpeechServer:
 â€œâ€â€
 Streaming speech server with model serving best practices</p>

<p>Combines:</p>
<ul>
  <li>Model serving (ML)</li>
  <li>Streaming audio (Speech)</li>
  <li>Validation (DSA)
 â€œâ€â€</li>
</ul>

<p>def <strong>init</strong>(self, model_path):
 # Load model (model serving pattern)
 self.model = self.load_model(model_path)</p>

<p># Validation (BST-like range checking)
 self.validator = AudioValidator()</p>

<p># Monitoring
 self.metrics = StreamingMetrics()</p>

<p>def load_model(self, model_path):
 â€œ"â€Load model with caching (from model serving)â€â€â€
 import joblib
 return joblib.load(model_path)</p>

<p>async def process_audio_stream(self, audio_chunks):
 â€œâ€â€
 Process streaming audio</p>

<p>Uses patterns from all topics
 â€œâ€â€
 for chunk in audio_chunks:
 # Validate input (BST validation pattern)
 is_valid, violations = self.validator.validate(chunk)</p>

<p>if not is_valid:
 print(fâ€Invalid chunk: {violations}â€)
 continue</p>

<p># Process chunk (model serving)
 start_time = time.time()
 result = self.model.predict(chunk)
 latency = time.time() - start_time</p>

<p># Monitor (model serving)
 self.metrics.record_prediction(latency)</p>

<p>yield result</p>

<p>class AudioValidator:
 â€œ"â€Validate audio chunks (similar to BST validation)â€â€â€</p>

<p>def <strong>init</strong>(self):
 # Define valid ranges (like BST min/max)
 self.amplitude_range = (-1.0, 1.0)
 self.length_range = (100, 10000) # samples</p>

<p>def validate(self, chunk):
 â€œâ€â€
 Validate chunk falls within ranges</p>

<p>Like BST validation with [min, max] bounds
 â€œâ€â€
 violations = []</p>

<p># Check amplitude range
 if np.min(chunk) &lt; self.amplitude_range[0]:
 violations.append(â€œAmplitude too lowâ€)
 if np.max(chunk) &gt; self.amplitude_range[1]:
 violations.append(â€œAmplitude too highâ€)</p>

<p># Check length range
 if len(chunk) &lt; self.length_range[0]:
 violations.append(â€œChunk too shortâ€)
 if len(chunk) &gt; self.length_range[1]:
 violations.append(â€œChunk too longâ€)</p>

<p>return len(violations) == 0, violations
``</p>

<hr />

<h2 id="production-patterns">Production Patterns</h2>

<h3 id="1-multi-channel-audio-streaming">1. Multi-Channel Audio Streaming</h3>

<p>``python
class MultiChannelStreamingProcessor:
 â€œâ€â€
 Process multiple audio streams simultaneously</p>

<p>Use case: Conference calls, multi-mic arrays
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, num_channels=4):
 self.num_channels = num_channels
 self.channel_buffers = [ChunkBuffer() for _ in range(num_channels)]
 self.processors = [StreamingProcessor() for _ in range(num_channels)]</p>

<p>async def process_multi_channel(self, channel_chunks: dict):
 â€œâ€â€
 Process multiple channels in parallel</p>

<p>Args:
 channel_chunks: Dict {channel_id: audio_chunk}</p>

<p>Returns: Dict {channel_id: result}
 â€œâ€â€
 import asyncio</p>

<p># Process channels in parallel
 tasks = []
 for channel_id, chunk in channel_chunks.items():
 task = self.processors[channel_id].process_chunk_async(chunk)
 tasks.append((channel_id, task))</p>

<p># Wait for all results
 results = {}
 for channel_id, task in tasks:
 result = await task
 results[channel_id] = result</p>

<p>return results</p>

<p>def merge_results(self, channel_results: dict):
 â€œâ€â€
 Merge results from multiple channels</p>

<p>E.g., speaker diarization, beam forming
 â€œâ€â€
 # Simple merging: concatenate transcriptions
 merged_text = []</p>

<p>for channel_id in sorted(channel_results.keys()):
 result = channel_results[channel_id]
 if result:
 merged_text.append(fâ€[Channel {channel_id}]: {result[â€˜textâ€™]}â€)</p>

<p>return â€˜\nâ€™.join(merged_text)</p>

<h1 id="usage-4">Usage</h1>
<p>multi_processor = MultiChannelStreamingProcessor(num_channels=4)</p>

<h1 id="stream-audio-from-4-microphones">Stream audio from 4 microphones</h1>
<p>async def process_meeting():
 while True:
 # Get chunks from all channels
 chunks = {
 0: mic1.get_chunk(),
 1: mic2.get_chunk(),
 2: mic3.get_chunk(),
 3: mic4.get_chunk()
 }</p>

<p># Process in parallel
 results = await multi_processor.process_multi_channel(chunks)</p>

<p># Merge and display
 merged = multi_processor.merge_results(results)
 print(merged)
``</p>

<h3 id="2-adaptive-chunk-size">2. Adaptive Chunk Size</h3>

<p>``python
class AdaptiveChunkingProcessor:
 â€œâ€â€
 Dynamically adjust chunk size based on network/compute conditions</p>

<p>Smaller chunks: Lower latency but higher overhead
 Larger chunks: Higher latency but more efficient
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, min_chunk_ms=50, max_chunk_ms=200):
 self.min_chunk_ms = min_chunk_ms
 self.max_chunk_ms = max_chunk_ms
 self.current_chunk_ms = 100 # Start with middle value
 self.latency_history = []</p>

<p>def adjust_chunk_size(self, recent_latency_ms):
 â€œâ€â€
 Adjust chunk size based on latency</p>

<p>High latency â†’ smaller chunks (more responsive)
 Low latency â†’ larger chunks (more efficient)
 â€œâ€â€
 self.latency_history.append(recent_latency_ms)</p>

<p>if len(self.latency_history) &lt; 10:
 return self.current_chunk_ms</p>

<p># Calculate average latency
 avg_latency = np.mean(self.latency_history[-10:])</p>

<p># Adjust chunk size
 if avg_latency &gt; 150: # High latency
 # Reduce chunk size for better responsiveness
 self.current_chunk_ms = max(
 self.min_chunk_ms,
 self.current_chunk_ms - 10
 )
 print(fâ€â†“ Reducing chunk size to {self.current_chunk_ms}msâ€)</p>

<p>elif avg_latency &lt; 50: # Very low latency
 # Increase chunk size for efficiency
 self.current_chunk_ms = min(
 self.max_chunk_ms,
 self.current_chunk_ms + 10
 )
 print(fâ€â†‘ Increasing chunk size to {self.current_chunk_ms}msâ€)</p>

<p>return self.current_chunk_ms</p>

<p>async def process_with_adaptive_chunking(self, audio_stream):
 â€œ"â€Process stream with adaptive chunk sizingâ€â€â€
 for chunk in audio_stream:
 start_time = time.time()</p>

<p># Process chunk
 result = await self.process_chunk(chunk)</p>

<p># Calculate latency
 latency_ms = (time.time() - start_time) * 1000</p>

<p># Adjust chunk size for next iteration
 next_chunk_ms = self.adjust_chunk_size(latency_ms)</p>

<p>yield result, next_chunk_ms
``</p>

<h3 id="3-buffering-strategy-for-unreliable-networks">3. Buffering Strategy for Unreliable Networks</h3>

<p>``python
class NetworkAwareStreamingBuffer:
 â€œâ€â€
 Buffer audio to handle network issues</p>

<p>Maintains smooth playback despite packet loss
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, buffer_size_seconds=2.0, sample_rate=16000):
 self.buffer_size = int(buffer_size_seconds * sample_rate)
 self.buffer = np.zeros(self.buffer_size, dtype=np.float32)
 self.write_pos = 0
 self.read_pos = 0
 self.underrun_count = 0
 self.overrun_count = 0</p>

<p>def write_chunk(self, chunk):
 â€œâ€â€
 Write audio chunk to buffer</p>

<p>Returns: Success status
 â€œâ€â€
 chunk_size = len(chunk)</p>

<p># Check for buffer overrun
 available_space = self.buffer_size - (self.write_pos - self.read_pos)
 if chunk_size &gt; available_space:
 self.overrun_count += 1
 print(â€œâš ï¸ Buffer overrun - dropping oldest dataâ€)
 # Drop oldest data
 self.read_pos = self.write_pos - self.buffer_size + chunk_size</p>

<p># Write to circular buffer
 for i, sample in enumerate(chunk):
 pos = (self.write_pos + i) % self.buffer_size
 self.buffer[pos] = sample</p>

<p>self.write_pos += chunk_size
 return True</p>

<p>def read_chunk(self, chunk_size):
 â€œâ€â€
 Read audio chunk from buffer</p>

<p>Returns: Audio chunk or None if underrun
 â€œâ€â€
 # Check for buffer underrun
 available_data = self.write_pos - self.read_pos
 if available_data &lt; chunk_size:
 self.underrun_count += 1
 print(â€œâš ï¸ Buffer underrun - not enough dataâ€)
 return None</p>

<p># Read from circular buffer
 chunk = np.zeros(chunk_size, dtype=np.float32)
 for i in range(chunk_size):
 pos = (self.read_pos + i) % self.buffer_size
 chunk[i] = self.buffer[pos]</p>

<p>self.read_pos += chunk_size
 return chunk</p>

<p>def get_buffer_level(self):
 â€œ"â€Get current buffer fill level (0-1)â€â€â€
 available = self.write_pos - self.read_pos
 return available / self.buffer_size</p>

<p>def get_stats(self):
 â€œ"â€Get buffer statisticsâ€â€â€
 return {
 â€˜buffer_levelâ€™: self.get_buffer_level(),
 â€˜underrunsâ€™: self.underrun_count,
 â€˜overrunsâ€™: self.overrun_count
 }</p>

<h1 id="usage-5">Usage</h1>
<p>buffer = NetworkAwareStreamingBuffer(buffer_size_seconds=2.0)</p>

<h1 id="writer-thread-receiving-from-network">Writer thread (receiving from network)</h1>
<p>async def receive_audio():
 async for chunk in network_stream:
 buffer.write_chunk(chunk)</p>

<p># Adaptive buffering
 level = buffer.get_buffer_level()
 if level &lt; 0.2:
 print(â€œâš ï¸ Low buffer, may need to increaseâ€)</p>

<h1 id="reader-thread-processing">Reader thread (processing)</h1>
<p>async def process_audio():
 while True:
 chunk = buffer.read_chunk(chunk_size=1600) # 100ms at 16kHz
 if chunk is not None:
 result = await process_chunk(chunk)
 yield result
 else:
 await asyncio.sleep(0.01) # Wait for more data
``</p>

<hr />

<h2 id="advanced-optimization-techniques">Advanced Optimization Techniques</h2>

<h3 id="1-model-warm-up">1. Model Warm-Up</h3>

<p>``python
class WarmUpStreamingProcessor:
 â€œâ€â€
 Pre-warm model for lower latency on first request</p>

<p>Cold start can add 100-500ms latency
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model):
 self.model = model
 self.is_warm = False</p>

<p>def warm_up(self, sample_rate=16000):
 â€œâ€â€
 Warm up model with dummy input</p>

<p>Call during initialization
 â€œâ€â€
 print(â€œWarming up modelâ€¦â€)</p>

<p># Create dummy audio chunk
 dummy_chunk = np.random.randn(int(sample_rate * 0.1)) # 100ms</p>

<p># Run inference to warm up
 for _ in range(3):
 _ = self.model.predict(dummy_chunk)</p>

<p>self.is_warm = True
 print(â€œModel warm-up completeâ€)</p>

<p>def process_chunk(self, chunk):
 â€œ"â€Process with warm-up checkâ€â€â€
 if not self.is_warm:
 self.warm_up()</p>

<p>return self.model.predict(chunk)</p>

<h1 id="usage-6">Usage</h1>
<p>processor = WarmUpStreamingProcessor(model)
processor.warm_up() # Do this during server startup
``</p>

<h3 id="2-gpu-batching-for-throughput">2. GPU Batching for Throughput</h3>

<p>``python
class GPUBatchProcessor:
 â€œâ€â€
 Batch multiple streams for GPU efficiency</p>

<p>GPUs are most efficient with batch processing
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model, max_batch_size=16, max_wait_ms=50):
 self.model = model
 self.max_batch_size = max_batch_size
 self.max_wait_ms = max_wait_ms
 self.pending_batches = []</p>

<p>async def process_chunk_batched(self, chunk, stream_id):
 â€œâ€â€
 Add chunk to batch and process when ready</p>

<p>Returns: Future that resolves with result
 â€œâ€â€
 future = asyncio.Future()
 self.pending_batches.append((chunk, stream_id, future))</p>

<p># Process batch if ready
 if len(self.pending_batches) &gt;= self.max_batch_size:
 await self._process_batch()
 else:
 # Wait for more requests or timeout
 asyncio.create_task(self._process_batch_after_delay())</p>

<p>return await future</p>

<p>async def _process_batch(self):
 â€œ"â€Process accumulated batch on GPUâ€â€â€
 if not self.pending_batches:
 return</p>

<p># Extract batch
 chunks = [item[0] for item in self.pending_batches]
 stream_ids = [item[1] for item in self.pending_batches]
 futures = [item[2] for item in self.pending_batches]</p>

<p># Pad to same length
 max_len = max(len(c) for c in chunks)
 padded_chunks = [
 np.pad(c, (0, max_len - len(c)), mode=â€™constantâ€™)
 for c in chunks
 ]</p>

<p># Stack into batch
 batch = np.stack(padded_chunks)</p>

<p># Run batch inference on GPU
 results = self.model.predict_batch(batch)</p>

<p># Distribute results
 for result, future in zip(results, futures):
 future.set_result(result)</p>

<p># Clear batch
 self.pending_batches = []</p>

<p>async def _process_batch_after_delay(self):
 â€œ"â€Process batch after timeoutâ€â€â€
 await asyncio.sleep(self.max_wait_ms / 1000.0)
 await self._process_batch()</p>

<h1 id="usage-7">Usage</h1>
<p>gpu_processor = GPUBatchProcessor(model, max_batch_size=16)</p>

<h1 id="multiple-concurrent-streams">Multiple concurrent streams</h1>
<p>async def process_stream(stream_id):
 async for chunk in audio_streams[stream_id]:
 result = await gpu_processor.process_chunk_batched(chunk, stream_id)
 yield result</p>

<h1 id="run-multiple-streams-in-parallel">Run multiple streams in parallel</h1>
<p>await asyncio.gather(*[
 process_stream(i) for i in range(10)
])
``</p>

<h3 id="3-quantized-models-for-edge-devices">3. Quantized Models for Edge Devices</h3>

<p>``python
import torch
import torchaudio</p>

<p>class EdgeOptimizedStreamingASR:
 â€œâ€â€
 Streaming ASR optimized for edge devices</p>

<p>Uses INT8 quantization for faster inference
 â€œâ€â€</p>

<p>def <strong>init</strong>(self, model_path):
 # Load and quantize model
 self.model = torch.jit.load(model_path)
 self.model = torch.quantization.quantize_dynamic(
 self.model,
 {torch.nn.Linear, torch.nn.LSTM},
 dtype=torch.qint8
 )
 self.model.eval()</p>

<p>def process_chunk_optimized(self, audio_chunk):
 â€œâ€â€
 Process chunk with optimizations</p>

<ul>
  <li>INT8 quantization: 4x faster</li>
  <li>No gradient computation</li>
  <li>Minimal memory allocation
 â€œâ€â€
 with torch.no_grad():
 # Convert to tensor
 audio_tensor = torch.from_numpy(audio_chunk).float()
 audio_tensor = audio_tensor.unsqueeze(0) # Add batch dim</li>
</ul>

<p># Extract features (optimized)
 features = torchaudio.compliance.kaldi.mfcc(
 audio_tensor,
 sample_frequency=16000,
 num_ceps=13
 )</p>

<p># Run inference
 output = self.model(features)</p>

<p># Decode
 transcription = self.decode(output)</p>

<p>return transcription</p>

<p>def decode(self, output):
 â€œ"â€Simple greedy decodingâ€â€â€
 # Get most likely tokens
 tokens = torch.argmax(output, dim=-1)</p>

<p># Convert to text (simplified)
 transcription = self.tokens_to_text(tokens)</p>

<p>return transcription</p>

<h1 id="benchmark-quantized-vs-full-precision">Benchmark: Quantized vs Full Precision</h1>
<p>def benchmark_models():
 â€œ"â€Compare quantized vs full precisionâ€â€â€
 full_model = load_model(â€˜model_fp32.ptâ€™)
 quant_model = EdgeOptimizedStreamingASR(â€˜model_int8.ptâ€™)</p>

<p>audio_chunk = np.random.randn(1600) # 100ms at 16kHz</p>

<p># Full precision
 start = time.time()
 for _ in range(100):
 _ = full_model.predict(audio_chunk)
 fp32_time = time.time() - start</p>

<p># Quantized
 start = time.time()
 for _ in range(100):
 _ = quant_model.process_chunk_optimized(audio_chunk)
 int8_time = time.time() - start</p>

<p>print(fâ€FP32: {fp32_time:.2f}sâ€)
 print(fâ€INT8: {int8_time:.2f}sâ€)
 print(fâ€Speedup: {fp32_time / int8_time:.1f}xâ€)
``</p>

<hr />

<h2 id="real-world-integration-examples">Real-World Integration Examples</h2>

<h3 id="1-zoom-like-meeting-transcription">1. Zoom-like Meeting Transcription</h3>

<p>``python
class MeetingTranscriptionService:
 â€œâ€â€
 Real-time meeting transcription</p>

<p>Similar to Zoomâ€™s live transcription
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.asr_model = load_asr_model()
 self.active_sessions = {}</p>

<p>def start_session(self, meeting_id):
 â€œ"â€Start transcription sessionâ€â€â€
 self.active_sessions[meeting_id] = {
 â€˜participantsâ€™: {},
 â€˜transcriptâ€™: [],
 â€˜start_timeâ€™: time.time()
 }</p>

<p>async def process_participant_audio(self, meeting_id, participant_id, audio_stream):
 â€œâ€â€
 Process audio from single participant</p>

<p>Returns: Real-time transcription
 â€œâ€â€
 session = self.active_sessions[meeting_id]</p>

<p># Initialize participant
 if participant_id not in session[â€˜participantsâ€™]:
 session[â€˜participantsâ€™][participant_id] = {
 â€˜processorâ€™: StatefulStreamingProcessor(self.asr_model),
 â€˜transcript_bufferâ€™: []
 }</p>

<p>participant = session[â€˜participantsâ€™][participant_id]
 processor = participant[â€˜processorâ€™]</p>

<p>async for chunk in audio_stream:
 # Process chunk
 result, is_complete = processor.process_chunk(chunk)</p>

<p>if is_complete:
 # Add to transcript
 timestamp = time.time() - session[â€˜start_timeâ€™]
 transcript_entry = {
 â€˜participant_idâ€™: participant_id,
 â€˜textâ€™: result[â€˜textâ€™],
 â€˜timestampâ€™: timestamp,
 â€˜confidenceâ€™: result.get(â€˜confidenceâ€™, 1.0)
 }</p>

<p>session[â€˜transcriptâ€™].append(transcript_entry)</p>

<p>yield transcript_entry</p>

<p>def get_full_transcript(self, meeting_id):
 â€œ"â€Get complete meeting transcriptâ€â€â€
 if meeting_id not in self.active_sessions:
 return []</p>

<p>transcript = self.active_sessions[meeting_id][â€˜transcriptâ€™]</p>

<p># Format as readable text
 formatted = []
 for entry in transcript:
 time_str = format_timestamp(entry[â€˜timestampâ€™])
 formatted.append(
 fâ€[{time_str}] Participant {entry[â€˜participant_idâ€™]}: {entry[â€˜textâ€™]}â€
 )</p>

<p>return â€˜\nâ€™.join(formatted)</p>

<p>def format_timestamp(seconds):
 â€œ"â€Format seconds as MM:SSâ€â€â€
 minutes = int(seconds // 60)
 secs = int(seconds % 60)
 return fâ€{minutes:02d}:{secs:02d}â€</p>

<h1 id="usage-8">Usage</h1>
<p>service = MeetingTranscriptionService()
service.start_session(â€˜meeting-123â€™)</p>

<h1 id="process-audio-from-multiple-participants">Process audio from multiple participants</h1>
<p>async def transcribe_meeting():
 participants = [â€˜user1â€™, â€˜user2â€™, â€˜user3â€™]</p>

<p># Process all participants in parallel
 tasks = [
 service.process_participant_audio(
 â€˜meeting-123â€™,
 participant_id,
 get_audio_stream(participant_id)
 )
 for participant_id in participants
 ]</p>

<p># Collect transcriptions
 # Collect tasks concurrently
 results = await asyncio.gather(*tasks, return_exceptions=True)
 for res in results:
 if isinstance(res, Exception):
 print(fâ€Stream error: {res}â€)
 else:
 for entry in res:
 print(fâ€[{entry[â€˜timestampâ€™]:.1f}s] {entry[â€˜participant_idâ€™]}: {entry[â€˜textâ€™]}â€)
``</p>

<h3 id="2-voice-assistant-backend">2. Voice Assistant Backend</h3>

<p>``python
class VoiceAssistantPipeline:
 â€œâ€â€
 Complete voice assistant pipeline</p>

<p>ASR â†’ NLU â†’ Action â†’ TTS
 â€œâ€â€</p>

<p>def <strong>init</strong>(self):
 self.asr = StreamingASR()
 self.nlu = IntentClassifier()
 self.action_executor = ActionExecutor()
 self.tts = TextToSpeech()</p>

<p>async def process_voice_command(self, audio_stream):
 â€œâ€â€
 Process voice command end-to-end</p>

<p>Returns: Audio response
 â€œâ€â€
 # 1. Speech Recognition
 transcription = await self.asr.transcribe_stream(audio_stream)
 print(fâ€User said: {transcription}â€)</p>

<p># 2. Natural Language Understanding
 intent = self.nlu.classify(transcription)
 print(fâ€Intent: {intent[â€˜nameâ€™]} (confidence: {intent[â€˜confidenceâ€™]:.2f})â€)</p>

<p># 3. Execute Action
 if intent[â€˜confidenceâ€™] &gt; 0.7:
 response_text = await self.action_executor.execute(intent)
 else:
 response_text = â€œIâ€™m not sure what you mean. Could you rephrase that?â€</p>

<p># 4. Text-to-Speech
 response_audio = self.tts.synthesize(response_text)</p>

<p>return {
 â€˜transcriptionâ€™: transcription,
 â€˜intentâ€™: intent,
 â€˜response_textâ€™: response_text,
 â€˜response_audioâ€™: response_audio
 }</p>

<p>async def continuous_listening(self, audio_source):
 â€œâ€â€
 Continuously listen for wake word + command</p>

<p>Efficient always-on listening
 â€œâ€â€
 wake_word_detector = WakeWordDetector(â€˜hey assistantâ€™)</p>

<p>async for chunk in audio_source:
 # Check for wake word (lightweight model)
 if wake_word_detector.detect(chunk):
 print(â€œğŸ¤ Wake word detected!â€)</p>

<p># Start full ASR
 command_audio = await self.capture_command(audio_source, timeout=5.0)</p>

<p># Process command
 result = await self.process_voice_command(command_audio)</p>

<p># Play response
 play_audio(result[â€˜response_audioâ€™])</p>

<p>async def capture_command(self, audio_source, timeout=5.0):
 â€œ"â€Capture audio command after wake wordâ€â€â€
 command_chunks = []
 start_time = time.time()</p>

<p>async for chunk in audio_source:
 command_chunks.append(chunk)</p>

<p># Check timeout
 if time.time() - start_time &gt; timeout:
 break</p>

<p># Check for end of speech (silence)
 if self.is_silence(chunk):
 break</p>

<p>return np.concatenate(command_chunks)</p>

<p>def is_silence(self, chunk, threshold=0.01):
 â€œ"â€Detect if chunk is silenceâ€â€â€
 energy = np.sqrt(np.mean(chunk ** 2))
 return energy &lt; threshold</p>

<h1 id="usage-9">Usage</h1>
<p>assistant = VoiceAssistantPipeline()</p>

<h1 id="continuous-listening">Continuous listening</h1>
<p>await assistant.continuous_listening(microphone_stream)
``</p>

<hr />

<h2 id="performance-metrics--slas">Performance Metrics &amp; SLAs</h2>

<h3 id="latency-tracking">Latency Tracking</h3>

<p>``python
class StreamingLatencyTracker:
 â€œâ€â€
 Track end-to-end latency for streaming pipeline</p>

<p>Measures:</p>
<ul>
  <li>Audio capture latency</li>
  <li>Network latency</li>
  <li>Processing latency</li>
  <li>Total latency
 â€œâ€â€</li>
</ul>

<p>def <strong>init</strong>(self):
 self.metrics = {
 â€˜capture_latencyâ€™: [],
 â€˜network_latencyâ€™: [],
 â€˜processing_latencyâ€™: [],
 â€˜total_latencyâ€™: []
 }</p>

<p>async def process_with_tracking(self, audio_chunk, capture_timestamp):
 â€œâ€â€
 Process chunk with latency tracking</p>

<p>Args:
 audio_chunk: Audio data
 capture_timestamp: When audio was captured</p>

<p>Returns: (result, latency_breakdown)
 â€œâ€â€
 # Network latency (time from capture to arrival)
 network_start = time.time()
 network_latency = (network_start - capture_timestamp) * 1000
 self.metrics[â€˜network_latencyâ€™].append(network_latency)</p>

<p># Processing latency
 processing_start = time.time()
 result = await self.process_chunk(audio_chunk)
 processing_end = time.time()
 processing_latency = (processing_end - processing_start) * 1000
 self.metrics[â€˜processing_latencyâ€™].append(processing_latency)</p>

<p># Total latency
 total_latency = (processing_end - capture_timestamp) * 1000
 self.metrics[â€˜total_latencyâ€™].append(total_latency)</p>

<p>latency_breakdown = {
 â€˜network_msâ€™: network_latency,
 â€˜processing_msâ€™: processing_latency,
 â€˜total_msâ€™: total_latency
 }</p>

<p>return result, latency_breakdown</p>

<p>def get_latency_stats(self):
 â€œ"â€Get latency statisticsâ€â€â€
 stats = {}</p>

<p>for metric_name, values in self.metrics.items():
 if values:
 stats[metric_name] = {
 â€˜p50â€™: np.percentile(values, 50),
 â€˜p95â€™: np.percentile(values, 95),
 â€˜p99â€™: np.percentile(values, 99),
 â€˜meanâ€™: np.mean(values),
 â€˜maxâ€™: np.max(values)
 }</p>

<p>return stats</p>

<p>def check_sla(self, sla_ms=100):
 â€œâ€â€
 Check if meeting SLA</p>

<p>Returns: (is_meeting_sla, violation_rate)
 â€œâ€â€
 if not self.metrics[â€˜total_latencyâ€™]:
 return True, 0.0</p>

<p>violations = sum(1 for lat in self.metrics[â€˜total_latencyâ€™] if lat &gt; sla_ms)
 violation_rate = violations / len(self.metrics[â€˜total_latencyâ€™])</p>

<p>is_meeting_sla = violation_rate &lt; 0.01 # &lt; 1% violations</p>

<p>return is_meeting_sla, violation_rate</p>

<h1 id="usage-10">Usage</h1>
<p>tracker = StreamingLatencyTracker()</p>

<h1 id="process-with-tracking">Process with tracking</h1>
<p>result, latency = await tracker.process_with_tracking(chunk, capture_time)</p>

<h1 id="check-sla">Check SLA</h1>
<p>is_ok, violation_rate = tracker.check_sla(sla_ms=100)
if not is_ok:
 print(fâ€âš ï¸ SLA violation rate: {violation_rate:.1%}â€)</p>

<h1 id="get-detailed-stats">Get detailed stats</h1>
<p>stats = tracker.get_latency_stats()
print(fâ€P95 latency: {stats[â€˜total_latencyâ€™][â€˜p95â€™]:.1f}msâ€)
``</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<p>âœ… <strong>Chunk audio correctly</strong> - Balance latency vs context 
âœ… <strong>Manage state</strong> - RNN/LSTM models need previous chunks 
âœ… <strong>Optimize latency</strong> - Smaller chunks, quantization, prefetching 
âœ… <strong>Handle errors gracefully</strong> - Network failures, audio glitches 
âœ… <strong>Validate inputs</strong> - Like BST range checking 
âœ… <strong>Monitor performance</strong> - Latency, error rate, throughput 
âœ… <strong>WebSocket for streaming</strong> - Bidirectional, low-latency</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/speech-tech/0008-streaming-speech-pipeline/">arunbaby.com/speech-tech/0008-streaming-speech-pipeline</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#latency" class="page__taxonomy-item p-category" rel="tag">latency</a><span class="sep">, </span>
    
      <a href="/tags/#pipeline" class="page__taxonomy-item p-category" rel="tag">pipeline</a><span class="sep">, </span>
    
      <a href="/tags/#real-time" class="page__taxonomy-item p-category" rel="tag">real-time</a><span class="sep">, </span>
    
      <a href="/tags/#streaming" class="page__taxonomy-item p-category" rel="tag">streaming</a><span class="sep">, </span>
    
      <a href="/tags/#websockets" class="page__taxonomy-item p-category" rel="tag">websockets</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#speech-tech" class="page__taxonomy-item p-category" rel="tag">speech-tech</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0008-validate-binary-search-tree/" rel="permalink">Validate Binary Search Tree
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master BST validation to understand data integrity in tree structures, critical for indexing and search systems.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0008-model-serving-architecture/" rel="permalink">Model Serving Architecture
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          22 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Design production-grade model serving systems that deliver predictions at scale with low latency and high reliability.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ai-agents/0008-agent-workflow-patterns/" rel="permalink">Agent Workflow Patterns
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">â€œBetter workflows beat better models.â€ â€” Dr. Andrew Ng
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Streaming+Speech+Processing+Pipeline%20https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0008-streaming-speech-pipeline%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fspeech-tech%2F0008-streaming-speech-pipeline%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/speech-tech/0008-streaming-speech-pipeline/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/speech-tech/0007-audio-preprocessing/" class="pagination--pager" title="Audio Preprocessing &amp; Signal Processing">Previous</a>
    
    
      <a href="/speech-tech/0009-keyword-spotting/" class="pagination--pager" title="Real-time Keyword Spotting">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
