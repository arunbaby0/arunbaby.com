<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Memory Architectures - Arun Baby</title>
<meta name="description" content="related_dsa_day: 5 related_ml_day: 5 related_speech_day: 5">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Memory Architectures">
<meta property="og:url" content="https://www.arunbaby.com/ai-agents/0005-memory-architectures/">


  <meta property="og:description" content="related_dsa_day: 5 related_ml_day: 5 related_speech_day: 5">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Memory Architectures">
  <meta name="twitter:description" content="related_dsa_day: 5 related_ml_day: 5 related_speech_day: 5">
  <meta name="twitter:url" content="https://www.arunbaby.com/ai-agents/0005-memory-architectures/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-21T11:00:41+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ai-agents/0005-memory-architectures/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Memory Architectures">
    <meta itemprop="description" content="related_dsa_day: 5related_ml_day: 5related_speech_day: 5">
    <meta itemprop="datePublished" content="2025-12-21T11:00:41+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ai-agents/0005-memory-architectures/" itemprop="url">Memory Architectures
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction-the-amnesia-problem">1. Introduction: The Amnesia Problem</a></li><li><a href="#2-the-cognitive-memory-hierarchy">2. The Cognitive Memory Hierarchy</a><ul><li><a href="#21-sensory-memory-the-input">2.1 Sensory Memory (The Input)</a></li><li><a href="#22-short-term--working-memory-context">2.2 Short-Term / Working Memory (Context)</a></li><li><a href="#23-long-term-memory-episodic">2.3 Long-Term Memory (Episodic)</a></li><li><a href="#24-semantic-memory-world-knowledge">2.4 Semantic Memory (World Knowledge)</a></li></ul></li><li><a href="#3-the-generative-agents-architecture-the-gold-standard">3. The “Generative Agents” Architecture (The Gold Standard)</a><ul><li><a href="#31-the-memory-stream">3.1 The Memory Stream</a></li><li><a href="#32-the-retrieval-function">3.2 The Retrieval Function</a></li><li><a href="#33-the-reflection-tree-synthesizing-wisdom">3.3 The Reflection Tree (Synthesizing Wisdom)</a></li></ul></li><li><a href="#4-memgpt-llm-as-an-operating-system">4. MemGPT: LLM as an Operating System</a></li><li><a href="#5-procedural-memory-the-skill-library-voyager">5. Procedural Memory: The Skill Library (Voyager)</a></li><li><a href="#6-code-implementing-generative-retrieval-conceptual">6. Code: Implementing Generative Retrieval (Conceptual)</a></li><li><a href="#7-summary">7. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p>related_dsa_day: 5
related_ml_day: 5
related_speech_day: 5</p>

<p><strong>“The difference between a Chatbot and a Partner is Memory.”</strong></p>

<h2 id="1-introduction-the-amnesia-problem">1. Introduction: The Amnesia Problem</h2>

<p>LLMs are fundamentally <strong>stateless</strong>. If you send a request to standard GPT-4, it processes it and immediately “forgets” it. The only state that exists is the <strong>Context Window</strong> you physically pass in with every call.</p>

<p>For a chatbot session, this is manageable. You pass the chat history <code class="language-plaintext highlighter-rouge">[(User, "Hi"), (Bot, "Hi")]</code> in every turn.
But for an <strong>Autonomous Agent</strong> running for days, handling thousands of steps, this breaks down.</p>
<ol>
  <li><strong>Cost:</strong> Passing 128k tokens per step is prohibitively expensive.</li>
  <li><strong>Capacity:</strong> Even 1M tokens isn’t enough for a whole lifetime of experiences.</li>
  <li><strong>Focus:</strong> “Context Stuffing” reduces intelligence. If you feed the model too much noise (irrelevant history), it hallucinates or gets distracted.</li>
</ol>

<p>To build true agency, we need to engineer a <strong>Memory Hierarchy</strong> similar to the human brain, managing the flow of information from short-term perception to long-term storage.</p>

<hr />

<h2 id="2-the-cognitive-memory-hierarchy">2. The Cognitive Memory Hierarchy</h2>

<p>Just like computers have layers (Registers -&gt; RAM -&gt; Hard Drive), agents utilize different tiers of memory to balance speed, cost, and capacity.</p>

<h3 id="21-sensory-memory-the-input">2.1 Sensory Memory (The Input)</h3>
<p>The raw user prompt and System Instructions.</p>
<ul>
  <li><em>Capacity:</em> Limited by the immediate Context Window logic.</li>
  <li><em>Mechanism:</em> Direct Prompt Injection.</li>
  <li><em>Role:</em> Immediate perception of the “Now.”</li>
</ul>

<h3 id="22-short-term--working-memory-context">2.2 Short-Term / Working Memory (Context)</h3>
<p>This tracks the <em>current</em> conversation or active task. It holds the “Scratchpad” of thoughts.</p>
<ul>
  <li><strong>The Challenge:</strong> The “Context Stuffing” problem.</li>
  <li><strong>Strategy 1: Sliding Window.</strong> Keep last $N$ turns. Simple, but forgets the beginning of the plan.</li>
  <li><strong>Strategy 2: Summarization.</strong> As the window fills, call the LLM to summarize the oldest 10 turns into a paragraph (“I successfully downloaded the data and cleaned it”). Inject this summary and drop the raw logs.</li>
  <li><strong>Strategy 3: Entity Extraction.</strong> Extract specific variables (“User Name: Alice”, “Goal: Fix Bug”) and store them in a state dictionary, reducing the text needed to purely essential facts.</li>
</ul>

<h3 id="23-long-term-memory-episodic">2.3 Long-Term Memory (Episodic)</h3>
<p>Storage that persists <em>across</em> sessions. “What did we do last Tuesday?”</p>
<ul>
  <li><strong>Implementation:</strong> <strong>Vector Databases (RAG)</strong> (Pinecone, Milvus).</li>
  <li><strong>Mechanism:</strong>
    <ol>
      <li>Event happens (“User reset password”).</li>
      <li>Embed text -&gt; Vector.</li>
      <li>Store metadata <code class="language-plaintext highlighter-rouge">{"date": "2023-10-01", "type": "auth"}</code>.</li>
      <li>Retrieve when relevant.</li>
    </ol>
  </li>
</ul>

<h3 id="24-semantic-memory-world-knowledge">2.4 Semantic Memory (World Knowledge)</h3>
<p>Facts about the world, distinct from events. “The user uses Python 3.10.”</p>
<ul>
  <li><strong>Implementation:</strong> <strong>Knowledge Graphs</strong> (Neo4j) or Structured SQL.</li>
  <li><strong>Why:</strong> Vectors are fuzzy. If you ask “Who is Alice’s manager?”, a vector search might return “Alice manages Bob” (similar words, wrong relationship). A Graph query <code class="language-plaintext highlighter-rouge">(Alice)-[:REPORTS_TO]-&gt;(Manager)</code> is precise.</li>
</ul>

<hr />

<h2 id="3-the-generative-agents-architecture-the-gold-standard">3. The “Generative Agents” Architecture (The Gold Standard)</h2>

<p>In 2023, Stanford/Google researchers published <em>“Generative Agents: Interactive Simulacra of Human Behavior”</em>. They created “Smallville,” a simulation where 25 AI agents lived in a town. They remembered relationships, planned parties, and gossiped.
Their memory architecture is the blueprint for <strong>Human-Like Memory</strong>.</p>

<h3 id="31-the-memory-stream">3.1 The Memory Stream</h3>
<p>Every observation is a distinct object in a time-ordered list.</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">[09:00] Observed Alice drinking coffee.</code></li>
  <li><code class="language-plaintext highlighter-rouge">[09:05] Observed Bob walking in.</code></li>
  <li><code class="language-plaintext highlighter-rouge">[09:06] Alice said "Hi Bob".</code></li>
</ul>

<h3 id="32-the-retrieval-function">3.2 The Retrieval Function</h3>
<p>How do we decide what to remember? They used a weighted score of 3 factors to retrieve top memories for a given query:
\(Score = \alpha \cdot \text{Recency} + \beta \cdot \text{Importance} + \gamma \cdot \text{Relevance}\)</p>

<ol>
  <li><strong>Recency:</strong> Exponential decay. I care about what happened 5 minutes ago more than 5 years ago.
    <ul>
      <li>$Score = 0.99^{\text{decay_hours}}$</li>
    </ul>
  </li>
  <li><strong>Importance:</strong> A static score separating “Noise” from “Signal”. “Ate toast” (1/10). “House on fire” (10/10).
    <ul>
      <li><em>Implementation:</em> Ask the LLM to rate the importance of every new memory on ingestion.</li>
    </ul>
  </li>
  <li><strong>Relevance:</strong> The standard Cosine Similarity to the module’s current query.</li>
</ol>

<h3 id="33-the-reflection-tree-synthesizing-wisdom">3.3 The Reflection Tree (Synthesizing Wisdom)</h3>
<p>If you only store raw logs, the agent never “learns.” It just remembers details.</p>
<ul>
  <li><em>Process:</em> Periodically (e.g., every 100 observations), the agent takes a batch of memories and asks: <em>“What does this mean?”</em></li>
  <li><em>Input:</em> “Alice drank coffee Mon”, “Alice drank coffee Tue”, “Alice drank coffee Wed”.</li>
  <li><em>Insight:</em> “Alice is addicted to coffee.”</li>
  <li><em>Action:</em> Save this <strong>Insight</strong> as a new memory node.</li>
  <li><em>Result:</em> Future queries retrieve the Insight, not the 50 raw logs. This mocks human generalization.</li>
</ul>

<hr />

<h2 id="4-memgpt-llm-as-an-operating-system">4. MemGPT: LLM as an Operating System</h2>

<p><strong>MemGPT</strong> (Memory-GPT) proposes a different analogy.</p>
<ul>
  <li><strong>LLM Context = RAM.</strong> (Fast, expensive, volatile).</li>
  <li><strong>Vector DB = Hard Drive.</strong> (Slow, cheap, persistent).</li>
  <li><strong>Paging:</strong> The OS (Prompt logic) manages “Virtual Memory.” It swaps data in and out of the Context Window based on need.</li>
  <li><strong>Mechanism:</strong> The LLM itself decides what to “write to disk” (save to DB) and what to “read from disk” (search DB) via function calls.</li>
  <li><strong>Impact:</strong> Enables agents to run “forever” (infinite context) by actively managing their own memory slots, just <code class="language-plaintext highlighter-rouge">malloc</code> and <code class="language-plaintext highlighter-rouge">free</code>.</li>
</ul>

<hr />

<h2 id="5-procedural-memory-the-skill-library-voyager">5. Procedural Memory: The Skill Library (Voyager)</h2>

<p>How does an agent “learn to code”?</p>
<ul>
  <li><strong>Naive Agent:</strong> Writes code -&gt; Fails -&gt; Rewrites -&gt; Succeeds -&gt; Forgets.</li>
  <li><strong>Voyager Agent (Minecraft):</strong> Writes code -&gt; Fails -&gt; Rewrites -&gt; Succeeds -&gt; <strong>Saves Function to Disk</strong>.</li>
  <li><strong>Skill Retrieval:</strong> Next time the goal is “Mine Diamond”, it checks its <code class="language-plaintext highlighter-rouge">skills/</code> folder. “Ah, I have a <code class="language-plaintext highlighter-rouge">mine_block</code> function.”</li>
  <li><strong>Result:</strong> The agent gets faster and more capable over time, building a library of tools it wrote itself. This is akin to “Muscle Memory.”</li>
</ul>

<hr />

<h2 id="6-code-implementing-generative-retrieval-conceptual">6. Code: Implementing Generative Retrieval (Conceptual)</h2>

<p>The formula for the Stanford retrieval function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">retrieve_memories</span><span class="p">(</span><span class="n">query_vector</span><span class="p">,</span> <span class="n">memory_stream</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
 <span class="sh">"""</span><span class="s">
 Ranks memories by Recency, Importance, and Relevance.
 </span><span class="sh">"""</span>
 <span class="n">scored_memories</span> <span class="o">=</span> <span class="p">[]</span>

 <span class="n">current_time</span> <span class="o">=</span> <span class="nf">now</span><span class="p">()</span>

 <span class="k">for</span> <span class="n">memory</span> <span class="ow">in</span> <span class="n">memory_stream</span><span class="p">:</span>
 <span class="c1"># 1. Relevance: Cosine Similarity
</span> <span class="n">relevance</span> <span class="o">=</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">query_vector</span><span class="p">,</span> <span class="n">memory</span><span class="p">.</span><span class="n">vector</span><span class="p">)</span>

 <span class="c1"># 2. Recency: Exponential Decay
</span> <span class="n">time_diff</span> <span class="o">=</span> <span class="n">current_time</span> <span class="o">-</span> <span class="n">memory</span><span class="p">.</span><span class="n">timestamp</span>
 <span class="n">recency</span> <span class="o">=</span> <span class="mf">0.99</span> <span class="o">**</span> <span class="n">time_diff</span><span class="p">.</span><span class="n">hours</span>

 <span class="c1"># 3. Importance: Pre-calculated static score (1-10)
</span> <span class="n">importance</span> <span class="o">=</span> <span class="n">memory</span><span class="p">.</span><span class="n">importance_score</span> <span class="o">/</span> <span class="mf">10.0</span>

 <span class="c1"># Combined Score
</span> <span class="n">total_score</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">recency</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">importance</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">relevance</span><span class="p">)</span>

 <span class="n">scored_memories</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">total_score</span><span class="p">,</span> <span class="n">memory</span><span class="p">))</span>

 <span class="c1"># Python's sort is stable
</span> <span class="n">scored_memories</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

 <span class="k">return</span> <span class="p">[</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">scored_memories</span><span class="p">[:</span><span class="mi">3</span><span class="p">]]</span>
</code></pre></div></div>

<hr />

<h2 id="7-summary">7. Summary</h2>

<p>Memory is the bedrock of identity.</p>
<ul>
  <li><strong>Short-term memory</strong> allows for coherent conversation.</li>
  <li><strong>Long-term memory</strong> allows for personalization.</li>
  <li><strong>Reflection</strong> allows for learning.</li>
</ul>

<p>Without robust memory architectures, AI agents will forever be “Goldfish”—brilliant in the moment, but incapable of growth. In the next section, we move from components to architectures, looking at the major frameworks like LangChain and AutoGen.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#generative-agents" class="page__taxonomy-item p-category" rel="tag">generative-agents</a><span class="sep">, </span>
    
      <a href="/tags/#long-term-memory" class="page__taxonomy-item p-category" rel="tag">long-term-memory</a><span class="sep">, </span>
    
      <a href="/tags/#memgpt" class="page__taxonomy-item p-category" rel="tag">memgpt</a><span class="sep">, </span>
    
      <a href="/tags/#memory" class="page__taxonomy-item p-category" rel="tag">memory</a><span class="sep">, </span>
    
      <a href="/tags/#vector-db" class="page__taxonomy-item p-category" rel="tag">vector-db</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ai-agents" class="page__taxonomy-item p-category" rel="tag">ai-agents</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Memory+Architectures%20https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0005-memory-architectures%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0005-memory-architectures%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ai-agents/0005-memory-architectures/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ai-agents/0004-tool-calling-fundamentals/" class="pagination--pager" title="Tool Calling Fundamentals">Previous</a>
    
    
      <a href="/ai-agents/0006-agent-frameworks-landscape/" class="pagination--pager" title="Agent Frameworks Landscape">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
