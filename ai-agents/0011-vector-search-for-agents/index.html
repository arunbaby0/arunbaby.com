<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Vector Search for Agents - Arun Baby</title>
<meta name="description" content="“Finding a Needle in a High-Dimensional Haystack.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Vector Search for Agents">
<meta property="og:url" content="https://www.arunbaby.com/ai-agents/0011-vector-search-for-agents/">


  <meta property="og:description" content="“Finding a Needle in a High-Dimensional Haystack.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Vector Search for Agents">
  <meta name="twitter:description" content="“Finding a Needle in a High-Dimensional Haystack.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ai-agents/0011-vector-search-for-agents/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-14T23:01:57+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ai-agents/0011-vector-search-for-agents/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="">
  
  <div class="sidebar sticky">
  
    


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="https://www.arunbaby.com/">
        <img src="/assets/images/profile-photo.png" alt="Arun Baby" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="https://www.arunbaby.com/" itemprop="url">Arun Baby</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Becoming <strong>Unlabelable</strong></p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">India</span>
        </li>
      

      
        
          
            <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
            <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i><span class="label">Google Scholar</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Vector Search for Agents">
    <meta itemprop="description" content="“Finding a Needle in a High-Dimensional Haystack.”">
    <meta itemprop="datePublished" content="2025-12-14T23:01:57+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ai-agents/0011-vector-search-for-agents/" itemprop="url">Vector Search for Agents
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction-the-agents-filing-system">1. Introduction: The Agent’s Filing System</a></li><li><a href="#2-the-mathematics-of-meaning-embeddings">2. The Mathematics of Meaning: Embeddings</a><ul><li><a href="#21-the-latent-space">2.1 The Latent Space</a></li><li><a href="#22-distance-metrics">2.2 Distance Metrics</a></li></ul></li><li><a href="#3-the-curse-of-dimensionality">3. The Curse of Dimensionality</a></li><li><a href="#4-algorithms-inside-the-engine">4. Algorithms: Inside the Engine</a><ul><li><a href="#41-hnsw-hierarchical-navigable-small-worlds">4.1 HNSW (Hierarchical Navigable Small Worlds)</a></li><li><a href="#42-ivf-inverted-file-index">4.2 IVF (Inverted File Index)</a></li></ul></li><li><a href="#5-keyword-vs-semantic-the-hybrid-debate">5. Keyword vs. Semantic: The “Hybrid” Debate</a><ul><li><a href="#51-hybrid-search">5.1 Hybrid Search</a></li><li><a href="#52-sparse-vectors-splade">5.2 Sparse Vectors (SPLADE)</a></li></ul></li><li><a href="#6-implementation-strategy-for-agents">6. Implementation Strategy for Agents</a><ul><li><a href="#61-thresholding-the-confidence-cutoff">6.1 Thresholding (The Confidence Cutoff)</a></li><li><a href="#62-namespace-partitioning">6.2 namespace Partitioning</a></li></ul></li><li><a href="#7-code-simulating-ann-with-clustering">7. Code: Simulating ANN with Clustering</a></li><li><a href="#8-summary">8. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Finding a Needle in a High-Dimensional Haystack.”</strong></p>

<h2 id="1-introduction-the-agents-filing-system">1. Introduction: The Agent’s Filing System</h2>

<p>In the physical world, if you want to find a specific book in a library, you use the Dewey Decimal System. It’s a rigid, hierarchical, and precise way of organizing knowledge. If you know the code, you find the book.</p>

<p>In the world of AI Agents, we don’t look for “Books” by their ID. We look for <strong>Concepts</strong>. We want to ask: <em>“Find me all documents related to project delays caused by weather,”</em> and we want the system to return a report titled “Q3 Logistics Failure” even though the word “weather” never appears in it (perhaps it uses “storm” or “hurricane”).</p>

<p>This capability—<strong>Semantic Search</strong>—is powered by <strong>Vector Search</strong>. For an AI Agent, the Vector Database is not just a storage bin; it is the <strong>Associative Memory</strong> cortex. It allows the agent to recall relevant experiences, skills, and facts based on <em>meaning</em> rather than <em>keywords</em>.</p>

<p>To build production-grade agents, you cannot treat the Vector DB as a black box. You must understand how it works, why it fails, and how to tune it. In this post, we will dive deep into the mathematics of embeddings, the algorithms behind Approximate Nearest Neighbors (ANN), and the architectural patterns for scaling agent memory.</p>

<hr />

<h2 id="2-the-mathematics-of-meaning-embeddings">2. The Mathematics of Meaning: Embeddings</h2>

<p>Before we search, we must define what we are searching.</p>

<h3 id="21-the-latent-space">2.1 The Latent Space</h3>
<p>Imagine a 2D graph.</p>
<ul>
  <li>X-axis: “Royalness”</li>
  <li>Y-axis: “Masculinity”</li>
</ul>

<p>In this space:</p>
<ul>
  <li><strong>King</strong> might be at <code class="language-plaintext highlighter-rouge">[0.9, 0.9]</code>.</li>
  <li><strong>Queen</strong> might be at <code class="language-plaintext highlighter-rouge">[0.9, 0.1]</code>.</li>
  <li><strong>Man</strong> might be at <code class="language-plaintext highlighter-rouge">[0.1, 0.9]</code>.</li>
  <li><strong>Woman</strong> might be at <code class="language-plaintext highlighter-rouge">[0.1, 0.1]</code>.</li>
</ul>

<p>The magic of embeddings is that <strong>Math becomes Meaning</strong>.
\(\text{King} - \text{Man} + \text{Woman} \approx \text{Queen}\)
\([0.9, 0.9] - [0.1, 0.9] + [0.1, 0.1] = [0.9, 0.1]\)</p>

<p>Modern Embedding Models (like OpenAI’s <code class="language-plaintext highlighter-rouge">text-embedding-3-small</code> or <code class="language-plaintext highlighter-rouge">bge-m3</code>) don’t use 2 dimensions. They use <strong>1,536</strong> or <strong>3,072</strong> dimensions. They map text into a hypersphere where “meaning” is preserved as geometric distance.</p>

<h3 id="22-distance-metrics">2.2 Distance Metrics</h3>
<p>How do we measure if two thoughts are “close”?</p>

<ol>
  <li><strong>Euclidean Distance (L2):</strong> The straight-line distance between two points.
    <ul>
      <li><em>Usage:</em> Rarely used for text. Good for clustering physical coordinates.</li>
    </ul>
  </li>
  <li><strong>Dot Product:</strong> The sum of the products of components.
    <ul>
      <li><em>Formula:</em> $A \cdot B = \sum A_i B_i$</li>
      <li><em>Usage:</em> Valid <em>only</em> if vectors are normalized (magnitude of 1). If vectors have different lengths, Dot Product is biased towards longer vectors (longer texts).</li>
    </ul>
  </li>
  <li><strong>Cosine Similarity:</strong> The angle between two vectors.
    <ul>
      <li>
        <table>
          <tbody>
            <tr>
              <td><em>Formula:</em> $\frac{A \cdot B}{</td>
              <td> </td>
              <td>A</td>
              <td> </td>
              <td>\cdot</td>
              <td> </td>
              <td>B</td>
              <td> </td>
              <td>}$</td>
            </tr>
          </tbody>
        </table>
      </li>
      <li><em>Usage:</em> The industry standard for text. It ignores the length/magnitude of the vector and focuses purely on the direction (semantic content). A document repeated twice (“Hello. Hello.”) has the same direction as “Hello.”, so Cosine Similarity says they are identical (1.0).</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="3-the-curse-of-dimensionality">3. The Curse of Dimensionality</h2>

<p>Why can’t we use a normal database (Postgres/MySQL) for this?</p>

<p>If you have 1 million documents, and you want to find the nearest neighbor to a query vector, the naive approach is <strong>Brute Force (KNN - K-Nearest Neighbors)</strong>.</p>
<ol>
  <li>Take the Query Vector.</li>
  <li>Calculate Cosine Similarity with <strong>all 1,000,000</strong> vectors in the DB.</li>
  <li>Sort them.</li>
  <li>Take the top 5.</li>
</ol>

<p>This is $O(N)$. With 1536 dimensions, calculating 1 million dot products takes seconds. For an agent that needs to “think” in milliseconds, this is too slow.</p>

<p>We need a way to find the nearest neighbors <em>without</em> looking at everyone. This is the <strong>Nearest Neighbor Search (NNS)</strong> problem. Since exact search is too slow, we accept <strong>Approximate Nearest Neighbors (ANN)</strong>. We trade 1% accuracy for 100x speed.</p>

<hr />

<h2 id="4-algorithms-inside-the-engine">4. Algorithms: Inside the Engine</h2>

<p>How do Vector Databases (Pinecone, Weaviate, Milvus, Qdrant) actually work? They use indexing structures.</p>

<h3 id="41-hnsw-hierarchical-navigable-small-worlds">4.1 HNSW (Hierarchical Navigable Small Worlds)</h3>
<p>This is currently the gold standard. It dominates the industry because of its balance of speed and recall (accuracy).</p>

<p><strong>The Concept:</strong> Six Degrees of Kevin Bacon.
Imagine a social network. You want to find “Kevin Bacon.”</p>
<ul>
  <li><strong>Layer 0 (The Ground):</strong> Everyone is connected to their close friends. To get from “You” to “Kevin,” you’d have to hop through millions of friends.</li>
  <li><strong>Layer 1 (The Express):</strong> Some people are “Connectors” (Long-range links). You skip from “You” -&gt; “Governor of your State”.</li>
  <li><strong>Layer 2 (The Super Express):</strong> “Governor” -&gt; “President”.</li>
</ul>

<p><strong>HNSW</strong> builds a multi-layered graph.</p>
<ul>
  <li><strong>Search Process:</strong>
    <ol>
      <li>Start at the top layer (Super Express). Look for a node somewhat close to the query.</li>
      <li>Zoom in. Drop to the next layer. The graph is denser here. Move closer.</li>
      <li>Repeat until you hit layer 0 (all data). Now you are in the local neighborhood of the answer. Perform a quick greedy search.</li>
    </ol>
  </li>
  <li><strong>Pros:</strong> Extremely fast (logarithmic time complexity $O(\log N)$). High Recall (95%+).</li>
  <li><strong>Cons:</strong> Memory hungry. The graph topology takes up RAM. Adding/Deleting items requires graph repair (can be slowish, though modern implementations optimize this).</li>
</ul>

<h3 id="42-ivf-inverted-file-index">4.2 IVF (Inverted File Index)</h3>
<p>Used by FAISS (Facebook AI Similarity Search).</p>

<p><strong>The Concept:</strong> Clustering (Voronoi Cells).</p>
<ol>
  <li><strong>Train:</strong> Take a sample of vectors. Run K-Means clustering to find 1,000 “Centroids” (buckets).</li>
  <li><strong>Index:</strong> Assign every document in the DB to its nearest Centroid.</li>
  <li><strong>Search:</strong>
    <ul>
      <li>Take the Query Vector.</li>
      <li>Find the nearest Centroid (e.g., Centroid #42).</li>
      <li><strong>Only</strong> search the vectors inside Centroid #42. Ignore the other 999 buckets.</li>
    </ul>
  </li>
</ol>

<ul>
  <li><strong>Pros:</strong> Very memory efficient. Fast.</li>
  <li><strong>Cons:</strong> <strong>The Edge Problem.</strong> If the query lands right on the edge of Centroid #42, but the true nearest neighbor is just across the border in Centroid #43, you will miss it. (Mitigation: Search <code class="language-plaintext highlighter-rouge">nprobe</code> nearest buckets, i.e., check top 5 buckets).</li>
</ul>

<hr />

<h2 id="5-keyword-vs-semantic-the-hybrid-debate">5. Keyword vs. Semantic: The “Hybrid” Debate</h2>

<p>Semantic search is not magic. It has blind spots.</p>

<ul>
  <li><strong>Scenario:</strong> You search for “Error 504 on user 998811”.</li>
  <li><strong>Semantic Search:</strong> Finds documents about “Server errors” and “Users”. It might find “Error 500 on user 123456” because mechanically, the numbers look similar in vector space (or are treated as noise).</li>
  <li><strong>Keyword Search (BM25):</strong> Finds documents containing the exact string “998811”.</li>
</ul>

<p><strong>Agents need both.</strong>
If your agent is looking up a specific SKU, Semantic Search will fail. It will return “Similar products” rather than “The exact product.”</p>

<h3 id="51-hybrid-search">5.1 Hybrid Search</h3>
<p>The modern standard is <strong>Reciprocal Rank Fusion (RRF)</strong>.</p>
<ol>
  <li>Run Vector Search -&gt; get Top 50.</li>
  <li>Run Keyword Search (BM25) -&gt; get Top 50.</li>
  <li>Combine the lists. Give a boost to items that appear in both.</li>
  <li>Return Top 5.</li>
</ol>

<h3 id="52-sparse-vectors-splade">5.2 Sparse Vectors (SPLADE)</h3>
<p>A newer approach. Instead of a dense vector (1536 floats), we generate a <strong>Sparse Vector</strong> (size of vocabulary, mostly zeros).</p>
<ul>
  <li>The model learns to “expand” the document.</li>
  <li>Input: “Apple”</li>
  <li>Learned Expansion: “Apple, Fruit, iPhone, Tech, Pie, Macintosh…”</li>
  <li>We utilize this for keyword search that includes synonyms automatically.</li>
</ul>

<hr />

<h2 id="6-implementation-strategy-for-agents">6. Implementation Strategy for Agents</h2>

<p>When building an agent, you must tune your retrieval parameter dynamics.</p>

<h3 id="61-thresholding-the-confidence-cutoff">6.1 Thresholding (The Confidence Cutoff)</h3>
<p>Agents default to being helpful. If you ask a RAG agent “Why is the moon made of cheese?”, and your DB contains a recipe for Cheesecake, the vector search might return the cheesecake recipe (Closest match). The agent then hallucinates connection.</p>

<ul>
  <li><strong>Solution:</strong> Set a <code class="language-plaintext highlighter-rouge">similarity_threshold</code> (e.g., 0.75). If the top result is $&lt; 0.75$, the tool should return <strong>“No relevant context found.”</strong></li>
  <li><strong>Agent Logic:</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span> <span class="o">=</span> <span class="nf">search</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="k">if</span> <span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">score</span> <span class="o">&lt;</span> <span class="mf">0.75</span><span class="p">:</span>
    <span class="k">return</span> <span class="sh">"</span><span class="s">I checked the database but found no information on that.</span><span class="sh">"</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div>    </div>
    <p>This allows the agent to fallback to reasoning or asking the user for clarification.</p>
  </li>
</ul>

<h3 id="62-namespace-partitioning">6.2 namespace Partitioning</h3>
<p>Don’t dump everything into one index.
Use <strong>Namespaces</strong> (Multi-tenancy).</p>
<ul>
  <li>Structure: <code class="language-plaintext highlighter-rouge">User_123_History</code>, <code class="language-plaintext highlighter-rouge">Global_Knowledge</code>, <code class="language-plaintext highlighter-rouge">Project_X_Docs</code>.</li>
  <li>Search: When the Agent wants to “Recall earlier conversation,” it queries only <code class="language-plaintext highlighter-rouge">User_123_History</code>. Limiting the search space improves accuracy and speed.</li>
</ul>

<hr />

<h2 id="7-code-simulating-ann-with-clustering">7. Code: Simulating ANN with Clustering</h2>

<p>To understand IVF, let’s build a toy implementation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="c1"># 1. Generate Toy Data
# 10,000 random vectors of dim 128
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">128</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># 2. Build the Index (Training Phase)
# Create 100 Clusters (Voronoi Cells)
</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span><span class="p">)</span>
<span class="n">kmeans</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># 3. Assign Data to Buckets
# bucket_map[cluster_id] = [vector_indices...]
</span><span class="n">bucket_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">)}</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
    <span class="n">bucket_map</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

<span class="c1"># 4. Search Function
</span><span class="k">def</span> <span class="nf">search_ivf</span><span class="p">(</span><span class="n">query_vec</span><span class="p">,</span> <span class="n">nprobe</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="c1"># Step A: Find closest clusters (coarse search)
</span>    <span class="c1"># Distance to the 100 centroids
</span>    <span class="n">dists_to_centroids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">cid</span><span class="p">,</span> <span class="n">centroid</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">):</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">query_vec</span> <span class="o">-</span> <span class="n">centroid</span><span class="p">)</span>
        <span class="n">dists_to_centroids</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">dist</span><span class="p">,</span> <span class="n">cid</span><span class="p">))</span>
    
    <span class="c1"># Sort and pick top 'nprobe' buckets
</span>    <span class="n">dists_to_centroids</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span>
    <span class="n">target_clusters</span> <span class="o">=</span> <span class="p">[</span><span class="n">cid</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">cid</span> <span class="ow">in</span> <span class="n">dists_to_centroids</span><span class="p">[:</span><span class="n">nprobe</span><span class="p">]]</span>
    
    <span class="c1"># Step B: Search inside those buckets (fine search)
</span>    <span class="n">candidates_indices</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">cid</span> <span class="ow">in</span> <span class="n">target_clusters</span><span class="p">:</span>
        <span class="n">candidates_indices</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">bucket_map</span><span class="p">[</span><span class="n">cid</span><span class="p">])</span>
        
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Scanning </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">candidates_indices</span><span class="p">)</span><span class="si">}</span><span class="s"> vectors instead of 10,000...</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="n">best_dist</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">inf</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">best_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">candidates_indices</span><span class="p">:</span>
        <span class="n">vec</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">query_vec</span> <span class="o">-</span> <span class="n">vec</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dist</span> <span class="o">&lt;</span> <span class="n">best_dist</span><span class="p">:</span>
            <span class="n">best_dist</span> <span class="o">=</span> <span class="n">dist</span>
            <span class="n">best_idx</span> <span class="o">=</span> <span class="n">idx</span>
            
    <span class="k">return</span> <span class="n">best_idx</span><span class="p">,</span> <span class="n">best_dist</span>

<span class="c1"># Test
</span><span class="n">query</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">128</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>
<span class="n">idx</span><span class="p">,</span> <span class="n">dist</span> <span class="o">=</span> <span class="nf">search_ivf</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Nearest Neighbor Found: </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s"> with distance </span><span class="si">{</span><span class="n">dist</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="8-summary">8. Summary</h2>

<p>Vector Search gives your agent a <strong>Long-Term Memory</strong>.</p>
<ul>
  <li><strong>Embeddings</strong> map thoughts to geometry.</li>
  <li><strong>HNSW</strong> navigates that geometry at lightning speed by building a “Small World” graph.</li>
  <li><strong>Hybrid Search</strong> fixes the blind spots of pure semantics by re-introducing keyword precision.</li>
</ul>

<p>Without Vector Search, an agent is just a goldfish—smart, but trapped in the “Now” of its context window. With it, the agent becomes a scholar, capable of drawing on vast archives of knowledge to inform its actions.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#ann" class="page__taxonomy-item p-category" rel="tag">ann</a><span class="sep">, </span>
    
      <a href="/tags/#embeddings" class="page__taxonomy-item p-category" rel="tag">embeddings</a><span class="sep">, </span>
    
      <a href="/tags/#faiss" class="page__taxonomy-item p-category" rel="tag">faiss</a><span class="sep">, </span>
    
      <a href="/tags/#hnsw" class="page__taxonomy-item p-category" rel="tag">hnsw</a><span class="sep">, </span>
    
      <a href="/tags/#vector-search" class="page__taxonomy-item p-category" rel="tag">vector-search</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ai-agents" class="page__taxonomy-item p-category" rel="tag">ai-agents</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Vector+Search+for+Agents%20https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0011-vector-search-for-agents%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0011-vector-search-for-agents%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ai-agents/0011-vector-search-for-agents/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ai-agents/0010-document-processing/" class="pagination--pager" title="Document Processing for Agents">Previous</a>
    
    
      <a href="/ai-agents/0012-context-window-management/" class="pagination--pager" title="Context Window Management">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
