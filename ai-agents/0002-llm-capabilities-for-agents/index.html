<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>LLM Capabilities for Agents - Arun Baby</title>
<meta name="description" content="“The Engine of Autonomy: Understanding the Agentic ‘Brain’.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="LLM Capabilities for Agents">
<meta property="og:url" content="https://www.arunbaby.com/ai-agents/0002-llm-capabilities-for-agents/">


  <meta property="og:description" content="“The Engine of Autonomy: Understanding the Agentic ‘Brain’.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="LLM Capabilities for Agents">
  <meta name="twitter:description" content="“The Engine of Autonomy: Understanding the Agentic ‘Brain’.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ai-agents/0002-llm-capabilities-for-agents/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-14T21:26:06+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ai-agents/0002-llm-capabilities-for-agents/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="">
  
  <div class="sidebar sticky">
  
    


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="https://www.arunbaby.com/">
        <img src="/assets/images/profile-photo.png" alt="Arun Baby" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="https://www.arunbaby.com/" itemprop="url">Arun Baby</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Becoming <strong>Unlabelable</strong></p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">India</span>
        </li>
      

      
        
          
            <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
            <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i><span class="label">Google Scholar</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="LLM Capabilities for Agents">
    <meta itemprop="description" content="“The Engine of Autonomy: Understanding the Agentic ‘Brain’.”">
    <meta itemprop="datePublished" content="2025-12-14T21:26:06+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ai-agents/0002-llm-capabilities-for-agents/" itemprop="url">LLM Capabilities for Agents
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction-the-silicon-cortex">1. Introduction: The Silicon Cortex</a></li><li><a href="#2-the-agentic-capability-hierarchy">2. The Agentic Capability Hierarchy</a><ul><li><a href="#21-steerability--instruction-following">2.1 Steerability &amp; Instruction Following</a></li><li><a href="#22-reasoning-system-2-thinking">2.2 Reasoning (System 2 Thinking)</a></li><li><a href="#23-grounding--tool-use">2.3 Grounding &amp; Tool Use</a></li><li><a href="#24-context-window-management">2.4 Context Window Management</a></li></ul></li><li><a href="#3-dissecting-reasoning-paradigms">3. Dissecting Reasoning Paradigms</a><ul><li><a href="#31-chain-of-thought-cot">3.1 Chain of Thought (CoT)</a></li><li><a href="#32-tree-of-thoughts-tot">3.2 Tree of Thoughts (ToT)</a></li><li><a href="#33-self-refinement-reflexion">3.3 Self-Refinement (Reflexion)</a></li></ul></li><li><a href="#4-the-model-landscape-choosing-your-brain">4. The Model Landscape: Choosing Your Brain</a><ul><li><a href="#41-frontier-models-the-big-brains">4.1 “Frontier” Models (The Big Brains)</a></li><li><a href="#42-open-weights-models-the-private-brains">4.2 Open Weights Models (The Private Brains)</a></li><li><a href="#43-specialized-models-the-savants">4.3 Specialized Models (The Savants)</a></li></ul></li><li><a href="#5-benchmarking-agency">5. Benchmarking Agency</a><ul><li><a href="#51-humaneval--mbpp-coding">5.1 HumanEval &amp; MBPP (Coding)</a></li><li><a href="#52-agentbench--webshop">5.2 AgentBench / WebShop</a></li><li><a href="#53-swe-bench-the-everest">5.3 SWE-Bench (The Everest)</a></li></ul></li><li><a href="#6-fine-tuning-for-agency">6. Fine-Tuning for Agency</a><ul><li><a href="#61-the-prompt-engineering-first-rule">6.1 The “Prompt Engineering First” Rule</a></li><li><a href="#62-when-to-fine-tune">6.2 When to Fine-Tune</a></li></ul></li><li><a href="#7-the-future-reasoning-tokens">7. The Future: Reasoning Tokens</a></li><li><a href="#8-summary">8. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“The Engine of Autonomy: Understanding the Agentic ‘Brain’.”</strong></p>

<h2 id="1-introduction-the-silicon-cortex">1. Introduction: The Silicon Cortex</h2>

<p>If an AI Agent is a vehicle for automation, the <strong>Large Language Model (LLM)</strong> is its engine. The performance, reliability, and “intelligence” of your agent are fundamentally capped by the capabilities of the underlying model.</p>

<p>You cannot build a sophisticated autonomous software engineer using a model that struggles with basic logic puzzles. Similarly, using a massive, expensive reasoning model for a simple intent classification task is a waste of resources.</p>

<p>In this deep dive, we will move beyond the marketing hype of “AI” and dissect the specific <strong>neuro-symbolic capabilities</strong> required for an LLM to function as an effective agent. We will explore reasoning paradigms, the mechanics of context, the landscape of models available today, and the benchmarks that actually matter.</p>

<hr />

<h2 id="2-the-agentic-capability-hierarchy">2. The Agentic Capability Hierarchy</h2>

<p>Not all LLMs can be agents. A model might be excellent at creative writing (high perplexity, good flow) but terrible at agency. Agency requires a specific set of skills, often referred to as the <strong>“Cybernetic Stack”</strong> of LLMs.</p>

<h3 id="21-steerability--instruction-following">2.1 Steerability &amp; Instruction Following</h3>
<p>The most basic requirement. Can the model follow a rule?</p>
<ul>
  <li><strong>The Problem:</strong> Pre-trained base models (like raw GPT-3) are just “Pattern Completers.” If you prompt <em>“The capital of France is”</em>, it completes <em>“Paris.”</em> If you prompt <em>“Do not output the capital of France”</em>, a base model might get confused because it sees “Capital of France” and autocompletes “Paris.”</li>
  <li><strong>The Solution:</strong> <strong>RLHF (Reinforcement Learning from Human Feedback)</strong>. Models are specifically trained to prioritize <strong>Instructions</strong> over probability. Even newer techniques like <strong>DPO (Direct Preference Optimization)</strong> are used to fine-tune models to prefer helpful, obedient responses.</li>
  <li><strong>Agent Relevance:</strong> Agents rely on <strong>System Prompts</strong>—massive blocks of text defining rules (“Never delete files,” “Always return JSON”). A model with low steerability (like many early open-source models) will “forget” these rules as the conversation gets longer (Instruction Drift). High-quality agent models (GPT-4o, Claude 3.5) treat the System Prompt as a constitution.</li>
</ul>

<h3 id="22-reasoning-system-2-thinking">2.2 Reasoning (System 2 Thinking)</h3>
<p>Daniel Kahneman distinguishes between System 1 (Fast/Intuitive) and System 2 (Slow/Logical).</p>
<ul>
  <li><strong>Standard Generation:</strong> LLMs naturally operate in System 1. They generate the next token immediately based on surface-level statistics.</li>
  <li><strong>Agentic Need:</strong> Agents face multi-step logic puzzles. “If user is in US, check inventory A. If inventory A is empty, check inventory B, but only if it’s a weekday.”</li>
  <li><strong>Capabilities:</strong>
    <ul>
      <li><strong>Deductive Reasoning:</strong> $A \to B, B \to C, \therefore A \to C$.</li>
      <li><strong>Inductive Reasoning:</strong> Seeing 3 examples in the prompt and guessing the underlying rule.</li>
      <li><strong>Abductive Reasoning:</strong> Guessing the most likely cause of an observation (Essential for Debugging).</li>
    </ul>
  </li>
  <li><strong>Chain of Thought (CoT):</strong> The ability to generate “intermediate reasoning tokens” before the final answer. Research shows that models that cannot “think out loud” perform significantly worse on agentic tasks.</li>
</ul>

<h3 id="23-grounding--tool-use">2.3 Grounding &amp; Tool Use</h3>
<p>Pure LLMs live in a world of text hallucinations. Agents must live in reality.</p>
<ul>
  <li><strong>The Capability:</strong> The ability to map a fuzzy intent (“Book me a flight”) to a <strong>rigid schema</strong> (<code class="language-plaintext highlighter-rouge">book_flight(origin="SFO", dest="LHR", date="2024-01-01")</code>).</li>
  <li><strong>Structured Output:</strong> This is the killer feature of 2024. Models are now fine-tuned to output valid JSON or XML. This isn’t just “writing code”; it’s about adhering to a specific <strong>syntax constraint</strong> while maintaining <strong>semantic intent</strong>.</li>
  <li><strong>Constraint Satisfaction:</strong> A good agent model respects <code class="language-plaintext highlighter-rouge">enums</code> (e.g., “Only choose from [RED, GREEN, BLUE]”). A bad model might output “YELLOW” because it “liked that color better.”</li>
</ul>

<h3 id="24-context-window-management">2.4 Context Window Management</h3>
<p>The Agent’s “Working Memory.”</p>
<ul>
  <li><strong>Needle in a Haystack:</strong> Can the model find one specific fact (e.g., a password or ID) buried in 100,000 tokens of logs?</li>
  <li><strong>recency Bias (Attention Sinks):</strong> Models tend to pay most attention to the <strong>Beginning</strong> (System Prompt) and the <strong>End</strong> (User Query). The “Middle” is often a dead zone.</li>
  <li><strong>Agent Relevance:</strong> As an agent works, its history log grows. If the model’s retrieval capability degrades as context fills, the agent becomes “senile”—forgetting what it did 5 minutes ago.</li>
</ul>

<hr />

<h2 id="3-dissecting-reasoning-paradigms">3. Dissecting Reasoning Paradigms</h2>

<p>How do we extract “Reasoning” from a next-token predictor?</p>

<h3 id="31-chain-of-thought-cot">3.1 Chain of Thought (CoT)</h3>
<ul>
  <li><em>Prompt:</em> “What is 23 * 45?”</li>
  <li><em>Standard:</em> “1035” (Might be hallucinated).</li>
  <li><em>CoT:</em> “Let’s break it down. 20 * 45 = 900. 3 * 45 = 135. 900 + 135 = 1035.”</li>
  <li><strong>Why it works:</strong> It spreads the computation across more tokens. The model effectively writes its own “scratchpad” data to the context window, which it can then attend to for the subsequent tokens.</li>
</ul>

<h3 id="32-tree-of-thoughts-tot">3.2 Tree of Thoughts (ToT)</h3>
<p>For complex planning, linear thinking isn’t enough.</p>
<ul>
  <li><strong>Mechanism:</strong> The agent explores multiple “branches” of possibilities.</li>
  <li><em>Branch 1:</em> “I could search Google.” -&gt; <em>Eval:</em> Too slow.</li>
  <li><em>Branch 2:</em> “I could check the cache.” -&gt; <em>Eval:</em> Fast, but might be stale.</li>
  <li><em>Selection:</em> “I will check the cache first.”</li>
  <li><strong>Implementation:</strong> This requires the model to hold multiple conflicting states in its head (or the runtime to manage multiple prompt chains).</li>
</ul>

<h3 id="33-self-refinement-reflexion">3.3 Self-Refinement (Reflexion)</h3>
<p>The ability to critique one’s own output.</p>
<ul>
  <li><em>Draft 1:</em> Writes code.</li>
  <li><em>Self-Prompt:</em> “Are there any bugs in this code?”</li>
  <li><em>Critique:</em> “Yes, I missed an edge case for null inputs.”</li>
  <li><em>Draft 2:</em> Rewrites code.</li>
  <li><strong>Agent Relevance:</strong> This is crucial for autonomous coders. They must “Loop until passing.”</li>
</ul>

<hr />

<h2 id="4-the-model-landscape-choosing-your-brain">4. The Model Landscape: Choosing Your Brain</h2>

<p>Which model should you use for your agent?</p>

<h3 id="41-frontier-models-the-big-brains">4.1 “Frontier” Models (The Big Brains)</h3>
<p>These are closed-source, massive models (likely 1T+ parameters).</p>
<ul>
  <li><strong>GPT-4o (OpenAI):</strong> The reigning champion for general agency.
    <ul>
      <li><em>Pros:</em> Best-in-class Instruction Following, native Function Calling fine-tuning, highly reliable JSON mode. Fast.</li>
      <li><em>Cons:</em> Expensive, “Lazy” (sometimes refuses to code full solutions), strict safety filters (refusals).</li>
    </ul>
  </li>
  <li><strong>Claude 3.5 Sonnet (Anthropic):</strong> The “Coder’s Choice.”
    <ul>
      <li><em>Pros:</em> Exceptional at coding and complex reasoning. Huge context window (200k) with excellent “Needle in Haystack” retrieval. Often feels more “human” and less robotic than GPT.</li>
      <li><em>Cons:</em> Function calling format is slightly different (uses XML heavily in training).</li>
    </ul>
  </li>
  <li><strong>Gemini 1.5 Pro (Google):</strong> The “Running Room.”
    <ul>
      <li><em>Pros:</em> Massive 1M-2M token context window. Multimodal (can watch videos/screencasts). Great for “Reading the manual” agents.</li>
      <li><em>Cons:</em> Historically slightly higher hallucination rates on rigid logic than GPT-4, though catching up fast.</li>
    </ul>
  </li>
</ul>

<h3 id="42-open-weights-models-the-private-brains">4.2 Open Weights Models (The Private Brains)</h3>
<p>Models you can host yourself.</p>
<ul>
  <li><strong>Llama 3.1 70B &amp; 405B (Meta):</strong>
    <ul>
      <li><em>Pros:</em> GPT-4 class performance for free (if you have the GPUs). Uncensored (mostly). You own your data. Excellent Tool Calling support.</li>
      <li><em>Cons:</em> Hosting 405B parameters is technically difficult and expensive ($$$ GPU clusters). The 70B model is the sweet spot for enterprise agents.</li>
    </ul>
  </li>
  <li><strong>Mistral Large / Mixtral:</strong>
    <ul>
      <li><em>Pros:</em> Efficient “Mixture of Experts” (MoE) architecture. Good reasoning-to-cost ratio.</li>
    </ul>
  </li>
</ul>

<h3 id="43-specialized-models-the-savants">4.3 Specialized Models (The Savants)</h3>
<ul>
  <li><strong>NexusRaven / Gorilla:</strong> Models fine-tuned <em>exclusively</em> on API calling. They might forget who the President is, but they can construct a perfect AWS CLI command from a fuzzy prompt better than GPT-4.</li>
  <li><strong>DeepSeek Coder:</strong> Models trained on massive github repos. Excellent for coding agents.</li>
</ul>

<hr />

<h2 id="5-benchmarking-agency">5. Benchmarking Agency</h2>

<p>How do we know if a model is a good agent? Standard NLP benchmarks (MMLU, HELM) are multiple choice. They don’t test agency. We use <strong>Agentic Benchmarks</strong>.</p>

<h3 id="51-humaneval--mbpp-coding">5.1 HumanEval &amp; MBPP (Coding)</h3>
<ul>
  <li><em>Task:</em> “Write a Python function to reverse a list.”</li>
  <li><em>Metric:</em> Pass@1 (Does the code run and pass tests on the first try?).</li>
  <li><em>Significance:</em> Coding is a proxy for rigorous logic and syntax adherence. Models good at code are usually good at agents.</li>
</ul>

<h3 id="52-agentbench--webshop">5.2 AgentBench / WebShop</h3>
<ul>
  <li><em>Task:</em> “Go to amazon.com, find a blue HDMI cable under $10, and add it to cart.”</li>
  <li><em>Metric:</em> Success Rate within N steps.</li>
  <li><em>Significance:</em> Tests environment navigation, HTML parsing, decision making. A model needs to handle dynamic observations (“out of stock”).</li>
</ul>

<h3 id="53-swe-bench-the-everest">5.3 SWE-Bench (The Everest)</h3>
<p>This is the gold standard for software engineering agents.</p>
<ul>
  <li><em>Task:</em> “Here is a real GitHub Issue from a popular repo (e.g., Django, scikit-learn). Fix it.”</li>
  <li><em>Process:</em> The agent is given the codebase. It must write a reproduction script, locate the bug, fix it, and pass the tests.</li>
  <li><em>Scores:</em>
    <ul>
      <li>GPT-4 (Unassisted): ~2%</li>
      <li>Devin/Specialized Agents: ~13-20%</li>
    </ul>
  </li>
  <li><em>Reality Check:</em> This shows how early we are. Most agents fail at real-world software engineering.</li>
</ul>

<hr />

<h2 id="6-fine-tuning-for-agency">6. Fine-Tuning for Agency</h2>

<p>Should you fine-tune your own model for your agent?</p>

<h3 id="61-the-prompt-engineering-first-rule">6.1 The “Prompt Engineering First” Rule</h3>
<p>Always exhaust prompt engineering (Context + Examples) first. It is cheaper, faster, and easier to debug. Fine-tuning adds a maintenance burden.</p>

<h3 id="62-when-to-fine-tune">6.2 When to Fine-Tune</h3>
<ol>
  <li><strong>Unique Toolset:</strong> If you have a proprietary internal API with 5,000 functions, GPT-4 won’t know it. Fine-tuning a Llama-3 model on your API docs (Input: “Refund user”, Output: <code class="language-plaintext highlighter-rouge">api.refund(uid)</code>) can make it an expert router.</li>
  <li><strong>Style/Tone Enforcement:</strong> If your agent needs to speak in a specific brand voice (e.g., “A 17th Century Pirate Lawyer”), fine-tuning is better than a long system prompt.</li>
  <li><strong>Latency/Cost (Distillation):</strong> You can train a small model (8B parameters) to mimic the outputs of a large model (GPT-4) for your specific task, allowing you to run it 10x cheaper and faster.</li>
</ol>

<hr />

<h2 id="7-the-future-reasoning-tokens">7. The Future: Reasoning Tokens</h2>

<p>We are on the cusp of models that “think” a new way.
Rumors of OpenAI’s “Project Strawberry” (Q<em>) suggest models that perform internal tree-search optimization (like AlphaGo) *during inference</em> before outputting a single token.</p>

<ul>
  <li><em>Current:</em> Inference is constant time (proportional to output length).</li>
  <li><em>Future:</em> Inference time is variable. “Take 10 seconds to think about this chess move.”</li>
</ul>

<p>This will transform agents from “Fast Guessers” to “Deep Thinkers,” potentially solving the “Zero-Shot Reliability” problem.</p>

<h2 id="8-summary">8. Summary</h2>

<p>The LLM is the <strong>Cognitive Substrate</strong> of the agent.</p>

<ul>
  <li><strong>Steerability</strong> ensures it listens to you.</li>
  <li><strong>Reasoning</strong> ensures it can plan.</li>
  <li><strong>Function Calling</strong> ensures it can act.</li>
  <li><strong>Context</strong> ensures it remembers.</li>
</ul>

<p>Building an agent starts with selecting the right model for the complexity of your task. A coding agent needs Claude 3.5 or GPT-4o. A classification agent might do fine with Llama-3-8B. The art is in matching the brain size to the problem size.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#benchmarks" class="page__taxonomy-item p-category" rel="tag">benchmarks</a><span class="sep">, </span>
    
      <a href="/tags/#cot" class="page__taxonomy-item p-category" rel="tag">cot</a><span class="sep">, </span>
    
      <a href="/tags/#deep-learning" class="page__taxonomy-item p-category" rel="tag">deep-learning</a><span class="sep">, </span>
    
      <a href="/tags/#llm" class="page__taxonomy-item p-category" rel="tag">llm</a><span class="sep">, </span>
    
      <a href="/tags/#reasoning" class="page__taxonomy-item p-category" rel="tag">reasoning</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ai-agents" class="page__taxonomy-item p-category" rel="tag">ai-agents</a>
    
    </span>
  </p>


        
      </footer>

      

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=LLM+Capabilities+for+Agents%20https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0002-llm-capabilities-for-agents%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0002-llm-capabilities-for-agents%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ai-agents/0002-llm-capabilities-for-agents/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ai-agents/0001-what-are-ai-agents/" class="pagination--pager" title="What are AI Agents?">Previous</a>
    
    
      <a href="/ai-agents/0003-prompt-engineering-for-agents/" class="pagination--pager" title="Prompt Engineering for Agents">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
