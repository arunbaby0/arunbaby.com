<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Video Analysis &amp; Real-World Perception - Arun Baby</title>
<meta name="description" content="“Time: The 4th Dimension of Vision.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Video Analysis &amp; Real-World Perception">
<meta property="og:url" content="https://www.arunbaby.com/ai-agents/0025-video-analysis/">


  <meta property="og:description" content="“Time: The 4th Dimension of Vision.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Video Analysis &amp; Real-World Perception">
  <meta name="twitter:description" content="“Time: The 4th Dimension of Vision.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ai-agents/0025-video-analysis/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-16T08:28:35+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ai-agents/0025-video-analysis/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="">
  
  <div class="sidebar sticky">
  
    


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="https://www.arunbaby.com/">
        <img src="/assets/images/profile-photo.png" alt="Arun Baby" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="https://www.arunbaby.com/" itemprop="url">Arun Baby</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Becoming <strong>Unlabelable</strong></p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">India</span>
        </li>
      

      
        
          
            <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
            <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i><span class="label">Google Scholar</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Video Analysis &amp; Real-World Perception">
    <meta itemprop="description" content="“Time: The 4th Dimension of Vision.”">
    <meta itemprop="datePublished" content="2025-12-16T08:28:35+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ai-agents/0025-video-analysis/" itemprop="url">Video Analysis &amp; Real-World Perception
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction-from-static-to-dynamic">1. Introduction: From Static to Dynamic</a></li><li><a href="#2-the-computational-cost-of-time">2. The Computational Cost of Time</a><ul><li><a href="#21-strategy-keyframe-extraction-smart-sampling">2.1 Strategy: Keyframe Extraction (Smart Sampling)</a></li><li><a href="#22-strategy-visual-embeddings-video-llama">2.2 Strategy: Visual Embeddings (Video-LLaMA)</a></li></ul></li><li><a href="#3-object-tracking-deva--sort">3. Object Tracking (DEVA / SORT)</a><ul><li><a href="#31-the-logic-detection-vs-tracking">3.1 The Logic: Detection vs Tracking</a></li><li><a href="#32-algorithms-sort-and-deva">3.2 Algorithms: SORT and DEVA</a></li></ul></li><li><a href="#4-3d-perception-and-depth">4. 3D Perception and Depth</a><ul><li><a href="#41-monocular-depth-estimation">4.1 Monocular Depth Estimation</a></li><li><a href="#42-slam-simultaneous-localization-and-mapping">4.2 SLAM (Simultaneous Localization and Mapping)</a></li></ul></li><li><a href="#5-deployment-the-edge-vs-cloud-trade-off">5. Deployment: The Edge vs. Cloud Trade-off</a></li><li><a href="#6-summary">6. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Time: The 4th Dimension of Vision.”</strong></p>

<h2 id="1-introduction-from-static-to-dynamic">1. Introduction: From Static to Dynamic</h2>

<p>So far, we have looked at Computer Vision as a series of snapshots. “Here is a screenshot—find the button.” “Here is a photo—find the cat.”
But the real world is <strong>Temporal</strong>. Reality does not exist in a single JPEG; it flows.</p>

<p>For an agent to operate in the physical world (Robotics, Autonomous Driving, Security) or the digital video world (YouTube Analysis), it must understand <strong>Time</strong>.</p>
<ul>
  <li><strong>Action Recognition:</strong> A static image of a person with their hand raised is ambiguous. Are they waving hello? Are they hailing a taxi? Are they stretching? You need the <em>previous</em> 10 frames to know the motion vector.</li>
  <li><strong>Physics Prediction:</strong> If a ball is in the air, where will it land? You need to observe its trajectory across frames to estimate velocity and gravity.</li>
  <li><strong>Causality:</strong> “The cup fell <em>because</em> the cat pushed it.” Causality requires observing the sequence <code class="language-plaintext highlighter-rouge">Event A -&gt; Event B</code>.</li>
</ul>

<p>In this final post of the Vision module, we will explore the architecture of <strong>Video Agents</strong>: How to process massive streams of visual data efficiently, track objects across time, and perceive depth in a 3D world.</p>

<hr />

<h2 id="2-the-computational-cost-of-time">2. The Computational Cost of Time</h2>

<p>Video is the heaviest modality in AI.</p>
<ul>
  <li><strong>Math:</strong>
    <ul>
      <li>1 Image (1080p) $\approx$ 6MB uncompressed.</li>
      <li>1 Second of Video (30fps) $\approx$ 180MB raw data.</li>
      <li>1 Hour of Video $\approx$ 600GB raw data.</li>
      <li>Even compressed (H.264), it is massive.</li>
    </ul>
  </li>
</ul>

<p>We cannot simply feed a raw 30fps video stream into a Vision Transformer or GPT-4V.</p>
<ul>
  <li><strong>Cost:</strong> GPT-4V charges ~$0.01 per frame. Processing 1 second (30 frames) would cost $0.30. Processing 1 minute would cost $18. This is economically impossible for most use cases.</li>
  <li><strong>Latency:</strong> Processing 30 images takes ~30 seconds of compute. The agent would lag behind reality instantly.</li>
</ul>

<h3 id="21-strategy-keyframe-extraction-smart-sampling">2.1 Strategy: Keyframe Extraction (Smart Sampling)</h3>
<p>The most common technique is to <em>not</em> watch the video. We watch a summary.</p>
<ul>
  <li><strong>Uniform Sampling:</strong> Only take 1 frame every second (1 FPS). For most human actions (walking, cooking), 1 FPS captures enough semantics.</li>
  <li><strong>Content Adaptive Sampling:</strong> We compare Frame $T$ and Frame $T-1$. We calculate the <strong>Pixel Difference</strong> (or Histogram Difference).
    <ul>
      <li>If <code class="language-plaintext highlighter-rouge">Diff &lt; Threshold</code>: The scene is static (e.g., an empty hallway). Discard Frame $T$.</li>
      <li>If <code class="language-plaintext highlighter-rouge">Diff &gt; Threshold</code>: Something moved. Keep Frame $T$.</li>
      <li><em>Result:</em> A Security Camera Agent might process 0 frames for 2 hours, then burst into processing 10 FPS when a person walks in.</li>
    </ul>
  </li>
</ul>

<h3 id="22-strategy-visual-embeddings-video-llama">2.2 Strategy: Visual Embeddings (Video-LLaMA)</h3>
<p>Instead of Tokenizing pixels (which is verbose), we use specialized <strong>Video Encoders</strong> (like VideoMAE or X-CLIP).</p>
<ul>
  <li><strong>Input:</strong> A 2-second clip (60 frames).</li>
  <li><strong>Mechanism:</strong> These models use “Spatiotemporal Attention”. They compress the redundancy. (The wall background didn’t change for 60 frames, so don’t encode it 60 times).</li>
  <li><strong>Output:</strong> A single dense vector representing “A man walking a dog”.</li>
  <li>The LLM reasoning happens on these high-level vectors, allowing it to “watch” hours of video by processing just a sequence of compressed vectors.</li>
</ul>

<hr />

<h2 id="3-object-tracking-deva--sort">3. Object Tracking (DEVA / SORT)</h2>

<p>In a single image, we detect “Car”. In the next image, we detect “Car”.
How does the agent know it is the <em>same</em> car?
This is the <strong>Identity Persistence</strong> problem. Without it, an agent counting cars on a highway would count the same car 30 times as it drives across 30 frames.</p>

<h3 id="31-the-logic-detection-vs-tracking">3.1 The Logic: Detection vs Tracking</h3>
<ul>
  <li><strong>Detection (YOLO):</strong> “There is a car at (100, 100).” (Independent per frame).</li>
  <li><strong>Tracking:</strong> “Object ID #45 (Car) moved from (100, 100) to (110, 100).”</li>
</ul>

<h3 id="32-algorithms-sort-and-deva">3.2 Algorithms: SORT and DEVA</h3>
<ol>
  <li><strong>SORT (Simple Online and Realtime Tracking):</strong>
    <ul>
      <li><strong>IOU Matching:</strong> If Box A in Frame 1 overlaps 90% with Box B in Frame 2, they are likely the same object.</li>
      <li><strong>Kalman Filter:</strong> This is the magic. It uses physics. If the car was moving Right at 10px/frame, the filter <em>predicts</em> where the box <em>should</em> be in Frame 3.</li>
      <li><em>Occlusion Handling:</em> If the car drives behind a tree (detection fails), the Kalman filter still “hallucinates” the predicted box for a few frames. When the car re-emerges, the tracker re-locks onto it, preserving ID #45.</li>
    </ul>
  </li>
  <li><strong>DEVA (Decoupled Video Segmentation):</strong>
    <ul>
      <li>A modern foundation model that propagates “Masks” across time.</li>
      <li><em>Usage:</em> You click a person in Frame 1 (SAM prompt). DEVA automatically segments that person in the next 1,000 frames using optical flow, even as they turn around or change shape.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="4-3d-perception-and-depth">4. 3D Perception and Depth</h2>

<p>Our cameras detect 2D pixels (<code class="language-plaintext highlighter-rouge">x, y, color</code>). But the world is 3D (<code class="language-plaintext highlighter-rouge">x, y, z</code>).
Information is lost during this projection.</p>
<ul>
  <li><em>Illusion:</em> A small toy car 1 meter away looks identical to a real car 50 meters away.</li>
  <li><em>Danger:</em> An autonomous drone agent needs to know the difference, or it will crash.</li>
</ul>

<h3 id="41-monocular-depth-estimation">4.1 Monocular Depth Estimation</h3>
<p>Humans use two eyes (Stereo) for depth. Can a single camera see depth?
Yes, using context. If a car is “floating” above the horizon line, it’s far away.
Models like <strong>Depth Anything</strong> or <strong>MiDaS</strong> are trained to predict a <strong>Depth Map</strong> (a heatmap where color = distance) from a single RGB image.</p>
<ul>
  <li><em>Agent Input:</em> RGB Image.</li>
  <li><em>Process:</em> Run Depth Anything.</li>
  <li><em>Agent Logic:</em> “The Pixels for the cup have a depth value of 0.5m. The pixels for the wall have 3.0m. Therefore the cup is reachable.”</li>
</ul>

<h3 id="42-slam-simultaneous-localization-and-mapping">4.2 SLAM (Simultaneous Localization and Mapping)</h3>
<p>For moving agents (drones/robots), sensing isn’t enough. They need a Map.
<strong>vSLAM</strong> (Visual SLAM) algorithms process the video stream to build a persistent 3D Point Cloud of the environment.</p>
<ul>
  <li>The agent builds a mental model: “I am at coordinate <code class="language-plaintext highlighter-rouge">(x,y,z)</code> in the Kitchen.”</li>
  <li>This allows “Memory of Place”. If you tell the agent “Go to the fridge”, it knows the 3D coordinates of the fridge from a previous conceptual map building session.</li>
</ul>

<hr />

<h2 id="5-deployment-the-edge-vs-cloud-trade-off">5. Deployment: The Edge vs. Cloud Trade-off</h2>

<p>Video Agents represent the extreme edge of “Edge AI”.</p>
<ul>
  <li><strong>Bandwidth:</strong> You physically cannot stream 6 cameras of 4K video to AWS. The uplink bandwidth doesn’t exist.</li>
  <li><strong>Latency:</strong> A self-driving car cannot wait 500ms for the cloud to say “Stop”.</li>
  <li><strong>Privacy:</strong> People do not want their home security feeds or factory floor footage sent to OpenAI.</li>
</ul>

<p><strong>The Hybrid Architecture:</strong></p>
<ul>
  <li><strong>The Edge Device (NVIDIA Jetson / Orin / Raspberry Pi):</strong>
    <ul>
      <li>Runs the high-frequency loops.</li>
      <li>Runs YOLO (Detection) and SORT (Tracking) at 30 FPS.</li>
      <li>Runs simple logic: “If Person enters Red Zone…”</li>
    </ul>
  </li>
  <li><strong>The Cloud (LLM):</strong>
    <ul>
      <li>Receives only <strong>Events</strong> and <strong>Snapshots</strong>.</li>
      <li><em>Edge Trigger:</em> “Person detected.” -&gt; Sends 1 JPG to Cloud.</li>
      <li><em>Cloud Reasoning:</em> “Is this person wearing a uniform? Yes. Ignore.”</li>
    </ul>
  </li>
</ul>

<p>This split—Perception on Edge, Reasoning in Cloud—is the only viable way to build scalable Video Agents today.</p>

<hr />

<h2 id="6-summary">6. Summary</h2>

<p>Video Analysis adds the dimension of Time to our agents, enabling them to understand Cause, Effect, and Physics.</p>

<ul>
  <li><strong>Sampling:</strong> Smart keyframing is essential to manage data volume.</li>
  <li><strong>Tracking:</strong> Assigns identities to objects across time using IOU and Kalman Filters.</li>
  <li><strong>Depth:</strong> Recovers the missing Z-axis using Monocular Depth estimation.</li>
  <li><strong>Edge AI:</strong> The necessary infrastructure to run these heavy perception loops locally.</li>
</ul>

<p>This concludes the <strong>Vision</strong> module of our curriculum. We have given our agents Eyes (ViT), Semantic Understanding (CLIP), UI Reading (OmniParser), Detection (YOLO), and Temporal Perception (Video).
Our agents can now Talk, Listen, and See. In the next section, we move to <strong>Orchestration &amp; Reliability</strong> (Days 26-30), focusing on how to make these complex systems robust in production.</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#autonomous-systems" class="page__taxonomy-item p-category" rel="tag">autonomous-systems</a><span class="sep">, </span>
    
      <a href="/tags/#computer-vision" class="page__taxonomy-item p-category" rel="tag">computer-vision</a><span class="sep">, </span>
    
      <a href="/tags/#object-tracking" class="page__taxonomy-item p-category" rel="tag">object-tracking</a><span class="sep">, </span>
    
      <a href="/tags/#spatial-awareness" class="page__taxonomy-item p-category" rel="tag">spatial-awareness</a><span class="sep">, </span>
    
      <a href="/tags/#temporal-consistency" class="page__taxonomy-item p-category" rel="tag">temporal-consistency</a><span class="sep">, </span>
    
      <a href="/tags/#video-processing" class="page__taxonomy-item p-category" rel="tag">video-processing</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ai-agents" class="page__taxonomy-item p-category" rel="tag">ai-agents</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0025-validate-bst/" rel="permalink">Validate Binary Search Tree
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The gatekeeper of data integrity. How do we ensure our sorted structures are actually sorted?
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0025-model-monitoring-systems/" rel="permalink">Model Monitoring Systems
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          15 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The silent killer of ML models is not a bug in the code, but a change in the world.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0025-speech-quality-monitoring/" rel="permalink">Speech Quality Monitoring
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          14 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">How do we know if the audio sounds “good” without asking a human?
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Video+Analysis+%26+Real-World+Perception%20https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0025-video-analysis%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0025-video-analysis%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ai-agents/0025-video-analysis/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ai-agents/0024-object-detection-segmentation/" class="pagination--pager" title="Object Detection &amp; Segmentation (YOLO, SAM)">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
