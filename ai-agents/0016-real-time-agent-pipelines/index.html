<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Real-Time Agent Pipelines - Arun Baby</title>
<meta name="description" content="“Speed is not a feature. Speed is the product.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Real-Time Agent Pipelines">
<meta property="og:url" content="https://www.arunbaby.com/ai-agents/0016-real-time-agent-pipelines/">


  <meta property="og:description" content="“Speed is not a feature. Speed is the product.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Real-Time Agent Pipelines">
  <meta name="twitter:description" content="“Speed is not a feature. Speed is the product.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ai-agents/0016-real-time-agent-pipelines/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-26T21:44:08+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ai-agents/0016-real-time-agent-pipelines/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Real-Time Agent Pipelines">
    <meta itemprop="description" content="“Speed is not a feature. Speed is the product.”">
    <meta itemprop="datePublished" content="2025-12-26T21:44:08+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ai-agents/0016-real-time-agent-pipelines/" itemprop="url">Real-Time Agent Pipelines
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction-the-200ms-wall">1. Introduction: The 200ms Wall</a></li><li><a href="#2-the-mechanics-of-latency-where-does-the-time-go">2. The Mechanics of Latency: Where does the time go?</a><ul><li><a href="#21-the-waterfall-serial-execution">2.1 The Waterfall (Serial Execution)</a></li><li><a href="#22-streaming-pipelining-parallel-execution">2.2 Streaming Pipelining (Parallel Execution)</a></li></ul></li><li><a href="#3-protocols-the-transport-layer">3. Protocols: The Transport Layer</a><ul><li><a href="#31-httprest-the-bad">3.1 HTTP/REST (The Bad)</a></li><li><a href="#32-server-sent-events-sse-the-okay">3.2 Server-Sent Events (SSE) (The Okay)</a></li><li><a href="#33-websockets-the-king">3.3 WebSockets (The King)</a></li><li><a href="#34-webrtc-the-god-tier">3.4 WebRTC (The God Tier)</a></li></ul></li><li><a href="#4-asyncio-the-python-backend">4. Asyncio: The Python Backend</a><ul><li><a href="#41-the-event-loop-mechanics-single-threaded-concurrency">4.1 The Event Loop Mechanics: Single-Threaded Concurrency</a></li><li><a href="#42-handling-streams-in-python-async-generators">4.2 Handling Streams in Python: Async Generators</a></li></ul></li><li><a href="#5-the-silent-killer-backpressure">5. The Silent Killer: Backpressure</a></li><li><a href="#6-architecture-pattern-the-actor-model-orchestrator">6. Architecture Pattern: The Actor Model (Orchestrator)</a></li><li><a href="#7-scaling-websockets-the-redis-pubsub-layer">7. Scaling WebSockets: The Redis Pub/Sub Layer</a></li><li><a href="#8-code-a-fastapi-websocket-agent">8. Code: A FastApi WebSocket Agent</a></li><li><a href="#9-summary">9. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“Speed is not a feature. Speed is the product.”</strong></p>

<h2 id="1-introduction-the-200ms-wall">1. Introduction: The 200ms Wall</h2>

<p>Historically, agent development focused on reasoning. Agents accepted a prompt, thought for 10 seconds, searched a vector database, formulated a plan, and eventually replied. For a Chatbot, an Email Assistant, or a Code Generator, 10 seconds is acceptable. The user is accustomed to waiting for a “complex” result.</p>

<p>However, for a <strong>Voice Agent</strong>, a <strong>Trading Agent</strong>, or a <strong>Real-Time Gaming Agent</strong>, 10 seconds is not just bad; it is broken.</p>

<p>Human conversation flows with a typical gap of <strong>200 milliseconds</strong> between speakers. This is the physiological “Turn-Taking Threshold”.</p>
<ul>
  <li><strong>200ms:</strong> A natural interface. The magic number for “Instant”. This is the speed of a witty comeback or a simple acknowledgement (“Yeah”, “Uh-huh”).</li>
  <li><strong>500ms:</strong> Noticeable lag, but acceptable for internet calls (Zoom/Teams/Meet). We have trained ourselves to tolerate this “Satellite Delay” in the post-COVID era.</li>
  <li><strong>1000ms+:</strong> Frustrating. Users start interrupting (“Hello? Are you there?”). This breaks the illusion of presence. The user assumes the connection has dropped or the agent has crashed.</li>
</ul>

<p>Building Real-Time Agents requires a complete paradigm shift from “Request-Response” (REST) to <strong>Event-Driven Streaming</strong> (WebSockets). It requires moving the metric of success from “Accuracy” to “Latency Jitter”. It requires us to abandon the comfort of blocking code for the complexity of asynchronous event loops. In this post, we will engineer the pipeline for ultra-low latency intelligence.</p>

<hr />

<h2 id="2-the-mechanics-of-latency-where-does-the-time-go">2. The Mechanics of Latency: Where does the time go?</h2>

<p>To solve latency, we must first understand it. Let’s dissect a typical Agent turn in a naive implementation to see where the seconds bleed away.</p>

<h3 id="21-the-waterfall-serial-execution">2.1 The Waterfall (Serial Execution)</h3>
<p>Imagine a standard HTTP pipeline implemented by a junior engineer:</p>

<ol>
  <li><strong>Transport (Network Upload):</strong> User sends audio -&gt; Server.
    <ul>
      <li><em>Time:</em> 50ms (on Fiber) to 500ms (on 3G).</li>
      <li><em>Constraint:</em> The server waits for the <em>entire</em> audio file to arrive before doing anything.</li>
    </ul>
  </li>
  <li><strong>Transcription (STT):</strong> Server receives the file. Calls OpenAI Whisper API.
    <ul>
      <li><em>Time:</em> 300ms (for a short sentence) to 2000ms (for a long monologue).</li>
      <li><em>Constraint:</em> The STT engine processes the whole file at once.</li>
    </ul>
  </li>
  <li><strong>Inference (LLM):</strong> STT Text -&gt; LLM. The LLM receives the prompt. It “thinks” (Time to First Token). It then generates the full response.
    <ul>
      <li><em>Time:</em> 2000ms.</li>
      <li><em>Constraint:</em> The system waits for the <em>entire</em> text response to be generated before moving to the next step.</li>
    </ul>
  </li>
  <li><strong>Synthesis (TTS):</strong> LLM Text -&gt; TTS Engine. The engine generates the audio waveform.
    <ul>
      <li><em>Time:</em> 500ms.</li>
      <li><em>Constraint:</em> The system waits for the <em>entire</em> audio file to be generated.</li>
    </ul>
  </li>
  <li><strong>Transport (Network Download):</strong> Server -&gt; User.
    <ul>
      <li><em>Time:</em> 50ms.</li>
    </ul>
  </li>
</ol>

<p><strong>Total:</strong> $50 + 300 + 2000 + 500 + 50 = 2900ms$ (2.9 seconds).
This is nearly 3 seconds of dead air. In a voice conversation, 3 seconds is an eternity. It is the awkward silence that kills the vibe.</p>

<h3 id="22-streaming-pipelining-parallel-execution">2.2 Streaming Pipelining (Parallel Execution)</h3>
<p>To get to sub-500ms, we cannot execute these steps sequentially. We must <strong>Pipeline</strong> them. We must treat data as a continuous river, not a series of buckets. This is often called <strong>Optimistic Execution</strong>.</p>

<ul>
  <li><strong>User:</strong> Speaks “Hello”.</li>
  <li><strong>STT (Stream):</strong> The server processes audio chunks as they arrive.</li>
  <li>T=10ms: “H”</li>
  <li>T=20ms: “Hel”</li>
  <li>T=30ms: “Hello”</li>
  <li><strong>LLM (Stream):</strong> The LLM receives “Hello”. It <strong>Immediately</strong> starts generating tokens. It does not wait for the sentence to be grammatically complete (though sometimes it should, which we will discuss in VAD).</li>
  <li><strong>LLM Output (Stream):</strong></li>
  <li>T=100ms: Generates “Hi” (Token 1).</li>
  <li>T=150ms: Generates “ there” (Token 2).</li>
  <li><strong>TTS (Stream):</strong> The TTS engine receives “Hi”. It immediately synthesizes those phonemes into audio bytes.</li>
  <li><strong>Network (Stream):</strong> The server pushes the audio bytes for “Hi” to the user.</li>
  <li>
    <p><strong>User:</strong> The user hears “Hi” while the LLM is <em>still generating</em> the rest of the sentence (“ there, how are you?”).</p>
  </li>
  <li><strong>Latency Cost:</strong> We only pay the “Time to First Token” (TTFT) of each stage, not the full processing time.</li>
  <li><strong>New Total:</strong> $50 (Net) + 200 (STT-Partial) + 300 (LLM-TTFT) + 100 (TTS-Partial) = 650ms$.</li>
  <li><em>Result:</em> The user perceives an instant response.</li>
</ul>

<hr />

<h2 id="3-protocols-the-transport-layer">3. Protocols: The Transport Layer</h2>

<p>How do we move bits this fast over the public internet? The choice of protocol determines your baseline physics.</p>

<h3 id="31-httprest-the-bad">3.1 HTTP/REST (The Bad)</h3>
<ul>
  <li><em>Architecture:</em> Universal Request-Response. Client sends request, waits, server sends response.</li>
  <li><em>Overhead:</em> TCP 3-Way Handshake + TLS Handshake on <em>every</em> interaction. This adds ~100ms RTT just to say “Hello”.</li>
  <li><em>Headers:</em> Every HTTP request carries bulky headers (Cookies, User-Agent, Accept-Encoding). Sending a 1KB audio chunk with 2KB of headers is inefficient (33% efficiency).</li>
  <li><em>Verdict:</em> Unusable for real-time audio. Good only for control signals (e.g. “Create Room”).</li>
</ul>

<h3 id="32-server-sent-events-sse-the-okay">3.2 Server-Sent Events (SSE) (The Okay)</h3>
<ul>
  <li><em>Architecture:</em> Client opens one HTTP connection (<code class="language-plaintext highlighter-rouge">Response-Type: text/event-stream</code>). Server keeps it open and pushes text events (<code class="language-plaintext highlighter-rouge">data: ...</code>) whenever it wants.</li>
  <li><em>Pros:</em> Great for Text Chat (ChatGPT uses this). Simple (Standard HTTP). Firewalls love it. Automatic reconnection logic in browsers.</li>
  <li><em>Cons:</em> <strong>Unidirectional.</strong> Server -&gt; Client only. If the user interrupts, the client cannot use the same connection to send the interruption signal. It must open a <em>new</em> POST request to send data. This adds 1x RTT (Round Trip Time) latency to the interruption, making the agent feel “deaf” for a split second.</li>
</ul>

<h3 id="33-websockets-the-king">3.3 WebSockets (The King)</h3>
<ul>
  <li><em>Architecture:</em> Starts as HTTP, then “Upgrades” to raw TCP. Full Duplex. Both sides send binary/text frames at any time exactly when they happen.</li>
  <li><em>Pros:</em> Lowest overhead. Ideal for Audio/Video streams. No headers per frame.</li>
  <li><em>Cons:</em> <strong>Stateful</strong>. You cannot use standard Load Balancers (Round Robin) easily because the client is “sticky” to one server process. If that server dies, the call drops. Implementing “sticky sessions” or a “router layer” is required for scale.</li>
  <li><em>Framing:</em> WebSockets manage “Framing” (knowing where one message ends and the next begins) for you, unlike raw TCP where you have to implement length-prefixing.</li>
</ul>

<h3 id="34-webrtc-the-god-tier">3.4 WebRTC (The God Tier)</h3>
<ul>
  <li><em>Architecture:</em> UDP (User Datagram Protocol) via RTP (Real-time Transport Protocol).</li>
  <li><em>Physics:</em> TCP (WebSockets) guarantees delivery. If a packet is lost, TCP pauses everything to re-transmit it. This causes “Lag Spikes”. UDP is “Fire and Forget”. If a packet is lost, it is lost forever.</li>
  <li><em>Audio Logic:</em> In voice, a lost packet is a 20ms glitch. A delayed packet is a 500ms lag. We prefer <strong>Glitch &gt; Lag</strong>. Therefore, UDP is superior.</li>
  <li><em>Cons:</em> Extremely complex to implement server-side (Requires DTLS encryption, ICE Candidate exchange, STUN/TURN servers for NAT traversal).</li>
  <li><em>Verdict:</em> Use WebSockets for text/simple audio agents. Use WebRTC for robust production voice (Zoom/Teams quality).</li>
</ul>

<hr />

<h2 id="4-asyncio-the-python-backend">4. Asyncio: The Python Backend</h2>

<p>Python is the language of AI, but naive Python (Flask/Django) kills real-time performance.
You <strong>must</strong> use <code class="language-plaintext highlighter-rouge">asyncio</code> (FastAPI/Quart/Litestar).</p>

<h3 id="41-the-event-loop-mechanics-single-threaded-concurrency">4.1 The Event Loop Mechanics: Single-Threaded Concurrency</h3>
<p>In a blocking server (e.g., Flask with Sync Workers):</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">handle_request</span><span class="p">():</span>
 <span class="n">audio</span> <span class="o">=</span> <span class="nf">receive_audio</span><span class="p">()</span> <span class="c1"># BLOCKS the thread for 2 seconds waiting for network
</span> <span class="n">text</span> <span class="o">=</span> <span class="nf">transcribe</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span> <span class="c1"># BLOCKS the thread for processing
</span> <span class="k">return</span> <span class="n">text</span>
</code></pre></div></div>
<p>If you have 10 users, you need 10 threads. 1000 users = 1000 threads. The OS spends significant CPU time just context-switching between threads (saving/loading registers). You hit the “C10K Problem” (Connecting 10k users) very quickly.</p>

<p>In <code class="language-plaintext highlighter-rouge">asyncio</code>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">handle_request</span><span class="p">():</span>
 <span class="n">audio</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">receive_audio</span><span class="p">()</span> <span class="c1"># Yields control.
</span> <span class="n">text</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">transcribe</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>
 <span class="k">return</span> <span class="n">text</span>
</code></pre></div></div>
<p>Here, <code class="language-plaintext highlighter-rouge">await</code> is a magical keyword. It tells the Event Loop: <em>“I am waiting for IO (Network/Disk). I release the CPU. Go do work for someone else. Wake me up when the data arrives.”</em>
One single thread can handle 10,000 connections because at any given millisecond, 9,999 of them are just waiting for packets.</p>

<h3 id="42-handling-streams-in-python-async-generators">4.2 Handling Streams in Python: Async Generators</h3>
<p>The most elegant pattern for pipelines is the <strong>Async Generator</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">llm_stream</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
 <span class="c1"># This call initiates the stream but doesn't block the whole function
</span> <span class="n">stream</span> <span class="o">=</span> <span class="k">await</span> <span class="n">openai</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o</span><span class="sh">"</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
 <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
 <span class="n">token</span> <span class="o">=</span> <span class="n">chunk</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">delta</span><span class="p">.</span><span class="n">content</span>
 <span class="k">if</span> <span class="n">token</span><span class="p">:</span>
 <span class="k">yield</span> <span class="n">token</span> <span class="c1"># Emits token to the consumer immediately
</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">tts_stream</span><span class="p">(</span><span class="n">text_iterator</span><span class="p">):</span>
 <span class="nb">buffer</span> <span class="o">=</span> <span class="sh">""</span>
 <span class="k">async</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">text_iterator</span><span class="p">:</span>
 <span class="nb">buffer</span> <span class="o">+=</span> <span class="n">token</span>
 <span class="c1"># Heuristic: Send to TTS only on sentence boundaries or commas
</span> <span class="c1"># This optimizes for audio quality (intonation) vs latency
</span> <span class="k">if</span> <span class="nf">ends_sentence</span><span class="p">(</span><span class="nb">buffer</span><span class="p">):</span>
 <span class="n">audio</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">synthesize</span><span class="p">(</span><span class="nb">buffer</span><span class="p">)</span>
 <span class="k">yield</span> <span class="n">audio</span>
 <span class="nb">buffer</span> <span class="o">=</span> <span class="sh">""</span>

<span class="c1"># The Pipeline
</span><span class="k">async</span> <span class="k">def</span> <span class="nf">handle_websocket</span><span class="p">(</span><span class="n">ws</span><span class="p">):</span>
 <span class="k">await</span> <span class="n">ws</span><span class="p">.</span><span class="nf">accept</span><span class="p">()</span>
 <span class="n">prompt</span> <span class="o">=</span> <span class="k">await</span> <span class="n">ws</span><span class="p">.</span><span class="nf">receive_text</span><span class="p">()</span>

 <span class="c1"># We pipe the generators together like Unix pipes
</span> <span class="c1"># Data flows: Network -&gt; iterator -&gt; iterator -&gt; Network
</span> <span class="n">text_stream</span> <span class="o">=</span> <span class="nf">llm_stream</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
 <span class="n">audio_stream</span> <span class="o">=</span> <span class="nf">tts_stream</span><span class="p">(</span><span class="n">text_stream</span><span class="p">)</span>

 <span class="k">async</span> <span class="k">for</span> <span class="n">audio_chunk</span> <span class="ow">in</span> <span class="n">audio_stream</span><span class="p">:</span>
 <span class="k">await</span> <span class="n">ws</span><span class="p">.</span><span class="nf">send_bytes</span><span class="p">(</span><span class="n">audio_chunk</span><span class="p">)</span>
</code></pre></div></div>

<p>This code is <strong>lazy</strong>. No processing happens until the final <code class="language-plaintext highlighter-rouge">async for</code> loop pulls data properly.</p>

<hr />

<h2 id="5-the-silent-killer-backpressure">5. The Silent Killer: Backpressure</h2>

<p>When you pipeline systems, speed mismatches occur. This leads to the <strong>Producer-Consumer Problem</strong>.</p>
<ul>
  <li><strong>LLM Generation:</strong> 50 tokens/sec (Fast).</li>
  <li><strong>TTS Synthesis:</strong> Real-time (1 sec audio takes 1 sec to play).</li>
  <li><strong>Network:</strong> Variable (Cellular 4G implies jitter).</li>
  <li><strong>Client Playback:</strong> Real-time.</li>
</ul>

<p><strong>Scenario:</strong>
The LLM generates a 3-minute speech in 5 seconds. It pushes all that text to the TTS. The TTS generates 3 minutes of audio in 30 seconds. It pushes all that audio to the WebSocket buffer.</p>
<ol>
  <li><strong>Memory Spike:</strong> Server RAM fills up with buffered audio.</li>
  <li><strong>Latency Increase (The Death Spiral):</strong> The user tries to interrupt (“Stop!”). The server receives “Stop”. It stops the LLM. <em>But the network buffer is already filled with 3 minutes of audio.</em> TCP guarantees delivery. The user hears the agent talking for 3 minutes <em>after</em> they shouted “Stop!”.</li>
</ol>

<p><strong>Solution: Flow Control (Backpressure)</strong></p>
<ul>
  <li><strong>Buffer Cap:</strong> Limit the output queue to 5 chunks. If the queue is full, <code class="language-plaintext highlighter-rouge">await</code> (pause) the producer.
 <code class="language-plaintext highlighter-rouge">await output_queue.put(audio)</code> -&gt; This line blocks if queue is full. This naturally slows down the LLM to match the Network speed.</li>
  <li><strong>Clearing:</strong> On interruption (Barge-In), you must support a <code class="language-plaintext highlighter-rouge">clear_buffer()</code> command.</li>
  <li>This requires accessing the internal buffer of the WebSocket or simply sending a control signal to the client: <code class="language-plaintext highlighter-rouge">"IGNORE_PREVIOUS_AUDIO"</code>.</li>
  <li>Server-side, you drop all pending items in the <code class="language-plaintext highlighter-rouge">output_queue</code>.</li>
</ul>

<hr />

<h2 id="6-architecture-pattern-the-actor-model-orchestrator">6. Architecture Pattern: The Actor Model (Orchestrator)</h2>

<p>For a robust pipeline, don’t write one monolithic <code class="language-plaintext highlighter-rouge">handle_websocket</code> function. Use an <strong>Orchestrator Pattern</strong>. Break every component into an independent “Worker” that communicates via Queues.</p>

<ul>
  <li><strong>Input Queue:</strong> Receives Audio Chunks from WS.</li>
  <li><strong>VAD Worker:</strong></li>
  <li>Consumes Input.</li>
  <li>Detects “Speech Start” -&gt; Emits “User Speaking” Event.</li>
  <li>Detects “Speech End” -&gt; Emits “User Utterance” Event.</li>
  <li><strong>STT Worker:</strong> Consumes Utterance. Emits Text.</li>
  <li><strong>Agent Brain:</strong> Consumes Text. Maintains State. Decides Action (RAG/Tool). Emits Start Token.</li>
  <li><strong>TTS Worker:</strong> Consumes Tokens. Emits Audio.</li>
  <li><strong>Output Queue:</strong> Sends to WebSocket.</li>
</ul>

<p>This decoupled architecture allows <strong>Complex State Management</strong>.</p>
<ul>
  <li><em>Scenario:</em> User speaks while Agent is talking.</li>
  <li><em>VAD Worker:</em> Detects speech energy. Sends <code class="language-plaintext highlighter-rouge">INTERRUPT</code> event.</li>
  <li><em>Orchestrator:</em></li>
  <li>Calls <code class="language-plaintext highlighter-rouge">TTS_Worker.cancel()</code></li>
  <li>Calls <code class="language-plaintext highlighter-rouge">LLM_Worker.cancel()</code></li>
  <li>Sends <code class="language-plaintext highlighter-rouge">ws.send_json({"type": "clear_audio"})</code> to client.</li>
  <li><em>Result:</em> Agent shuts up instantly, feeling “Listening”.</li>
</ul>

<hr />

<h2 id="7-scaling-websockets-the-redis-pubsub-layer">7. Scaling WebSockets: The Redis Pub/Sub Layer</h2>

<p>One server can handle 10k connections. But what if you need 1 Million?
You need multiple servers. But WebSockets are stateful.</p>
<ul>
  <li>User A connects to Server 1.</li>
  <li>User B connects to Server 2.</li>
  <li>Agent Logic runs on Server 3.</li>
</ul>

<p>How does Agent Logic send audio to User A? It doesn’t have the socket handle.</p>

<p><strong>Solution: Redis Pub/Sub (The Message Bus)</strong></p>
<ol>
  <li><strong>Gateway Layer:</strong> Holds the WebSockets. Does nothing but forward bytes.</li>
  <li><strong>Processing Layer:</strong> Runs the AI Logic.</li>
  <li><strong>Communication:</strong>
    <ul>
      <li>User A sends audio. Gateway 1 publishes to Redis Channel <code class="language-plaintext highlighter-rouge">input:user_a</code>.</li>
      <li>Processor subscribes to <code class="language-plaintext highlighter-rouge">input:user_a</code>. Processes it.</li>
      <li>Processor publishes audio to <code class="language-plaintext highlighter-rouge">output:user_a</code>.</li>
      <li>Gateway 1 subscribes to <code class="language-plaintext highlighter-rouge">output:user_a</code>. Forwards bytes to WebSocket.</li>
      <li><em>Pros:</em> Infinite scaling. Processing layer can crash without dropping the connection.</li>
      <li><em>Cons:</em> Adds ~5-10ms latency (Redis RTT).</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="8-code-a-fastapi-websocket-agent">8. Code: A FastApi WebSocket Agent</h2>

<p>Here is a more complete implementation demonstrating the connection lifecycle and asyncio gathering.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">fastapi</span> <span class="kn">import</span> <span class="n">FastAPI</span><span class="p">,</span> <span class="n">WebSocket</span><span class="p">,</span> <span class="n">WebSocketDisconnect</span>
<span class="kn">import</span> <span class="n">asyncio</span>
<span class="kn">import</span> <span class="n">uuid</span>
<span class="kn">import</span> <span class="n">json</span>

<span class="n">app</span> <span class="o">=</span> <span class="nc">FastAPI</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">ConnectionManager</span><span class="p">:</span>
 <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="n">self</span><span class="p">.</span><span class="n">active_connections</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">WebSocket</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

 <span class="k">async</span> <span class="k">def</span> <span class="nf">connect</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">websocket</span><span class="p">:</span> <span class="n">WebSocket</span><span class="p">):</span>
 <span class="k">await</span> <span class="n">websocket</span><span class="p">.</span><span class="nf">accept</span><span class="p">()</span>
 <span class="n">self</span><span class="p">.</span><span class="n">active_connections</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">websocket</span><span class="p">)</span>

 <span class="k">def</span> <span class="nf">disconnect</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">websocket</span><span class="p">:</span> <span class="n">WebSocket</span><span class="p">):</span>
 <span class="n">self</span><span class="p">.</span><span class="n">active_connections</span><span class="p">.</span><span class="nf">remove</span><span class="p">(</span><span class="n">websocket</span><span class="p">)</span>

<span class="n">manager</span> <span class="o">=</span> <span class="nc">ConnectionManager</span><span class="p">()</span>

<span class="nd">@app.websocket</span><span class="p">(</span><span class="sh">"</span><span class="s">/ws/audio</span><span class="sh">"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">websocket_endpoint</span><span class="p">(</span><span class="n">websocket</span><span class="p">:</span> <span class="n">WebSocket</span><span class="p">):</span>
 <span class="k">await</span> <span class="n">manager</span><span class="p">.</span><span class="nf">connect</span><span class="p">(</span><span class="n">websocket</span><span class="p">)</span>
 <span class="n">session_id</span> <span class="o">=</span> <span class="nf">str</span><span class="p">(</span><span class="n">uuid</span><span class="p">.</span><span class="nf">uuid4</span><span class="p">())</span>
 <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Session </span><span class="si">{</span><span class="n">session_id</span><span class="si">}</span><span class="s"> started</span><span class="sh">"</span><span class="p">)</span>

 <span class="c1"># Queues for inter-task communication
</span> <span class="c1"># maxsize=10 provides Backpressure!
</span> <span class="n">input_queue</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nc">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

 <span class="c1"># Start the Processing Worker in background
</span> <span class="n">worker_task</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="nf">create_task</span><span class="p">(</span><span class="nf">run_agent_pipeline</span><span class="p">(</span><span class="n">session_id</span><span class="p">,</span> <span class="n">input_queue</span><span class="p">,</span> <span class="n">websocket</span><span class="p">))</span>

 <span class="k">try</span><span class="p">:</span>
 <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
 <span class="c1"># 1. Receive packet
</span> <span class="c1"># receive_bytes blocks until data arrives
</span> <span class="n">data</span> <span class="o">=</span> <span class="k">await</span> <span class="n">websocket</span><span class="p">.</span><span class="nf">receive_bytes</span><span class="p">()</span>

 <span class="c1"># Put into queue. If worker is slow, this will eventually block,
</span> <span class="c1"># signaling the network layer to stop reading (TCP Window Scaling).
</span> <span class="k">await</span> <span class="n">input_queue</span><span class="p">.</span><span class="nf">put</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

 <span class="k">except</span> <span class="n">WebSocketDisconnect</span><span class="p">:</span>
 <span class="n">manager</span><span class="p">.</span><span class="nf">disconnect</span><span class="p">(</span><span class="n">websocket</span><span class="p">)</span>
 <span class="n">worker_task</span><span class="p">.</span><span class="nf">cancel</span><span class="p">()</span> <span class="c1"># Kill the worker
</span> <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Session </span><span class="si">{</span><span class="n">session_id</span><span class="si">}</span><span class="s"> ended</span><span class="sh">"</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">run_agent_pipeline</span><span class="p">(</span><span class="n">session_id</span><span class="p">,</span> <span class="n">input_queue</span><span class="p">,</span> <span class="n">ws</span><span class="p">):</span>
 <span class="sh">"""</span><span class="s">
 Simulates the AI processing pipeline independent of the network loop.
 </span><span class="sh">"""</span>
 <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Worker for </span><span class="si">{</span><span class="n">session_id</span><span class="si">}</span><span class="s"> started</span><span class="sh">"</span><span class="p">)</span>
 <span class="k">try</span><span class="p">:</span>
 <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
 <span class="n">audio_chunk</span> <span class="o">=</span> <span class="k">await</span> <span class="n">input_queue</span><span class="p">.</span><span class="nf">get</span><span class="p">()</span>

 <span class="c1"># Simulated VAD
</span> <span class="n">is_speech</span> <span class="o">=</span> <span class="nf">process_vad</span><span class="p">(</span><span class="n">audio_chunk</span><span class="p">)</span>
 <span class="k">if</span> <span class="n">is_speech</span><span class="p">:</span>
 <span class="c1"># Logic...
</span> <span class="k">pass</span>

 <span class="c1"># Simulated TTS Output
</span> <span class="c1"># await ws.send_bytes(generated_audio)
</span>
 <span class="k">except</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">CancelledError</span><span class="p">:</span>
 <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Worker cancelled</span><span class="sh">"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">process_vad</span><span class="p">(</span><span class="n">chunk</span><span class="p">):</span>
 <span class="c1"># Mock VAD
</span> <span class="k">return</span> <span class="bp">True</span>
</code></pre></div></div>

<hr />

<h2 id="9-summary">9. Summary</h2>

<p>Real-time is the frontier of Agent UX. It is where “Software Engineering” meets “AI”. To build a great voice agent, you must stop thinking about “Models” and start thinking about “Systems”.</p>

<ul>
  <li><strong>WebSockets/WebRTC</strong> are mandatory for transport.</li>
  <li><strong>Asyncio</strong> is the mandatory runtime pattern for Python.</li>
  <li><strong>Pipelining</strong> (Streaming) hides the latency of individual components.</li>
  <li><strong>Backpressure</strong> handling ensures the agent doesn’t sound “Laggy” during interruptions.</li>
  <li><strong>Orchestration</strong> decouples the brain from the mouth.</li>
</ul>

<p>These real-time pipelines are the foundation for <strong>Voice Agents</strong>, which add layers of complexity regarding STT accuracy, Cost Analysis, and the physics of sound.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ai-agents/0016-real-time-agent-pipelines/">arunbaby.com/ai-agents/0016-real-time-agent-pipelines</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#asyncio" class="page__taxonomy-item p-category" rel="tag">asyncio</a><span class="sep">, </span>
    
      <a href="/tags/#backpressure" class="page__taxonomy-item p-category" rel="tag">backpressure</a><span class="sep">, </span>
    
      <a href="/tags/#latency" class="page__taxonomy-item p-category" rel="tag">latency</a><span class="sep">, </span>
    
      <a href="/tags/#real-time" class="page__taxonomy-item p-category" rel="tag">real-time</a><span class="sep">, </span>
    
      <a href="/tags/#sse" class="page__taxonomy-item p-category" rel="tag">sse</a><span class="sep">, </span>
    
      <a href="/tags/#streaming" class="page__taxonomy-item p-category" rel="tag">streaming</a><span class="sep">, </span>
    
      <a href="/tags/#websockets" class="page__taxonomy-item p-category" rel="tag">websockets</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ai-agents" class="page__taxonomy-item p-category" rel="tag">ai-agents</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0016-merge-intervals/" rel="permalink">Merge Intervals
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          21 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master interval processing to handle overlapping ranges—the foundation of event streams and temporal reasoning in production systems.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0016-event-stream-processing/" rel="permalink">Event Stream Processing
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Build production event stream processing systems that handle millions of events per second using windowing and temporal aggregation—applying the same interva...</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Real-Time+Agent+Pipelines%20https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0016-real-time-agent-pipelines%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0016-real-time-agent-pipelines%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ai-agents/0016-real-time-agent-pipelines/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ai-agents/0015-planning-and-decomposition/" class="pagination--pager" title="Planning and Decomposition">Previous</a>
    
    
      <a href="/ai-agents/0017-voice-agent-architecture/" class="pagination--pager" title="Voice Agent Architecture">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
