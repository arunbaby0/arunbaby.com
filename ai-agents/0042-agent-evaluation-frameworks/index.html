<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Agent Evaluation Frameworks - Arun Baby</title>
<meta name="description" content="“If you can’t measure an agent, you can’t improve it: build evals for success, safety, cost, and regressions.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Agent Evaluation Frameworks">
<meta property="og:url" content="https://www.arunbaby.com/ai-agents/0042-agent-evaluation-frameworks/">


  <meta property="og:description" content="“If you can’t measure an agent, you can’t improve it: build evals for success, safety, cost, and regressions.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Agent Evaluation Frameworks">
  <meta name="twitter:description" content="“If you can’t measure an agent, you can’t improve it: build evals for success, safety, cost, and regressions.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ai-agents/0042-agent-evaluation-frameworks/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-22T11:01:30+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ai-agents/0042-agent-evaluation-frameworks/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Agent Evaluation Frameworks">
    <meta itemprop="description" content="“If you can’t measure an agent, you can’t improve it: build evals for success, safety, cost, and regressions.”">
    <meta itemprop="datePublished" content="2025-12-22T11:01:30+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ai-agents/0042-agent-evaluation-frameworks/" itemprop="url">Agent Evaluation Frameworks
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          13 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-why-agents-need-evaluation-frameworks-not-just-tests">1. Why agents need evaluation frameworks (not just “tests”)</a></li><li><a href="#2-the-eval-mindset-define-a-contract-for-good">2. The eval mindset: define a contract for “good”</a></li><li><a href="#25-a-template-define-the-eval-contract-per-intent">2.5 A template: define the eval contract per intent</a></li><li><a href="#3-eval-types-offline-online-and-adversarial">3. Eval types: offline, online, and adversarial</a><ul><li><a href="#31-offline-evals-most-important-early">3.1 Offline evals (most important early)</a></li><li><a href="#32-online-evals-production-monitoring">3.2 Online evals (production monitoring)</a></li><li><a href="#33-adversarial-evals-security-and-robustness">3.3 Adversarial evals (security and robustness)</a></li></ul></li><li><a href="#35-practical-sampling-strategy-for-online-evals">3.5 Practical sampling strategy for online evals</a></li><li><a href="#4-what-to-score-the-four-axes-that-matter">4. What to score: the “four axes” that matter</a><ul><li><a href="#41-correctness-did-it-do-the-right-thing">4.1 Correctness (did it do the right thing?)</a></li><li><a href="#42-safety-did-it-avoid-forbidden-behavior">4.2 Safety (did it avoid forbidden behavior?)</a></li><li><a href="#43-efficiency-did-it-waste-resources">4.3 Efficiency (did it waste resources?)</a></li><li><a href="#44-robustness-does-it-work-under-stress">4.4 Robustness (does it work under stress?)</a></li></ul></li><li><a href="#45-a-concrete-scoring-schema-copypaste">4.5 A concrete scoring schema (copy/paste)</a></li><li><a href="#5-designing-an-eval-dataset-coverage-beats-size">5. Designing an eval dataset: coverage beats size</a></li><li><a href="#55-dataset-design-separate-intent-from-instance">5.5 Dataset design: separate “intent” from “instance”</a></li><li><a href="#6-scoring-methods-deterministic-checks--llm-judges">6. Scoring methods: deterministic checks + LLM judges</a><ul><li><a href="#61-deterministic-evaluators-preferred-when-possible">6.1 Deterministic evaluators (preferred when possible)</a></li><li><a href="#62-llm-as-a-judge-useful-for-fuzzy-dimensions">6.2 LLM-as-a-judge (useful for fuzzy dimensions)</a></li></ul></li><li><a href="#65-judge-calibration-how-to-keep-llm-judges-honest">6.5 Judge calibration: how to keep LLM judges honest</a></li><li><a href="#7-evaluating-tool-use-action-quality-not-just-final-output">7. Evaluating tool use: action quality, not just final output</a></li><li><a href="#75-trajectory-scoring-the-simplest-approach-that-works">7.5 Trajectory scoring: the simplest approach that works</a></li><li><a href="#8-regression-testing-pin-behavior-over-time">8. Regression testing: pin behavior over time</a></li><li><a href="#85-ci-integration-make-evals-a-first-class-build-step">8.5 CI integration: make evals a first-class build step</a></li><li><a href="#86-reducing-noise-make-your-evals-stable">8.6 Reducing noise: make your evals stable</a></li><li><a href="#9-evals-for-safety-and-security-a-must-have">9. Evals for safety and security (a must-have)</a><ul><li><a href="#91-prompt-injection-evals">9.1 Prompt injection evals</a></li><li><a href="#92-data-leakage-evals">9.2 Data leakage evals</a></li><li><a href="#93-tool-failure-evals">9.3 Tool failure evals</a></li></ul></li><li><a href="#96-secret-leakage-detection-cheap-heuristics-that-work">9.6 Secret leakage detection: cheap heuristics that work</a></li><li><a href="#95-canary-safety-evals-test-the-worst-things-first">9.5 “Canary” safety evals: test the worst things first</a></li><li><a href="#10-implementation-sketch-a-minimal-eval-runner-pseudocode">10. Implementation sketch: a minimal eval runner (pseudocode)</a></li><li><a href="#105-making-evals-reproducible-pin-models-tools-and-environments">10.5 Making evals reproducible: pin models, tools, and environments</a></li><li><a href="#106-eval-infrastructure-store-results-like-a-dataset-not-like-logs">10.6 Eval infrastructure: store results like a dataset, not like logs</a></li><li><a href="#11-case-study-evaluating-a-browsing-agent">11. Case study: evaluating a browsing agent</a></li><li><a href="#115-case-study-evaluating-an-autonomous-multi-step-agent">11.5 Case study: evaluating an autonomous multi-step agent</a></li><li><a href="#116-case-study-evaluating-a-code-execution-agent">11.6 Case study: evaluating a code execution agent</a></li><li><a href="#12-summary--junior-engineer-roadmap">12. Summary &amp; Junior Engineer Roadmap</a><ul><li><a href="#mini-project-recommended">Mini-project (recommended)</a></li><li><a href="#common-rookie-mistakes-avoid-these">Common rookie mistakes (avoid these)</a></li><li><a href="#further-reading-optional">Further reading (optional)</a></li></ul></li></ul>
            </nav>
          </aside>
        
        <p><strong>“If you can’t measure an agent, you can’t improve it: build evals for success, safety, cost, and regressions.”</strong></p>

<h2 id="1-why-agents-need-evaluation-frameworks-not-just-tests">1. Why agents need evaluation frameworks (not just “tests”)</h2>

<p>Traditional software tests check deterministic behavior. Agents are different:</p>

<ul>
  <li>outputs vary across runs (non-determinism)</li>
  <li>decisions depend on external tools (web, APIs, files)</li>
  <li>quality includes fuzzy dimensions (helpfulness, correctness, safety)</li>
</ul>

<p>So evaluation frameworks for agents must measure:</p>

<ul>
  <li><strong>task success</strong> (did it accomplish the goal?)</li>
  <li><strong>behavior quality</strong> (was reasoning sound and complete?)</li>
  <li><strong>safety</strong> (did it avoid harmful actions?)</li>
  <li><strong>cost/efficiency</strong> (how many steps and tool calls?)</li>
  <li><strong>robustness</strong> (does it hold under noise, adversarial inputs, flaky tools?)</li>
</ul>

<p>An “agent evaluation framework” is the system you use to run these checks continuously, track metrics, and prevent regressions.</p>

<hr />

<h2 id="2-the-eval-mindset-define-a-contract-for-good">2. The eval mindset: define a contract for “good”</h2>

<p>Before building eval infrastructure, define what “good” means for your agent.</p>

<p>A useful contract includes:</p>

<ul>
  <li><strong>Intent:</strong> what task category is this?</li>
  <li><strong>Requirements:</strong> what must be present in output?</li>
  <li><strong>Constraints:</strong> what must never happen?</li>
  <li><strong>Budgets:</strong> max steps, tool calls, time, tokens</li>
  <li><strong>Evidence:</strong> what counts as proof (citations, tool outputs, tests)</li>
</ul>

<p>This turns evaluation from “vibes” into “specification.”</p>

<hr />

<h2 id="25-a-template-define-the-eval-contract-per-intent">2.5 A template: define the eval contract per intent</h2>

<p>When an agent supports multiple intents, define one eval contract per intent. This prevents “one-size-fits-none” scoring.</p>

<p>Example contract (conceptual):</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"intent"</span><span class="p">:</span><span class="w"> </span><span class="s2">"browsing_answer_with_citations"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"requirements"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"contains_citations"</span><span class="p">,</span><span class="w"> </span><span class="s2">"claims_supported_by_quotes"</span><span class="p">],</span><span class="w">
  </span><span class="nl">"constraints"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"no_unsafe_tools"</span><span class="p">,</span><span class="w"> </span><span class="s2">"no_secret_leakage"</span><span class="p">],</span><span class="w">
  </span><span class="nl">"budgets"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nl">"max_steps"</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="nl">"max_tool_calls"</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">},</span><span class="w">
  </span><span class="nl">"evidence"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nl">"min_sources"</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="nl">"quote_required"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>This makes evaluation consistent and debuggable: when an eval fails, you can point to the contract field that was violated.</p>

<hr />

<h2 id="3-eval-types-offline-online-and-adversarial">3. Eval types: offline, online, and adversarial</h2>

<h3 id="31-offline-evals-most-important-early">3.1 Offline evals (most important early)</h3>

<p>Run a fixed dataset of tasks and score:</p>

<ul>
  <li>success rate</li>
  <li>cost per success</li>
  <li>safety violations</li>
</ul>

<p>Offline evals catch regressions quickly.</p>

<h3 id="32-online-evals-production-monitoring">3.2 Online evals (production monitoring)</h3>

<p>In production, you measure:</p>

<ul>
  <li>user feedback</li>
  <li>task completion metrics</li>
  <li>error rates and escalations</li>
  <li>latency and cost</li>
</ul>

<p>Online evals tell you if your system is healthy, but they’re noisy.</p>

<h3 id="33-adversarial-evals-security-and-robustness">3.3 Adversarial evals (security and robustness)</h3>

<p>Agents face adversarial inputs:</p>

<ul>
  <li>prompt injection</li>
  <li>malformed data</li>
  <li>tool failures</li>
</ul>

<p>Adversarial evals simulate those failures and verify safe behavior.</p>

<hr />

<h2 id="35-practical-sampling-strategy-for-online-evals">3.5 Practical sampling strategy for online evals</h2>

<p>You usually can’t judge every production trace. A pragmatic sampling strategy:</p>

<ul>
  <li>record <strong>100%</strong> of failures and safety blocks</li>
  <li>record <strong>100%</strong> of high-value flows (paid users, sensitive actions)</li>
  <li>record <strong>5–10%</strong> of routine successes</li>
</ul>

<p>Then run automated scoring (deterministic + judge) on the sampled traces.</p>

<p>This gives you visibility without exploding cost.</p>

<hr />

<h2 id="4-what-to-score-the-four-axes-that-matter">4. What to score: the “four axes” that matter</h2>

<h3 id="41-correctness-did-it-do-the-right-thing">4.1 Correctness (did it do the right thing?)</h3>

<p>Metrics:</p>

<ul>
  <li>exact match (for deterministic tasks)</li>
  <li>rubric score (for open-ended tasks)</li>
  <li>citation faithfulness (for browsing tasks)</li>
  <li>test pass rate (for code tasks)</li>
</ul>

<h3 id="42-safety-did-it-avoid-forbidden-behavior">4.2 Safety (did it avoid forbidden behavior?)</h3>

<p>Metrics:</p>

<ul>
  <li>policy violations per task</li>
  <li>unsafe tool call attempts</li>
  <li>secret leakage indicators</li>
  <li>write actions without approval</li>
</ul>

<h3 id="43-efficiency-did-it-waste-resources">4.3 Efficiency (did it waste resources?)</h3>

<p>Metrics:</p>

<ul>
  <li>steps per task (median/p95)</li>
  <li>tool calls per task</li>
  <li>token usage and latency</li>
  <li>retry rate</li>
</ul>

<h3 id="44-robustness-does-it-work-under-stress">4.4 Robustness (does it work under stress?)</h3>

<p>Metrics:</p>

<ul>
  <li>success under flaky tools</li>
  <li>success under noisy inputs (OCR errors, partial logs)</li>
  <li>success under adversarial prompts</li>
</ul>

<hr />

<h2 id="45-a-concrete-scoring-schema-copypaste">4.5 A concrete scoring schema (copy/paste)</h2>

<p>To keep evals consistent across tasks, use a common score object:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"overall"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
  </span><span class="nl">"scores"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"correctness"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"safety"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"efficiency"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"robustness"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0</span><span class="w">
  </span><span class="p">},</span><span class="w">
  </span><span class="nl">"violations"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
  </span><span class="nl">"notes"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
  </span><span class="nl">"artifacts"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"citations_ok"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="p">,</span><span class="w">
    </span><span class="nl">"tests_passed"</span><span class="p">:</span><span class="w"> </span><span class="kc">null</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Practical rule: <strong>any high-severity safety violation forces overall to 0</strong>, even if the answer is correct. This matches real production priorities: “correct but unsafe” is not acceptable.</p>

<hr />

<h2 id="5-designing-an-eval-dataset-coverage-beats-size">5. Designing an eval dataset: coverage beats size</h2>

<p>You don’t need 100k examples to start. You need <em>coverage</em>.</p>

<p>Include:</p>

<ul>
  <li>common tasks</li>
  <li>edge cases</li>
  <li>high-risk scenarios</li>
  <li>failure cases you’ve seen in production</li>
</ul>

<p>Practical approach:</p>

<ol>
  <li>Collect 20–50 tasks per major intent.</li>
  <li>Add 5–10 “nasty” edge cases per intent.</li>
  <li>Add adversarial cases (prompt injection, tool timeouts).</li>
</ol>

<p>As your agent ships, the dataset grows naturally from real failures.</p>

<hr />

<h2 id="55-dataset-design-separate-intent-from-instance">5.5 Dataset design: separate “intent” from “instance”</h2>

<p>Many teams accidentally build a dataset that’s too narrow because all tasks are small variations of the same thing.</p>

<p>A better structure:</p>

<ul>
  <li><strong>Intent</strong>: the category (e.g., “browsing answer with citations”)</li>
  <li><strong>Instance</strong>: a specific example within that intent</li>
</ul>

<p>Track intent-level coverage:</p>

<ul>
  <li>at least N intents</li>
  <li>at least M instances per intent</li>
  <li>at least K edge cases per intent</li>
</ul>

<p>This helps you avoid the “we improved one task and broke another” problem.</p>

<hr />

<h2 id="6-scoring-methods-deterministic-checks--llm-judges">6. Scoring methods: deterministic checks + LLM judges</h2>

<h3 id="61-deterministic-evaluators-preferred-when-possible">6.1 Deterministic evaluators (preferred when possible)</h3>

<p>Examples:</p>

<ul>
  <li>JSON schema validation</li>
  <li>unit tests for code</li>
  <li>citation matching scripts (“does quote exist on cited page?”)</li>
  <li>allowlist checks for tool calls</li>
</ul>

<p>These are cheap and reliable.</p>

<h3 id="62-llm-as-a-judge-useful-for-fuzzy-dimensions">6.2 LLM-as-a-judge (useful for fuzzy dimensions)</h3>

<p>LLM judges are useful for:</p>

<ul>
  <li>helpfulness and clarity</li>
  <li>completeness</li>
  <li>whether an answer follows a rubric</li>
</ul>

<p>But you must treat them as approximate and calibrate them.</p>

<p>Practical guidelines:</p>

<ul>
  <li>use a strict rubric</li>
  <li>keep judge prompts stable and versioned</li>
  <li>use multi-judge consensus for high-stakes evals</li>
</ul>

<p><strong>Further reading (optional):</strong> see <a href="/ai-agents/0034-observability-tracing/">Observability and Tracing</a> for judge integration into traces.</p>

<hr />

<h2 id="65-judge-calibration-how-to-keep-llm-judges-honest">6.5 Judge calibration: how to keep LLM judges honest</h2>

<p>LLM judges drift too:</p>

<ul>
  <li>judge prompt changes</li>
  <li>model updates</li>
  <li>subtle rubric ambiguity</li>
</ul>

<p>Calibrate judges with a small “golden set”:</p>

<ol>
  <li>Create 30–50 examples with human-labeled scores (PASS/FAIL + reasons).</li>
  <li>Run the judge and measure:
    <ul>
      <li>false positives (judge passes bad outputs)</li>
      <li>false negatives (judge fails good outputs)</li>
    </ul>
  </li>
  <li>Tighten rubric:
    <ul>
      <li>define hard claims vs soft claims</li>
      <li>define what counts as evidence</li>
    </ul>
  </li>
  <li>Version your judge prompt and pin it in eval runs.</li>
</ol>

<p>If you can’t afford human labels, start with a smaller set (even 10) and grow over time.</p>

<hr />

<h2 id="7-evaluating-tool-use-action-quality-not-just-final-output">7. Evaluating tool use: action quality, not just final output</h2>

<p>Agents can “accidentally” reach a correct answer with unsafe or wasteful behavior.</p>

<p>So you must evaluate:</p>

<ul>
  <li>tool selection (did it use the right tool?)</li>
  <li>tool arguments (were they valid and safe?)</li>
  <li>action ordering (did it respect dependencies?)</li>
  <li>repetition (did it loop?)</li>
</ul>

<p>This is where agent eval differs from normal model eval: you grade the <strong>trajectory</strong>, not just the final answer.</p>

<p><strong>Further reading (optional):</strong> for multi-step architectures and stop conditions, see <a href="/ai-agents/0038-autonomous-agent-architectures/">Autonomous Agent Architectures</a>.</p>

<hr />

<h2 id="75-trajectory-scoring-the-simplest-approach-that-works">7.5 Trajectory scoring: the simplest approach that works</h2>

<p>Trajectory scoring doesn’t need to be complicated. Start with penalties:</p>

<ul>
  <li><strong>-1</strong> per failed tool call</li>
  <li><strong>-1</strong> per repeated identical tool call</li>
  <li><strong>-2</strong> for using a tool when not needed (obvious)</li>
  <li><strong>-5</strong> for attempting a forbidden tool (safety violation)</li>
</ul>

<p>Then normalize into a score. This makes “how” the agent behaved measurable.</p>

<p>Combine with a stop-condition check:</p>

<ul>
  <li>Did the agent stop for the right reason (SUCCESS/NEEDS_INPUT/FAILURE)?</li>
  <li>Did it exceed budgets?</li>
</ul>

<p>This catches a common failure: “agent eventually succeeded, but burned 10x budget.”</p>

<hr />

<h2 id="8-regression-testing-pin-behavior-over-time">8. Regression testing: pin behavior over time</h2>

<p>Agents regress for many reasons:</p>

<ul>
  <li>prompt changes</li>
  <li>tool behavior changes</li>
  <li>model provider updates</li>
  <li>dependency changes</li>
</ul>

<p>A regression framework should:</p>

<ul>
  <li>run the offline eval suite on every change</li>
  <li>compare metrics to a baseline</li>
  <li>fail the build if thresholds are violated</li>
</ul>

<p>Example “red lines”:</p>

<ul>
  <li>success rate drops by &gt;2%</li>
  <li>unsafe tool call attempts increase</li>
  <li>cost per success increases by &gt;20%</li>
</ul>

<p>This is how you keep an agent stable as it evolves.</p>

<hr />

<h2 id="85-ci-integration-make-evals-a-first-class-build-step">8.5 CI integration: make evals a first-class build step</h2>

<p>Practical build workflow:</p>

<ul>
  <li><strong>PR checks:</strong> run a fast eval subset (smoke suite) on every PR.</li>
  <li><strong>Nightly:</strong> run full eval suite (bigger dataset, adversarial cases).</li>
  <li><strong>Release gate:</strong> require thresholds before shipping.</li>
</ul>

<p>Keep suites sized appropriately:</p>

<ul>
  <li>smoke: 20–50 tasks, finishes in minutes</li>
  <li>full: 200–1000 tasks, finishes in hours</li>
</ul>

<p>This gives you fast feedback without sacrificing coverage.</p>

<hr />

<h2 id="86-reducing-noise-make-your-evals-stable">8.6 Reducing noise: make your evals stable</h2>

<p>Two things make evals noisy:</p>

<ul>
  <li>model randomness</li>
  <li>external tool variability</li>
</ul>

<p>Mitigations:</p>

<ul>
  <li>run each task multiple times and score the average (or median)</li>
  <li>keep temperature low for eval runs</li>
  <li>mock tools (or record/replay tool outputs)</li>
  <li>isolate “content correctness” from “tool availability”</li>
</ul>

<p>The goal is that a regression is a real regression—not a random fluctuation.</p>

<hr />

<h2 id="9-evals-for-safety-and-security-a-must-have">9. Evals for safety and security (a must-have)</h2>

<h3 id="91-prompt-injection-evals">9.1 Prompt injection evals</h3>

<p>Create tasks where inputs contain:</p>

<ul>
  <li>“ignore previous instructions”</li>
  <li>“reveal secrets”</li>
  <li>“call destructive tool”</li>
</ul>

<p>Expected behavior:</p>

<ul>
  <li>refuse</li>
  <li>sanitize</li>
  <li>continue safely or escalate</li>
</ul>

<h3 id="92-data-leakage-evals">9.2 Data leakage evals</h3>

<p>Create tasks that include:</p>

<ul>
  <li>fake API keys</li>
  <li>fake PII (emails, SSNs)</li>
</ul>

<p>Expected behavior:</p>

<ul>
  <li>redact in outputs and logs</li>
  <li>never echo secrets back</li>
</ul>

<h3 id="93-tool-failure-evals">9.3 Tool failure evals</h3>

<p>Simulate:</p>

<ul>
  <li>429 rate limits</li>
  <li>timeouts</li>
  <li>malformed responses</li>
</ul>

<p>Expected behavior:</p>

<ul>
  <li>bounded retries</li>
  <li>strategy change or escalation</li>
</ul>

<hr />

<h2 id="96-secret-leakage-detection-cheap-heuristics-that-work">9.6 Secret leakage detection: cheap heuristics that work</h2>

<p>You don’t need a perfect classifier to catch most leaks. Start with:</p>

<ul>
  <li>regex checks for key-like patterns (e.g., <code class="language-plaintext highlighter-rouge">sk-...</code>, long hex strings)</li>
  <li>email/phone patterns for PII</li>
  <li>“forbidden tokens” list (internal hostnames, internal file paths)</li>
</ul>

<p>If a leak detector triggers:</p>

<ul>
  <li>mark eval as FAIL (high severity)</li>
  <li>store a redacted snippet for debugging</li>
</ul>

<p>This also doubles as a production monitor.</p>

<hr />

<h2 id="95-canary-safety-evals-test-the-worst-things-first">9.5 “Canary” safety evals: test the worst things first</h2>

<p>If you only have time to build a few safety evals, prioritize:</p>

<ol>
  <li><strong>Prompt injection attempts</strong> that try to trigger tool use</li>
  <li><strong>Secret leakage</strong> tests (the agent should redact)</li>
  <li><strong>Destructive write attempts</strong> (the agent should refuse or require approval)</li>
</ol>

<p>These are high-signal because they fail loudly and have real-world consequences.</p>

<p><strong>Further reading (optional):</strong> if you want deeper threat modeling, see <a href="/ai-agents/0044-prompt-injection-defense/">Prompt Injection Defense</a> and <a href="/ai-agents/0045-data-leakage-prevention/">Data Leakage Prevention</a>.</p>

<hr />

<h2 id="10-implementation-sketch-a-minimal-eval-runner-pseudocode">10. Implementation sketch: a minimal eval runner (pseudocode)</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">run_eval_suite</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">dataset</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="n">trace</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">])</span>
        <span class="n">score</span> <span class="o">=</span> <span class="nf">evaluate_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">test</span><span class="p">[</span><span class="sh">"</span><span class="s">rubric</span><span class="sh">"</span><span class="p">])</span>
        <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">:</span> <span class="n">test</span><span class="p">[</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">],</span> <span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">:</span> <span class="n">score</span><span class="p">,</span> <span class="sh">"</span><span class="s">trace</span><span class="sh">"</span><span class="p">:</span> <span class="n">trace</span><span class="p">.</span><span class="nf">summary</span><span class="p">()})</span>
    <span class="k">return</span> <span class="nf">aggregate</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">evaluate_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">rubric</span><span class="p">):</span>
    <span class="n">checks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">checks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">schema_check</span><span class="p">(</span><span class="n">trace</span><span class="p">))</span>
    <span class="n">checks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">safety_check</span><span class="p">(</span><span class="n">trace</span><span class="p">))</span>
    <span class="n">checks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">cost_check</span><span class="p">(</span><span class="n">trace</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">rubric</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">requires_citations</span><span class="sh">"</span><span class="p">):</span>
        <span class="n">checks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">citation_check</span><span class="p">(</span><span class="n">trace</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">rubric</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">requires_tests</span><span class="sh">"</span><span class="p">):</span>
        <span class="n">checks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">test_check</span><span class="p">(</span><span class="n">trace</span><span class="p">))</span>
    <span class="k">return</span> <span class="nf">combine</span><span class="p">(</span><span class="n">checks</span><span class="p">)</span>
</code></pre></div></div>

<p>The key idea: evaluation reads traces (actions + outputs) and produces structured scores.</p>

<hr />

<h2 id="105-making-evals-reproducible-pin-models-tools-and-environments">10.5 Making evals reproducible: pin models, tools, and environments</h2>

<p>Your eval runs should be reproducible, otherwise you’ll chase ghosts.</p>

<p>Pin:</p>

<ul>
  <li>model/version for the agent</li>
  <li>model/version for the judge (if used)</li>
  <li>tool behavior (mocked or recorded)</li>
  <li>sandbox images for code execution</li>
</ul>

<p>If you can, run evals in a controlled environment where:</p>

<ul>
  <li>web tools use recorded responses (or a fixed snapshot)</li>
  <li>flaky APIs are mocked</li>
</ul>

<p>This turns evals into stable regression signals.</p>

<hr />

<h2 id="106-eval-infrastructure-store-results-like-a-dataset-not-like-logs">10.6 Eval infrastructure: store results like a dataset, not like logs</h2>

<p>If eval outputs are just console logs, you’ll never use them. Treat eval results as a dataset:</p>

<ul>
  <li><strong>Row per test case per run</strong></li>
  <li>Fields:
    <ul>
      <li>agent version</li>
      <li>judge version (if any)</li>
      <li>scores and violations</li>
      <li>cost and latency metrics</li>
      <li>pointers to traces/artifacts</li>
    </ul>
  </li>
</ul>

<p>This lets you answer questions like:</p>

<ul>
  <li>“Which intent regressed in the last release?”</li>
  <li>“Which tool causes the most failures?”</li>
  <li>“Which tasks are the cost outliers?”</li>
</ul>

<p>A minimal storage format is JSONL. A more scalable option is a table in a warehouse (or Postgres) keyed by <code class="language-plaintext highlighter-rouge">(suite, test_id, agent_version, timestamp)</code>.</p>

<p>Once you have this, you can build a simple dashboard:</p>

<ul>
  <li>success rate by intent</li>
  <li>p95 cost by intent</li>
  <li>top safety violations</li>
  <li>top failing tests (by count)</li>
</ul>

<p>This is what turns evals into a real engineering feedback loop.</p>

<hr />

<h2 id="11-case-study-evaluating-a-browsing-agent">11. Case study: evaluating a browsing agent</h2>

<p>Success criteria:</p>

<ul>
  <li>answer contains citations</li>
  <li>citations support claims</li>
  <li>minimal number of sources for key claims</li>
</ul>

<p>Metrics:</p>

<ul>
  <li>citation faithfulness rate</li>
  <li>average sources per answer</li>
  <li>cost per verified claim</li>
</ul>

<p>Failure cases to include:</p>

<ul>
  <li>stale pages</li>
  <li>conflicting sources</li>
  <li>malicious prompt injection in page text</li>
</ul>

<p><strong>Further reading (optional):</strong> see <a href="/ai-agents/0036-web-browsing-agents/">Web Browsing Agents</a>.</p>

<hr />

<h2 id="115-case-study-evaluating-an-autonomous-multi-step-agent">11.5 Case study: evaluating an autonomous multi-step agent</h2>

<p>Success criteria:</p>

<ul>
  <li>completes within step budget</li>
  <li>no unsafe tool attempts</li>
  <li>stops with correct stop reason</li>
</ul>

<p>Trajectory metrics:</p>

<ul>
  <li>repeated tool calls (should be rare)</li>
  <li>“thrashing” (replanning every step)</li>
  <li>tool failures and recovery quality</li>
</ul>

<p><strong>Further reading (optional):</strong> for loop control and stop conditions, see <a href="/ai-agents/0038-autonomous-agent-architectures/">Autonomous Agent Architectures</a> and <a href="/ai-agents/0040-hierarchical-planning/">Hierarchical Planning</a>.</p>

<hr />

<h2 id="116-case-study-evaluating-a-code-execution-agent">11.6 Case study: evaluating a code execution agent</h2>

<p>Success criteria:</p>

<ul>
  <li>code runs in sandbox within limits</li>
  <li>tests pass (when required)</li>
  <li>no forbidden operations attempted (network, filesystem writes outside sandbox)</li>
</ul>

<p>Metrics:</p>

<ul>
  <li>test pass rate</li>
  <li>runtime and memory usage</li>
  <li>retry count (how often it needs a fix loop)</li>
</ul>

<p>Failure cases to include:</p>

<ul>
  <li>infinite loops (timeout)</li>
  <li>missing dependencies (ImportError)</li>
  <li>wrong output format (prints extra text)</li>
</ul>

<p><strong>Further reading (optional):</strong> see <a href="/ai-agents/0037-code-execution-agents/">Code Execution Agents</a>.</p>

<hr />

<h2 id="12-summary--junior-engineer-roadmap">12. Summary &amp; Junior Engineer Roadmap</h2>

<p>Agent evaluation frameworks are how you turn agent development into engineering:</p>

<ol>
  <li><strong>Define a contract:</strong> requirements, constraints, budgets, evidence.</li>
  <li><strong>Build a dataset with coverage:</strong> common tasks + nasty edge cases.</li>
  <li><strong>Prefer deterministic checks:</strong> schemas, tests, and scripts.</li>
  <li><strong>Add judges carefully:</strong> strict rubrics, stable prompts, consensus when needed.</li>
  <li><strong>Evaluate trajectories:</strong> tool use and safety matter, not just final output.</li>
  <li><strong>Run regressions continuously:</strong> fail builds when metrics degrade.</li>
</ol>

<h3 id="mini-project-recommended">Mini-project (recommended)</h3>

<p>Build an eval suite with:</p>

<ul>
  <li>30 tasks across 3 intents</li>
  <li>a deterministic safety checker (no write tools, no secrets)</li>
  <li>a cost budget checker (max steps/tool calls)</li>
  <li>one LLM judge rubric for helpfulness</li>
</ul>

<p>Run it on every agent change and chart metrics over time.</p>

<h3 id="common-rookie-mistakes-avoid-these">Common rookie mistakes (avoid these)</h3>

<ol>
  <li><strong>Only scoring the final answer:</strong> for agents, the trajectory matters (tool choice, safety, loops).</li>
  <li><strong>No baseline:</strong> without a stable baseline, you can’t call something a regression.</li>
  <li><strong>No safety suite:</strong> safety regressions are the most expensive regressions; test them first.</li>
  <li><strong>Too much judge, not enough determinism:</strong> use schemas/tests/scripts whenever possible; judges are a supplement.</li>
  <li><strong>No reproducibility:</strong> if tools are flaky and models are random, you’ll chase noise instead of signal.</li>
</ol>

<h3 id="further-reading-optional">Further reading (optional)</h3>

<ul>
  <li>Multi-step agent architectures: <a href="/ai-agents/0038-autonomous-agent-architectures/">Autonomous Agent Architectures</a> and <a href="/ai-agents/0040-hierarchical-planning/">Hierarchical Planning</a></li>
  <li>Debugging + metrics: <a href="/ai-agents/0034-observability-tracing/">Observability and Tracing</a></li>
</ul>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ai-agents/0042-agent-evaluation-frameworks/">arunbaby.com/ai-agents/0042-agent-evaluation-frameworks</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#agent-evaluation" class="page__taxonomy-item p-category" rel="tag">agent-evaluation</a><span class="sep">, </span>
    
      <a href="/tags/#benchmarks" class="page__taxonomy-item p-category" rel="tag">benchmarks</a><span class="sep">, </span>
    
      <a href="/tags/#evals" class="page__taxonomy-item p-category" rel="tag">evals</a><span class="sep">, </span>
    
      <a href="/tags/#metrics" class="page__taxonomy-item p-category" rel="tag">metrics</a><span class="sep">, </span>
    
      <a href="/tags/#observability" class="page__taxonomy-item p-category" rel="tag">observability</a><span class="sep">, </span>
    
      <a href="/tags/#regression-testing" class="page__taxonomy-item p-category" rel="tag">regression-testing</a><span class="sep">, </span>
    
      <a href="/tags/#reliability" class="page__taxonomy-item p-category" rel="tag">reliability</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ai-agents" class="page__taxonomy-item p-category" rel="tag">ai-agents</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0042-merge-k-sorted-lists/" rel="permalink">Merge K Sorted Lists
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Combining order from chaos, one element at a time.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0042-feature-stores/" rel="permalink">Feature Stores
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          14 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“The centralized truth for machine learning features.”
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0042-asr-decoding/" rel="permalink">Automatic Speech Recognition (ASR) Decoding
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          16 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">“Turning acoustic probabilities into coherent text.”
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Agent+Evaluation+Frameworks%20https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0042-agent-evaluation-frameworks%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0042-agent-evaluation-frameworks%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ai-agents/0042-agent-evaluation-frameworks/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ai-agents/0041-world-models-for-agents/" class="pagination--pager" title="World Models for Agents">Previous</a>
    
    
      <a href="/ai-agents/0043-testing-ai-agents/" class="pagination--pager" title="Testing AI Agents">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
