<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.26.2 by Michael Rose
  Copyright 2013-2024 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Context Window Management - Arun Baby</title>
<meta name="description" content="“The Finite Canvas of Intelligence: Managing the Agent’s RAM.”">


  <meta name="author" content="Arun Baby">
  
  <meta property="article:author" content="Arun Baby">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Arun Baby">
<meta property="og:title" content="Context Window Management">
<meta property="og:url" content="https://www.arunbaby.com/ai-agents/0012-context-window-management/">


  <meta property="og:description" content="“The Finite Canvas of Intelligence: Managing the Agent’s RAM.”">



  <meta property="og:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">



  <meta name="twitter:site" content="@arunbaby0">
  <meta name="twitter:title" content="Context Window Management">
  <meta name="twitter:description" content="“The Finite Canvas of Intelligence: Managing the Agent’s RAM.”">
  <meta name="twitter:url" content="https://www.arunbaby.com/ai-agents/0012-context-window-management/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://www.arunbaby.com/assets/images/profile-photo.png">
    
  

  



  <meta property="article:published_time" content="2025-12-22T11:38:01+05:30">





  

  


<link rel="canonical" href="https://www.arunbaby.com/ai-agents/0012-context-window-management/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Baby Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">Arun Baby</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/dsa/"
                
                
              >DSA</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ml-system-design/"
                
                
              >ML Systems</a>
            </li><li class="masthead__menu-item">
              <a
                href="/speech-tech/"
                
                
              >Speech Tech</a>
            </li><li class="masthead__menu-item">
              <a
                href="/ai-agents/"
                
                
              >AI Agents</a>
            </li><li class="masthead__menu-item">
              <a
                href="/publications/"
                
                
              >Publications</a>
            </li><li class="masthead__menu-item">
              <a
                href="/statuses/"
                
                
              >Statuses</a>
            </li><li class="masthead__menu-item">
              <a
                href="/contact/"
                
                
              >Contact</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main" class="no-author-sidebar">
  
  <div class="sidebar sticky">
  
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Context Window Management">
    <meta itemprop="description" content="“The Finite Canvas of Intelligence: Managing the Agent’s RAM.”">
    <meta itemprop="datePublished" content="2025-12-22T11:38:01+05:30">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://www.arunbaby.com/ai-agents/0012-context-window-management/" itemprop="url">Context Window Management
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#1-introduction-the-ram-of-the-llm">1. Introduction: The RAM of the LLM</a></li><li><a href="#2-the-physics-of-attention-why-length-matters">2. The Physics of Attention: Why Length Matters</a><ul><li><a href="#21-the-quadratic-bottleneck-on2">2.1 The Quadratic Bottleneck: $O(N^2)$</a></li><li><a href="#22-the-kv-cache-key-value-cache">2.2 The KV Cache (Key-Value Cache)</a></li></ul></li><li><a href="#3-the-lost-in-the-middle-phenomenon">3. The “Lost in the Middle” Phenomenon</a></li><li><a href="#4-context-management-strategies">4. Context Management Strategies</a><ul><li><a href="#41-sliding-window-fifo">4.1 Sliding Window (FIFO)</a></li><li><a href="#42-summarization-the-rolling-summary">4.2 Summarization (The Rolling Summary)</a></li><li><a href="#43-entity-extraction-the-knowledge-graph">4.3 Entity Extraction (The Knowledge Graph)</a></li><li><a href="#44-selective-context-the-bouncer">4.4 Selective Context (The Bouncer)</a></li></ul></li><li><a href="#5-the-game-changer-context-caching">5. The Game Changer: Context Caching</a><ul><li><a href="#51-how-it-works">5.1 How it Works</a></li><li><a href="#52-the-megaprompt-agent-pattern">5.2 The “Megaprompt Agent” Pattern</a></li></ul></li><li><a href="#6-compression-techniques-lingua-franca">6. Compression Techniques: Lingua Franca</a><ul><li><a href="#61-token-optimization">6.1 Token Optimization</a></li></ul></li><li><a href="#7-code-a-context-buffer-implementation">7. Code: A Context Buffer Implementation</a></li><li><a href="#8-summary">8. Summary</a></li></ul>
            </nav>
          </aside>
        
        <p><strong>“The Finite Canvas of Intelligence: Managing the Agent’s RAM.”</strong></p>

<h2 id="1-introduction-the-ram-of-the-llm">1. Introduction: The RAM of the LLM</h2>

<p>If Vector Search is the “Hard Drive” of an agent (Long-term, slow, massive), the <strong>Context Window</strong> is its <strong>RAM</strong> (Short-term, fast, expensive, volatile).</p>

<p>Every time you send a request to an LLM, you are sending a state snapshot. The model reads the inputs, performs inference, and generates an output. It does not “remember” the previous request unless you pass it in again.</p>

<p>In 2023, context windows were small (4k - 8k tokens). This forced developers to be extremely disciplined, summarizing every conversation turn.
In 2025, we have models with <strong>1 Million+</strong> tokens (Gemini 1.5, Claude 3).
This leads to a dangerous fallacy: <em>“Why manage context? Just stuff everything in.”</em></p>

<p>This is wrong for three reasons:</p>
<ol>
  <li><strong>Cost:</strong> Input tokens cost money. Re-sending a 100-page manual on every API call allows you to burn through your budget in minutes. Even at cheap rates, $0.50 per call * 1000 calls = $500.</li>
  <li><strong>Latency:</strong> Reading 1M tokens takes time. The <strong>Time-to-First-Token (TTFT)</strong> scales linearly (or worse) with input length. Real-time agents cannot wait 30 seconds to “read the manual” before answering “Hello.”</li>
  <li><strong>Accuracy (The Dilution Effect):</strong> The more garbage you put in the context, the less attention the model pays to the signal. This is known as the <strong>Signal-to-Noise Ratio</strong> problem. A model given 1000 irrelevant sentences and 1 relevant sentence is significantly more likely to hallucinate than a model given just the 1 relevant sentence.</li>
</ol>

<p>Effective <strong>Context Management</strong> is the art of curating the perfect prompt—giving the model exactly what it needs to solve the <em>current</em> step, and nothing else.</p>

<hr />

<h2 id="2-the-physics-of-attention-why-length-matters">2. The Physics of Attention: Why Length Matters</h2>

<p>To understand why context matters, we must peek inside the Transformer architecture.</p>

<h3 id="21-the-quadratic-bottleneck-on2">2.1 The Quadratic Bottleneck: $O(N^2)$</h3>
<p>The core mechanism of an LLM is <strong>Self-Attention</strong>.
Every token in the sequence looks at every other token to calculate its “Attention Score.”</p>
<ul>
  <li>“The” looks at “cat”, “sat”, “on”, “mat”.</li>
  <li>“cat” looks at “The”, “sat”, “on”, “mat”.</li>
</ul>

<p>If you have sequence length $N$, the number of calculations is $N^2$.</p>
<ul>
  <li>1k tokens -&gt; 1M operations.</li>
  <li>100k tokens -&gt; 10B operations.</li>
</ul>

<p>While recent optimizations like <strong>FlashAttention-2</strong> and <strong>Ring Attention</strong> have reduced the memory footprint and constant factors, the fundamental physics remains: <strong>Longer Context = More Compute + More VRAM.</strong>
This is why prompt caching is a game changer—it allows us to re-use the computed attention matrices.</p>

<h3 id="22-the-kv-cache-key-value-cache">2.2 The KV Cache (Key-Value Cache)</h3>
<p>When you use a chat interface (like ChatGPT), why is the second message faster than the first?
Because of the <strong>KV Cache</strong>.</p>

<p>The model processes the prompt layer by layer. At each layer, it computes “Keys” and “Values” for each token.
Instead of discarding them, the inference engine (like vLLM) stores them in GPU memory (VRAM).
When you send the next token, the model doesn’t re-compute the past. It just retrieves the cached Keys/Values.</p>

<p><strong>Implication for Agents:</strong></p>
<ul>
  <li>Adding to the <strong>End (Prefix Caching):</strong> Cheap. The cache is preserved.</li>
  <li>Changing the <strong>Beginning (System Prompt):</strong> Expensive. It invalidates the entire cache for the subsequent tokens.</li>
  <li><em>Design Rule:</em> Keep your massive instructions static at the top. Append dynamic history at the bottom.</li>
</ul>

<hr />

<h2 id="3-the-lost-in-the-middle-phenomenon">3. The “Lost in the Middle” Phenomenon</h2>

<p>A landmark paper (Liu et al., 2023) discovered a quirk in LLM psychology. They tested models by placing a specific fact (the “Needle”) at different positions in a long context window (the “Haystack”).</p>

<ul>
  <li><strong>Primacy Bias:</strong> Models pay huge attention to the <strong>Beginning</strong>. The System Prompt sets the rules.</li>
  <li><strong>Recency Bias:</strong> Models pay huge attention to the <strong>End</strong>. The user’s last question defines the immediate task.</li>
  <li><strong>The Trough:</strong> Information buried in the exact middle (e.g., at token 50,000 of a 100,000 window) is frequently ignored or hallucinated.</li>
</ul>

<p><strong>Agent Strategy:</strong>
If you retrieve 50 documents via RAG, do not just dump them in the middle of the prompt.</p>
<ol>
  <li><strong>Curve Sorting:</strong> Sort your RAG chunks by relevance.</li>
  <li><strong>Placement:</strong> Put the <strong>Most Relevant</strong> chunk at the very bottom (closest to the question). Put the <strong>2nd Most Relevant</strong> at the very top. Put the least relevant in the middle.</li>
  <li><strong>Instruction Repeat:</strong> It is often useful to repeat the core instruction at the very end.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">[System Prompt] ... [Data] ... [User Query] ... [Constraint Reminder: remember to answer in JSON]</code>.</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="4-context-management-strategies">4. Context Management Strategies</h2>

<p>How do we compress “A Lifetime of Memories” into a finite window?</p>

<h3 id="41-sliding-window-fifo">4.1 Sliding Window (FIFO)</h3>
<p>The simplest approach.</p>
<ul>
  <li><strong>Structure:</strong> <code class="language-plaintext highlighter-rouge">[System Prompt] + [Msg N-10 ... Msg N]</code></li>
  <li><strong>Logic:</strong> When the buffer is full, drop the oldest message (<code class="language-plaintext highlighter-rouge">Msg N-11</code>).</li>
  <li><strong>Pros:</strong> Simple implementation. Constant cost. Predictable latency.</li>
  <li><strong>Cons:</strong> <strong>Catastrophic Amnesia.</strong> If the user told you their name (“I’m Alice”) 50 turns ago, and that message slides out of the window, the agent forgets who Alice is.</li>
</ul>

<h3 id="42-summarization-the-rolling-summary">4.2 Summarization (The Rolling Summary)</h3>
<p>Instead of dropping old messages, convert them into a compressed representation.</p>
<ul>
  <li><strong>Structure:</strong> <code class="language-plaintext highlighter-rouge">[System Prompt] + [Summary of Past] + [Recent Messages]</code></li>
  <li><strong>Process:</strong>
    <ol>
      <li>Wait until <code class="language-plaintext highlighter-rouge">Recent Messages</code> hits 10 items.</li>
      <li>Trigger a background LLM call (using a cheaper model like GPT-3.5-Turbo).</li>
      <li>Prompt: <em>“Summarize the following conversation, preserving key facts like names, dates, and decisions.”</em></li>
      <li>Update the <code class="language-plaintext highlighter-rouge">Summary</code> variable.</li>
      <li>Clear <code class="language-plaintext highlighter-rouge">Recent Messages</code>.</li>
    </ol>
  </li>
  <li><strong>Pros:</strong> Theoretically infinite conversation duration.</li>
  <li><strong>Cons:</strong> <strong>Lossy Compression.</strong> “Subtle details” (e.g., the user mentioned they dislike blue) are often lost in the summary.</li>
</ul>

<h3 id="43-entity-extraction-the-knowledge-graph">4.3 Entity Extraction (The Knowledge Graph)</h3>
<p>Instead of summarizing text, extract specific facts into a structured state.</p>
<ul>
  <li><strong>State:</strong> <code class="language-plaintext highlighter-rouge">{"user_name": "Alice", "current_project": "Website", "todos": ["Fix bug"]}</code></li>
  <li><strong>Process:</strong> An “Observer Agent” runs in parallel, watching the stream and updating the JSON state.</li>
  <li><strong>Context Injection:</strong> We inject this JSON into the System Prompt.</li>
  <li><strong>Pros:</strong> Extremely dense information. Zero hallucination of facts.</li>
  <li><strong>Cons:</strong> Hard to implement. Requires defining a schema for what is worth remembering.</li>
</ul>

<h3 id="44-selective-context-the-bouncer">4.4 Selective Context (The Bouncer)</h3>
<p>Use a specialized small model (BERT/Cross-Encoder) to act as a “Bouncer.”</p>
<ul>
  <li><strong>Scenario:</strong> You have 100 previous messages.</li>
  <li><strong>Question:</strong> “What was the error code?”</li>
  <li><strong>Bouncer:</strong> Scans the 100 messages independently. Scores them based on relevance to “error code”.</li>
  <li><strong>Selection:</strong> Identifies messages 42 and 43.</li>
  <li><strong>Construction:</strong> Constructs a prompt with ONLY messages 42 and 43.</li>
  <li><strong>Pros:</strong> Highest Signal-to-Noise ratio.</li>
  <li><strong>Cons:</strong> If the Bouncer misses the context, the Agent is blind.</li>
</ul>

<hr />

<h2 id="5-the-game-changer-context-caching">5. The Game Changer: Context Caching</h2>

<p>In late 2024, model providers (Anthropic, Google, OpenAI) introduced <strong>Prompt Caching</strong> APIs. This changed the economics of agency overnight.</p>

<h3 id="51-how-it-works">5.1 How it Works</h3>
<ul>
  <li><strong>Old Way:</strong> You pay to upload the 100-page manual on every API call.</li>
  <li><strong>New Way:</strong> You upload the manual once. You designate a <code class="language-plaintext highlighter-rouge">cache_control</code> checkpoint. The provider processes the manual, computes the KV Cache, and stores it on the GPU (for a few minutes/hours).</li>
  <li><strong>Subsequent Calls:</strong> You pass the <code class="language-plaintext highlighter-rouge">cache_id</code>. The provider <strong>skips</strong> the computation for the prefix.</li>
  <li><strong>Economics:</strong></li>
  <li><strong>Write Cost:</strong> Standard price.</li>
  <li><strong>Read Cost:</strong> ~90% Discount.</li>
  <li><strong>Speed:</strong> 2-5x faster (Processing pre-cached tokens is instant).</li>
</ul>

<h3 id="52-the-megaprompt-agent-pattern">5.2 The “Megaprompt Agent” Pattern</h3>
<p>This enables a new architecture. Instead of RAG (retrieving tiny snippets), you can just <strong>Global Context</strong>.</p>
<ul>
  <li><strong>Scenario:</strong> A Coding Agent.</li>
  <li><strong>Old Strategy:</strong> RAG search to find 3 relevant files.</li>
  <li><strong>New Strategy:</strong> Dump the <strong>Entire Codebase</strong> (100 files) into the Context Cache.</li>
  <li><strong>Benefit:</strong> The agent sees the <em>whole</em> architecture. It understands imports, global variables, and side effects that RAG would miss.</li>
  <li><strong>Limit:</strong> Still bounded by the hard limit (e.g., 200k for Claude). For huge repos, you still need RAG.</li>
</ul>

<hr />

<h2 id="6-compression-techniques-lingua-franca">6. Compression Techniques: Lingua Franca</h2>

<p>Can we make the text itself smaller?</p>

<h3 id="61-token-optimization">6.1 Token Optimization</h3>
<ul>
  <li><strong>Standard English:</strong> “I would like you to please go ahead and search for the file.” (12 tokens).</li>
  <li><strong>Compressed:</strong> “Search file.” (2 tokens).</li>
  <li>LLMs are surprisingly good at understanding “Caveman Speak” or specialized syntax.</li>
  <li><strong>LLMLingua:</strong> A research project (Microsoft) that uses a small model to remove “non-essential” tokens (stopwords, adjectives) from a prompt before sending it to the big model. It achieves 2-3x compression with &lt;1% accuracy drop.</li>
</ul>

<hr />

<h2 id="7-code-a-context-buffer-implementation">7. Code: A Context Buffer Implementation</h2>

<p>Abstracting the logic for a managed buffer that handles pruning.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tiktoken</span>
<span class="kn">import</span> <span class="n">json</span>

<span class="k">class</span> <span class="nc">ContextBuffer</span><span class="p">:</span>
 <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4</span><span class="sh">"</span><span class="p">):</span>
 <span class="n">self</span><span class="p">.</span><span class="n">max_tokens</span> <span class="o">=</span> <span class="n">max_tokens</span>
 <span class="c1"># Load the specific tokenizer for accurate counting
</span> <span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="p">.</span><span class="nf">encoding_for_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
 <span class="n">self</span><span class="p">.</span><span class="n">system_message</span> <span class="o">=</span> <span class="bp">None</span>
 <span class="n">self</span><span class="p">.</span><span class="n">history</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># List of dicts
</span> <span class="n">self</span><span class="p">.</span><span class="n">summary</span> <span class="o">=</span> <span class="sh">""</span>

 <span class="k">def</span> <span class="nf">set_system_message</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">content</span><span class="p">):</span>
 <span class="n">self</span><span class="p">.</span><span class="n">system_message</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">content</span><span class="p">}</span>

 <span class="k">def</span> <span class="nf">add_message</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">role</span><span class="p">,</span> <span class="n">content</span><span class="p">):</span>
 <span class="sh">"""</span><span class="s">Add a message and immediately prune if needed.</span><span class="sh">"""</span>
 <span class="n">self</span><span class="p">.</span><span class="n">history</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="n">role</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">content</span><span class="p">})</span>
 <span class="n">self</span><span class="p">.</span><span class="nf">_prune</span><span class="p">()</span>

 <span class="k">def</span> <span class="nf">_count_tokens</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">messages</span><span class="p">):</span>
 <span class="c1"># Rough estimation: content + role overhead
</span> <span class="n">text</span> <span class="o">=</span> <span class="sh">""</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">messages</span><span class="p">])</span>
 <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>

 <span class="k">def</span> <span class="nf">_prune</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="sh">"""</span><span class="s">
 Enforce the token limit by dropping the oldest messages.
 Always keep the System Message and Summary.
 </span><span class="sh">"""</span>
 <span class="c1"># Calculate current usage
</span> <span class="n">static_tokens</span> <span class="o">=</span> <span class="mi">0</span>
 <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">system_message</span><span class="p">:</span>
 <span class="n">static_tokens</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_count_tokens</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">system_message</span><span class="p">])</span>
 <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">summary</span><span class="p">:</span>
 <span class="n">static_tokens</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_count_tokens</span><span class="p">([{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">summary</span><span class="p">}])</span>

 <span class="n">current_history_tokens</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_count_tokens</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">history</span><span class="p">)</span>
 <span class="n">total</span> <span class="o">=</span> <span class="n">static_tokens</span> <span class="o">+</span> <span class="n">current_history_tokens</span>

 <span class="c1"># While loop to pop from the front (FIFO)
</span> <span class="k">while</span> <span class="n">total</span> <span class="o">&gt;</span> <span class="n">self</span><span class="p">.</span><span class="n">max_tokens</span> <span class="ow">and</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">history</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
 <span class="n">removed</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">history</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
 <span class="c1"># Optimization: Just subtract the tokens of the removed msg
</span> <span class="n">removed_count</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_count_tokens</span><span class="p">([</span><span class="n">removed</span><span class="p">])</span>
 <span class="n">total</span> <span class="o">-=</span> <span class="n">removed_count</span>
 <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Pruned message: </span><span class="si">{</span><span class="n">removed</span><span class="p">[</span><span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">][</span><span class="si">:</span><span class="mi">20</span><span class="p">]</span><span class="si">}</span><span class="s">...</span><span class="sh">"</span><span class="p">)</span>

 <span class="k">def</span> <span class="nf">update_summary</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">new_summary</span><span class="p">):</span>
 <span class="sh">"""</span><span class="s">Manually inject a summary (from an external process)</span><span class="sh">"""</span>
 <span class="n">self</span><span class="p">.</span><span class="n">summary</span> <span class="o">=</span> <span class="n">new_summary</span>

 <span class="k">def</span> <span class="nf">get_messages</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
 <span class="sh">"""</span><span class="s">Construct the final payload for the API.</span><span class="sh">"""</span>
 <span class="n">final_payload</span> <span class="o">=</span> <span class="p">[]</span>
 <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">system_message</span><span class="p">:</span>
 <span class="n">final_payload</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">system_message</span><span class="p">)</span>

 <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">summary</span><span class="p">:</span>
 <span class="c1"># Inject summary as a high-priority system note
</span> <span class="n">final_payload</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span>
 <span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
 <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Previous conversation summary: </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">summary</span><span class="si">}</span><span class="sh">"</span>
 <span class="p">})</span>

 <span class="n">final_payload</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">history</span><span class="p">)</span>
 <span class="k">return</span> <span class="n">final_payload</span>

<span class="c1"># Usage Simulation
</span><span class="nb">buffer</span> <span class="o">=</span> <span class="nc">ContextBuffer</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># Tiny limit for demo
</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">set_system_message</span><span class="p">(</span><span class="sh">"</span><span class="s">You are a helpful assistant.</span><span class="sh">"</span><span class="p">)</span>

<span class="nb">buffer</span><span class="p">.</span><span class="nf">add_message</span><span class="p">(</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Hello! My name is Alice.</span><span class="sh">"</span><span class="p">)</span>
<span class="nb">buffer</span><span class="p">.</span><span class="nf">add_message</span><span class="p">(</span><span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Hi Alice!</span><span class="sh">"</span><span class="p">)</span>
<span class="nb">buffer</span><span class="p">.</span><span class="nf">add_message</span><span class="p">(</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">What is the capital of France?</span><span class="sh">"</span><span class="p">)</span>
<span class="nb">buffer</span><span class="p">.</span><span class="nf">add_message</span><span class="p">(</span><span class="sh">"</span><span class="s">assistant</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Paris.</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># This message pushes us over the limit
</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">add_message</span><span class="p">(</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Tell me a very long story about dinosaurs...</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Output: "Pruned message: Hello! My name is Alice..."
</span>
<span class="c1"># Now the agent doesn't know Alice's name.
# Fix:
</span><span class="nb">buffer</span><span class="p">.</span><span class="nf">update_summary</span><span class="p">(</span><span class="sh">"</span><span class="s">User is named Alice.</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Now get_messages() includes the summary.
</span></code></pre></div></div>

<hr />

<h2 id="8-summary">8. Summary</h2>

<p>Context is the scarcest resource in the AI economy. It is the bottleneck for “Intelligence Density.”</p>
<ul>
  <li><strong>KV Caching</strong> explains the physical costs.</li>
  <li><strong>Context Caching APIs</strong> (Megaprompts) are removing the need for RAG in medium-sized tasks.</li>
  <li><strong>Sliding Windows</strong> and <strong>Summarization</strong> are the basic garbage collection algorithms of Agent Memory.</li>
</ul>

<p>A senior engineer treats Tokens like Bytes in network packets—optimizing, compressing, and caching them to build high-performance systems. An unoptimized agent is slow, expensive, and forgetful.</p>

<p>Managing memory allows us to tackle broader problems, but complex problems require <strong>Multi-Step Reasoning</strong> to deduce answers logically.</p>

<hr />

<p><strong>Originally published at:</strong> <a href="https://www.arunbaby.com/ai-agents/0012-context-window-management/">arunbaby.com/ai-agents/0012-context-window-management</a></p>

<p><em>If you found this helpful, consider sharing it with others who might benefit.</em></p>


        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#attention" class="page__taxonomy-item p-category" rel="tag">attention</a><span class="sep">, </span>
    
      <a href="/tags/#caching" class="page__taxonomy-item p-category" rel="tag">caching</a><span class="sep">, </span>
    
      <a href="/tags/#compression" class="page__taxonomy-item p-category" rel="tag">compression</a><span class="sep">, </span>
    
      <a href="/tags/#context-window" class="page__taxonomy-item p-category" rel="tag">context-window</a><span class="sep">, </span>
    
      <a href="/tags/#kv-cache" class="page__taxonomy-item p-category" rel="tag">kv-cache</a><span class="sep">, </span>
    
      <a href="/tags/#memory" class="page__taxonomy-item p-category" rel="tag">memory</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#ai-agents" class="page__taxonomy-item p-category" rel="tag">ai-agents</a>
    
    </span>
  </p>


        
      </footer>

      <div class="page__related page__related--full">
  <h2 class="page__related-title">Related across topics</h2>
  <style>
    /* Make section span full content width and use 2 equal columns */
    .page__related--full { float: inline-start; width: 100%; padding: 0; }
    .cross-related-grid { display: grid; grid-template-columns: repeat(2, minmax(0, 1fr)); gap: 2rem; }
    @media (max-width: 768px) { .cross-related-grid { grid-template-columns: 1fr; } }
    /* Ensure archive cards stretch nicely in the grid */
    .cross-related-grid .list__item, .cross-related-grid .grid__item { width: auto; float: none; margin: 0; }
  </style>
  <div class="cross-related-grid">
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/dsa/0012-add-two-numbers/" rel="permalink">Add Two Numbers
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          23 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Master digit-by-digit addition with linked lists: Handle carry propagation elegantly. Classic problem teaching pointer manipulation and edge cases.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/ml-system-design/0012-distributed-systems/" rel="permalink">Distributed ML Systems
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          25 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Design distributed ML systems that scale to billions of predictions: Master replication, sharding, consensus, and fault tolerance for production ML.
</p>
  </article>
</div>




<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/speech-tech/0012-multi-speaker-asr/" rel="permalink">Multi-Speaker ASR
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          25 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Build production multi-speaker ASR systems: Combine speech recognition, speaker diarization, and overlap handling for real-world conversations.
</p>
  </article>
</div>

  </div>
</div>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://twitter.com/intent/tweet?via=arunbaby0&text=Context+Window+Management%20https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0012-context-window-management%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.arunbaby.com%2Fai-agents%2F0012-context-window-management%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://www.arunbaby.com/ai-agents/0012-context-window-management/" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ai-agents/0011-vector-search-for-agents/" class="pagination--pager" title="Vector Search for Agents">Previous</a>
    
    
      <a href="/ai-agents/0013-multi-step-reasoning/" class="pagination--pager" title="Multi-Step Reasoning">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/arunbaby0" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/arunbaby0/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://scholar.google.co.in/citations?user=6fSYWhkAAAAJ" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-graduation-cap" aria-hidden="true"></i> Google Scholar</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 1990 - 2143 <a href="https://www.arunbaby.com">Arun Baby</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0JRJPEC9SS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0JRJPEC9SS', { 'anonymize_ip': false});
</script>








  </body>
</html>
